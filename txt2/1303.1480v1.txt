possible to use such models to represent, say, the gen­
eral medical knowledge possessed by a typical physi­
cian.
A major limitation of current graphical representa­
tions is that they are propositional . That is, they
*This work was supported by NSERC under its Operat­
ing Grants program and by the IRIS network. The author's
e-mail address is fbacchus@logos. uwaterloo. ca

lack quantifiers, which are essential for representing
general knowledge. W ith quantifiers one can repre­
sent an assertion about a whole class of individuals
using a single sentence, while in a propositional lan­
guage this would require a separate sentence for each
individual. As a result, important knowledge structur­
ing techniques, like taxonomies, cannot be applied to
propositional representations.
However, graphical representations have important ad­
vantages of their own. In particular, they support ef
ficient reasoning algorithms. These algorithms are fa
more efficient than the symbolic reasoning mechanism�
typical of more general representations.
This dichotomy of utility has lead to proposals fo'
hybrid uses of general and graphical representations
In particular, Breese et al. (BGW91] have proposul
the technique of knowledge based model construe
tion ( KBMC) : the automatic construction of propo
sitionalj graphical models for specific problems from '
larger knowledge base expressed in a general repre
sentation. Breese et al. provide a number of moti­
vations for this approach that extend the arguments
given above.
We refer the reader to [B GW91] for this motivation,
and take as our starting point that KBMC is a po­
tentially useful technique, certainly worth examining
in more detail. Our contribution, then, is to look
more closely at a particular mechanism for perform­
ing KBMC. In particular, we develop a mechanism in
which a first-order probability logic [Bac90b] is used
to represent the general knowledge base, and model
construction is performed using ideas arising from thr'
study of direct inference. Direct inference involve.
reasoning from general statistical knowledge to prob
abilities assigned to particular cases and has beei
worked on by a number of authors including [ BGHK92
Bac90b, Kyb61, Kyb74, Lev80, Lou87, Pol90, Sal71 ]
Our mechanism brings to light the important role ex
pressive-first-order probability logics can play in rep
resenting general probabilistic knowledge, and the im
portant relationship between KBMC and direct infer
ence.
In the sequel, we first introduce a probability logi•

220

Bacchus

that can be used for the representation of general
probabilistic and logical knowledge, and demonstrate
that it is capable of representing any Bayesian net­
work [Pea86]-pe'rhaps the most important of current
graphical representations. Then we discuss how ideas
from direct inference can be used to specify a model
construction procedure that can construct graphical
models for particular problems. We point out how
this idea is related to, but strictly more general than,
template models. Throughout our discussion we try to
point out various insights about the process of KBMC
offered by our approach. Finally, we close with some
conclusions and indications for future work.
2

Representing Gener al Prob abilistic
Knowledge

KBMC requires a mechanism for representing general
knowledge. This representation should be declarative,
for a number of good reasons that are beyond the scope
of this paper to discuss. Furthermore, the representa­
tion should have a precise semantics, so that we can
specify exactly the meaning of the expressions in the
knowledge base. Without precise semantics it would
be impossible to verify the accuracy of the knowledge
base.

tics of our language here ( see [Bac90b] for all such
details) . The specification simply formalizes the fol­
lowing notion: a formula with free variables might be­
come true or false depending on how the variables an:
instantiated. For example, bird(x) might be true when
x = Tweety but false when x =Clyde. A proportion
term, then, simply evaluates to the proportion of pos­
sible instantiations that make the formula true.
This language can express an wide variety of statistical
assertions ( [Bac90b] gives an extensive collection of ex­
amples) . It can also express whatever can be expressed
in first-order logic, so essential structuring mechanisms
like taxonomies can be applied.
Example 1 Let the domain contain, among other
things, a collection of coins, and a collection of coin­
tossing events. 1 In addition to some obvious symbols,
let our language include the predicate CoinToss(e)
which is true of an individual e iff e is an coin-tossing
event; Goin(x) which is true of x iff xis a coin; and
Object(e,x) which holds of the individuals e and x iff
e is an event and x is the object of that event: the ob­
ject of a coin-tossing event is the particular coin that
is tossed. Now we can express the following:

1. Ve,x.GoinToss(e) 1\ Object(e,x) ---> Goin(x).
That is, the object of any coin toss is always a.
coin.

Since logical representations meet our desiderata, we
propose as a representation mechanism a first-order
logic for statistical information, developed by Bacchus
[Bac90a] . This logic is basically first-order logic aug­
mented to allow the expression of various assertions
about proportions.
Syntactically, we augment an ordinary collection of
first-order symbols with symbols useful for express­
ing numeric assertions, e.g., '1', '+', ';::: '. In ad­
dition to allowing the generation of ordinary first­
order formulas we also allow the generation of nu­
meric assertions involving proportions. For example,
[P(x)]x = 0.75, expresses the assertion that 75% of
the individuals in the domain have property P, while
0.45 ::; [R(x,y)](x,y} ::; 0.55 expresses the assertion
that between 45% and 55% of all pairs of domain in­
dividuals stand in relation R. In general, if a is an
existent formula and x is a vector of n variables, the
proportion term [a] x denotes the proportion of n-ary
vectors of domain individual that satisfy the formula
a. Most of the statistical information we wish to ex­
press will in fact be statements of conditional probabil­
ity denoting relative proportions. For example, [ai .Bl x
will denote the proportion of n-ary vectors of domain
individuals among those that satisfy ,B which also sat­
isfy a. We can then express various statistical asser­
tions by expressing various constraints on the values
that these proportion terms can take. For example,
by asserting that [Q(x)IP(x)]x = 0.5 we are asserting
that the domain we are considering is such that 1/2 of
the P's are Q's.
We will not give a formal specification of the seman-

2. Vx.Fair(x)
+-+
[Heads(e)IGoinToss(e) J'
Object(e,x)]e E (.49,.51). We agree to call an;,
coin x fair iff approximately 50% of the events in
which it is tossed result in heads. This example
demonstrates the useful interplay between univer·
sal quantification and the proportion terms.
3. [[Heads(e)IGoinToss(e) 1\ Object(e,x)]e
E
(0.49,0.51)1Goin(x)Jx = 0.95. This formula says
that 95% of all coins are such that approximately
50% of the events in which they are tossed re­
sult in heads. That is, 95% of the coins in the
domain are approximately fair. This example
demonstrates the useful ability to nest proportion
statements.
3

Representing Bayesian Networks

Using the logic described in the previous section we
can represent a large knowledge base of general logi­
cal and statistical information by a collection of sen.
tences. It is not difficult to see that any discrete val­
ued Bayesian network can easily be represented in thf
1The explicit inclusion of events in the domain of indi­

viduals is similar to the inclusion of other abstract object,:
like time points or situations (as in the situation calculu.>

[MH69]). There may be philosophical objections, but tech­
nical difficulties can be avoided if we restrict ourselves t;
a finite collection of distinct e vents.

221

Probability Log ics for KBMC

logic.2 Here we will give a particular scheme for repre­
senting an arbitrary network, although there are many
other schemes possible.

its parents Xf(i,l),...,Xf(i,qi)· This matrix of cone
tional probabilities consists of a collection of individm
equations each of the form

Any Bayesian network is completely specified by two
pieces of information: ( 1 ) a product decomposition of
the joint distribution which specifies the topological
structure of the network, and (2) matrices of condi­
tional probability values which parameterize the nodes
in the network [Pea88J . Consider an arbitrary network
B. Let the nodes in B be the set {X1,...,Xn}- Each
node Xi has some set of parents {XJ(i,l)> ...,Xf(i,qi)},
where f ( i, j) gives the index of node Xi's j-th parent,
and qi is the number of parents of Xi- Furthermore
each node Xi can take one of some discrete set of val­
ues {v1,...,Vki}, where ki is the number of different
values for xi.

Pr(Xi

The topological structure of B is completely specified
by the equation
Pr(X1,..., Xn)= Pr(X1IX!(l,l),...,Xf(l,ql))x
···X Pr(XniX t(n,l)> ...,Xf(n,q n))That is, the topological structure of B is equivalent
to an assertion about how the joint distribution over
the nodes X1-Xn can be decomposed into a product
of lower-order conditionals. Actually, this equation is
shorthand. Its full specification is that this product
decomposition holds for every collection of values the
nodes XcXn can take on.
We can translate this equation into a sentence of our
logic by creating a fnnction symbol for every node Xi;
for convenience we use the same symbol Xi. Now the
above structure equation can be rewritten as the sen­
tence

Vz1,···,Zn.[X1(e)=z1/\ ···1\Xn(e)=zn]e=
Xt(1,1)=Z£(1,1) 1\ . ..
X1(e)= z1
1\Xt(1,q1) =Zf(1,q1) e x

1

[

X

[

Xn(e) =Zn

J

I

.. ]

X f(n,1) = Zf(n,1)1\ .
1\X f(n,qn) =Zf(n,qn)

.
e

Here we have treated the multi-valued nodes as func­
tion symbols XrXn in our language. Our translated
sentence asserts that for every particular set of val­
ues the X1-Xn can take on, the proportion of events e
that achieve that set of values can be computed from
the lower-order relative proportions. The universal
quantification ensures that this product decomposition
holds of every collection of values.
Having completely specified the topological structure
of B, we can equally easily specify the conditional
probability parameters in our language. For each node
Xi, B provides the probability of Xi taking on any of
its allowed values under every possible instantiation of
2It is also possible, with a few technical caveats, to rep­
resent networks with continuous valued nodes. But here
we restrict our attention to discrete valued nodes.

=

tiiXt(i,l) =t f(i,l),···,Xf(i, qi)

=

tf(i,q i))=p,

where tj is some value for variable Xj, and p is some
numeric probability value.
To translate these equations into sentences of our logic
we create new constant symbols for every possible
value ti of every node Xi; for convenience we use the
same symbol ti. Now the above equation can be rewrit­
ten as the sentence

[Xi(e)= ti l

Xt (i,l)(e) � t f(i:! )1\ · ··
1\xf(l,ql) ( e ) -_ t f(l_ ,ql.)

]=
e

p.

Here we have simply rewritten the conditional proba
bility equations as equations involving the proportiu
of events in which Xi takes on value ti.
The above procedure can be applied to any networ�
Thus we make the following observation. Any discret.
valued Bayesian network can be represented as a Cur
lection of sentences in the knowledge base.
W hat is important to point out about this transla
tion is that the translated assertions represent templat.:
networks. As pointed out in [BGW91] most probabilis
tic networks in use in consultation systems are actuall}'
template models. That is, the nodes represent gener­
alized events which get instantiated to the particular
event under consideration. For example, a node rep­
resenting "Disease D" will be instantiated to "Patient
John R. Smith has disease D, " a node representing
"Blood test shows low white cell count" will be instan­
tiated to "Blood test T0906 for patient John R. Smith
shows low white cell count," etc. In our representation
the template nature of the networks is made explicit:
our formulas refer to proportions over classes of sim­
ilar events not particular events. As we will see this
is not a limitation in representational power, rather it
is simply a more accurate representation which allows
for greater modularity. Propositional networks refer
ring to particular events are to be generated from the
knowledge base via model construction techniques.
4

Simple Model Construction

To introduce the basic ideas that underlie our mode'
construction technique consider a knowledge base thai
consists simply of a collection of template Bayesian
networks, each one applicable to different types of
events.
To specify that each different decomposition, and col­
lection of conditional probability parameters, is appli­
cable to a different class of events we only need add the
event type as an extra conditioning formula. For ex­
ample, say that we have two networks both suitable for
diagnosing abdominal pain. However, one of the net
works is designed for women in late-term pregnane}.

222

Bacchus

\fz1, z2, zs.[Xl (e)=z1 1\X2(e) z2 1\Xs(e)=zsiAbdominalPain(e) 1\-.Pregnancy(e)]e
= [X1(e)=z1 IAbdominalPain(e) 1\-.Pregnancy(e)]e
x [X2(e)
z2 IX1(e) z1 1\AbdominalPain(e) 1\--.Pregnancy(e)]e
x [Xs(e)= zsiX1(e)=Z1 1\X2(e) =z2 1\AbdominalPain(e) 1\-.Pregnancy(e)]e,
=

(1)

=

=

\fz1,Z 2,zs.[Y1(e)=z1 1\Y2(e)=z2 1\Ys(e) zsiAbdominaiPain(e) 1\Pregnancy(e)]e
= [¥1(e)= z1IAbdominalPain(e) 1\Pregnancy(e)]e
X [Y2(e) = z2 IY1(e) =z1 1\AbdominalPain(e) 1\Pregnancy(e)]e
x [Ys(e) = zs1Y1(e)=z1 1\AbdominalPain(e) 1\Pregnancy(e)]e·
=

(2)

.

F igure 1: Alternate Structures for Abdominal Pain
while the other is suitable for other patients with ab­
dominal pain. Our general knowledge base might con­
tain the two formulas (Equations 1 and 2) shown in
F igure 1.
In this example the events involving abdominal pain
and pregnancy have a different network models (i.e.,
structural decompositions) with entirely different vari­
ables than the events where there is no pregnancy. In
a similar manner we can represent a whole collection
of disjoint types of events, where each event type is
modeled by a different probabilistic structure.
In this case the model construction technique in
this case would simply locate the appropriate tem­
plate model using information about the particular
event being reasoned about. For example, if the
event is EOOl and we know AbdominalPain(EOOl) 1\
Pregnancy(EOOl ), i.e., the event being reasoned about
involves adominal pain in a pregnant patient, we would
construct a network model for reasoning about EOOl
using the second template model. This network would
have the structure
Pr(Y1, Y2 , Y3)

Pr(Y1) x Pr(Y2 1Yl) x Pr(Y31Yl),
and would be parameterized by the values specified
in the knowledge base for the Yi variables. Since
the constructed network is now specific to event EOOl
we can drop the extra condition AbdominalPain(e) 1\
Pregancy(e) as we know that EOOl satisfies these con­
ditions. Now we have an event specific network that
can be used to reason about the probable values of the
variables Yi in the particular event.
=

We can see that the model constructor is simply "in­
stantiating" the general template model with the par­
ticular event EOOl. By using the same structure and
probability parameters as the class of abdominal pain­
pregnancy events we are assigning probabilities to the
particular event EOOl that are identical to the statis­
tics we have about that general class of events. This
is an example of direct inference, where we use statis­
tics over a class of similar events to assign probabilities
to a particular event. For example, when we assign a
probability of 1/2 to the event of heads on a particu­
lar coin toss based on statistics from a series of coin
tosses we are performing direct inference. This kind of
inference is pervasive in reasoning under uncertainty.3
3
See Kyburg

[K yb83a]

for further arguments pointing

Simple model construction of this kind is not tha,
interesting however. We could easily accomplish the
same thing with a control structure that chooses frou
some collection of networks. The main difference b
that here we have an explicit, declarative, represen
tation of which network is applicable to what type cr
event. Furthermore, it also serves to illustrate the b<1
sic idea behind our approach to KBMC.
5

More General Model Construction

In general we will not have explicit template models
in our knowledge base for all of the events we wish
to reason about. Indeed, this is exactly the point of
the KBMC approach: we want to deal with situation:>
beyond the ability of template models.
Our knowledge base will more likely contain informa
tion about conditional probabilities isolated to neigh
borhoods of related variables. For example, instead
of having an explicit product decomposition for all of
the relevant variables, as in the above examples, th•·
knowledge base might simply contain the individua.
product terms, i.e., the neighborhood information, in
isolation. It will be up to the model construction pro­
cedure to link these individual terms into a joint dis
tribution. Consider Pearl's classic Holmes's burglar:··
example. It is unlikely that Holmes has in his knowl­
edge base an explicitly represented decomposition of
the form shown in Equation 3 (Figure 2). Such a de
composition is simply far too specific. Rather Holme­
would more typically have information like that shown
in Equation 4 (Figure 2). In this case Holmes has the
knowledge (a) in 75% of the events in which a house
with an alarm is burglarized, the alarm will sound;
(b) in 45% of the events in which an alarm sounds
near where a person lives that person will report the
alarm; (c) the specific knowledge that Watson lives
near Holmes's house and that Holmes's house has an
alarm. The advantage of knowledge in this more gen­
eral form is that it can be used to reason about many
other types of events. For example, the statistic�!
knowledge (a) can be used to reason about any alarn1
in any house, e.g., if Holmes learns that his parent&
house alarm has been tripped; similarly (b) can b
out the prevalence of "direct inference" in probabilisti
reasoning.

Probability Logics for KBMC

(3)

(4)

223

[Burglary(e,MyHouse) A AlarmSound(e,MyHouse) A ReportsAlarm( e,Watson,MyHouse)]e
[AlarmSound(e,MyHouse) !Burglary(e,MyHouse)]e
x [ReportsAlarm(e,Watson,MyHouse) IAlarmSound(e,MyHouse)]e=

( a)
(b)
( c)

[AlarmSound(e,x)jBurglary(e,x) AHouseWithAlarm(x)](e,x) .75
[ReportsAlarm(e,y,x)l
AlarmSound(e,x) A HouseWithAlarm(x) A LivesNear(x,y)](e,x,y)
LivesNear(MyHouse,Watson) A HouseWithAlarm(MyHouse)
=

=

0 45
.

Figure 2: An overly Specific Decomposition vs. General Information
used for reasoning about reports from any neighbor,
e.g., if Mrs. Gibbons reported the alarm instead of Dr.
Watson.
Holmes will also have other pieces of statistical infor­
mation, e.g. , statistics about the event that a house
has been burglarized given that a police car is parked
outside, and other pieces of information specific to the
particular event being reasoned about. The task, then,
of a model construction procedure is to use the infor­
mation specific to the particular event being reasoned
about to decide which local pieces of statistical infor­
mation are relevant and how they should be linked
into a Bayesian network representation. Once a net­
work has been constructed it can be used to quickly
perform a range of complex reasoning about the par­
ticular event.
There are three issues that arise when constructing a
Bayesian network model of the particular event we are
reasoning about. First, the model construction proce­
dure must have some information about the variables
(properties of the event in question) that we wish to in­
clude in the constructed network. Second, we must use
information about the particular event to locate ap­
propriate pieces of local statistical information in the
knowledge base. And third, we must combine these
local pieces of information into a network.
5.1

The Set of Variables

Some information must be supplied about what collec­
tion of variables we want to model in the constructed
network. In the simplest case we will just supply
a query about the particular event under consider­
ation along with some additional information about
that event. For example, we might be reasoning
about event E002 and the guery might be expressed
as Burglary(E002)?; i.e., did a burglary occur as
part of this event? We might also have the informa­
tion ReportsAlarm(E002,Watson,MyHouse), i.e. , Dr.
Watson reported an alarm at Holmes's house during
this event. If the knowledge base is similar to that
given above, the procedure could determine that it can
chain probabilistic influence from a report by Watson
to belief in the alarm sounding, and then from there
to a belief in a burglary, i.e. , to an inference about
the query. Given that this is the only chain of influ­
ence it can find in the knowledge base linking alarm
reports and burglaries, the constructed network will

only contain a burglary node, an alarm sound nod< ,
and an alarm report node. That is, in a strictly query
driven KBMC procedure the constructed model will
only contain variables relevant to the particular quen
Alternately, we could supply the procedure with mor
information. For example, we could specify a set c
variables that we wish to include in the constructed
model. For example, we could specify that we are also
interested in reasoning about earthquakes and radio
broadcasts. If the knowledge base has local statistics
about the frequency of alarms sounding given earth­
quakes, and radio reports given earthquakes, a larger
Bayesian network could be constructed that includes
nodes for these variables. The links between these
variables would be determined by the local statistic··
contained in the knowledge base. For example, if w
know the frequency of alarm triggers given earthquak'
events, we would plac�a link from the earthquake noel-)
to the alarm node in the constructed network.
As in the simple query driven case, however, the proc.o
dure would still be able to add additional intermediat. ·
variables that link the variables in the set of inter
est. These intermediate variables would be found b:·
looking through the knowledge base for chains of in
ftuences between the specified variables. For example,
if we inform the procedure to build a model of some
set of diseases {D1,...,Dn} and some set of symp
toms {81,...,Bm}, it can search for chains of local
conditional probabilities linking members of these tw•>
sets. Hence, the constructed network will generally
contain additional intermediary nodes describing the
causal processes known to link the diseases with the
various symptoms, just as the alarm sound informa­
tion linked burglaries and alarm reports in the query
driven case.
It seems likely that we would want to amortize the ef­
fort of constructing the Bayesian network over a whole
range of queries. Hence, we will probably want to sup­
ply the model constructor with more information than
just a single query.
5.2

Locating the Appropriate Local Statistic.

Inform�tion about the particular event will help deter
mine which collection of local statistics are appropri
ate. The issue of choosing appropriate statistics is a
the heart of the difficulties in direct inference. Q],

224

Bacchus

approaches to direct inference revolved around try­
ing to find appropriate reference classes from which
statistics can be. drawn [Kyb83b]. More recent work
has taken an approach based on the principle of indif­
ference that dispenses with the notion of a. reference
class altogether [BGHK92]. In general, however, de­
termining the probabilities to assign to a particular
event given a collection of statistical information about
classes of similar events is a very difficult problem. For
a practical enterprise like KBMC, however, we can use
the work on direct inference to derive general guide­
lines as to what statistics to consider. For example,
all approaches to direct inference validate the subset
or specificity preference: one should choose the most
specific statistics applicable to the event in question.
Similarly, if we have statistical information about a
specific individual involved in the event we should use
that.
Information about the particular event can alter both
the parameterization and the structure of the con­
structed Bayesian network. This flexibility is not pos­
sible with simple template models. Consider the fol­
lowing example.
Example 2 Say that the local information shown in
F igure 3 was contained in the knowledge base. And
say that our information about the particular event
was ReportsAlarm(E002, Watson, MyHouse). If it is
decided that AlarmSound should be placed in the con­
structed network, either because it is a variable of in­
terest or because it is in a chain of influences to a
variable of interest, then the procedure would have to
choose how to parameterize the link from the MyHouse
alarm sound node and the Watson alarm report node.

The only statistic we have about the chance of an
alarm report given an alarm concerns the class of peo­
ple who live near the house whose alarm sounded. In
this case we know Dr. Watson is a member of this class,
i.e., LivesNear(MyHouse, Watson), so item 1 gives the
most specific known probability of a report given an
alarm. However, we do have a more specific statistic
for Dr. Watson, item 3, in the case of a report when
there is no alarm, indicating that Watson is a bit of a
practical joker. Hence, this more specific value would
be used for the probability of a report given no alarm.
On the other hand if the event in question involved
a report by Mrs. Gibbons, we would be forced to use
the more general statistics 1 and 2 to parameterize the
alarm-report/alarm-sound link as we have no specific
statistics for Mrs. Gibbon's alarm reports.
Example 3 Let the knowledge base be as in F igure 3,

except augmented by the additional statistical infor­
mation shown in F igure 4. That is, in this case Holmes
has a special alarm installed by a security company
AlarmMonitorCompany with a direct line to their of­
fice, and from the company's literature about the ac­
curacy of their alarm systems Holmes has come to ac­
cept the above statistical assertion about the reliabil­
ity of their alarm reports. Now if the event was Re-

portsAlarm(E003, AlarmMonitorCompany, MyHous<

there would be no need for the model constructio '
procedure to include an intermediary node of alan ,
sound, nor would the direction of the links be require.
to go from burglaries towards alarm reports. Instea' I
it could use this statistic, as the particular event EOO
is a member of this class of events, to link the alan
report node directly to the burglary node, and a quit·
different network structure would result.

J

5.3

Linking the Local Pieces

Once appropriate local statistics are obtained from the
·database we have enough information to link various
nodes in the network. That is, each local statistic will
serve to parameterize a link between two nodes in the
network. A difficulty that arises here is justifying thi ,
composition.
All we really know about the probability distributio: .
describing the interaction between the variables ar
the local conditional probabilities. There will in gen
eral be many different joint probability distribution",
that are consistent with these local conditional prob
abilities. In linking up the nodes in a manner d€
termined solely by the local information we are cou
structing a particular joint distribution, one in whie1
the local conditional probabilities determine a produ..;
decomposition. An important question is: to what ex
tent is such a procedure justified? Lewis [LI59] prow" I
some results which show that by taking the produc
of local conditional probabilities one obtains a best es
timator in the sense of Kullback-Leibler cross-entrop)
[KL51]. But his results do not cover all of the cases
that might occur. Another justification comes from re­
cent work that applies the principle of indifference to
reasoning about change [BGHK93]. For an enterprise
like KBMC, however, we will again want to use gen ­
eral principles derived from such work. One general
principle arising from [BGHK93], and earlier work by
Hunter [Hun89], is that when the variables are causally
related, as compared to being simply correlated, using
the product of the local conditional probabilities can
be justified by principles of indifference.
A related difficulty occurs when we have some but nm
all of the information required to specify the parame
terization of the network. For example, we might hav
statistics about a number of distinct causes for an d
feet, but we might not have statistics about their join.
effect. Pearl [Pea88] has suggested the use of "proto
typical structures" like noisy OR gates. There is P.J
underlying probabilistic model from which noisy 0 I
gates arise, and when it is reasonable to assume tha'
this model holds in a domain, prototypical structure ;
of this form could be used. Alternately, the indiffer
ence considerations of [BGHK93, Hun89] can also t,.
used in certain cases to complete the joint distribution
over the different causes.

Probability Logics for KBMC

225

1.

[ReportsAlarm(e,y,x)l
AlarmSound( e, x) 1\ HouseWithAlarm(x) 1\ LivesNear(x,y)](e,x,y) 0.45
2. [ReportsAlarm(e,y,x)l
·AlarmSound(e,x) 1\ HouseWithAlarm(x) 1\ LivesNear(x,y)](e,x,y) 0.05
3. [ReportsAlarm(e,Watson,x)l
·AlarmSound(e,x) 1\ HouseWithAlarm(x) 1\ LivesNear(x,Watson)](e,x) 0.15
4. HouseWithAlarm(MyHouse) 1\ LivesNear(MyHouse,Watson)
5. LivesNear(MyHouse, Gibbons)
=

=

=

F igure 3: Knowledge Base for Example 2
6.
7.

[Burglary(e,MyHouse) IReportsAlarm(e,AlarmMonitorCompany,MyHouse)]e
[Burglary(e,MyHouse) I•ReportsAlarm(e,AlarmMonitorCompany, MyHouse)]e

=

0.90

=

0.05

Figure 4: Additional Knowledge for Example 3
6

Conclusions and Future Work

We have outlined a mechanism for KBMC of Bayesian
networks from a knowledge base expressed in a first­
order probabilistic logic. Although we have only been
able to present a sketch of how the mechanism works
we have discussed the main ideas behind the proposal:
( 1) identify the variables of interest either through a
query driven process or through information provided
by the user; ( 2) locate local statistics, relevant to the
particular event being reasoned about, by using prin­
ciples from work on direct inference, like specificity, to
prefer certain local st�tistics over others; ( 3) construct
chains of probabilistic influence from these local statis­
tics; (4) construct an event specific network by using
the chains of probabilistic influence to specify the arcs
in the network, and by using the local statistics to
parameterize the nodes, perhaps filling in missing pa­
rameters by using prototypical structures or principles
of indifference. The resulting network can then be used
to reason probabilistically about the particular event.
The mechanism can be actualized fairly easily in
straightforward cases. In such cases the chains of influ­
ence are easy to locate: the individual links are explic­
itly expressed in the knowledge base. If the statistics
in the knowledge base are of a form such that select­
ing the most appropriate statistics reduces to simple
specificity considerations and if we have sufficient sta­
tistical information, we can easily parameterize the re­
sulting structure. Such a mechanism, although limited
in some ways, already offers a considerable increase in
flexibility over current template models.
One issue we have not addressed here is a mechanism
for representing temporal information, but as shown
by Bacchus et al. [B TH91] first-order logic is suffi­
cient for representing a range of temporal ontologies.
Hence, once an appropriate temporal ontology is de­
cided upon, it is possible that the representation could
be extended to allow for temporal information. If
the temporal structure is discrete we could also al-

low the formation of proportion statements over time
points, thus allowing the expression of various asser­
tions about discrete stochastic processes. A related
issue that can be addressed is the representation of
utilities. Extending our representation to utilities and
temporal information, and the KBMC procedure WP
proposed to generate, e.g., influence diagrams, is hU
interesting area for future research. Current work uH
this model is focused on filling in the details of th.
mechanism we have sketched, and on building a pro
totype system.
In conclusion, we feel that our proposal is a wori<
able one, that, with sufficient resources, can be turnt'•:
into a prototype implementation. Work on this is con
tinuting. Such an implementation holds the promis
of a useful KBMC procedure that would be far mor,
general than current template models. There are, o
course, limitations to the approach, limitations that.
stem mainly from problems that arise during dire.�r
inference. Given a very general knowledge base of sta­
tistical information it will not always be possible tl)
choose the "most appropriate" statistical information
for an event. For example, we might have conflict­
ing statistical information that cannot be resolved by
specificity. Nevertheless, we can still obtain useful re­
sults in less general but, we hope, still practical, con­
texts.
References

Fahiem Bacchus. Lp, a logic for rep
resenting and reasoning with statistica'
knowledge. Computational Intelligenu
6(4): 209-231, 1990.
[Bac90b] Fahiem Bacchus.
Representing an 1
Reasoning With Probabilistic Knowledgt
MIT-Press, Cambridge, Massachusett�
1990.
[BGHK92] F . Bacchus, A. J. Grove, J. Y. Halpern.
and D. Koller. From statistics to belief. h•

[Bac90a]

226

Bacchus

Proceedings of the AAAI National Confer­
ence, pages 602-608, 1992.
[BGHK93) F . Bacchus, A. J. Grove, J. Y. Halpern,
and D. Koller. Forming beliefs about a
changing world. In preparation, 1993.
[BGW91]

John S. Breese, Robert Goldman, and
Michael P. Wellman. Knowledge-based
construction of probabilistic and deci­
Unpub­
sion models: An overview.
lished manuscript, presented as introduc­
tion to AAAI-91 Workshop on Knowledge
Based Model Construction. Available from
Michael P. Wellman, USAF Wright Labo­
ratory, Wright Patterson Air Force Base,
OH., 1991.

[BTH91)

Fahiem Bacchus, Josh Tenenberg, and
Koomen Hans. A non-reified temporal
logic. Artificial Intelligence, 52:87-108,
1991.

[Hun89)

D. Hunter. Causality and maximum en­
tropy updating. International Journal
of Approximate Reasoning, 3(1):379--406,
1989.

[KL51)

S. Kullback and R. A. Leibler. Information
and sufficiency. Annals of Mathematical
Statistics, 22:79-86, 1951.

[Kyb61)

Henry E. Kyburg, Jr. Probability and the
Logic of Rational Belief Wesleyan Univer­
sity Press, Middletown, Connecticut, 1961.

[Kyb74)

Henry E. Kyburg, Jr. The Logical Foun­
dations of Statistical Inference. D. Reidel,
Dordrecht, Netherlands, 1974.

[Kyb83a)

Henry E. Kyburg, Jr. Epistemology and
Inference. University of Minnesota Press,
1983.

[Kyb83b)

Henry E. Kyburg, Jr. The reference
class. Philosophy of Science, 50(3):374397, September 1983.

[Lev80)

Isaac Levi. The Enterprise of Knowledge.
MIT-Press, Cambridge, Massachusetts,
Cambridge, Massachusetts, 1980.

[LI59]

P. M. Lewis II. Approximation probabil­
ity distribution's to reduce storage require­
ments. Information and Control, 2: 214225, 1959.

[Lou87]

Ronald P. Loui. Theory and Computa­
tion of Uncertain Inference and Decision.
PhD thesis, The University of Rochester,
September 1987.

[MH69]

John McCarthy and Patrick J. Hayes.
Some philosophical problems from the
standpoint of artificial intelligence. In Ma­
chine Intelligence 4, pages 463-502. Edin­
burgh University Press, 1969.

[OS90)

[Pea86]

[Pea88]

[Pol90)

[Sal71]

Robert M. Oliver and John Q. Smith, edl·
tors. Influence Diagmms, Belief Nets and
Decision Analysis. Wiley, 1990.
Judea Pearl. Fusion, propagation, and
structuring in belief networks. Artificial
Intelligence, 29:241-288, 1986.
Judea Pearl. Probabilistic Reasoning in In­
telligent Systems. Morgan Kaufmann, San
Mateo, California, 1988.
John L. Pollock. Nomic Probabilities and
the Foundations of Induction. Oxford Uni­
versity Press, Oxford, 1990.
Wesley Salmon. Statistical Explanation
and Statistical Relevance. University of
Pittsburgh Press, Pittsburgh, 1971.

