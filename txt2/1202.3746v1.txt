As a result, the accurate and computationally efficient
estimation of discrete energy-based models is a fundamental open problem in machine learning and computational statistics.
Maximum likelihood is the most commonly used estimator for probabilistic models. Its prevalence is based
on two key properties: asymptotic consistency and
maximal asymptotic efficiency [Fisher, 1922, p. 316].1
Essentially, an estimator is asymptotically consistent
if it converges to the true parameters as the sample
size goes to infinity. An asymptotically consistent estimator is maximally efficient if the variance in the
estimated parameters attains the minimum possible
value among all consistent estimators as the sample
size goes to infinity.2
However, the criterion function underlying standard
maximum likelihood estimation requires the computation of normalized joint probabilities under the model,
which in turn requires the explicit computation of
the partition function. When the computation of the
partition function is intractable, the computation of
the maximum likelihood estimator is clearly also intractable. This creates a fundamental tension between
classical estimation theory, which suggests the use of
maximum likelihood estimation due to its optimal statistical properties, and complexity theory, which states
1
Note that these properties hold subject to particular regularity conditions that are satisfied by many model
classes including discrete log-linear models like Markov
random fields, Boltzmann machines and conditional random fields.
2
Note that both properties implicitly assume that the
model is well-specified (the true distribution can be represented by the model class), although the statements can
be generalized when this is not the case.

that the computation required to perform maximum
likelihood estimation is infeasible for a general discrete
energy-based model.
The estimation of discrete energy-based models thus
requires a trade-off between the statistical and computational properties of estimators. Two fundamentally
different approaches have been proposed for this problem. The first approach is to approximately maximize
the likelihood. Examples of the approximation approach include stochastic approximation-based maximum likelihood methods [Younes, 1989], as well as
approximation methods based on belief propagation
[Wainwright et al., 2003].
The second approach is to select an alternative estimation criterion that explicitly avoids the computation
of the partition function. Pseudolikelihood is perhaps
the best-known alternative estimation criterion for discrete energy-based models [Besag, 1975]. It avoids the
need to compute the partition function by defining a
criterion function consisting of a sum of log-conditional
probabilities. Since any conditional probability can be
written as a ratio of model probabilities with the same
value of the partition function, the partition function
analytically cancels away.
Recent research in machine learning has seen the proposal of a large number of alternative deterministic
estimators for discrete energy-based models that resemble the original pseudolikelihood estimator proposed by Besag [1975], as well as various forms of
composite likelihood estimators [Lindsay, 1988]. This
set of alternative estimators includes ratio matching
[Hyvärinen, 2007], generalized score matching [Lyu,
2009], contrastive estimation [Smith and Eisner, 2005]
and non-local contrastive objectives [Vickrey et al.,
2010]. While previous work has focused on proposing new estimators and empirical comparisons among
existing estimators [Marlin et al., 2009], virtually nothing is known about the asymptotic relative efficiencies
of these alternative estimators.
Our primary interest in this paper is the unification
and theoretical analysis of deterministic estimators for
discrete energy-based models. We present a novel generalized estimator and show that it subsumes the majority of the deterministic estimators we have mentioned thus far. Our generalized estimator is itself a
special case of an M-estimator [van der Vaart, 2000].
This allows us to apply the standard asymptotic theory for M-estimators to obtain a generic expression for
the asymptotic covariance matrix of our generalized
estimator. We use these results to study the relative
efficiency of pseudolikelihood and ratio matching. We
present a theorem bounding the asymptotic covariance
matrix for ratio matching by the asymptotic covari-

ance matrix for pseudolikelihood. We show that neither of these estimators is strictly more efficient than
the other.

2

Discrete Energy-Based Models

Discrete energy-based models fall into two broad categories: discriminative and generative models. We describe the discriminative case as it subsumes the generative case. A discriminative discrete energy-based
model is a joint distribution over a set of random variables X, conditioned on an instantiation of a second
set of random variables Y .3 We assume that there
are DX random variables X = (X1 , ..., XDX ) in the
first set and DY random variables Y = (Y1 , ..., YDY )
in the second set. We assume that each random variable Xd takes values in a discrete set Xd . The set
of random variables X thus takes values in the set
X = X1 × ... × XDX . The set of random variables Y
similarly takes values in the set Y = Y1 × ... × YDY ,
but the domain of each variable Yd is unconstrained.
The model is defined through an energy function
Eθ (x, y), which depends on the model parameters θ
and a pair of instantiations x = (x1 , ..., xDX ), y =
(y1 , ..., yDY ). We denote the dimensionality of the parameter vector by Dθ . The joint probability and partition function for the discriminative dEBM are shown
in equations 2.1 and 2.2.
1
exp (−Eθ (x, y))
Zθ (y)
X
Zθ (y) =
exp (−Eθ (x, y))
x∈X

Pθ (X = x|Y = y) =

(2.1)
(2.2)

The generative dEBM can be seen as a special case of
the discriminative dEBM where the set of conditioning
variables Y is empty. In this case, we use the simplified notation Eθ (x) for the energy function and Zθ
for the partition function. Unlike Bayesian networks
[Pearl, 1988], energy-based models are not automatically normalized. The exponentiated energies must
be explicitly divided by the partition function to yield
a valid joint distribution. The partition function is a
sum over all joint instantiations of the X random variables with the conditioning instantiation y fixed (or
empty for the generative case) . The number of terms
in this sum grows exponentially as a function of DX
and it quickly becomes intractable to compute for both
discriminative and generative dEBMs. We illustrate
several common instances of the dEBM framework below, including binary Markov random Fields (MRFs),
3
We reverse the standard roles of X and Y in conditional models so that the conditional model simplifies to a
generative model over the variables X when the conditioning set it empty.

binary restricted Boltzmann machines (RBMs) and binary conditional random fields (CRFs).
• Binary MRF: The binary MRF model is parameterized by a symmetric DX ×DX quadratic interaction
matrix W [Kindermann et al., 1980]. It is the discrete
analogue of the multivariate Gaussian distribution.
EθMRF (x) = −xT W x

(2.3)

• Binary CRF: A binary CRF model is parameterized by a symmetric DX × DX quadratic interaction
matrix W and a bi-linear interaction term linking the
features Y to the labels X. It is the discriminative
analogue of the binary MRF model [Lafferty et al.,
2001].
EθCRF (x, y)

T

T

= −(x W x + x V y)

(2.4)

• Binary RBM: The binary RBM model is parameterized by K filter vectors Wk of size DX × 1 and K
scalar offsets ck . This form of the energy function is
obtained by starting from a bipartite binary random
field model, the more common visible/hidden representation of the RBM, and analytically summing out
over the hidden variables [Smolensky, 1986].
EθRBM (x) = −

K
X

log(1 + exp(xT Wk + ck ))

(2.5)

k=1

Perhaps the most significant difference between MRFs,
CRFs and RBMs is that MRFs and CRFs have energy
functions that are linear in their parameters (they are
log-linear models), while the RBM does not. In addition, the RBM model is not identifiable due to the fact
that permuting the K filters leaves the model probability invariant. The estimators that we study in this
paper are applicable to all dEBMs, but log-linear models are more amenable to theoretical analysis due to
the fact that their log likelihoods are convex.

3

A Generalized Estimator

In this section, we introduce a novel generalized estimator that subsumes many of the classical and
recently-proposed estimators for discrete energy-based
models. In the first part of this section, we define the
generalized estimator. The generalized estimator is itself a member of a larger family of estimators called
M-estimators [van der Vaart, 2000, Ch. 5]. In the subsequent parts of this section, we derive a generic expression for the asymptotic covariance matrix of the
generalized estimator using the standard asymptotic
theory of M-estimators.

3.1

Definition of the Estimator

Our generalized estimator deals with the intractability of the partition function in exactly the same way
as Besag’s classical pseudolikelihood estimator [Besag,
1975]. It is based on the use of ratios of model probabilities with the same partition function, ensuring that
the partition functions analytically cancel away. To
emphasize the point that the estimator does not rely
on the availability of normalized probabilities, we introduce the special notation Qθ (x) in Equation 3.8 to
denote the model probability computed up to the normalization term.
Qθ (x) = exp (−Eθ (x))
Rθ (x, A) = P

Q (x)
Pθ (x)
P θ
′ =
′
x′ ∈A Pθ (x )
x′ ∈A Qθ (x )

(3.8)
(3.9)

We define the probability ratio function Rθ (x, A) in
Equation 3.9. This function takes an arbitrary data
configuration x ∈ X and an arbitrary set A ⊆ X and
computes the ratio of the probability of x over the sum
of the probabilities of each configuration x′ contained
in the set A.
We can now define the generalized estimation criterion
function fθ (x1:N ) as seen in Equation 3.10. We use
the shorthand notation x1:N to indicate a data set
containing N data cases x1 , ..., xN . The estimation
criterion is restricted to decompose additively across
the N data cases.
fθ (x1:N ) =
mθ (x) =

N
1 X
mθ (xn )
N n=1

C
1 X
gc (Rθ (x, Nc (x))
C c=1

(3.10)

(3.11)

For each data case, we apply the function mθ (x) defined in Equation 3.11. mθ (x) consists of a sum over
C components. For each component c, we compute
the probability ratio Rθ (x, Nc (x)) and then pass it
through a transfer function gc (). The probability ratio is computed with respect to a set Nc (x) ⊆ X that
we refer to as the neighborhood of x in component c.
The neighborhood function can depend both on the
component c and the data configuration x. The transfer function gc () can depend on the component c only.
When the number of components and the size of the
neighborhoods are both relatively small, the resulting
criterion function can be computed exactly and optimized without requiring any other approximation.
3.2

M-Estimators

Following van der Vaart [2000, Ch. 5], an M-estimator
is defined through a criterion function that is repre-




C
∂mθ (x)
1 X ′
∂Eθ (x)
∂Eθ (x)
=
g (Rθ (x, Nc (x)))Rθ (x, Nc (x)) −
+ ERθ (x,Nc (x))
∂θi
C c=1 c
∂θi
∂θi

(3.6)

C

∂ 2 mθ (x)
1 X  ′′
=
gc (Rθ (x, Nc (x)))Rθ2 (x, Nc (x)) + gc′ (Rθ (x, Nc (x)))Rθ (x, Nc (x))
∂θi ∂θj
C c=1


 


∂Eθ (x)
∂Eθ (x)
∂Eθ (x)
∂Eθ (x)
· −
+ ERθ (x,Nc (x))
−
+ ERθ (x,Nc (x))
∂θj
∂θj
∂θi
∂θi



C
1 X ′
∂ 2 Eθ (x)
∂ 2 Eθ (x)
gc (Rθ (x, Nc (x)))Rθ (x, Nc (x)) −
+ ERθ (x,Nc (x))
+
C c=1
∂θi ∂θj
∂θi ∂θj


C
1 X ′
∂Eθ (x) ∂Eθ (x)
−
g (Rθ (x, Nc (x)))Rθ (x, Nc (x))CRθ (x,Nc (x))
,
C c=1 c
∂θj
∂θi

sentable as the empirical average of a known estimating function mθ (xn ) over N samples xn drawn from
an underlying true probability distribution P∗ (x) as
seen in Equation 3.12. Note that we can equivalently
define the estimator as an expectation
PN under the empirical distribution PN (x) = N1
n=1 δ(x, xn ) where
δ(a, b) is the Kronecker delta function. Our generalized estimator parameterizes the estimating function
mθ (xn ), creating a useful restriction on the space of
all M-estimators.
N
X
1 X
fθ (x1:N ) =
mθ (xn ) =
PN (x)mθ (x)
N n=1
x∈X
(3.12)

The two key theoretical properties of M-estimators are
asymptotic consistency and asymptotic efficiency. The
advantage of working within the M-estimator framework is that generic conditions necessary for these
properties to hold are well established.
3.3

Consistency

Following van der Vaart [2000, Ch. 5], we define fθ∗
to be the limit as N goes to infinity of the criterion
function fθ (x1:N ), as given in Equation 3.13. We define θ∞ to be an optimizer of fθ∗ . Note that it is not
necessarily assumed that the true distribution is in the
model class so that P∗ is not in general equal to Pθ∞ .
X
P∗ (x)mθ (x)
(3.13)
fθ∗ =
x∈X
We denote an M-estimator computed by maximizing
a criterion function fθ (x1:N ) based on a sample of N
data cases drawn from P∗ by θ̂N . The estimator θ̂N is
said to be consistent if the limit as N goes to infinity
of D(θ̂N − θ∞ ) goes to zero under a suitable distance
metric D. All of the models we consider have parameters defined over Euclidean spaces, so we will simply

(3.7)

take D to be the standard Euclidean distance. The
requirements for an M-estimator to be consistent are
given in van der Vaart [2000, Theorem 5.7]. Establishing the consistency of estimators for log-linear models
can be relatively simple because the criterion functions
are often convex. It is generally not possible to establish consistency for non-identifiable models like RBMs
without placing further restrictions on the parameters.
3.4

Asymptotic Normality

An estimator is said to be asymptotically
normal if it
√
is consistent and the sequence N (θ̂N −θ∞ ) converges
in distribution to a multivariate normal N (0, Σ). The
conditions on the estimating function mθ (x) required
to ensure asymptotic normality are given in van der
Vaart, Theorem 5.21. Our main interest is the asymptotic covariance matrix Σ, which can be defined in
terms of mθ (x) as shown in Equation 3.14.
Σ = (Hθ∞ )−1 Jθ∞ (Hθ∞ )−1
 2

∂ mθ (x)
(Hθ∞ )ij = EP∗ (x)
∂θi ∂θj θ∞

∂mθ (x) ∂mθ (x)
(Jθ∞ )ij = EP∗ (x)
∂θi
∂θj

(3.14)

θ∞



To define the asymptotic covariance matrix for our
generalized estimator, we need only supply the first
and second derivatives for our parametrization of
the estimating function mθ (x).
The results are
given in Equations 3.6 and 3.7. We use the notation ERθ (x,Nc (x)) [a(x)] and CRθ (x,Nc (x)) (a(x), b(x))
to denote expectations and covariances under
Rθ (x, Nc (x)). We define the expectation below. The
definition of the covariance follows from the expectation in the usual way. These definitions are sensible
since Rθ (x, Nc (x)) acts as a properly normalized dis-

tribution over the set Nc (x).
X
ERθ (x,Nc (x)) [a(x)] =
Rθ (x′ , Nc (x))a(x′ )
x′ ∈Nc (x)
Despite appearing complex, Equations 3.6 and 3.7 can
be derived with relative ease starting from Equation
3.11. More importantly, the equations depend only on
the definition of the ratio function Rθ (x, Nc (x)), the
first and second derivatives of the transfer function
gc (R) and the energy function of the model Eθ (x).
All of these quantities can be easily derived for specific models and estimators. In the next section, we
show how several classical and recent estimators can
be viewed as special cases of the generalized estimator.

4

Applications

Many classical and recently proposed estimators for
dEBMs are subsumed by our generalized estimator
through appropriate choices of the number of components C, the transfer function gc (R) and the neighborhood function Nc (x). To simplify the exposition,
we focus on estimators for binary data and generative
dEBMs only.
4.1

Maximum Likelihood

The maximum likelihood principle states that we
should select the parameters θ that assign the highest probability to the observed data. The maximum
likelihood criterion function is given in Equation 4.15.
fθML (x1:N ) =

N
1 X
log Pθ (xn )
N n=1

(4.15)

The maximum likelihood estimator can be put into
our generalized framework using the settings C = 1,
g1 (R) = log(R), and N1 (x) = X for all x. This
choice of neighborhood function exactly recreates the
full partition function since Rθ (x, N1 (x)) = Pθ (x) for
all x. The log transfer function has the interesting
property that log′ (R)R = 1 and log′′ (R)R2 = −1.
This cancels the entire first line of Equation 3.7. If
we further assume that the model is well specified
(P∗ = Pθ∞ ), we quickly arrive at the classical result
that HθML
= −JθML
and the asymptotic covariance
∞
∞
matrix for maximum likelihood is equal to the inverse
of the Fisher information matrix I.4

−1 ML
−1
−1
ΣML = (HθML
Jθ∞ (HθML
= (JθML
= I −1
∞ )
∞ )
∞ )


∂Eθ (x) ∂Eθ (x)
(JθML
,
∞ )ij = CP∗ (x)
∂θj
∂θi

4
Note that all expressions must be evaluated at θ = θ∞
after taking derivatives, as in Equation 3.14, but we suppress the explicit notation here and in subsequent expressions due to space limitations.

4.2

Pseudolikelihood

The classical pseudolikelihood criterion consists of a
sum of all one-dimensional model log-conditional distributions log Pθ (xd |x−d ) as shown in Equation 4.16.
The notation x−d indicates the instantiations of all of
the random variables except for xd .

fθP L (x1:N ) =

N D
1 XX
log Pθ (xdn |x−dn )
N D n=1

(4.16)

d=1

Pseudolikelihood can be put into our generalized
framework using the settings C = D, Nd (x) =
¬d
{x, x
}, and g(R) = log(R). The notation x¬d indicates the instantiation formed by starting with x and
flipping the value of xd . This choice of neighborhood
function leads to Rθ (x, Nd (x)) = Pθ (xd |x−d ), which
recovers the conditional distributions used to define
pseudolikelihood.
As in the maximum likelihood case, use of the log
transfer function cancels the entire first line of Equation 3.7. If we again assume that the model is well
specified (P∗ = Pθ∞ ), we can easily obtain a simple
form for (HθP∞L )ij :

−EP∗ (x)

"


#
D
∂Eθ (x) ∂Eθ (x)
1 X
CP ∗(xd |x−d )
,
D
∂θj
∂θi
d=1

(4.17)

The expression for JθP∞L is somewhat more complicated. We state (JθP∞L )ij below in an unexpanded form
that is useful for subsequent comparisons.
"




D 
1 X
∂Eθ (x)
∂Eθ (x)
EP∗ (xd |x−d )
−
D
∂θi
∂θi
d=1


#
D 
1 X
∂Eθ (x)
∂Eθ (x)
·
EP∗ (xd |x−d )
−
D
∂θj
∂θj

EP∗ (x)

d=1

(4.18)

4.3

Ratio Matching

The ratio matching criterion function can be interpreted as a sum of ℓ2 distances between pairs of onedimensional conditional distributions under the model
and the empirical distribution. The ratio matching criterion function is given in Equation 4.19. Note that the
original presentation of ratio matching defines the estimator in terms of a function h(a) = 1/(1 + a) applied
¬d
to ratios of the form P (x)/P (x
) [Hyvärinen, 2007],
but this unnecessarily obscures the close relationship

Transfer Functions

between ratio matching and pseudolikelihood.
0

fθRM (x1:N )

N D
1 XX X 
=−
PN (Xd = ξ|x−dn )
N D n=1
d=1 ξ∈{0,1}
2
− Pθ (Xd = ξ|x−dn )
(4.19)

−1
−2
−3

The ratio matching criterion function can be reduced
to a much simpler form as shown by Hyvärinen [2007].
We give this simplified form in Equation 4.20. Note
that the extra summation over the states of the dth
variable in Equation 4.19 is critical to obtaining this
reduced form of the estimator shown below.
fθRM (x1:N )

N D
1 XX
(1 − Pθ (xdn |x−dn ))2
=−
N D n=1
d=1

(4.20)

Ratio matching can be obtained in our generalized
¬d
framework by the settings C = D, Nd (x) = {x, x
},
2
and gd (R) = −(1 − R) . We immediately see that the
choice of the number of components and the neighborhood structure is identical to pseudolikelihood. The
two estimators differ only in terms of the transfer functions. We plot the log(R) transfer function as well
as the −(1 − R)2 transfer function in Figure 1. We
note that the two functions are very similar, differing
mainly in the fact that −(1 − R)2 is equal to −1 at
R = 0 while log(R) diverges to −∞ as R goes to zero.
The fact that ratio matching uses a different transfer function means that the expression for the asymptotic covariance matrix is yet more complex. However,
it does simplify under the assumption of no model
mismatch. We obtain the following expression for
(HθRM
∞ )ij :
−

D
h
2 X
EP∗ (x) P∗2 (xd |x−d )
D
d=1




∂Eθ (x)
∂Eθ (x)
· EP∗ (xd |x−d )
−
∂θi
∂θi




∂Eθ (x)
∂Eθ (x) i
· EP∗ (xd |x−d )
−
∂θj
∂θj

We state (JθRM
∞ )ij below. We introduce a new piece
of notation VdP∗ = (P∗ (xd |x−d ))(1 − P∗ (xd |x−d )) to
indicate the Bernoulli variance of the dth conditional
distribution.
"




D
∂Eθ (x)
∂Eθ (x)
2 X d
EP∗ (x)
VP∗ EP∗ (xd |x−d )
−
D
∂θi
∂θi
d=1



#
D
2 X d
∂Eθ (x)
∂Eθ (x)
·
VP∗ EP∗ (xd |x−d )
−
D
∂θj
∂θj
d=1

−4
−5
0

log(R)
−(1−R)2

0.2

0.4

0.6

0.8

1

R

Figure 1: This figure shows the log(R) transfer function compared to the −(1 − R)2 transfer function used
by ratio matching.
4.4

Generalized Score Matching

The inductive principle underlying generalized score
matching also involves an ℓ2 distance and pairs of onedimensional conditional distributions. In this case,
however, the ℓ2 distance is applied to the difference
of inverses conditional probabilities, as seen in Equation 4.21 [Lyu, 2009].
fθGSM (x1:N ) = −
−

N X
D 
X

n=1 d=1

1
Pθ (xdn |x−dn )
2

1
PN (xdn |x−dn )

(4.21)

The generalized score matching criterion can be reduced to a form that only depends on ratios of probabilities as shown in Equation 4.22.5 .
N D 
1 X X  Pθ (xdn |x−dn ) −2
GSM
fθ
(x1:N ) = −
N D n=1
1 − Pθ (xdn |x−dn )
d=1
 P (x |x

θ dn −dn )
−2
(4.22)
1 − Pθ (xdn |x−dn )
Generalized score matching can be put in our generalized framework using the settings C = D, Nd (x) =
¬d
{x, x
}, and gd (R) = (R/(1−R))−2 −2(R/(1−R)). In
the interest of space, we omit the form of the asymptotic covariance matrix for generalized score matching
as we will not analyze it further in the current paper.
4.5

Contrastive Estimation and Non-Local
Contrastive Objectives

Contrastive estimation [Smith and Eisner, 2005] and
non-local contrastive objectives [Vickrey et al., 2010]
5
Note that the original reduced form due to Lyu contained an error that was corrected in Marlin et al. [2009]

are both closely related to composite conditional likelihood estimators [Lindsay, 1988]. Both estimators
use multiple components, log(R) for the transfer function and ratios of unnormalized probabilities defined
by neighborhoods. The main contribution of the contrastive estimation method is that it makes use of relatively large, structured neighborhoods that can be
summed over efficiently.
The main feature of the contrastive objectives method
is the proposal of a method for iteratively increasing
the size of the neighborhoods. The goal of the iterative
procedure is to dynamically construct neighborhoods
where the configurations are not one-neighbors as in
psuedolikelihood. In the interest of space, we again
omit the forms of the asymptotic covariance matrix
for these methods as we will not analyze them further
in the current paper.

5

Relative Efficiency Results

It is well known that maximum likelihood attains the
minimum possible asymptotic covariance of any consistent estimator. Prior work has considered the relative
efficiency of pseudolikelihood compared to maximum
likelihood [Liang and Jordan, 2008]. Our main interest
in this work is establishing results regarding the relative efficiency of pseudolikelihood and ratio matching.
It is well known that pseudolikelihood is consistent
for well specified log-linear models since the criterion
function is convex. The ratio matching criterion function is not convex, but it is still consistent for identifiable models in the well-specified case as shown by
Hyvärinen [2007].
Our main theorem shows that the asymptotic covariance matrix for ratio matching can be both upper
and lower bounded by scalar multiples of the pseudolikelihood asymptotic covariance matrix in the wellspecified case. The inequalities in the theorem are with
respect to the standard positive definite ordering relation where A 4 B implies that B − A is positive
definite.
Theorem 5.1. Let Vmax and Vmin be the maximum and minimum values of the Bernoulli variance
P∗ (xd |x−d )(1 − P∗ (xd |x−d )) over all x ∈ X and
d ∈ {1...D}. Let qmax and qmin be the maximum and
minimum values of P∗ (xd |x−d ) over all x ∈ X and
d ∈ {1...D}. The asymptotic covariance matrix for
ratio matching satisfies:
V2min P L
V2
Σ 4 ΣRM 4 max
ΣP L
4
4
qmax
qmin
Proof: We begin with JθRM
∞ . We can easily see that
the only difference between JθRM
and JθP∞L is that each
∞

term in JθRM
is weighted by 2VdP∗ = 2P∗ (xd |x−d )(1 −
∞
P∗ (xd |x−d )). Since Vmax and Vmin upper and lower
bound VdP∗ by definition, we have that
4V2min JθP∞L 4 JθRM
4 4V2max JθP∞L .
∞
Under the assumption that P∗ (xd |x−d ) is equal to a
constant q, HθRM
simplifies 2q 2 HθP∞L . Since qmax and
∞
qmin upper and lower bound P∗ (xd |x−d ) by definition
and −HθRM
and −HθP∞L are positive definite, we have
∞
2
2
that −2qminHθP∞L 4 −HθRM
4 −2qmax
HθP∞L . By in∞
verting all of the matrices we obtain:
−

1
2
2qmax

−1
(HθP∞L )−1 4 −(HθRM
4−
∞ )

1
(HθP∞L )−1 .
2
2qmin

We can combine the two sets of bounds to prove the
theorem as shown below.
V2min P L V2min P L −1 P L P L −1
Σ = 4 (Hθ∞ ) Jθ∞ (Hθ∞ )
4
qmax
qmax
−1 RM
−1
4 ΣRM = (HθRM
Jθ∞ (HθRM
∞ )
∞ )

4

V2max P L V2max P L −1 P L P L −1
Σ = 4 (Hθ∞ ) Jθ∞ (Hθ∞ )
4
qmin
qmin



Since the coefficients in the bound are constructed
by minimizing and maximizing over all conditional
probabilities and variances, they are not independent.
In particular, we must have that qmax = 1 − qmin .
The minimal conditional variance Vmin is obtained using the most extreme conditional probabilities. This
means that Vmin = qmin (1 − qmin ) = (1 − qmax )qmax =
qmin qmax . The maximal variance Vmax is independent of the other quantities, but can be expressed
in terms of the conditional probability that is closest
to 0.5 without exceeding it. We label this probability qmid and express the maximal variance as Vmax =
qmid (1 − qmid ). This allows us to re-express the bound
as follows:
2
2
qmin
qmid
(1 − qmid )2 P L
PL
RM
Σ
4
Σ
4
Σ
4
2
qmax
qmin
2
2
Since qmin ≤ qmax , we have that qmin
/qmax
≤ 1. Since
2
qmin ≤ qmid ≤ (1 − qmid ), we have that qmid
(1 −
2 4
qmid ) /qmin ≥ 1. As a result, we have that the same
bound applies to the pseudolikelihood covariance matrix as well:
2
qmin
q 2 (1 − qmid )2 P L
ΣP L 4 ΣP L 4 mid 4
Σ
2
qmax
qmin

We now consider the problem of quantifying the difference between ΣRM and ΣP L . One standard way of
comparing the size of covariance matrices is the ratio of
their determinants. The determinant of a positive definite matrix is the volume of the corresponding ellipsoid. The determinant has a very intuitive interpretation in this setting: it can be thought of as the volume

(a) PL vs ML

(b) RM vs ML

(c) RM vs PL

Figure 2: Efficiency difference for PL&ML (a), RM&ML (b) and RM&PL (c) versus the bound width.
of parameter space in which the estimated parameters
are likely to fall for large N . In the present case, a more
convenient measure is the log of the determinant ratio,
which is simply the difference of log determinants. Letq2 (1−q
)2
V2
q2
V2
= mid q4 mid ,
ting l = q4min = q2min and h = qmax
4
max
max
min
min
we obtain the result:
 PL 
 RM 


|lΣ |
|Σ |
|hΣP L |
log
≤ log
≤ log
|ΣP L |
|ΣP L |
|ΣP L |

We randomly sample 1000 sets of true model parameters from a zero mean spherical Gaussian distribution with standard deviation one. We then compute
the determinant of the asymptotic covariance matrix
for maximum likelihood, pseudolikelihood and ratio
V2
matching, as well as the bound coefficients l = q4min

We see that the width of the bound on the difference
in asymptotic efficiency between pseudolikelihood and
ratio matching in the log determinant sense depends
directly on the width of the interval log(h) − log(l).
We note that in certain special cases the bound can
be exact. For example, if the true distribution is uniform, all the conditional probabilities will be equal to
1/2|X |. In this case, the covariance matrices for pseudolikelihood and ratio matching will be identical and
the width of the bound in the above sense will also be
exactly zero. On the other hand, the maximum width
of the bound is unbounded as qmin goes to zero. For an
energy-based model, this would require that a subset
of the parameter values approach positive or negative
infinity.

The results are given in Figure 2 for PL and ML (a),
RM and ML (b) and RM and PL (c). In each plot,
the second estimator in the pair is more efficient for
a given sample in the log-determinant sense if the
plotted point lies above zero (this indicates that the
second estimator in the pair has an asymptotic covariance matrix with smaller volume). As expected,
we see that ML is always more efficient than both
PL and RM in the log-determinant sense. As predicted by the theoretical development in the previous section, the maximum difference in efficiency between ratio matching and pseudolikelihood in the logdeterminant sense scales with the bound width, although we see that the bound can be quite loose. The
plots clearly show that neither estimator is always
more efficient than the other in the log-determinant
sense. We also note that neither is more efficient
than the other in the stricter positive definite ordering. Indeed, ΣRM − ΣP L is almost always an indefinite matrix with some positive and some negative eigenvalues. For example, the parameter setting
W = [2, 2, 2, −2, −2, −2] leads to ΣRM − ΣP L having
the eigenvalues: [10.8, 0, 0, 0, 0, −6.5].

Dθ log(l) ≤ log |ΣRM | − log |ΣP L | ≤ Dθ log(h)

5.1

Simulations

We perform a simulation study in low dimensions
where the exact computation of the partition function
as well as the asymptotic covariance matrices is feasible. The simulation uses a third-order binary Boltzmann machine in three dimensions with the following
energy function: Eθ (x) = −(W1 (1 − x1 )(1 − x2 )(1 −
x3 ) + W2 x1 (1 − x2 )(1 − x3 ) + W3 (1 − x1 )x2 (1 − x3 ) +
W4 (1 − x1 )(1 − x2 )x3 + W5 x1 x2 (1 − x3 ) + W6 x1 (1 −
x2 )x3 ). This energy function thus has six parameters
while the full joint distribution has seven degrees of
freedom.

max

V2

. We plot the log determinant differand h = qmax
4
min
ences for each set of sampled parameters relative to
the bound width log(h) − log(l) for each pair of estimators.

6

Conclusions and Future Work

We present a generalized estimator that unifies several
classical and recently proposed estimators. This uni-

fication is valuable for several reasons. First, it highlights the small number of dimensions along which the
estimators differ: the choice of transfer function, the
number of components and the neighborhood structure. Second, it allows us to derive generic asymptotic
results by applying M-estimation theory to the generalized estimator. In this paper, we use these results to
study the relative asymptotic efficiency of pseudolikelihood and ratio matching.
The unifying perspective offered by the generalized estimator also raises many interesting questions for future study. For example, can we obtain general results regarding the relative efficiency of estimators as
a function of properties of the transfer function and
neighborhood structure? Can we design transfer functions that are maximally efficient for a given neighborhood structure and model class? How does the choice
of transfer function affect robustness of the estimators? Alternative estimators with a large number of
components will again necessitate the use of stochastic approximation algorithms. The question of how the
efficiency of such estimators compares to the efficiency
of stochastic maximum likelihood estimators is also of
great interest.

Acknowledgments
This work was supported by the Pacific Institute for
the Mathematical Science, Mitacs, the CIFAR Neural
Computation and Adaptive Perception program and
the Natural Sciences and Engineering Research Council of Canada.

References
D.H. Ackley, G.E. Hinton, and T.J. Sejnowski. A
learning algorithm for Boltzmann machines. Cognitive science, 9(1):147–169, 1985.
J. Besag. Statistical analysis of non-lattice data. The
Statistician, 24(3):179–195, 1975.
R. A. Fisher. On the mathematical foundations of
theoretical statistics. Philosophical Transactions of
the Royal Society of London Series A., 222:309–368,
1922.
Aapo Hyvärinen. Some extensions of score matching. Computational Statistics & Data Analysis, 51
(5):2499–2512, 2007.
E. Ising. Beitrag zur theorie des ferromagnetismus.
Zeitschrift für Physik, 31(1):253–258, 1925.
R. Kindermann, J.L. Snell, and American Mathematical Society. Markov random fields and their applications. American Mathematical Society, 1980.

J.D. Lafferty, A. McCallum, and F.C.N. Pereira. Conditional Random Fields: Probabilistic Models for
Segmenting and Labeling Sequence Data. In Proceedings of the 18th International Conference on
Machine Learning, pages 282–289, 2001.
P. Liang and M.I. Jordan. An asymptotic analysis of
generative, discriminative, and pseudolikelihood estimators. In Proceedings of the 25th International
Conference on Machine learning, pages 584–591,
2008.
B.G. Lindsay. Composite likelihood methods. Contemporary Mathematics, 80(1):221–239, 1988.
S. Lyu. Interpretation and generalization of score
matching. In Proceedings of the 25th Conference
on Uncertainty in Artificial Intelligence, pages 359–
366, 2009.
B.M. Marlin, K. Swersky, B. Chen, and N. de Freitas. Inductive principles for restricted Boltzmann
machine learning. In Proceedings of the 13th International Conference on Artificial Intelligence and
Statistics, pages 509–516, 2009.
J. Pearl. Probabilistic reasoning in intelligent systems:
networks of plausible inference. Morgan Kaufmann,
1988.
N.A. Smith and J. Eisner. Contrastive estimation:
Training log-linear models on unlabeled data. In
Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 354–362,
2005.
P. Smolensky. Information processing in dynamical
systems: Foundations of harmony theory. Parallel distributed processing: explorations in the microstructure of cognition, Vol. 1, pages 194–281,
1986.
A.W. van der Vaart. Asymptotic statistics. Cambridge
University Press, 2000.
D. Vickrey, C. Lin, and D. Koller. Non-Local Contrastive Objectives. In Proceedings of the 27th International Conference on Machine Learning, 2010.
M. Wainwright, T. Jaakkola, and A. Willsky.
Tree-reweighted belief propagation algorithms and
approximate ML estimation via pseudo-moment
matching. In Workshop on Artificial Intelligence
and Statistics 9, 2003.
M. Welling, M. Rosen-Zvi, and G. Hinton. Exponential
family harmoniums with an application to information retrieval. In Advances in Neural Information
Processing Systems 17, pages 1481–1488, 2005.
L. Younes. Parametric inference for imperfectly observed Gibbsian fields. Probability Theory and Related Fields, 82(4):625–645, 1989.

