to make any decision based on such hypotheses.

This pa-

In real-world problems, it is observed that usually only a

per studies the theoretical properties of MRE

few target variables are most relevant in explaining any

and develops an algorithm for nding multiple

given evidence. For example, there are many possible dis-

top MRE solutions. Our study shows that MRE

eases in a medical domain, but a patient can have at most

relies on an implicit soft relevance measure in

a few diseases at one time, as long as he or she does not

automatically identifying the most relevant tar-

delay treatments for too long. It is desirable to nd diag-

get variables and pruning less relevant variables

nostic hypotheses containing only those relevant diseases.

from an explanation. The soft measure also en-

Other diseases should be excluded from further tests or

ables MRE to capture the intuitive phenomenon

treatments. In a recent work, Yuan and Lu [12] propose

of explaining away encoded in Bayesian net-

an approach called Most Relevant Explanation (MRE) to

works.

Furthermore, our study shows that the

generate explanations containing only the most relevant tar-

solution space of MRE has a special lattice struc-

get variables for given evidence in Bayesian networks. Its

ture which yields interesting dominance relations

main idea is to traverse a trans-dimensional space contain-

among the solutions. A K-MRE algorithm based

ing all the partial instantiations of the target variables and

on these dominance relations is developed for

nd one instantiation that maximizes a relevance measure

generating a set of top solutions that are more

called generalized Bayes factor [3].

representative. Our empirical results show that

shown in [12] to be able to nd precise and concise ex-

MRE methods are promising approaches for ex-

planations. This paper provides a study of the theoretical

planation in Bayesian networks.

properties of MRE and offers further evidence for its valid-

The approach was

ity. The study shows that MRE relies on an implicit soft
relevance measure that enables the automatic identication

1

Introduction

of the most relevant target variables and pruning of less relevant variables from an explanation. Furthermore, the solu-

Bayesian networks offer compact and intuitive graphical

tion space of MRE has a special lattice structure that allows

representations of uncertain relations among the random

two interesting dominance relations among the solutions to

variables of a domain and provide a foundation for many

be dened. These dominance relations are used to design

diagnostic expert systems.

and develop a K-MRE algorithm for nding a set of top

However, these systems typi-

cally focus on disambiguating single-fault diagnostic hy-

explanations that are more representative.

potheses because it is hard to generate just right multiple-

results show that MRE methods are promising approaches

fault hypotheses that contain only the most relevant faults.

for explanation in Bayesian networks.

Maximum a Posteriori (MAP) assignment and Most Probable Explanation (MPE) are two explanation methods for
Bayesian networks that nd a complete assignment to a
set of target variables as the best explanation for given evidence and can be applied to generate multiple-fault hypotheses. A priori, the set of target variables is often large
and can be in tens or even hundreds for a real-world diag-

Our empirical

The remainder of the paper is structured as follows. We
rst review methods for explanation in Bayesian networks,
including Most Relevant Explanation. Then we introduce
several theoretical properties of Most Relevant Explanation. We also develop a K-MRE algorithm for generating
multiple top explanations and evaluate it empirically.

632

YUAN ET AL.
3

A (0.016)
Input

Output

C (0.15)

networks. However, they often fail to nd just-right ex-

D (0.1)

planations containing the most relevant target variables.

(a)

Many existing methods make simplifying assumptions and

Input
current100%
noCurr 0%

def 10%
ok 90%

Related Work

Many methods exist for explaining evidence in Bayesian

B (0.1)

B

UAI 2009

focus on singleton explanations [5, 7]. However, singleton

A
def 2%
ok 98%

explanations may be underspecied and are unable to fully
explain given evidence. For the running example, the pos-

D

C

output of B

def 10%
ok 90%

noCurr 90%

output of D

A, B, C, and D failing independently
0.391, 0.649, 0.446, and 0.301 respectively. Therefore,
(¬B) is the best singleton explanation1 . However, B alone
does not fully explain the evidence. C or D has to be interior probabilities of

output of A

def 15%
ok 85%

current 10%

current 2%

are

noCurr 98%

output of C

current 1%

current 1%

noCurr 99%

noCurr 99%

volved. Actually, if we are not focusing on faulty states,

(D)
Total Output

(0.699) is the best singleton explanation. It is clearly

not an adequate explanation for the evidence.

current 4%
noCurr 96%

For a domain in which target variables are interdependent,

(b)

multivariate explanations are often more natural for exFigure 1: (a) A probabilistic digital circuit and (b) a corresponding diagnostic Bayesian network

plaining given evidence.

However, existing methods of-

ten produce hypotheses that are overspecied. MAP nds
a conguration of a set of target variables that maximize
the joint posterior probability given partial evidence on
the other variables.

2

For the running example, if we set

A, B, C and D as the target variables, MAP will nd
(A∧¬B∧¬C ∧D) as the best explanation. However, given
that B and C are faulty, A and D are somewhat redundant

A Running Example

for explaining the evidence. MPE nds an explanation with
Let us rst introduce a running example used throughout
this paper.

Consider the circuit in Figure 1(a) adapted

from [9, 12]. Gates

A, B, C

and

D are defective if they are

closed. The prior probabilities that the gates close independently are

0.016, 0.1, 0.15

and

0.1

respectively. Connec-

tions between the gates may not work properly with certain
small probabilities. The circuit can be modeled with a diagnostic Bayesian network as shown in Figure 1(b). Nodes

A, B, C

and

D

correspond to the gates in the circuit and

each has two states: defective and ok. Others are input

even more variables. Several other approaches use the dependence relations encoded in Bayesian networks to prune
independent variables [10, 11].

They will nd the same

explanation as MAP because all of the target variables are
dependent on the evidence. Yet several other methods measure the quality of an explanation using the likelihood of the
evidence [1]. Unfortunately they will overt and choose

(¬A ∧ ¬B ∧ ¬C ∧ ¬D)

as the explanation, because the

likelihood of the evidence given that all the target variables
fail is almost

1.0.

or output nodes and have two states: current or noCurr.

There have been efforts trying to generate more appropri-

Uncertainty is introduced to the model such that an output

ate explanations. Henrion and Druzdzel [6] assume that a

node is in state current with a certain probability less than

system has a set of pre-dened scenarios as potential ex-

1.0

if its parent gate, when exists, is defective and any

planations and nd the scenario with the highest posterior

of its other parents is in state current. Otherwise, it is

probability. Flores et al. [4] propose to grow an explanation

in noCurr state with probability

1.0.

For example, node

output of B takes state current with probability 0.99 if
parent gate B is in state defective and parent Input is in
state current.

Input

and

T otal Output

in the

Bayesian network are both in the state current. The task
is to diagnose the system and nd the best fault hypotheses.
Based on our knowledge of the domain, we know there are
three basic scenarios that most likely lead to the observa-

A is defective; (2) B
B and D are defective.
tion: (1)

able at each step while maintaining the probability of each
explanation above certain threshold. Nielsen et al. [8] use
a different measure called causal information ow to grow

Suppose we observe that current ows through the circuit,
which means that nodes

tree incrementally by branching the most informative vari-

and

C

are defective; and (3)

the explanation trees. Because the explanations in the trees
have to branch on the same variable(s), they may still contain redundant variables. Finding more concise hypotheses
also have been studied in model-based diagnosis [2]. The
approach focus on truth-based systems and cannot be easily
generalized to deal with Bayesian networks.
1

We use a variable and its negation to stand for its ok and

defective states respectively

UAI 2009
4

YUAN ET AL.

Most Relevant Explanation

633
5

There are two most essential properties for a good expla-

A Theoretical Study

5.1

Theoretical properties of MRE

nation. First, the explanation should be precise, meaning
it should explain the presence of the evidence well. Sec-

We now discuss several theoretical properties of MRE.

ond, the explanation should be concise and only contain the

Since MRE relies heavily on the

most relevant variables. The above discussions show that

ating its explanations, it is not surprising that these proper-

existing approaches for explaining evidence in Bayesian

ties are mostly originated from

networks often generate explanations that are either under-

properties can be found in the appendix.

specied (imprecise) or overspecied (inconcise).

GBF

GBF .

measure in gener-

The proofs of these

First, we note that GBF can be expressed in a different way

To address the limitations, Yuan and Lu [12] propose a

using the belief update ratio.

method called Most Relevant Explanation (MRE) to au-

Denition 3. The belief update ratio of

tomatically identify the most relevant target variables for

r(x1:k1 ; e), is dened as

x1:k1

given

e,

given evidence in Bayesian networks. First, explanation in
Bayesian networks is formally dened as follows.
Denition 1.

Given a set of target variables

Bayesian network and evidence

e

X

in a

x1:k

of

X, i.e., X1:k ⊆ X and X1:k 6= ∅.

(4)

GBF can then be expressed as the ratio between the belief
update ratios of
given

MRE is then dened as follows [12].
Denition 2. Let

P (x1:k |e)
.
P (x1:k )

on the remaining vari-

ables, an explanation for the evidence is a partial instantiation

r(x1:k ; e) ≡

X be a set of target variables,

and

x1:k1

and alternative explanations

x1:k1

e, i.e.,

e be

GBF (x1:k1 ; e) =

the evidence on the remaining variables in a Bayesian net-

r(x1:k1 ; e)
.
r(x1:k1 ; e)

(5)

work. Most Relevant Explanation is the problem of nding an explanation
Bayes Factor score

x1:k that has the maximum Generalized
GBF (x1:k ; e), i.e.,

M RE(X, e) ≡ arg maxx1:k ,X1:k ⊆X,X1:k 6=∅ GBF (x1:k ; e) ,
(1)
where

GBF

The most important property of MRE is that it is able to
weigh the relative importance of multiple variables and
only include the most relevant variables in explaining the
given evidence. The degree of relevance is evaluated using
a measure called conditional Bayes factor (CBF) implicitly

is dened as

GBF (x1:k1 ; e) ≡

encoded in the GBF measure and dened as follows.

P (e|x1:k1 )
.
P (e|x1:k1 )

(2)

Denition 4. The conditional Bayes factor of hypothesis

y1:m for given evidence e conditional on x1:k is dened as

Therefore, MRE traverses the trans-dimensional space containing all the partial assignments of
ment that maximizes the

GBF

score.

Potentially, MRE

can use any measure that provides a common ground for
comparing the partial instantiations of the target variables.

GBF

CBF (y1:m ; e|x1:k ) ≡

X and nds an assign-

is chosen because it is shown to provide a plausible

P (e|y1:m , x1:k )
.
P (e|y1:m , x1:k )

Then, we have the following theorem.
Theorem 1. Let the conditional Bayes factor of y1:m given

measure for representing the degree of evidential support

x1:k

in recent studies in Bayesian conrmation theory [3].

ratio of the alternative explanations

be less than or equal to inverse of the belief update

MRE was shown to be able to generate precise and con-

CBF (y1:m ; e|x1:k ) ≤

cise explanations for the running example [12]. The best
explanation according to MRE is:

GBF (¬B, ¬C; e) = 42.62 .

(3)

e and write GBF (¬B, ¬C).
explanation than both (¬A) (39.44)

For simplicity we often omit
(¬B, ¬C ) is a better

(6)

x1:k , i.e.,
1
,
r(x1:k ; e)

(7)

the following holds

GBF (x1:k ∪ y1:m ; e) ≤ GBF (x1:k ; e).

(8)

and (¬B, ¬D ) (35.88), because its prior and posterior probabilities are both relatively high; The posterior probabilspectively.

0.394, 0.391,

0.266

Therefore,

CBF (y1:m , e|x1:k ) provides a soft measure on

re-

the relevance of a new set of variable states with regard to

Therefore, MRE seems able to automatically

an existing explanation and can be used to decide whether

ities of the explanations are

and

GBF

identify the most relevant target variables and states as the

or not to include them in an existing explanation.

explanations for given evidence.

also encodes a decision boundary, the inverse belief update

634

YUAN ET AL.

ratio of alternative explanations

x1:k

given

e,

UAI 2009

which pro-

a

A

b

B

c

C

vides a threshold on how important the remaining variables
ab

should be in order to be included in the current explanation.

aB

ac

aC

Ab

AB

Ac

AC

bc

bC

Bc

BC

1

CBF (y1:m ; e|x1:k ) is greater than or equal to r(x1:k ;e) ,
y1:m is regarded as relevant and will be included. Otherwise, y1:m will be excluded from the explanation.

If

abc

abC

aBc

aBC

Abc

AbC

ABc

ABC

Figure 2: Solution space of Most Relevant Explanation

Theorem 1 has several intuitive and desirable corollaries.
First, the following corollary shows that, for any explanation

x1:k

with belief update ratio greater than or equal to

1.0, adding any independent variable to the explanation will
decrease its GBF score [12].

x1:k be an explanation with r(x1:k ; e) ≥
1.0, and y be a state of a variable Y such that
P (y|x1:k , e) ≤ P (y|x1:k ). Then

Corollary 3. Let

x1:k be an explanation with r(x1:k ; e) ≥
1.0, and y be a state of variable Y independent from variables in x1:k and e. Then

Corollary 1. Let

GBF (x1:k ∪ {y}; e) ≤ GBF (x1:k ; e).

GBF (x1:k ∪ {y}; e) ≤ GBF (x1:k ; e).

(11)

(9)
This is again an intuitive result; a variable state whose posterior probability decreases for given evidence should not
be part of an explanation for the evidence.

Therefore, adding an irrelevant variable dilutes the explanative power of an existing explanation. MRE is able to
automatically prune such variables. This is clearly a desir-

The above theoretical results can be veried using the running example. For example,

able property.
Note that we focus on the explanations with belief update
ratio greater than or equal to 1.0. We believe that an explanation whose probability decreases given the evidence

>

GBF (¬B, ¬C)
GBF (¬B, ¬C, A) & GBF (¬B, ¬C, D)

>

GBF (¬B, ¬C, A, D) .

is unlikely to be a good explanation for the evidence.
Corollary 1 requires the additional variable
pendent from both

X1:k

and

E.

Y

to be inde-

The assumption is rather

strong. The following corollary relaxes it to be that
conditionally independent from

E

given

X1:k

Y

is

The results suggest that GBF has the intrinsic capability
to penalize higher-dimensional explanations and prune less
relevant variables.

and shows

the same result still holds.

5.2

x1:k be an explanation with r(x1:k ; e) ≥
1.0, and y be a state of a variable Y conditionally independent from variables in e given x1:k . Then

Corollary 2. Let

GBF (x1:k ∪ {y}; e) ≤ GBF (x1:k ; e).

(10)

Explaining away

One unique property of Bayesian networks is that they can
model the so called explaining away phenomenon using the

V

structure, i.e., a single variable with two or more parents.

This structure intuitively captures the situation where an
effect has multiple causes. Observing the presence of the
effect and one of the causes reduces the likelihood of the
presence of the other causes. It is desirable to capture this

Corollary 2 is a more general result than corollary 1 and
captures the intuition that conditionally independent variables add no additional information to an explanation in
explaining given evidence, even though the variable may
be marginally dependent on the evidence. Also note that
these properties are all relative to an existing explanation.
It is possible that a variable is independent from the evidence given one explanation, but becomes dependent on
the evidence given another explanation.

In other words,

GBF score is not monotonic. Looking at variables one by
one does not guarantee to nd the optimal solution.
The above results can be further relaxed to accommodate
cases where the posterior probability of
than its prior, i.e.,

y given e is smaller

phenomenon when generating explanations.
MRE seems able to capture the explaining away effect using CBF. CBF provides a measure on how relevant a new
variable is to an existing explanation.

In an explaining-

away situation, if one of the causes is already present in
the current explanation, other causes typically do not receive high CBF scores.

Again for the running example,

(¬B, ¬C) and (¬A) are both good explanations for the evidence by themselves. The CBF of ¬A given only e (the
effect) is equal to its GBF (39.44), which is rather high.

(¬B, ¬C) (one of the causes) is also obCBF (¬A; e|¬B, ¬C) becomes rather low and is
only equal to 1.03. Clearly, CBF is able to capture the exHowever, when
served,

plaining away phenomenon in this example.

UAI 2009
5.3

YUAN ET AL.

635
GBF(¬ B,

¬ C) = 42.62
¬ B, ¬ C) = 42.15
GBF(¬ B, ¬ C, D) = 39.93
GBF(A, ¬ B, ¬ C, D) = 39.56

Dominance relations

GBF(A,

MRE has a solution space with an interesting lattice structure similar to the graph in Figure 2 for three binary target

GBF(¬ A) = 39.44
GBF(¬ A, B) = 36.98
GBF(¬ A, C) = 35.99
GBF(¬ B,

¬ D) = 35.88

variables. The graph contains all the partial assignments of
the target variables. Two explanations are linked together

Table 1: The top solutions ranked by GBF. The solutions in

if they only have a local difference, meaning they either

boldface are the top minimal solutions.

have the same set of variables with one variable in different
states, or one explanation has one fewer variable than the
other explanation with all the other variables being in the
same states.
There are two dominance relations among these potential
solutions that are implied by Figure 2. The rst concept is
strong dominance.
Denition 5. An explanation

x1:k

other explanation y1:m if and
GBF (x1:k ) ≥ GBF (y1:m ).
If

x1:k

strongly dominates an-

only if

x1:k ⊂ y1:m

and

y1:m , x1:k is clearly a better
y1:m , because it not only has a no-worse

strongly dominates

explanation than

explanative score but also is more concise. We only need
to consider

x1:k

when nding multiple top MRE explana-

tions. The second concept is weak dominance.
Denition 6. An explanation

x1:k

y1:m if and
GBF (x1:k ) > GBF (y1:m ).
other explanation

In this case,

x1:k

the solutions.
The dominance relations dened in the last section allow
us to develop a K-MRE algorithm to nd a set of top solutions that are more representative.

Let us look at the

running example again to illustrate the idea.

The expla-

nations in Table 1 have the highest GBF scores. If we simply select top three explanations solely based on GBF, we
will obtain these rather similar explanations: (¬B, ¬C),
(A, ¬B, ¬C), and (¬B, ¬C, D), which are rather similar.
Since (A, ¬B, ¬C), (¬B, ¬C, D), and (A, ¬B, ¬C, D)
are strongly dominated by (¬B, ¬C), we should only consider (¬B, ¬C) out of those four explanations. Similarly,
(¬A, B) and (¬A, C) are strongly dominated by (¬A).
These dominated explanations should be excluded from the

weakly dominates an-

only if

output all the top solutions rather than selecting any one of

x1:k ⊃ y1:m

and

top solution set. In the end, we get the set of top explanations shown in boldface in Table 1, which is clearly more
diverse and representative than the original set. MAP and

has a strictly larger

GBF

score than

MPE clearly do not have this nice property.

but the latter is more concise. It is possible that we

Therefore, our proposed K-MRE algorithm works as fol-

can include them both and let the decision makers to decide

lows. Whenever we generate a new explanation, we check

whether they prefer higher score or conciseness. However,

its score against the best solution pool. If it is lower than

y1:m ,

we believe that we only need to include

x1:k ,

because its

the worst score in the pool, reject the new explanation. If

K

higher GBF score indicates that the extra variable states are

there are fewer than

relevant to explain given evidence and should be included

new explanation is higher than the worst score in the pool,

in the explanation.

we consider adding the new explanation to the top pool. We

Based on the two kinds of dominance relations, we dene
the concept minimal.

best solutions or if the score of the

rst check whether the new solution is strongly or weakly
dominated by any of the top explanations. If so, reject the
new explanation. Otherwise, we add the new explanation

Denition 7. An explanation is minimal if it is neither

to the top pool. However, we then need to check whether

strongly nor weakly dominated by any other explanation.

there are existing top explanations that are dominated by
the newly added explanation. If yes, these existing expla-

In case we want to nd multiple top explanations, we only

nations should be excluded. Otherwise we delete the top

need to consider the minimal explanations, because they

explanation with the least score.

are the most representative ones.

6

7

K-MRE Algorithm

7.1

Empirical Results
Experimental design

In many decision problems, outputting the single top solution may not be the best practice.

Decision makers typ-

We tested the K-MRE algorithm on a set of benchmark

ically would like multiple competing options to choose

models, including Alarm, Circuit, Hepar, Munin, and

from. This is especially important when there are multi-

SmallHepar. We chose these several models because we

ple solutions that are almost equally good. For the circuit

have the diagnostic versions of these networks, whose vari-

example, all three basic explanations will lead to the same

ables have been annotated into three categories: target, ob-

observation. However, we can only recover one explana-

servation, and auxiliary. For generating the test cases, we

tion if we are satised with one top solution. It is better to

used the networks as generative models and sampled with-

636

YUAN ET AL.

UAI 2009

Precision

1.0

K=1
F 1

0.8

1.0

1.0

1.0

1.0

0.6

0.8

0.8

0.8

0.8

0.4

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

1 0.0

0.0

0.0

Singleton
F-MAP

0.2

P-MAP
MRE

0.0
0

0.5

0

Recall

K=3
F=0
K=3
F 1

0.5

1

0

0.5

1

0.0
0

0.5

1

1.0

1.0

1.0

0.8

0.8

0.8

1.0

0.8

0.6

0.6

0.6

0.8

0.6

0.4

0.4

0.6

0.4

0.2

0.4

0.5

1

0.0
0

0.5

1

0.5

1

0

0.5

1

1.0

1.0

1.0

1.0

1.0

0.8

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.2

0.0

0.0

0.0
0

0.5

1

0

0.5

1

0

0.5

0.5

1

1.0

1.0

1.0

1.0

0.8

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.2

0.0

0.0

0.0

0.0

0.5

0

1

1

0

0.5

1

0

0.5

1

0.0

0

1

1.0

0

0.5

0.0

0.0

0



0.0

0

0.2


K=1
. 
F=2
)





K=3
. 
F=2
)




1

0.2

0.2

0.0
0

0.5

0.4

0.2

0.0

0

1.0

0.5

1

0

0.5

0.0
0

1

0.5

1

1.0

Singleton

P MAP

1.0

1.0

1.0

1.0

0.8

F MAP

MRE

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.6

I-rate 0.4
K1F1 0.2

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0

0.0

0.0

0.0

0.0

Alarm

Circuit

Hepar

Munin

SmallHepar

Figure 3: Precision vs recall plots of the results by four algorithms, Marginal, P-MAP, F-MAP, and MRE, on a set of
benchmark diagnostic Bayesian networks.K shows the number of top solutions generated. F shows the least number
of faulty target variables in test cases. F Score shows the F-Scores of the results of the algorithms. Marginal algorithm
did not appear in rows K3F1 and K3F2 because it has only one solution.

out replacement from their prior probability distributions.

centage of faulty states correctly identied among all faulty

We only kept those test cases with at least one abnormal ob-

explanation variables) and recall (the percentage of faulty

servation and used the abnormal observations as evidence.

states correctly identied among all faulty variables in test

Since Circuit and SmallHepar have 4 and 3 target variables

cases) of these algorithms in Figure 3.

respectively, we collected as many test cases as possible.

sample results on F-Score, which is dened as

Munin also has 4 target variables but each with many more
states. Hepar and Alarm have 9 and 12 target variables re-

F-Score

spectively. We collected 50 test cases for the last three net-

=

We also include

2 × (precision × recall)
.
(precision + recall)

(12)

works. We also extracted from them the test cases which
contain at least two faulty target variables for separate ex-

7.2

Results and analysis

periments on multiple-fault test cases.
Our experiments compared MRE with MAP given their
similarities. We tested two versions of the MAP algorithm,
one focusing on all the target variables (F-MAP) and the
other only on the target variables selected by MRE (PMAP). In addition, we compared with the Marginal algorithm, which neglects the interdependence among the target variables and uses the marginal posterior probabilities
to determine the most likely states of the target variables.
We plot the accuracy statistics, including precision (the per-

We make the following observations from these results.
First, MRE is able to achieve higher precision and/or recall
rates in identifying the faulty target variables than the other
algorithms on all the networks except Munin. An outstanding example is the SmallHepar network. Marginal, F-MAP
and P-MAP all failed badly on this model in identifying the
faulty variables, while MRE was able to achieve reasonable
performance. It is clearly desirable given that one major
goal of diagnosis or explanation is to identify problems,
e.g.

faulty states.

We investigated the results of Munin

UAI 2009

YUAN ET AL.

network further and found that all target variables of these

637
8

Concluding Remarks

test cases are in faulty states. Marginal and F-MAP have
exactly the same statistics, which suggests that the target

In this paper, we discuss several theoretical properties of

variables may have weak correlations with each other. This

Most Relevant Explanation (MRE) and develop an algo-

puts MRE in disadvantage because MRE takes into account

rithm for nding multiple top MRE solutions. Our study

such weak correlations and generate concise explanations

shows that MRE relies on an implicit soft relevance mea-

with fewer target variables. On average, the explanations of

sure in automatically identifying the most relevant target

4.3 variables out of 12 target variables for
Alarm, 1.7/4 for Circuit, 4/9 for Hepar, 2.5/4 for Munin,
and 2.3/3 for SmallHepar. For networks with strong correMRE identies

variables and pruning less relevant variables from an explanation.

The soft measure also enables MRE to cap-

ture the intuitive phenomenon of explaining away encoded

lations among the target variables, e.g. Circuit and Hepar,

in Bayesian networks. Furthermore, we dene two dom-

MRE has much higher precision/recall rates. The sample

inance relations among the explanations that are implied

F-score results in the case of K1F1 further conrmed the

by the structure of the solution space of MRE. These rela-

observation.

tions allow us to design and develop a K-MRE algorithm

Second, by comparing rows K1F1 vs.
K1F2 vs.

K3F1 and

K3F2, we found that using multiple top

for nding top MRE solutions that are much more representative.

solutions helps MRE signicantly in improving the preci-

Our empirical results agree quite well with the theoretical

sion/recall rates than the other algorithms. With multiple

understanding of MRE. The results show that MRE is ef-

solutions, we kept the results with the maximum precision

fective in identifying the most relevant target variables, es-

rates. The results seem to support our claim that K-MRE

pecially the true faulty target variables. Furthermore, K-

was able to generate solutions that are more representative.

MRE seems able to generate more representative top ex-

It is somewhat surprising that the precision/recall rates of

planations than K-MAP methods.

F-MAP were not improved at all on the networks, but those

is especially suitable for systems in which target variables

of P-MAP were improved. Our hypothesis is that, since the

are strong correlated with each other and can generate more

explanations by F-MAP are more grained because more

precise and concise explanations for these systems.

variables are involved, its top explanations tend to agree
with each other on the faulty variables and differ mostly
in the less important non-faulty variables. Generating multiple top solutions could not really help F-MAP much in
improving its accuracy statistics.

We believe that MRE

This research has many future works. It is desirable to understand the theoretical complexity of MRE. It has a solution space even larger than MAP and is believed to be at
least as hard. Currently we rely on an exhaustive search
algorithm for solving MRE and K-MRE. More efcient

Third, although P-MAP gets the target variables identied

methods for solving MRE need be developed to make it

by MRE as input, it still failed badly on the SmallHepar

applicable to large real-world problems.

network in identifying faulty states of the target variables.
It did not show any signicant advantage over F-MAP on

Acknowledgement

other networks either. The results suggest that relying on

National Science Foundation grant IIS-0842480.

posterior probabilities may not work well in certain diag-

experimental data have been obtained using SMILE, a

nostic systems.

Bayesian inference engine developed at the Decision Sys-

Fourth, although multiple-fault cases are believed to be

tems Laboratory at University of Pittsburgh and available

more difcult because of their low likelihood, the algo-

at

This research was supported by the
All

http://genie.sis.pitt.edu.

rithms in our experiments seem able to maintain the same
level of accuracy rates in face of multiple-fault test cases
(rows K1F2 and K3F2).

References

We hope to apply the pro-

posed methods to real-world systems and test cases to gain

[1] U. Chajewska and J. Y. Halpern.

Dening explanation in

more insights.

probabilistic systems. In Proceedings of the Thirteenth An-

Last but not least, the Marginal algorithm is efcient and

(UAI97), pages 6271, San Francisco, CA, 1997. Morgan

sometimes can achieve similar accuracy rates with other

Kaufmann Publishers.

methods.

However, since it does not take into account

the dependence among the target variables, its results can

nual Conference on Uncertainty in Articial Intelligence

[2] J. de Kleer and B. C. Willams. Diagnosis with behavioral
modes. In Proceedings of IJCAI-89, pages 104109, 1989.

be arbitrarily bad if the dependence are strong. It is evident on the Circuit network for which the accuracy rates
of Marginal algorithm are much lower than other methods.
The results suggest that we have to be cautious about the
use of the Marginal algorithm in certain systems.

[3] B. Fitelson.

Likelihoodism, Bayesianism, and relational

conrmation. Synthese, 156(3), June 2007.
[4] J. Flores, J. A. Gamez, and S. Moral. Abductive inference
in Bayesian networks: nding a partition of the explanation
space.

In Eighth European Conference on Symbolic and

638

YUAN ET AL.

UAI 2009

Quantitative Approaches to Reasoning with Uncertainty,

Proof of Corollary 1: The corollary follows immediately from

ECSQARU'05, pages 6375. Springer Verlag, 2005.

Theorem 1. We can also prove it in the following way.

[5] D. E. Heckerman, E. J. Horvitz, , and B. N. Nathwani. To-

GBF (x1:k ∪ {y}; e)
P (x1:k ∪ {y}|e)(1 − P (x1:k ∪ {y}))
=
P (x1:k ∪ {y})(1 − P (x1:k ∪ {y}|e))
P (x1:k |e)P (y)(1 − P (y)P (x1:k ))
=
P (x1:k )P (y)(1 − P (y)P (x1:k |e))
P (x1:k |e)(1 − P (y)P (x1:k ))
=
.
P (x1:k )(1 − P (y)P (x1:k |e))

ward normative expert systems: Part I the pathnder project.
Methods of Information in Medicine, 31:90105, 1992.
[6] M. Henrion and M. J. Druzdzel. Qualitative propagation and
scenario-based schemes for explaining probabilistic reasoning. In P. Bonissone, M. Henrion, L. Kanal, and J. Lemmer,
editors, Uncertainty in Articial Intelligence 6, pages 17
32. Elsevier Science Publishing Company, Inc., New York,
N. Y., 1991.
[7] J. Kalagnanam and M. Henrion. A comparison of decision

Because

analysis and expert rules for sequential diagnosis. In Pro-

P (x1:k |e) ≥ P (x1:k ), we have the following:
GBF (x1:k ∪ {y}; e)
P (x1:k |e)(1 − P (y)P (x1:k ))
=
P (x1:k )(1 − P (y)P (x1:k |e))
P (x1:k |e)(1 − P (x1:k ) + (1 − p(y))P (x1:k ))
=
P (x1:k )(1 − P (x1:k |e) + (1 − p(y))P (x1:k |e))
P (x1:k |e)(1 − P (x1:k ) + (1 − p(y))P (x1:k ))
≤
P (x1:k )(1 − P (x1:k |e) + (1 − p(y))P (x1:k ))
P (x1:k |e)(1 − P (x1:k ))
≤
P (x1:k )(1 − P (x1:k |e))
= GBF (x1:k ; e) .

ceedings of the 4th Annual Conference on Uncertainty in
Articial Intelligence (UAI-88), pages 253270, New York,
NY, 1988. Elsevier Science.
[8] U. Nielsen, J.-P. Pellet, and A. Elisseeff. Explanation trees
for causal Bayesian networks. In Proceedings of the 24th
Annual Conference on Uncertainty in Articial Intelligence
(UAI-08), 2008.
[9] D. Poole and G. M. Provan. What is the most likely diagnosis? In P. Bonissone, M. Henrion, L. Kanal, and J. Lemmer,
editors, Uncertainty in Articial Intelligence 6, pages 89
105. Elsevier Science Publishing Company, Inc., New York,
N. Y., 1991.

Proof of Corollary 2: This corollary can be proved in a similar
[10] S. E. Shimony. The role of relevance in explanation I: Irrel-

way as Corollary 1.

evance as statistical independence. International Journal of
Approximate Reasoning, 8(4):281324, June 1993.
[11] L. van der Gaag and M. Wessels. Efcient multiple-disorder
diagnosis by strategic focusing, pages 187204. UCL Press,
London, 1995.
[12] C. Yuan and T.-C. Lu. A general framework for generating
multivariate explanations in Bayesian networks. In Proceedings of the Twenty-Third National Conference on Articial
Intelligence (AAAI-08), 2008.

Appendix

Because

Proof of Theorem 1:

GBF (x1:k ∪ y1:m ; e)
P (x1:k ∪ y1:m |e)(1 − P (x1:k ∪ y1:m ))
=
P (x1:k ∪ y1:m )(1 − P (x1:k ∪ y1:m |e)
P (x1:k |e)P (y1:m |x1:k , e)(1 − P (y1:m |x1:k )P (x1:k ))
=
P (x1:k )P (y1:m |x1:k )(1 − P (y1:m |x1:k , e)P (x1:k |e))
=

1
P (x1:k |e) 1 − P (x1:k ) + P (y1:m |x1:k ) − 1
P (x1:k ) 1 − P (x1:k |e) + P (y 1|x ,e) − 1
1:m

The above equation is less than or equal to

⇔

GBF (x1:k ∪ {y}; e)
P (x1:k ∪ {y}|e)(1 − P (x1:k ∪ {y}))
=
P (x1:k ∪ {y})(1 − P (x1:k ∪ {y}|e)
P (x1:k |e)P (y|x1:k , e)(1 − P (y|x1:k , e)P (x1:k ))
=
P (x1:k )P (y|x1:k )(1 − P (y|x1:k )P (x1:k |e))
P (x1:k |e)P (y|x1:k )(1 − P (y|x1:k )P (x1:k ))
=
P (x1:k )P (y|x1:k )(1 − P (y|x1:k )P (x1:k |e))
P (x1:k |e)(1 − P (y|x1:k )P (x1:k ))
=
.
P (x1:k )(1 − P (y|x1:k )P (x1:k |e)

1:k

GBF (x1:k ; e) when

1
−1
P (y1:m |x1:k )
1
−1
P (y1:m |x1:k ,e)

≤

P (y1:m |x1:k , e)(1 − P (y1:m |x1:k ))
P (y1:m |x1:k )(1 − P (y1:m |x1:k , e))

≤

⇔ CBF (y1:m ; e|x1:k )

≤

1 − P (x1:k )
1 − P (x1:k |e)
P (x1:k )
P (x1:k |e)
1
.
r(x1:k ; e)

P (x1:k |e) ≥ P (x1:k ), we have

GBF (x1:k ∪ {y}; e)
P (x1:k |e)(1 − P (y|x1:k )P (x1:k ))
=
P (x1:k )(1 − P (y|x1:k )P (x1:k |e)
P (x1:k |e)(1 − P (x1:k ) + (1 − p(y|x1:k ))P (x1:k ))
=
P (x1:k )(1 − P (x1:k |e) + (1 − p(y|x1:k ))P (x1:k |e))
P (x1:k |e)(1 − P (x1:k ) + (1 − p(y|x1:k ))P (x1:k ))
≤
P (x1:k )(1 − P (x1:k |e) + (1 − p(y|x1:k ))P (x1:k ))
P (x1:k |e)(1 − P (x1:k ))
≤
P (x1:k )(1 − P (x1:k |e))
= GBF (x1:k ; e) .
Proof of Corollary 3:
This corollary follows immediately from Theorem 1.

