attempts to identify a network structure that best satisfies
these constraints (Spirtes et al., 2000). The difficulty with
this approach is that reliably identifying the conditional independence properties, and optimizing the network structure are both challenging problems (Margaritis, 2003). A
much more common approach is the score based method,
where one first posits a criterion by which a given Bayesian
network structure can be evaluated on a given data set. The
task is then to search for a Bayesian network structure that
optimizes the score. Since model selection is such a critical issue, score based approaches are typically based on
well established statistical principles such as minimum description length (MDL) (Rissanen, 1987; Lam & Bacchus,
1994; Friedman & Goldszmidt, 1998; Van Allen & Greiner,
2000) or Bayesian estimation. The use of Bayesian scoring
approaches was developed in (Cooper & Herskovits, 1992),
culminating in the BDe score of (Heckerman et al., 1995),
which is currently one of the best known standards. These
scores offer sound, well motivated model selection criteria
for Bayesian network structure. The main problem with
using these scores, however, is that they create intractable
optimization problems. That is, it is NP-hard to compute

the optimal network for the Bayesian scores (Chickering,
1996). In fact, recently it has been shown that optimizing
Bayesian network structure is NP-hard, in the large sample limit, for all consistent scoring criteria, including MDL,
BIC and the Bayesian scores (Chickering et al., 2003).
Due to the known intractability of structure optimization,
the literature on Bayesian network structure learning has
been dominated by heuristic algorithms for searching the
space of networks, including greedy local search, random re-starts, simulated annealing and genetic algorithms
(Goldenberg & Moore, 2004; Moore & Wong, 2003; Heckerman et al., 1995; Elidan et al., 2002; Larranaga et al.,
1996). However, recently it has been observed that searching the space of variable orderings can be more effective
than searching the space of network structures (Larranaga
et al., 1996; Teyssier & Koller, 2005), since the space of
orderings is much smaller. This approach exploits the fundamental insight of (Cooper & Herskovits, 1992; Buntine,
1991) that, for a fixed variable order, the optimal network
(of bounded in-degree) and parameters can be computed in
polynomial time (but exponential in the in-degree bound).
In this paper we offer an alternative approach to the problem of learning a Bayesian network model from data. Our
idea is to follow the strategy from combinatorial optimization, where, when faced with an intractable combinatorial
problem, one first formulates a convex relaxation that can
be solved efficiently, and then rounds the “soft” solution
to obtain an approximate “hard” solution to the original
problem. Here, we propose an efficient relaxation of the
Bayesian network structure learning problem—solving for
the structural features that determine the graph, the variable ordering that determines the edge orientation, and
the model parameters—in a single, compact optimization.
First, we show that, given a fixed variable order, the maximum likelihood structure and parameters can be found in
polynomial time and space using a sparse exponential family representation, without any restriction on the number
of parents for any variable. Second, given a fixed variable
order, we show how feature selection based on the minimum description length principle can be addressed simultaneously with parameter optimization. Finally, to optimize
the order, we introduce a compact matrix representation of
total orderings that allows a tight semidefinite relaxation.
We evaluate our overall technique on natural and synthetic
data sets, and find that convex relaxation is a very promising approach to this problem, even though the underlying
search problem is inherently discrete.

2

Bayesian network representations

A Bayesian network is normally defined by a directed
acyclic graph over variables X1 , ..., Xn , where the prob-

ability of a configuration x is given by
P (x)

=

n
Y

P (xj |xπ(j) )

j=1

=

exp

X
jab

1(xj =ab) ln θjab



(1)

Here θ denotes the parameters of the model; j ranges over
conditional probability tables (CPTs), one for each variable Xj ; 1(·) denotes the indicator function; xj denotes
the local subconfiguration of x on (xj , xπ(j) ); a denotes
the set of values for child variable xj ; and b denotes the
set of configurations for xj ’s parents xπ(j) . The form
(1) shows how Bayesian networksare a form of exponential model P (x) = exp w> φ(x) , using the substitution
wjab = ln θjab . Here φ(x) denotes the feature vector
(...1(xj =ab) ...)> over j, a, b. In fact, we will adopt a general exponential form representation for Bayesian networks
in this paper, since we will exploit many of the advantages
it offers over the traditional CPT based representation.
Rather than start with CPT entries θ, one can alternatively
represent a Bayesian network in a general exponential form


X

P (x) = exp 
wj> φj (xj , xπ(j) ) − A(wj , xπ(j) ) 
j

where

A(wj , xπ(j) )

=

log

X
a

exp wj> φj (a, xπ(j) )



!

Here A(wj , xπ(j) ) is the log normalization constant for the jth conditional probability distribution.
We use φj (xj , xπ(j) ) to denote the feature vector
(...1(xj =a,xπ(j) =b) ...)> over a, b, and use wj to denote
the local weight vector (...wjab ...)> over a, b. Thus,
together φj and wj specify the local conditional probability distribution P (xj |xπ(j) ) and allow the traditional
CPT parameter entries to be efficiently recovered by
θjab = exp(wj> φj (a, b) − A(wj , b)).
As we will see below, one key aspect of the exponential form is that it expresses P (x) as a convex function
of the parameters w, which will lead to convenient optimization problems. Another important advantage of the
exponential form, however, is that it allows a sparse representation of the conditional distributions. That is, we can
represent P (xj |xπ(j) ) given a subset of features from the
set of possibilities {1(xj =a,xπ(j) =b) : a ∈ Vals(xj ), b ∈
Vals(xπ(j) )}. In general, this allows one to represent
P (xj |xπ(j) ) compactly even if the number of parent variables is large. Such a sparse feature representation of a
CPT is similar to exploiting context specific independence
(Boutilier et al., 1996) or local structure (Friedman & Goldszmidt, 1998). In fact, these compact representations can

be recovered as a special case. The size of a feature based
representation for a CPT is never larger than the traditional
table based representation, and can be arbitrarily smaller.
Below we find that the feature based representation is particularly advantageous from the perspective of learning a
Bayesian network from data, since it nicely reduces the
problem of structure learning, largely, to identifying the
features used to define the model. The only other issue
is to acquire an effective ordering for the variables in the
model. Overall, the problem of learning a Bayesian network from data can be decomposed into the three problems
of: learning a set of features, learning a variable ordering,
and learning a corresponding set of parameters. Below we
propose an approach that tackles all three subproblems simultaneously.

3

Parameter estimation

Before tackling each subproblem in turn, we first establish
some preliminary results that will be needed later. The first
and simplest subproblem is estimating the parameters w
given a fixed ordering π and feature set φ.
Given complete training data D = [x1 ; ...; xT ], the negative loglikelihood loss can be expressed
i
Xh
L(w) =
A(wj , xiπ(j) ) − wj> φj (xij , xiπ(j) )
i,j

=

X
j,bj

with the dual. The dual is derived by formulating a tight
concave lower bound on L̃(u), which can then be maximized to recover an equivalent result to L̃(u). A tight lower
bound can easily be derived in this case using the convex
conjugate function for A(u, b), given by
X
A∗ (µ, b) = max µ>
µab log µab
b u − A(u, b) =
u

a

=

Here we use e to denote the vector of all 1s. This function is also convex (Boyd & Vandenberghe, 2004), and the
dual variables µ satisfy the relation µab = E[φab (x)|b]
(Wainwright & Jordan, 2003).
Since our Bayesian network representation is based on using indicator features, φab (x, xπ ) = 1(x=a,xπ =b) , we
can furthermore obtain a convenient one-to-one relationship between the dual variables µab and the CPT parameter entries θab by noting that µab = E[φab (x)|xπ = b] =
P (x = a|xπ = b) = θab . Therefore, we can think of the
convex conjugate function for A(u, b) as being expressed
in the CPT parameters directly:
A∗ (θ, b)

β
L̃(w) = kwk2 + L(w) =
(2)
2


h
i
X β
X
>
 kwj k2 +
#bj A(wj , bj ) − φ̄bj wj 
2
j
bj

Here β is a regularization parameter. Note that the weights
that minimize L̃(w) correspond to a MAP estimate of w,
with prior w ∼ N (0, βI).

The objective (2) decomposes as an independent sum over
j, so we can consider the minimization of each individual
objective separately. To reduce the notational burden, denote the jth component of L̃(w) by
h
i
X
β
>
#b A(u, b) − φ̄b u (3)
kuk2 +
L̃(u) =
2
b

Although L̃(u) is a convex minimization objective, it turns
out that to derive our results below we will need to work

=

where θ b ≥ 0 and e> θ b = 1

−H(θ b )

The key property that the conjugate function provides is
that it establishes a concave lower bound. In fact, since
A(u, b) is convex in u it can be shown that

i
h
#bj A(wj , bj ) − wj> φ̄bj

P #(a b )
where φ̄bj = aj #bj j j φj (aj , bj ). Since A(wj , b) is
a convex function of wj , this leads to a convex minimization problem for wj . However, since overfitting is always a
concern, we will find it advantageous to minimize the regularized loglikelihood loss

where µb ≥ 0 and e> µb = 1

−H(µb )

A(u, b)

=

max

θ b ≥0, e> θ b =1

θ>
b u + H(θ b )

(Wainwright & Jordan, 2003). Using this fact we obtain
min L̃(u)

(4)


u



X
β
>
#b max θ >
kuk2 +
b u + H(θ b ) − u φ̄b
u 2
θb
b
X


β
2
= max min kuk +
#b H(θ b ) − u> (φ̄b − θ b )
u 2
θ

= min

b

Note that the minimum and the maximum can be interchanged here because the problem is convex and there is
no duality gap (Boyd & Vandenberghe, 2004). Taking the
derivative of the inner objective with respect to u yields
X

#b φ̄b − θ b = 0, so that
∇u = βu −
b

∗

u (θ)


1X
#b φ̄b − θ b , and therefore
β

=

(5)

b

L̃(θ)

=

max
θ

"

X
b

#

#bH(θ b ) −

(6)


1 X
#b φ̄b − θ b
2β

2

b

where θ ≥ 0, e> θ b = 1 for all b. Thus the dual to the
minimum regularized loglikelihood loss problem is a regu-

larized concave maximum entropy problem. Given a solution to the dual problem θ ∗ a corresponding primal solution
can be easily recovered using (5).

relaxed form of the entire Bayesian network learning problem within a polynomial convex optimization framework.

For implementation, the primal problem is more convenient
than the dual because it is unconstrained. In our implementation below we used a Newton method to efficiently solve
(3). The dual formulation is required to establish our theoretical results below, however.1

5

4

Strategy

Of course, the main goal in this paper is not to perform parameter estimation, but to learn the structure of a
Bayesian network model from data. The exponential family representation and maximum entropy frameworks offer a new perspective on this problem. Rather than scoring a Bayesian network and performing a discrete search
in structure space, our goal will be to formulate a polynomial time approach that addresses each of the three
subproblems—feature generation (and selection), parameter estimation, and variable ordering—in a joint convex
optimization that relies on reasonable convex relaxations
of the discrete subproblems when necessary.
We pursue the following strategy. First, we generate a
“universal” set of features that allows us to express any
maximum likelihood solution exactly. Our first result below shows that in fact this can be achieved in polynomial
time and space given a fixed variable ordering. Second,
we select a subset of the generated features using the minimum description length principle (Rissanen, 1987; Lam
& Bacchus, 1994). Our main result here is that, using the
maximum entropy estimation framework developed above,
MDL feature selection and parameter optimization can be
performed simultaneously in a novel convex relaxation. Finally, we include variable ordering in the framework by
extending the previous optimization formulation to also
search over variable orders. Our third main result is that
a search over variable orders can be efficiently encoded by
a compact set of semidefinite constraints on a matrix representation of the ordering. Overall, we are able to solve a
1
We note that in the Bayesian network learning literature it
is common to adopt a Bayesian perspective on parameter estimation as well as structure learning (Heckerman et al., 1995;
Cooper & Herskovits, 1992). For parameter estimation, the standard approach is to use a Dirichlet prior over each of the conditional probability vectors θ jb , with corresponding prior parameters αjb . Here the posterior mean estimate is given by
P
θ̂jab = (#(jab) + αjab )/(#(jb) + a αjab ). Although the
Bayesian posterior estimate appears to be quite different from the
solution to (6), the two estimates in fact behave similarly. For
example, in the large sample limit both estimates θ̂ jb converge
to E[φj |b]. With no data and uniform α, both approaches produce the same maximum entropy estimates. The advantage of the
quadratic regularizer in (6) is that it allows us to express a convex formulation of the minimum description length principle for
structure learning, as we will see below.

Feature generation

Our first result is that, given a fixed variable order, a set
of features sufficient to represent any maximum likelihood
Bayesian network can be found in polynomial time and is
polynomially large. This result holds without restriction on
the number of parents of any variable. In fact, the result
is straightforward, but relies heavily on the sparse feature
representation. They key idea is that one can use linear dependence of feature responses on augmented training data
to identify key features and eliminate other features from
consideration.
First, note that since the conditional probabilities are locally defined and the variable ordering is known, we can
solve the feature generation problem for each variable xj
independently. Next, assume that the variable indices are
sorted according to the ordering so that the set of possible
parents of xj is {x1 , ..., xj−1 }. Let σ(j) = {1, ..., j − 1}
denote the set of ancestors of j under the ordering. Then
given a set of complete training data (row vectors) represented in a T × n data matrix, D = [x1 ; ...; xT ], only the
first j columns of D are relevant for xj .
To identify a universal set of features, it suffices to consider a locally augmented data matrix where we copy each
ancestor configuration, xiσ(j) , Vj times and replace xij with
each of its possible values. Here Vj = |Vals(xj )|. Call the
resulting matrix D̃j ; so if Dj is T × j then D̃j is (T Vj ) × j.
Proposition 1 For any two exponential form representations φ1 , w1 and φ2 , w2 : if w1> φ1 (a, xiσ(j) ) =
w2> φ2 (a, xiσ(j) ) for all i = 1, ..., T and all a ∈ Vals(xj ),
then P1 (xij |xiσ(j) ) = P2 (xij |xiσ(j) ) for all i = 1, ..., T .
Proof First note that the assumption
implies that

P
=
A1 (w1 , xiσ(j) ) = log a exp w1> φ1 (a, xiσ(j) )


P
log a exp w2> φ2 (a, xiσ(j) ) = A2 (w2 , xiσ(j) ) for all

i. Therefore we must also have − log P1 (xij |xiσ(j) ) =
A1 (w1 , xiσ(j) ) − w1> φ1 (a, xiσ(j) ) = A2 (w2 , xiσ(j) ) −
w2> φ2 (a, xiσ(j) ) = − log P2 (xij |xiσ(j) ).
Thus if one set of features φ1 spans another set φ2 on the
augmented data matrix D̃j for each variable xj , then the
optimal parameter estimate for φ1 (either maximum likelihood or regularized likelihood) on D has to be at least
as good as the best parameter estimate for φ2 ; that is,
L̃(w1∗ , φ1 , D) ≤ L̃(w2∗ , φ2 , D).
Of course, there are many possible features to consider.
There is a unique feature φjab corresponding to an indicator function φjab (xj , xρ(j) ) = 1(xj =a,xρ(j) =b) for each

particular subset of ancestor variables, ρ(j) ⊂ σ(j), and
each particular value a for xj and value b for xρ(j) . Nevertheless, it is a trivial observation that the maximum rank
of any possible span is bounded by T Vj , since this is the
length of each feature response vector on the augmented
training set D̃j . Therefore, there must exist a set of no
more than T Vj features that allows the exponential form
representation achieve the maximum likelihood score (or
regularized likelihood score) of any Bayesian network on
the training data Dj .
To find this set of features in polynomial time we exploit
the fact that every compound feature φjab can be decomposed as a product for features defined on shorter patterns
φjab (xj , xρ(j) )
= 1(xj =a,xρ(j) =b)
=

1(xj =a) 1(xρ1 (j) =b1 ) ...1(xρk (j) =bk )

=

φja (xj )φρ1 (j)b1 (xρ1 (j) )...φρk (j)bk (xρk (j) )

(7)

Naturally we would like to build a span consisting of the
shortest possible feature patterns, since this would result
in a simpler Bayesian network representation. Define the
length of φjab to be the number of variables in its definition
(7). Then we have the following proposition.
Proposition 2 If a compound feature φjab is spanned by a
set of shorter features, then φjab is unnecessary.
P
Proof Assume φjab =
f wf φf on D̃j for some set of
shorter feature patterns f ∈ F . Then any extended feature that uses φjab can be spanned by features based on
shorter patterns. In particular, if φjcd = φjab φg1 ...φgk on
P
D̃j , then we must also have φjcd = f wf φf φg1 ...φgk on
D̃j , where the feature patterns in the second expansion are
strictly shorter than the first.
This leads to a polynomial time algorithm for generating a set of shortest features with maximum span on D̃j ;
see Figure 1. To establish that this procedure does indeed run in polynomial time, consider the lattice of feature patterns. The lattice is searched from shortest patterns
to longest. Once a pattern is pruned, no extension of it
will ever be considered (and correctness will be preserved
by Proposition
2). However, for each increase in rank, at
Pj
most `=1 Vals(x` ) features are added, while the maximum rank is T Vj .
One drawback of this procedure is that it can generate a
large number of parents for xj , even though the representation remains polynomially large. In fact, this feature generation process is guaranteed to overfit the data, in the sense
that it yields a representation that can achieve the maximum
likelihood of any Bayesian network. Clearly, some sort of
feature selection process is required to yield a reasonable
model, which we now consider.

Feature generation procedure for xj on augmented D̃j :
Φ(0) = {φ∅ } (the constant 1 feature)
for k = 1, 2, ... (while rank increased)
Φ(k) ← ∅
for each φf ∈ Φ(k−1)
Ψ ← {φ
b` φf : ` 6∈ f, b`∈ Vals(x
` )}

S
S
(`)
(`)
Φ
Φ
∪
Ψ
>
rank
if rank
`≤k
`≤k
Φ(k) ← Φ(k) ∪ Ψ

Figure 1: Feature generation procedure

6

Feature selection

We base our feature selection strategy on the minimum description length principle (Rissanen, 1987; Lam & Bacchus, 1994; Friedman & Goldszmidt, 1998). Here we continue to assume a fixed variable ordering is given. The
idea is to start with a large set of “universal” features
φ = (...φjab ...)> generated by the procedure outlined previously. To perform feature selection in this large set, we
augment the exponential representation with feature selection variables η. That is, for each feature φjab we establish
a corresponding selector variable ηjab ∈ {0, 1}, in addition to the corresponding weight wjab . Let Nj = diag(η j )
be the diagonal matrix of selector values corresponding to
variable xj . We can then write


X

wj> Nj φj (xj , xπ(j) )−A(wj , xπ(j) ) 
P (x) = exp
j

A(wj , xπ(j) )

=

log

X

exp wj> Nj φj (a, xπ(j) )

a



If ηjab = 1 then the feature φjab is selected, otherwise it
is dropped.
We would like to solve for the set of features η and parameters w that minimize the total description length of
the data and the Bayesian network model (in exponential
form). This can be formulated as a joint optimization of
min

η∈{0,1}F

c> η +

log T >
e η + min L̃(w, η, D)
w
2

(8)

Here the last term is the cost of encoding the training data
D = [x1 ; ...; xT ] using the optimal parameters w for the
network structure specified by η. This uses a standard
result from information theory (Cover & Thomas, 1991;
Friedman & Goldszmidt, 1998) that an optimal
P code for
data D given a model P (x) has length − i log P (xi ).
(Although here we alter this principle slightly to use the
regularlized loss L̃ rather than the plain loglikelihood loss
L. This simplifies the derivation below.)
The first term in (8) measures the length of the description
for an exponential family representation for the Bayesian

network structure specified by η. In particular, for each
feature φjab selected by indicator ηjab we fix the description length cost
X
log |Vals(b` )|
cjab = |b| log n + log |Vals(a)| +
`

where the first term is the cost of encoding the list of variables in feature φjab , and the remaining terms are the cost
of encoding the specific values for these variables.
The second term in (8) is the cost of encoding each weight
parameter wjab , where the precision is chosen in the manner discussed in (Friedman & Goldszmidt, 1998).
Now we would like to solve for the structure η and parameters w that minimize the total description cost (8). Unfortunately, this is a combinatorial optimization problem over
η, and even more problematic, even if η were relaxed, the
MDL objective (8) is not jointly convex in η and w. Fortunately, the dual form of the regularized loss allows us to
re-express this problem as a convex minimization over η,
ignoring the integer constraints.
Using the fact that (6) is equivalent to (4), yields an equivalent optimization problem to (8), but now using maximum
entropy instead of loglikelihood loss:
log T >
e η+
(9)
c> η +
2
#
"
!
X
X
1 > >
#bH(θ b ) −
max
δ j Nj Nj δ j
θ
2β
j

min

η∈{0,1}F

b

P

where δ j = bj #bj (φ̄bj − θ bj ). Recall that, thus far,
we have assumed that η ∈ {0, 1}F , and therefore Nj> Nj =
Nj , since n2jab = ηjab . This allows us to rewrite (9) as
minη g(η) where
g(η)

=

log T >
e η+
(10)
max c> η +
θ
2
#
"
!
X
X
1 >
#bH(θ b ) −
δ j Nj δ j
2β
j
b

Crucially, g(η) is a pointwise maximum of linear functions of η, and is therefore convex (Boyd & Vandenberghe,
2004).
Thus, by combining regularized maximum entropy parameter estimation with the description length penalty, we obtain a natural convex relaxation of the minimum description length principle simply by relaxing the structure indicator variables to be soft indicators in the interval [0, 1].
Remarkably, this formulation allows one to simultaneously
optimize the (soft) structure and parameters in a polynomial size convex optimization problem.
To solve this problem in practice, we use a quasi-Newton
method, BFGS (Nocedal & Wright, 1999) with backtrack-

ing line search to efficiently minimize g(η). BFGS progressively approximates the Hessian matrix by accumulating gradient information ∇g(η) at successive η points.
Fortunately, g(η) and ∇g(η) are both computable by solving the inner concave maximization on θ (which in fact is
equivalent to solving the primal minimization problem on
w). In particular, g(η) is given by (10), and
∇g(η)

such that δ ∗j =

=

P

bj

c+

log T
1 X ∗.2
δ
e−
2
2β j j



#b φ̄bj − θ ∗jb for the optimal in-

ner solution θ ∗ . Here, .2 denotes componentwise squaring.

7

Variable ordering

Our final step is to consider variable ordering as part of the
optimization process. Once again, we will find that one
can solve for the optimal ordering, while performing feature selection and parameter optimization simultaneously.
Since no order is given, we first generate features for each
variable xj assuming all other variables are potential parents. Then, as in the previous section, we introduce feature
selection variables η = (...ηjf ...)> and reduce the model
complexity by minimizing the description length criterion.
As before, we begin by assuming the feature selection variables are {0, 1} valued. The issue now is that we need
to add constraints to the η variables to ensure that a valid
Bayesian network structure is obtained. For example, since
activating a feature φjf for one variable means that the remaining variables in the pattern f must be parents of j, no
feature pattern f can be activated for more than one variable j ∈ f . We can encode this constraint locally for each
feature pattern by the constraints
X

ηjf

≤

1

for all f

(11)

j∈f

In fact, the local constraints are simple linear equalities that
pose little additional burden on the optimization. Unfortunately, ensuring consistency locally within a feature pattern
f is easy, but ensuring consistency globally between feature
patterns f and h is more difficult.
Our strategy for enforcing global consistency is to introduce a {0, 1}n×n matrix S that encodes a total ordering on
the variables. In particular, we let Sij = 1 denote the case
that i precedes j in the ordering, and Sij = 0 denotes that
i follows j. For a matrix S to encode a total ordering it has
to be
antisymmetric:
transitive:
reflexive:

Sij = 1 − Sji for all i 6= j

(12)

Sij + Sjk ≤ Sik + 1 for all distinct i, j, k
Sii = 1 for all i

(The diagonal of S is not terribly important, but we set it 1
for convenience.) The feature selection variables can then
be forced to respect a global ordering by the constraints
ηjf

≤

Sij for all f, i ∈ f, i 6= j

(13)

Thus, we obtain the result that for {0, 1} valued variables η
and S we can enforce local and global consistency by linear constraints. This yields an obvious convex formulation
for the entire relaxed problem.
min

η∈[0,1]F ,S∈[0,1]n×n

g(η) subject to (11), (12) and (13)

One remaining problem with the formulation is that it requires a large, cubic number of constraints in (12) to encode
the transitivity constraint. The cubic complexity can be reduced to a quadratic number of constraints by exploiting a
few basic facts about relation matrices.
Proposition 3 Let T be an upper triangular matrix with
1’s above the main diagonal. A {0, 1} valued matrix S
encodes a total ordering if and only if S = I + U + (T −
U )> for some {0, 1} valued upper triangular U above the
diagonal such that I +U +U > and I +(T −U )+(T −U )>
are both equivalence relations.
Proof The idea is to show that transitivity is preserved in
{0, 1} matrices when one converts between an antisymmetric and symmetric relation. Let T and U be defined as
above. Let M = I +U +U > , N = I +(T −U )+(T −U )>
and S = I + U + (T − U )> , and assume all values are in
{0, 1}. Clearly M and N are symmetric and S is antisymmetric.
We establish that for all i, j, k, such that i 6= j, j 6= k and
i 6= k, that Sij ∧ Sjk ⇒ Sik if and only if Mij ∧ Mjk ⇒
Mik and Nij ∧ Njk ⇒ Nik . The argument is by cases over
the six possible orderings of i, j, k.
Case 1: If i < j < k, then Sij = Mij , Sjk = Mjk , and
Sik = Mik . Therefore Sij ∧ Sjk ⇒ Sik iff Mij ∧ Mjk ⇒
Mik .
Case 2: If i < k < j, then Sjk = Njk , Ski = Nki , and
Sji = Nji . Therefore Sij ∧Sjk ⇒ Sik iff ¬Sij ∨¬Sjk ∨Sik
iff Sji ∨¬Sjk ∨¬Ski iff Nji ∨¬Njk ∨¬Nki iff Njk ∧Nki ⇒
Nji .
Case 3: If k < i < j, then Ski = Mki , Sij = Mij ,
and Skj = Mkj . Therefore Sij ∧ Sjk ⇒ Sik iff ¬Sij ∨
¬Sjk ∨ Sik iff ¬Sij ∨ Skj ∨ ¬Ski iff ¬Mij ∨ Mkj ∨ ¬Mki
iff Mki ∧ Mij ⇒ Mkj .
The remaining three cases are similar.
Thus, from the proposition, we can enforce transitivity by
using a quadratic number of constraints by:
S

=

I + U + (T − U )>

I + U + U>
E − U − U>

=
=

DD>
CC >

De

=

e,

(14)

Ce = e

where D and C are further auxiliary square {0, 1} matrices,
and E denotes the matrix of all 1s. Unfortunately, the two
quadratic constraints are not convex, but they can be approximated by the semidefinite relaxations I + U + U > 
DD> , E − U − U >  CC > . In our experiments below,
we relaxed the {0, 1} valued variables to [0, 1] and used the
semidefinite constraints. The implementation only requires
a small modification to the BFGS strategy of the previous
section, where the quasi-Newton step now needs to respect
these constraints. We solved the convex constrained optimization problem by using a simple barrier method (Boyd
& Vandenberghe, 2004), using a log barrier function for
the linear inequality constraints (11) and (13), plus a log
determinant barrier to enforce the semidefinite constraints
in (14), thus ensuring a feasible search.
The result is the first comprehensive Bayesian network
technique we are aware of that solves for an approximate
variable ordering, feature set, and optimal weights in a
joint, polynomial, convex optimization. Our results below
show that this approach can produce competitive results.
One final issue to deal with is rounding a “soft” solution
produced by the above convex optimization, to produce
a variable ordering and a hard set of features to define a
proper Bayesian network. We do not as yet have any approximation guarantees for any rounding approach we have
developed so far. In our experiments below, we simply
used a greedy rounding scheme that successively checks
the largest non-integer η variable, determines whether it is
possible for it to be set to 1 without violating any consistency checks, and if so, rounds the variable greedily to 0 or
1 depending on which value yields the smallest value in the
MDL objective (8) (keeping the current optimal Bayesian
network parameters fixed). This is sufficient to yield reasonable results, although we would still like to investigate
more sophisticated rounding approaches.

8

Experimental evaluation

We conducted a set of experiments on both synthetic and
real data to evaluate our proposed algorithms and compare
them to benchmark greedy heuristic search techniques. To
measure performance of the different learning techniques,
we measured the loglikelihood loss they achieve on held
out test data after training. To isolate the effects of the different approximation stages, we conducted two sets of experiments: in the first set the variable ordering was held
fixed, while in the second we used the relaxed ordering
search of Section 7. In each case, for the greedy search
algorithms, we considered both BDe and BIC scores.
For the fixed order experiments, we first considered three

Table 1: Synthetic experiments, comparing fixed order
learning methods (given the correct variable order), with
training sample size 50. Loglikelihood loss.
Data Set

Convex

BIC

BDe

Synthetic 1
Synthetic 2
Synthetic 3

4.4753
5.3385
5.1641

4.5725
5.4263
5.2677

4.5264
5.3854
5.1788

Table 2: UCI data set experiments, comparing fixed order learning methods (given a random variable order), with
training sample size 50. Loglikelihood loss.

Table 3: Synthetic experiments, comparing methods that
learn both structure and order, with training sample size
50. Loglikelihood loss.
Data Set

Convex

BIC

BDe

Synthetic 1
Synthetic 2
Synthetic 3

4.4887
5.3413
5.1581

4.5731
5.4265
5.2692

4.5072
5.3489
5.2105

Table 4: UCI data set experiments, comparing methods that
learn both structure and order, with training sample size 50.
Loglikelihood loss.

Data Set

Convex

BIC

BDe

Data Set

Convex

BIC

BDe

breast
cleve
corral
diabetes
glass2
heart
mofn
pima

5.0275
8.7238
4.6109
5.5130
3.3533
8.7866
7.6434
5.2982

5.4698
8.9061
4.6686
5.6224
3.5833
8.8927
7.6734
5.3460

5.2899
8.9984
4.5084
5.5942
3.4047
8.9570
7.8376
5.3635

breast
cleve
corral
diabetes
glass2
heart
mofn
pima

5.2643
8.5974
4.7056
5.5823
3.4870
8.5889
7.5659
5.3823

5.2289
8.8418
4.5577
5.6098
3.5803
8.8684
7.7125
5.3369

5.2491
9.1324
4.5360
5.6210
3.3950
9.0956
8.0055
5.3885

different artificial network structures, consisting of 5 variables and 4, 5, 6 edges respectively, each instantiated with
random CPT entries. To set the regularization parameters
for the convex relaxation technique we used an initial training sample and hold-out test sample, and then repeated the
training and test procedures 10 times with the parameters
fixed on independently generated data. We evaluated each
learning technique by measuring the loglikelihood loss on
an independent test set of size 1000 drawn from the same
distribution as the training data. The results in the tables
were averaged over the 10 repeats. We compared the results of the convex relaxation algorithm described in Section 6 to the K2 search algorithm of (Cooper & Herskovits,
1992), using both the BDe and BIC criteria as optimization
objectives. All algorithms were given the correct variable
ordering in these synthetic experiments.
Table 1 shows the results obtained by the convex relaxation
technique versus the greedy search algorithms on a training
sample of size 50 drawn from the synthetic Bayesian network models. Here we can see that the convex technique
outperforms the greedy heuristic search procedures, using
both the BDe and BIC scores. However, the run time for
the convex relaxation procedure (including rounding) was
10, 25 and 30 seconds respectively, while the K2 algorithm
on required 0.05 seconds on these problems.
Next, we conducted an experiment on real data. Here
we used several UCI data sets: corral, breast, cleve, diabetes, glass2, heart, pima and mofn. For each dataset, we

ran the learning algorithms 10 times on different random
training/test partitions, using the training set for Bayesian
network learning and test set for performance evaluation.
Each algorithm was run with the same fixed variable order,
where in this case the order was just chosen randomly. For
the convex relaxation technique, we used one preliminary
training/test split to set the regularization parameters. Table 2 shows the average loglikelihood loss obtained on the
test partitions, training on a disjoint subset of size 50 examples, for each of the learning methods. Here we see that
the convex approach holds an advantage over the greedy
search techniques, for both BDe and BIC scores. Once
again, however, the run times of the convex relaxation approach are greater than the K2 algorithm, requiring from
10-100 times more time to produce the final results.
Although these results are preliminary, they suggest that the
ability to avoid local minima in a discrete structure search
can lead to good solutions. On the other hand, the major
disadvantages for the approach we have presented so far is
that it runs slower than heuristic greedy search and requires
regularization parameters to be set, where as the BDe and
BIC scores are parameter free. Beyond improving run time,
one significant direction for future research is to reduce the
reliance on regularization parameters.
Next, we repeated the previous experiments using the combined structure and order optimization algorithm of Section 7. Here we compared to greedy heuristic search using
the standard edge addition, deletion and reversal operators,

again considering both the BDe and BIC optimization criteria. Each greedy search was started from an empty network
and restarted 4 times when reaching a local minimum, by
randomly adding and deleting edges. However, other than
not imposing a variable ordering, the algorithms were run
exactly as described above for the fixed order case.
Table 3 shows the results obtained by the convex relaxation technique versus the greedy search algorithms on
the synthetic problems. Here we see a modest advantage for the convex over greedy search methods. However,
once again, the convex relaxation procedure runs about 100
times slower. Interestingly, the solution quality is close
to the fixed order case, which only benefited slightly from
having the correct variable ordering.
Table 4 shows the results obtained by the convex relaxation
technique versus the greedy search algorithms on the UCI
data sets. Here the quality of the outcome is mixed. The
convex relaxation procedure obtains the best solution quality on 4 out of 8 data sets, while the greedy heuristic search
using BDe obtains the best results on 2 out of 8, and BIC
obtains the best results on 2 out of 8. More interestingly,
comparing these results to the fixed order technique, which
just uses a random variable ordering, shows that the fixed
order approach (with convex relaxation) still obtains the
best results on 4 out of 8 data sets. This outcome seems
to suggest that the relaxed ordering constraints imposed in
Section 7 might not be sufficiently tight to ensure a good
solution. Improving the quality of the relaxed ordering constraints remains an important question for future research.

9

Conclusion

We have presented what we feel is a promising new perspective on learning Bayesian network structure from data.
The technique simultaneously searches for variable order,
parameter settings, and features in a joint convex optimization. We feel that this approach might open the way to a
new class of algorithms for learning Bayesian networks that
ultimately might lead to guaranteed approximation quality. Beyond approximation guarantees and algorithmic improvements, other significant directions for future research
include considering the problem of learning in the presence of missing data or latent variables (Elidan & Friedman, 2005), and attempting to extend the current analysis
to Bayesian scores (Cooper & Herskovits, 1992; Heckerman et al., 1995).

References
Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996).
Context-specific independence in Bayesian networks. Proceedings UAI.
Boyd, S., & Vandenberghe, L. (2004). Convex optimization. Cambridge U. Press.

Buntine, W. (1991). Theory refinement on Bayesian networks.
Proceedings UAI.
Chickering, M. (1996). Learning Bayesian networks is NPcomplete. In D. Fisher and H. Lenz (Eds.), Learning from data:
Artificial intelligence and statistics, vol. 5. Springer.
Chickering, M., Meek, C., & Heckerman, D. (2003). Largesample learning of Bayesian networks is NP-hard. Proc. UAI.
Cooper, G., & Herskovits, E. (1992). A Bayesian method for
the induction of probabilistic networks from data. Machine
Learning, 9.
Cover, T., & Thomas, J. (1991). Elements of information theory.
Wiley.
Elidan, G., & Friedman, N. (2005). Learning hidden variable
networks: The information bottleneck approach. Journal of
Machine Learning Research, 6.
Elidan, G., Ninio, M., Friedman, N., & Schuurmans, D. (2002).
Data perturbation for escaping local maxima in learning. Proceedings AAAI.
Friedman, N., & Goldszmidt, M. (1998). Learning Bayesian networks with local structure. In M. Jordan (Ed.), Learning in
graphical models. MIT Press.
Goldenberg, A., & Moore, A. (2004). Tractable learning of large
Bayes net structure from sparse data. Proceedings ICML.
Heckerman, D., Geiger, D., & Chickering, M. (1995). Learning
Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 9.
Lam, W., & Bacchus, F. (1994). Learning Bayesian belief networks: an approach based on the MDL principle. Computational Intelligence, 10.
Larranaga, P., Kuijpers, C., Murga, R., & Yurramendi, Y. (1996).
Learning Bayesian network structures by searching for the best
ordering with genetic algorithms. IEEE Trans. Systems, Man,
Cybernetics, 26.
Margaritis, D. (2003). Learning Bayesian network model structure from data. Doctoral dissertation, CMU, CS.
Moore, A., & Wong, W.-K. (2003). Optimal reinsertion: A new
search operator for accelerated and more accurate Bayesian
network structure learning. Proceedings ICML.
Nocedal, J., & Wright, S. (1999).
Springer.

Numerical optimization.

Pearl, J. (1988). Probabilistic reasoning in intelligent systems.
Morgan Kaufmann.
Rissanen, J. (1987). Stochastic complexity. Roy. Stat. Soc. B, 49.
Spirtes, P., Glymour, C., & Scheines, R. (2000). Causation, prediction and search. MIT Press.
Teyssier, M., & Koller, D. (2005). Ordering-based search: A
simple and effective algorithm for learning Bayesian networks.
Proceedings UAI.
Van Allen, T., & Greiner, R. (2000). Model selection criteria for
learning belief nets. Proceedings ICML.
Wainwright, M., & Jordan, M. (2003). Graphical models, exponential families, and variational inference (Technical Report
TR-649). UC Berkeley, Dept. Statistics.

