this paper along with several examples.

1

Introduction

The Bayesian networks technology provides a rep­
resentation language for uncertain beliefs and infer­
ence algorithms for drawing sound conclusions from
such representations. Bayesian Network is a directed,
acyclic graph in which the nodes represent random
variables, and the arcs between the nodes represent
possible probabilistic dependence between the vari­
ables. The success of the representation is mainly
due to the development of many probabilistic infer­
ence algorithms [3, 4, 5, 6]. While most of the algo­
rithms can efficiently perform simple queries such as
the marginal probability of each node given evidence,
they have not efficiently addressed the problem of more
general queries such as joint or conditional probabili­
ties of any combination of nodes.
The recent work of Symbolic Probabilistic Inference
(SPI) [1, 2] has made a significant step in this direc­
tion. SPI is a goal-driven method which is incremen­
tal with respect to both queries and observations. In
response to this research we have extended the evi­
dence potential (EP) algorithm [3] with the same fea­
tures. We call the extension symbolic evidence poten­
tial inference (SEPI). Unlike traditional Bayesian Net
inferencing algorithms, both SPI and SEPI are goal
directed, performing only those calculations that are
required to respond to queries. While in SPI, opera­
tions are done on a search tree constructed from the
original network, in SEPI, a clique-tree structure ob­
tained from the EP algorithm is the basic framework
for recursive query processing.
In SEPI, the EP algorithm [3] is used as the "pre­
processing" step in which various probabilities such as
"set-chain" conditional [3) and marginal probabilities
of each clique are computed based on the clique tree.
The second step in SEPI is to process the query with
a recursive mechanism similar to the SPI algorithm.
A query is directed to the root clique and decomposed
into queries for the clique's subtrees. This recursive
process continues until a particular query can be an­
swered at the clique at which it is directed. The answer

Symbolic Probabilistic Inference with Evidence Potential

is then computed and returned to the next higher level
in the clique tree. Once a clique has responses from
all of its subtrees it can compute its own response to
its predecessor clique. This process terminates when
the root clique processes all the responses from its sub­
trees.
With similar mechanisms for caching and incorporat­
ing evidence as in SPI, the calculation in SEPI is also
incremental with respect to both query and evidences.
However, since all the necessary probability distribu­
tions are stored in the "pre-processing" step, the SEPI
algorithm is more efficient.
The paper is organized as follows. Section 2 briefly de­
scribes the EP algorithm which includes the construc­
tion of the clique tree. Section 3 describes the SEPI
algorithm. A systematic recursive query and caching
procedure will be presented. Some illustrative exam­
ples are given in Section 4, followed by the concluding
remarks in Section 5.
2

Evidence Potential Algorithm

In this section, we will briefly review the evidence po­
tential (EP) algorithm [3]. The algorithm first orga­
nizes the original network into clique tree, where each
clique is a group of nodes not necessary mutually ex­
clusive. It then performs inference by passing messages
between cliques in a similar way to the distributed al­
gorithm [4].
The first part of the algorithm is to form a clique tree.
This part consists of five steps
1. Marry Parents: link predecessors of a node to­
gether
2. Remove Arc Directions: remove directions of all
arcs
3. Fill in: generate new arcs between nodes when­
ever necessary to form a "perfect" graph
4. Find Cliques: form node clusters/cliques
5. Order Cliques, and Find Residuals and Separa­
tors: form cluster tree
After the clique tree is formed, the second part of the
algorithm is to calculate the marginal probability of
each node. Before this can be done, the "evidence
potential" and "separator potential" likelihoods [3] are
calculated for each cluster.
The second part of the algorithm consists of the fol­
lowing:
1. Calculating Evidence Potentials and Separator
potentials: they are calculated from the prior
node conditionals in each clique.
2. Calculate Set-Chain Conditionals: namely, the
conditional probability of the residuals given the

Figure 1: A Example Network
separators of each clique.
3. Calculate Joint Probability for each clique: from
joint, we then can calculate individual node pos­
teriors (marginal) probability.
To illustrate this algorithm, the example given in [3]
is shown in Figure 1. The corresponding clique tree
and the set-chain conditional of each clique is shown
in Figure 2. It is clear that the joint probability of
the whole network can be obtained by multiplying all
the set-chain conditionals together with the marginal
probabilities of all the root cliques. Any query can
then be obtained from the joint probability. The basic
idea of EP algorithm is to decompose and factor the
original formulae so that only minimum operations are
required to answer the queries.
3

Symbolic Inference with Evidence
Potential

The procedure described in the previous section can
be considered as the pre-processing step for the generic
query algorithm to be described. We call this new al­
gorithm symbolic evidence potential inference (SEPI).
In this algorithm, the goal is to calculate the results of
arbitrary queries. The idea is to derive an efficient in­
ference algorithm which takes advantage of the clique­
tree structure of the EP algorithm.
The SEPI algorithm consists of several major process­
ing steps. The first step is to organize the nodes of
a Bayesian network into a clique tree structure and

83

84

Chang and fung

Note that if the successor clique has nothing to do with
the query, i.e., (Z \Co) n T(C;)
0, then no query
will be sent to that clique.
=

P(LE!T)

At the clique C;, when the request arrives for a prob­
ability distribution represented by P{XIS;}, if such
a distribution had already been computed earlier and
cached, it can be returned immediately. However, usu­
ally it will be necessary to send requests to the clique's
successors in order to compute the response. Since it
can be easily shown that

P(B/LE)

P{XIS;}

=

Lfl P(XnT(C,.i)IS;j)P(Rc,IS;)
Rei

j

(4)

where Rc, is the residual nodes of C;, C;j is the j- th
child clique of C;, and S;j is the separators between
C; and C;j, the request to each child C;j will be
P(XnT(C;i)IS;j)·
(5)
The recursive process continues until it reaches the leaf
node or the request can be answered from the cached
results.

Figure 2: Cluster Tree and Set-Chain Conditional
calculate and store the various probability functions
as described in the previous section (e.g., set-chain
conditional and joint probability distribution). In the
second step, queries from the user are directed to the
root clique of the tree. The query is decomposed into
queries for the clique's subtrees. This recursive pro­
cedure continues until a particular query can be an­
swered.
The general format for a query received by SEPI is as
a conditional probability, namely, P{XIY}, where X
and Y are sets of nodes in the network. This query is
first transformed into joint distribution format P(Z)::::
P{X, Y} and directed to the root clique. In order to
answer the query, it would be sufficient to calculate
P(Z \ CoiC0), where Co is the root clique. This is
because we can calculate the query by
P(Z)

=

L

P(Z \CaiCo)P(Co)

(1)

Co\Z

where the prior probability P(Co) is available at the
clique Co. According to the EP algorithm, the clique
tree is organized in a way that the separators are the
overlapping nodes between the successor and prede­
cessor cliques. Denote the separators between the root
clique and the child clique C; as S;, then
P(Z \CaiCo)

=

P(Z \Co IS;)

To handle the evidence, just substitute the observed
values into all the clique distributions involving the
observed node. This operation is very simple in which
the particular dimension of the observed value is sim­
ply eliminated and the rest of the distribution remain
the same. The substitution needs to be done for all
the distributions including the cached results stored
in each clique which involves the observed node. Af­
ter the substitution, all the other operations can be
applied on distributions with the reduced dimensions.
As in the SPI algorithm, three major operations are
needed in the SEPI algorithm: multiplication, sum­
mation and substitution. Multiplication calculates the
product of two distributions, summation calculates the
sum of a distribution over a set of variables, and substi­
tution calculates the result of substituting an observed
value for a node into a distribution.
Examples

4

With the network given in Figure 1, we will now il­
lustrate the SEPI algorithm with several query exam­
ples. First, assuming the query we are interested is
P(AXS), the recursive algorithm works as follows.
•

•

(2)

Define T(Ci) as all the nodes in the subtree rooted
from C;, then the new request to be sent to each child
C; is
P((Z \Co) n T(C;)IS;)
(3)

•

The query P(AXS) is received at the root clique
(AT), based on eqn. (2) and (3), a new query
P(XSIT) is generated and sent to the successor
clique (TLE)
The query P(XSIT) is received at the clique
(TLE); similarly, a new query P(XSILE) is gen­
erated and sent to the successor clique (LEB)
The query P(XSILE) is received at the clique
(LEB), based on (5), new queries P(SIBL) and
P(XIEB) are generated and sent to the successors
(BLS) and (EBD) respectively.

Symbolic Probabilistic Inference with Evidence Potential

•

- The query P(SIBL) is received at the clique
(BLS) which is available in the cache due to
the pre-processing.
- The query P(XIEB) is received at clique
(EBD), a new query P(XIE) is generated
and sent to the successor clique (EX)
- The query P(XIE) is received at clique (EX)
which is available.
At clique (EBD), compute the query P(XIEB)
by
(6)
P(XIEB) 2::: P(XIE)P(DIBE)
D

evidence and have good potential for parallel process­
ing.
In this paper, we develop a similar query algorithm
based on the combination of evidence potential algo­
rithm and the SPI inference mechanism. Rather than
converting the network into a SPI search tree, we con­
struct a "clique tree" based on the evidence potential
algorithm. Additionally, the evidence potential algo­
rithm is used as the pre-processing step where all the
necessary probability distributions for answering the
query are computed and stored in each clique.

=

•

At clique (LEB), compute the query P(XSILE)
by
P(X SILE)

•

=

2::: P(SIBL)P(XIEB)P(BILE)
B

(7)
At clique (TLE), compute the query P(XSIT) by
P(XSIT)

=

2::: P(XSILE)P(LEIT)

(8)

LE
•

At root clique ( AT), compute the query P(AXS)

by

P(AXS)

=

2::: P(XSIT)P(AT)

(9)

T

Assume in the second example that the node E is
observed and the observed value is E•. To calcu­
late the posterior probability of the same query, we
first substitute the observed value into all the distri­
butions in the cliques related to the observed node.
These include P(DIBE), P(BILE), and P(LEIT) in
the cliques (TLE), (LEB), and (EBD) respectively.
The substitution operation simply eliminates the par­
ticular dimension corresponding to the observed value
in the distributions. Then the same procedure as de­
scribed above to calculate the query can be applied
using the distributions with new reduced dimensions.
The result is therefore,
P(AXSIE E•)
LT LL [LB [P(SIBL) LD [p(XIE.)P(DIBE•)J
P(BILE•)J P(LE.IT)] P(AT)
(10)
=

5

=

Conclusion

SPI algorithm [1, 2] is the latest inference algorithms
in which the emphasis is on the efficient generic query.
The main goal of these algorithms is to respond to
arbitrary queries in an efficient manner. In these al­
gorithms the network is first converted into a search
tree and the probabilities are manipulated by symbol­
ically decomposing or factoring the formulae. These
methods are incremental with respect to queries and

Similar to the SPI algorithm, queries are directed to
the root clique of the tree. They are decomposed into
queries for the clique's subtrees. This recursive pro­
cedure continues until a particular query can be an­
swered. The answer is then computed and returned to
the next higher level. The algorithm and the computa­
tion are simple. With a similar mechanism for caching
and incorporating eviden.ce as in the SPI algorithm,
the calculation is also incremental with respect to both
query and evidence. However, the SEPI algorithm is
more efficient since all the necessary probability distri­
butions are stored in the pre-processing step. A ver­
sion of the SEPI algorithm as well as the SPI algorithm
have been implemented, preliminary results from sev­
eral examples show that with the prep-processing step,
the query process of the SEPI algorithm is faster than
the SPI algorithm.

References

[1] B. D'Ambrosio. Symbolic probabilistic inference in
belief nets. 1990.
[2] R. Shachter A. Del Favero and B. D'Ambrosio.
Symbolic probabilistic inference: A probabilistic
perspective. Proceeding of AAAI, 1990.
[3] S. L. Lauritzen and D. J. Spiegelhalter. Local
computations with probabilities on graphical struc­
tures and their application in expert systems. Jour­
nal Royal Statistical Society B, 50, 1988.
[4] Judea Pearl. Fusion, propagation, and structuring
in belief networks. Artificial Intelligence, 29, 1986.
[5] Judea Pearl. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann Publishers, 1988.
[6] Ross D. Shachter. Intelligent probabilistic infer­
ence. In L.N. Kana! and J.F. Lemmer, editors,
Uncertainty in Artificial Intelligence. Amsterdam:
North-Holland, 1986.

85

