strumental variables can be used to estimate

r

how the distribution of an effect will respond
to a manipulation of its causes, even in the
presence of unmeasured common causes (con­
founders).

f

In typical instrumental variable

1

s

z----.x ---•Y

estimation, instruments are chosen based on
domain knowledge.

fy

There is currently no

statistical test for validating a continuous
variable as an instrument.

In this paper,

we introduce the concept of semi-instrument,
which generalizes the concept of instrument:

Figure 1: One Instrumental Variable

each instrument is a semi-instrument, but the
converse does not hold.

We show that in

the framework of additive models, under cer­
tain conditions, we can test whether a vari­
able is semi-instrumental. Moreover, adding
some distribution assumptions, we can test
whether two semi-instruments are instrumen­

tal.

We give algorithms to test whether a.

variable is semi-instrumental, and whether
two semi-instruments are both instrumental.
These algorithms can be used to test the ex­
perts' choice of instruments, or to identify in­
struments automatically.

random variable
on

X

X

is the cause of Y, then intervening

will change Y, but intervening on Y should not

change X. 1 By regressing Y on
effect of the intervention of

X

X,

we can estimate the

on Y, up to a constant,

if no unmeasured common cause of

X

and Y exists.

However, this advantage becomes problematic if we
know, or even suspect, that there are some unmea­
sured common causes of

X

and Y, in which case we

cannot estimate the direct effect of

X

on Y.

Consider the the causal structure illustrated in figure
1. Suppose that Z is not observable for the moment.

Key Words: Causality, Instrumental Variables

Among

X,

Y, Ex, and fy, only

X andY

are observed

variables. The functional relations among them are:

1

INTRODUCTION

One of the major advantages of knowing the causal
relations among the variables we are interested in is
that we can use the causal structure to predict the
effects of interventions. For example, if we know that a

1 Here by intervention we mean the manipulation of the
value of one random variable, with the assumption that
this manipulation can affect other variables only through
the manipulated variable. We also assume no feedback,
i.e., no directed cycles, and no reversible causal relations,
in a causal graph. See Spirtes et al (2001)

CHU ET AL.

84

2

UAI2001

PRIOR WORK

There has been intensive study of the use of instru­
ments in the econometric literature. Much work has
focused on the study of the efficiency of an instrument

h

(Nelson et al 1990, Shea 1997, and Zivot et al 1998).

�

...._

instrument

l

s

X

z

strument for

ey

r

f

Z1 is an in­
X and Y, provided we already have an
Z2 for X and Y (Wu 1973, Newey 1985,

There are studies of whether a variable

and Magdalinos 1994), but how to find the first in­
strument is still an open problem.

There is no test

available to find out whether a continuous variable is

y
___.;t(

an instrument, without knowing that some other vari­
4

able is an instrument.

In practice, people usually resort to domain knowledge

g

to determine whether a variable is an instrument. As
some studies show, moreover, it is often very difficult
to find instruments, and a wrong choice may signifi­
cantly affect the final result (Bartels 1991, Heckman

Figure 2: One Common Cause

1995, and Bound et al 1995). Therefore, even in the
case where we do have some domain knowledge to help
us identify instruments, some kind of testing procedure
is still useful in that it can serve as a double check. It

X
Y
E[t:Yitx]
Note that

ey

X

and

(1)
(2)

f(Z) +ex
s(X) +ey
h(Ex)

=

(3)
E[EyiX]
>.(X). The

are dependent, that is,

will be a non-constant function of

X,

say,

regression of Y on X will give:

E[YIXJ

=

s( X)

+

E[EyjX]

=

s(X),

X- E[XIZ].

X

on

Z

3

APPROACH

W ithin the framework of linear models, the fact that a
variable is an instrument imposes no constraint on the
Consider the causal structure illustrated in figure 1,

even up to a constant.

However, with the help of variable
Ex by regressing

variable is an instrument.

joint marginal distribution of the observed variables.

s( X) +>.( X)

Because we have no way to estimate >.(X), we also
have no way to identify

is the goal of this paper to find out whether, under
certain reasonable conditions, we can test whether a

Z, we can estimate

where only

Z, X, and Y

to get an estimate of ex =

Then, we can regress Y on X and EX to

=

= s(X)

s(X) + E[EY!Ex]

+ E[t:yiEx,X]
=

s(X) + h(Ex). 2

where f and

An additive regression method will give an estimate of
and h(Ex) simultaneously. 3

s(X)
Here

Z

is called an instrumental variable for

Y, by which we mean that
and

X.

Z

Z

X

and

is a direct cause of

is independent of all other causes of

Y

X,

except

(Note that X must be a direct cause of Y.)
2Here we use the fact that X and

conditional on
figure 1.

Ex,

In

particular, we assume that:

get:

E[YIX,t:x]

are observable. Assume for

now that all the functional relations are linear.

Ey

are independent

which is implied by the causal graph in

3For the proof of the identifiability of
constant, see Newey et a! (1999).

s(X),

up to a

s are

X

=

fZ+cx

Y

=

sX +t:y

(4)
(5)

non-zero constants.

Now we construct another model based on the causal

2. Let
g'
t:x. Let Z'

structure illustrated in figure

81

=

s-

g'
7' and ey

=

Ey+

y

g'
=

=

c

=/:- 0,

z, X1 =X,

4Pearl (1995) shows that, for discrete data, Z being an
instrument for X and Y imposes some constraint, which is
called by him as instrumental inequality, on the joint dis­
tribution of Z, X, andY. This inequality is not sufficient
for Z to be an instrument, and, as pointed in this paper, it
cannot be extended to the continuous models. This paper
does not give an instrument testing procedure for discrete
data.

Y1
gf Z' + s1 X1 + f�. Clearly Y1
Y, hence
(Z,X,Y) has the same distribution as (Z1,X1,Y1).
However, we notice that in the first case, Z is an in­
strument for X and Y, while in the second case, Z1 is
not an instrument, but a common cause of X' and Y1•
Actually, we can set g' to be any value, and adjust s1
X and Y1 Y. This
and E� accordingly so that X'
and

=

=

=

=

means that, a joint distribution implied by a model

where

85

CHU ET AL.

UAI2001

Z

is an instrument could also be implied by

uncountably many other models where

Z

is not an in­

strument. Based on the joint distribution, we cannot
tell whether

Z

figure represents the correct model.
Note that a linear model is a special case of the ad­
ditive model. Thus, given that we cannot determine
whether a random variable is an instrument in a linear
model, we could not, in general, determine whether it
is an instrument in an additive model. Therefore, we
should look for some other conditions, preferably a lit­
tle bit weaker than those required by an instrument, so
that a random variable that satisfies these conditions
can be identified in an additive model.
One such candidate set of conditions is what we call

is an instrument.

The above analysis suggests two possible approaches
for instrument testing. First, we may extend the space
of linear structural models to a larger space. The idea
is that the space of linear structural models already im­
p oses strong constraints on each variable so that being
an instrument does not impose any extra constraints.

However, i n a larger space, it is possible that being
an instrument does imply some conditions not neces­
sarily satisfied by all models in that space. Another
approach is to get more candidates for instrumental
variables, and see whether the instrument assumption
imposes some constraint on the joint distribution of
these candidate variables.
In this section we shall try a combination of both ap­

the semi-instrumental conditions. A random variable
is a

Z

semi-instrument

for

X

2. Z is an exogenous variable in M;
3. Z is the cause of X, and X is a cause of Y in M;

4. If Z is also a cause of Y, then the direct effect of
Z on Y is a linear function of the direct effect of
Z on X.
Note that if a random variable

ment variables. In the first stage of the test, we shall
exclude some, but not necessarily all, non-instruments.

g(Z),

In the second stage of the test, we shall determine

X, say,

whether all the remaining ones are instruments, pro­

if the following

1. The joint distribution of Z, X, andY can be rep­
resented by an additive model M consisting only
of Z, X, and Y as the observed variables;

for X and

vided we still have at least two candidates left.

Y

conditions are satisfied:

proaches. That is, we shall propose a two-stage proce­
dure. Suppose we are given a set of candidate instru­

and

Y,

Z is

a semi-instrument

then the direct effect of

Z

on

Y,

say,

is a linear function of the direct effect of Z on

f(Z). That is, there is a pair of real numbers
that g(Z) = af( Z ) +b. We shall call a the
linear coefficient of the semi-instrument Z.
(a, b) such

It is easy to see that the semi-instrumental assump­
3.1

SEMI-INSTRUMENTAL VARIABLES

tion is weaker than the instrumental assumption: All

FOR THE ADDITIVE

instruments are semi-instruments (with a linear coeffi­

NONPARAMETRIC MODELS

cient 0), but not all semi-instruments are instruments.
Moreover, in general, in a linear model, an exogenous

Here by an additive nonparametric model we mean

Z that is a common cause of X

that, for each endogenous variable, its expectation

be an instrument, is a semi-instrument for X andY,

and Y, which could not

given its parents is a linear combination of univari­

because both its effect on X, i.e., f, and its effect on

ate functions of its parents, plus some error term with

Y,

unknown distribution. We further assume that all ex­

ing the space of linear models to the space of additive

i.e., g, are linear functions. Therefore, by extend­

ogenous variables are independent, and that they are

models, we find that the instrument assumption does

independent of all the error terms. We do allow de­

impose some constraints on the possible kinds of ef­

pendence between the error terms of

X

and

Y.

Later

fects of

Y

Z

on Y: Only models where the effect of

Z

on

nonparametric model interchangeably.

compatible with the distribution implied by a model

Figure 1 and figure

2

give two very simple additive

models. We can take them

as

two alternative hypothe­

ses about the underlying model that generates a sam­
ple with observable variables
lem of testing whether

Z

Z,

X, and Y. The prob­

is an instrument for X and

Y is equivalent to the problem of determining which

where

is a linear function of the effect of

Z

in this paper we shall use additive model and additive

Z

on X are

is an instrument.

To test whether a random variable

Z

is a semi­

instrument for X and Y, theoretically we should check
whether all the four conditions are satisfied.

How­

ever, it turns out that not all these four conditions are
testable.

For example, the second condition, i.e., Z

CHU ET AL.

86

UAI2001

is an exogenous variable, cannot be tested if we only
have the joint distribution of

X, Y,

and

Z.

From the

joint distribution only, there is no way to tell whether

Z

and the error term associated with

X,

say,

t:x,

dependent.
On the other hand, we do have many cases where it is
reasonable to assume that the first three conditions are
satisfied. For example, it is typical that when testing
whether

Z

is an instrument for

X and

Ey

r

are

Y, we are deal­

1

s

---•Y

ing with an additive model described by conditions 1
to 4, and our question is whether

Z

is also a direct

cause ofY.
Assuming that the first three conditions are satisfied,
under certain smoothness conditions, to test whether
the fourth condition is also satisfied is equivalent to
testing whether

E[YjX, t:x] is a linear combination of
X and a univariate function

Figure

a univariate function of

3:

Two Common Causes

of t:x:
Theorem 1

Consider the additive model given by fig­
ure 2. Suppose that (X, t:x) has a joint density, and
that all the functions, i.e., J, g, s, and h, are differen­
tiable. 5 Then E[YIX, t:x] is a linear combination of
a univariate function of X and a univariate function
oft:x, and Var(YjZ,t:x) = Var(YIE[XIZ],t:x), if and
only if g(Z) = af(Z) + b for some pair of real numbers
(a, b).

4. If

The second step is the measurability test for the null
hypothesis that Var ( Y j Z, t:x)
1. Regress

X

2. Regress

Y on Z and t:x

The above theorem suggests an algorithm, which we

semi-instrument testing algorithm,

will call the

Z

3.

to test

test has two steps, the first step is the additivity test

4.

2. Regress

to estimate

Yon X and Z

t:x =X� E[XjZ]

with a surface smoother,

and score the fit of this model with some score
function that penalizes model complexity.

3.

Regress

Y

on

X

and

t:x,

with a surface smoother,

t:x

with an additive

smoother, and score the fit of this model with
some score function that penalizes model com­
plexity.
s
These two conditions are much stronger than what we
need. Actually, we only need to assume the boundary of
the support of the (X, t:x ) has no positive probability, (see
proof of Theorem 2.3 in Newey et a! (1999)), and that all
the functions are absolutely continuous. We choose these
two conditions because they are more intuitive.

Regress Y on

E(XIZJ

and

t:x

with a surface

If the regression of Y on

Z

and fX has a smaller

hypothesis. Otherwise, accept it. 6

3.2

X on Z

and

sum of residuals, i.e., RA < RN, reject the null

X and a univariate

t:x:

Regress

E[XjZ]

to estimate

uals.

for the null hypothesis thatE [Yj X, t:x] is a linear com­
bination of a univariate function of

1.

Z

smoother, and let RN be the sum of the resid­

is a semi-instrument for a sample S gener­

ated from an additive model given by figure 2. This

function of

on

= Var(YIE[XIZJ, t:x):

and let RA be the sum of the residuals.

For the proof, see the appendix.

whether

the additive model has a better score, accept

the null hypothesis. Otherwise, reject it.

TWO SEMI-INSTRUMENTS

If the test for whether a random variable, say, zl' is a
semi-instrument for

X andY

gives a negative result,

there is not much left to do with

Z1.

However, if the

test says that zl is a semi-instrument, we will face
another problem: Is

Z1

an instrument?

We have pointed out that this question cannot be an­
swered if we only have the joint distribution of

Z1, X,

andY. However, from the Bayesian point of view, with
some further assumption, if there is a second semi­
instrument, say, z2' we might be able to determine
6
Here we might also want to do a bootstrap estimation
of the distribution of RN- RA. If we adopt this approach,
we could generate bootstrap samples by adding permuta­
tions of the residuals obtained by regressing Y on E[XID]
and ex to the fitted values ofY obtained from the same
regression.

87

CHU ET AL.

UAI2001

whether Z1 and Z2 are both instruments. (If not both
of them are instrument, we would not be able to tell
whether both are non-instrumental, or only one is non­
instrumental. ) The following theorem gives the condi­
tion when two semi-instruments are both instruments
almost surely.
Let Z1 and Z2 be two independent ran­
dom variables that are both semi-instruments for X
and Y. Let a1 and a2 be the linear coefficients of Z1
and Z2 respectively. Suppose a1 and az are indepen­
dent, and each has a distribution function that has one
and only one point of discontinuity, 0. 7 If Z1 and Z2
have the same linear coefficients, then with probability
1, Z1 and Z2 are both instruments.

Theorem 2

For the proof, see the appendix.
Assume that the sample S was generated from the
causal structure illustrated in figure 3, and that both
Z1 and Z2 are semi-instruments for X andY. We can
use the following algorithm, called the double instru­
ments testing algorithm, to test whether Z1 and Z2
have the same linear coefficient: 8
1. Create a new variable Z ft (Z1) + h(Z2), where
ft(Zt) == E[XIZ1], !2(Z2) = E[XIZ2].
=

2. Test whether Z is a semi-instrument.
This algorithm is based on the following observation:
Assume that g1 and gz are differentiable, and that Z1
and Z2 have a joint density. Then g1(ZI) + 92(Z2) =
a(ft(Z1) + h (Z2)) + b for some (a,b) iff g1(Z1) =
aft (Z1) + b1 and g2(Z2) = af2(Z2) + b2 for some
b1 + b2 =b. 9

testing and double instrument testing. It turns out
that, in order for the semi-instrument testing to
work, we need to find a better additive regression
method that can handle dependent predictors, which
seems currently not available. However, the double­
instruments testing algorithm does work for a subset
of additive models: the models where the influence of
X on Y is linear. This subset includes a very impor­
tant class of models: the linear models. 10
4.1

SEMI-INSTRUMENT TESTING

The semi-instrument testing algorithm requires a sur­
face smoother and an additive smoother. We use the
Splus functions loess as the surface smoother, and
gam as the additive smoother. The gam function im­
plements the back-fitting algorithm proposed in Hastie
et al (1990). The loess function is an implementation
of the local polynomial regression. We use BIC score
function to score the fitted models returned by gam
and loess respectively.
We generate 6 samples from the following 2 models:

X = Z2 +t:x
2
Y; = X + ciZ3
where E[t:ylt:x]

=

�:i-, c1

1: gam

and

SIMULATION

size
200
1000
5000
200
1000
5000

We have done some simulation studies to estimate the
performance of the two algorithms for semi-instrument
7
Note that here by imposing a distribution on a1 and
a2, which are actually parameters of our models, we have
adopted a Bayesian perspective. Also, the conditions for
the distribution are stronger than required. What we really

need is to ensure that the P(at == a2) = P(at = a2 = 0) >
0. That is, we want to assume that it is possible that
a1 = a2, and if a1 = a2, it is almost sure that a1 = a2 = 0,
which means that both Zt and Z2 are instruments.
8
Note that from the classical point of view, this algo­
rithm can be used to reject the null hypothesis that Zt and
Z2 are both instruments in the cases where Zt and Z2 have
different linear coefficients. However, to make it a testing
algorithm for double instruments, a certain Bayesian as­
sumption about the prior distributions of the linear coeffi­
cients of Zt and Z2, like the one proposed in Theorem 2,
is required.
9
The proof of this observation is similar to that of The­
orem 1.

=o:

0, and c2 = 1.

These two models share the same Z, t:x, X, and fy,
differ in the effect of Z on Y, and hence differ in Y.
In the first model, Z is an instrument, hence a semi­
instrument. In the second model, Z is not a semi­
11 For each model, we generated 3 sam­
instrument.
ples with sizes 200, 1000, and 5000 respectively.
Table

4

+ Ey

model
1
1
1
2
2
2

loe ss
gam

models comparison

BIC
486.8
2530.3
12438.3
350. 9
1404. 6
6670.7

loess

BIC

239.4
1041.8
5053.4
238.8
1041.8
5053. 4

From the above data, we can see that the gam model is
10

The second algorithm works for the models where the
inftuence of X on Y is linear because in this case we can
modify the algorithm so that we do not need to apply ad­
ditive regression method to models with dependent predic­
tors. For a detailed discussion, see section 4.2. ·
11
The distribution of these variables are: Z is uniform

between 0 and 5. There is also a latent variable T that is
uniform between 0 and 2. EX is the sum ofT and a normal
noise with standard deviation 0.5, Ey is the sum of T2 and
a normal noise with standard deviation 0.5.

CHU ET AL.

88

always much worse than the loess model, no matter
whether

Z

to get

is a semi-instrument or not. This implies

2. Let Z

that no matter whether the null hypothesis is true,

Z

i.e.,

The most plausible explanation of

tx

and

X,

the performance of gam is signif­
4.

with some further conditions, we could still make the
double instruments testing algorithm work. Consider

Zr
Z2 are semi-instruments, we further assume that
s(X) = eX + d, i.e., the direct effect of X on Y is
a linear function in X, then we will be able to test
whether Z1 and Z2 have the same linear coefficients.
the rnodel given by figure 3. If besides assuming

and

a2h(Z2) + b2,

and do

=eX+d + ty

+

Y

Z1, Z2,

where

Z1, Z2

Y

=

Let Z

=

!t(Zt) + h(Z2),

Var(YIZr , Z2)

=

� Var ( ctx + ty)

+

are

ar

ctx

+

models:

E[tyJtx]

+ Ey
=

t�, and Ct

=

0, e2

=

0.2, and C3

=

1.

€y'

Z2, tx, X, and
z 1 and z2'
model, Z1 and Z2

but differ in the linear coefficients of

and hence differ in

Y.

In the first

have the same linear coefficients. In the second model,
there is a small difference in the linear coefficients. In
13

For

Table 2: Values of BICa -BlOt for the 12 samples
sample size

Ct =

50

11.35

0

C2 =

0.2

C3

=

1

8.79

-95.57

100

14.23

1.60

-333.81

200

13.07

-2.53

-419.26

500

19.10

-35.56

-1233.05

The entries of the above table are the values of BIC1BIGa for each sample.

Var (ar !t (Zt) + a2!2(Z2)jZ )

A positive

value means that the

null hypothesis is accepted for that sample. From the
table, we can see that to detect the significant differ­

a1

=

ence between the linear coefficients of

a2.

double instruments testing algorithm,

Zt

and

Z2,

a

sample of size 50 is sufficient. But when the difference

The following algorithm, which is called the

linear

is small, we need a sample size of 200.

compares the

mean square error of regressing Y on

Z1

Z2,
Z =

and

with the mean square error of regressing Y on

DISCUSSION AND FUTURE

5

WORK

It can be used to test whether the two

semi-instruments Z1 and Z2 have the same linear coef­

ficients, assuming the direct effect of
in

3

it is easy to see that:

with the equality holds only when

E [X IZt , Z2].

have the same linear coefficients if

These three models share the same Z r,

ty + do.

Var(YjZ)

=

where

=

Var(etx + ty)
+

Z2

Yi = X + CiZi

we further have:

(c + ai)[ft(Zr) + h(Z2)]

and

X= z; + Zi +Ex

r:y.

If Z1 and Z2 have the same linear coefficient, i.e.,

a2, 12

using additive nonpara­

200, and 500 respectively.

independent of each other, and jointly independent of
r:x an

Z2

each model, we generated 4 samples with sizes 50, 100,

9r(Z l) + 9 2(Z2)

is additive in

and

the third model, the difference is significant.

= (c + at)ft(ZI) + (c + a2}h(Z2) + ctx + ty + do.
That is,

Z1

from the following

Despite the lack of good additive regression method,

Y

Z1

on

To test the above algorithm, we generated 12 samples

DOUBLE INSTRUMENTS TESTING

=

Y

BIC1 < BICa.

and in some cases fails to converge.

92(Zr) = atfr(Zr) + br, g2(Z2)
= br + b2 + d we have:

Regress Yon Z using linear

BICa.

back-fitting algorithm often gets trapped in a plateau,

Let

E[XJZr, Z2].

metric regression, and compute the BIC score

icantly worse than that of loess. It seems that the

4.2

=

3. Regress

this phenomenon is that because of the dependence
between

E[XJZr, Z2].

regression, and compute the BIC score BIC1.

is a semi-instrument, the test procedure will

always reject it!

UAI2001

X

on Y is linear

X:

5.1

THREE ASSUMPTIONS

The semi-instrument testing algorithm assumes that
the first three semi-instrumental conditions are sat-

1.

Use additive regression to regress X on Zt and

Z2

2
1 Note that by Theorem 2, under certain distribution
assumptions, a1 = a2 implies that a1 = a2 = 0 w.p.l., i.e.,

both zl and z2 are instruments.

13The distribution of these variables are: Z1 and Z2 both

are uniform between 0 and 4. There is also a latent variable

T that is uniform between 0 and 2.

€X

is the sum ofT and

a normal noise with standard deviation 0.5, €y is the sum
of T2 and a normal noise with standard deviation 0.5.

UAI 2001

CHU ET AL.

isfied. While in general we cannot test whether
a random variable satisfies all the first three semi­
instrumental conditions, it is interesting to know
whether we can test for one of them, especially the
second semi-instrumental condition: Z is an exogenous
variable. The answer is: in principle, this assumption
can be tested by the method of instrument, if we have
an instrument for Z and X. However, it is easy to see
that this will lead to an infinite regression.
Another assumption key to the double instruments
testing algorithm is: The prior probability that Z is
instrumental is positive, while the prior probability is
0 for a semi-instrument to have linear coefficient a if
a -::/::- 0.
This raises a question: Why does the value
0 have a special status in the range of possible values
of the linear coefficients of a semi-instrument? Here
we want to give an argument for the plausibility of
this assumption: If we take the set of possible causal
structures among X, Y, Z1 and Z2 as a discrete sample
space, it is reasonable to assign a positive prior prob­
ability to one element in the space, i.e., the structure
where both Z1 and Z2 are instruments, which means
that both Z1 and Z2 have linear coefficients 0. On the
other hand, if a semi-instrument is not an instrument,
there is no specific reason to believe that its linear co­
efficient should take any specific non-zero value.
We make a third assumption in an effort to modify the
double instruments testing algorithm so that it has suf­
ficient power: We assume that the direct effect of X
on Y is a linear function of X. We notice that this is a
rather strong assumption for an additive model. More­
over, because currently we do not have a suitable addi­
tive regression method for the semi-instrument testing,
we also have to assume, without any testing, that Z1
and Zz are semi-instruments. Nevertheless, the mod­
ified double instruments testing algorithm is general
enough to provide a double instruments test for linear
models.
5.2

-;

FUTURE WORK

To make the semi-instrument testing powerful, we will
continue to look for some additive regression method
that is suitable for the case where the predictors are
dependent. 14 Alternatively, we may also try to find
some new ways of testing semi-instruments where the
problem of the dependence of the predictors will not
significantly affect the test results.
14
Tom Minka suggested that by letting the back-fitting
algorithm run for a sufficient number of iterations, it will
eventually return a good fitted model, or we can use least

squares to the the best fitted model directly ( without iter­
ations ) . We have yet to test whether this will work.

89

Acknowledgments

This paper was supported by NSF grant DMS9873442.
Reference

[1] Bartels, L. (1991), "Instrumental and 'Quasi­
Instrumental' Variables," in American Journal of Po­
litical Science, 35, 777-800.
[2] Bound, J., Jaeger, D., Baker, R. (1995), "Prob­
lems with Instrumental Variables Estimation When
the Correlation Between the Instruments and the En­
dogenous Explanatory Variable is Weak," in Journal
of the American Statistical Ass oc iat ion, Vol. 90, 443450.
[3] Fennessey, J., d'Amico, R. (1980), "Collinearity,
Ridge Regression, and Investigator Judgment," in So­
ciological Methods and Research 8, 309-340.
[4] Gozalo, P., Linton, 0. (2001) "Testing Additiv­
ity in Generalized Nonparametric Regression Models
with Estimated Parameters," forthcoming in Journal
of Econometrics.

[5] Hastie, T., Tibshirani, R. (1990), Generalized Ad­
ditive Models. New York : Chapman and Hall.
[6] Heckman, J. (1995), "Instrumental Variables: A
Cautionary Tale," NBER Technical Working Paper
No. 185.

[7] Magdalinos, M. (1994), "Testing Instrument Ad­
missibility: Some Refined Asymptotic Results", in
Econometrica Vol. 62, 373-403
[8] Nelson, C., Startz, R. (1990), "The Distribution of
the Instrumental Variables Estimator and Its t-Ratio
when the Instrument is a Poor One," in Journal of
Business, vol. 63, S125-S140.
[9] Newey, W. (1985), "Generalized Method of Mo­
ments Specification Testing," in Journal of Economet­
rics, Vol. 29, 229-256.
[10] Newey, W., Powell, J., Vella, F . (1999), "Nonpara­
metric Estimation of Triangular Simultaneous Equa­
tions Models," in Econometrica Vol. 67, 565-603.

[11] Pearl, J. (1995) "On the Testability of Causal
Models with Latent and Instrumental Variables," in
P. Besnard and S. Hanks (Eds.), Uncertainty in Ar­
tificial Intelligence 11, 435-443. San Francisco, CA:
Morgan Kaufmann
[12] Shea, J. (1997), "Instrument Relevance in Multi­
variate Linear Models: A Simple Measure," in Review
of Economics and Statistics Vol. 79, 348-352.
(13] Spirtes, P., Glymour, C. and Scheines, R. (2001).

CHU ET AL.

90

Causation, Prediction, and Search,

2nd ed. New York,

N.Y.: MIT Press.

[14] Wu, D. (1973), "Alternative Tests oflndependence
Between Stochastic Regressors and Disturbances," in
Econometrica Vol. 41, 733-750.
[15] Zivot, E., Startz, R., Nelson, C., (1998), "Valid
Confidence Intervals and Inference in the Presence of
Weak Instruments," in International Economic Re­
view, Vol. 39, 1119-44.
Appendix
Proof of Theorem 1 .

Because (X, Ex) has a joint density, and j, g, s , and h
are differentiable, it immediately follows that E[Y]X =
x, Ex = u] is differentiable with respect to X and t:x
with probability one.
Also note that E[Y]X

=

x, Ex =u]

=E[s(X) +g(Z)) +t:y]X =x, EX = u]
=

s(x) +E[g(Z)]j(Z)

=

x

�

u]

+

h(u),

because conditional on f(Z) =X� Ex =x
is independent of X =x and t:x =u

�

u,

g(Z)

It is easy to see that if g(Z) aj(Z) +b, i.e., the direct
effect of Z on Y is a linear function of the direct effect
of Z on X, then with probability one, E[Y]X, t:x] =
s(X) +aX + h(Ex) � at:x + b, i.e., E[Y]X, t:x] is a
linear combination of a univariate function of X and
a univariate function of t:x. Moreover, we have:
=

Var(Y]E[X]Z], Ex)
=Var(s(j(Z) +EX) +g(Z) +Ey lf(Z), Ex )
=

=

Var(t:ylf(Z), ex)

=

Var(t:yjt:x)

Var(Ey]Z,t:x) = Var(Y]Z,t:x)

To show the converse, suppose:
E[Y]X

=

x,t:x =u]

Let g0(x� u)

=

=

s1 (x) +h1(u).

E[g(Z)]f(Z)

= x

�

] we have:

u ,

8go(x u)
1
d
= 9o(x� u)
d (sl ( x) � s(x))
x
ax
:::::::> nb ( x� u) is constant in u
�

=

:::::::>

ub is a constant

:::::::>

go is a linear function

Note that the assumption that Var(YjE[XjZ], t:x) =
Var(Y]Z,ex) implies that Var(g(Z)]j(Z)) = 0, which
again implies that g(Z) E[g(Z)]j(Z)] w.p.l. There­
fore, we have:
=

g(Z)

=

af(Z) +b, where a, bare constants.

UAI2001

Proof of Theorem 2 .

Let L1 be the linear coefficient of Z1 , L2 the linear
coefficient of Z2, and f-lL, the distribution of £1. Then:

P(L t
P(Lt

=

=

£2

=

O]Lt

=

L2, L1 =/: 0)

£ 2)

=

=

P(Lt L2 0)
= 1, for:
P(L1 L2)
=

fn�.\{O} P(L2

=

=

=

lt)df-lL1 (lt)

=

0

