Gather
Evidence

Evidence

&

Figure

Recommen-

dations

1:

An illustration of the context

a property of large domains with respect to human
cognitive activity.
There are fixed 'natural' subdomains. At
any time, a human reasoner focuses attention
at only one of them. He can acquire evidence
from and form queries about only one of them
at a time. He may shift attention from one
to another from time to time.

WHAT IS LOCALIZATION?

localization

t

System
User

System

LOCALIZATION

define informally what we will call

~t

Actions

Expert

We consider the following general context where an ex­
pert system is to be used (Figure 1): The human user
plays the central role between an expert system and a
real world domain. To know the state o f the domain
{e.g. diagnosis), the user gathers evidence from the
domain and enters the evidence along with queries to
the expert system. The system may provide a recom­
mendation or may prompt further gathering of infor­
mation. We would like the system to be efficient in
inference computation. How can this aim be realized
when the domain is large?
We

Queries

� t

as

Localization can be seen in many large domains. For
example, in many medical domains (e.g. neuromuscu­
lar diagnosis [Xiang et al. 19911) medical practitioners
acquire information about a patient by history taking,
physical examination, and performing a set of special­
ized tests. Each such activity involves a subdomain
which contains possibly dozens of alternative ques­
tions, procedures, test sites, etc., and diseases which
can be differentiated. Each such subdomain can hold
the person's attention for a period of time. During
this period, he updates his belief on disease hypothe­
ses based on acquired evidence, carefully weighs the

Exp l ori ng Localization

importance of alternative means to gather information
under the current situation, and selects the best alter­
native to perform next. Although one subdomain may
have an influence on another, the influence is summa­
rized by the common disease hypotheses which they
both can (partially) differentiate.
We distinguish 'interesting' variables from 'relevant'
variables. Call the set U of variables in a subdomain
'interesting' to the human reasoner if this subdomain
captures his current attention. A set V of variables
outside the current subdomain may have a bearing on
U due to two reasons. First, the background knowl­
edge on U may provide partial background knowledge
on V. Second, obtained evidence on U may change
one's belief on V. Thus V is 'relevant' to U but is
not currently 'interesting' to the reasoner. However,
we can often find a set I (I C U) of variables which
summarizes all the influence on U from V such that V
can be discarded from the reasoner's current attention.
This issue is treated more formally in Section 4.1.
1.2

IS LOCALIZATION USEFUL?

W hen localization exists, a large domain can be rep­
resented in an expert system according to the natu­
ral subdomains. Each subdomain is represented by a
subsystem. During a consultation session, only the
subsystem corresponding to the current subdomain
consumes computational resources. This subsystem is
called active. When a user's attention shifts to a differ­
ent subdomain, the evidence acquired in the previously
active subsystem can be absorbed by the newly ac­
tive subsystem through summarizing variables. Since
no computational resources are consumed by the in­
active subsystems, computational savings can be ob­
tained without loss of inference accuracy.
Our observation of localization was made based on our
experience in the domain of neuromuscular diagnosis
(ib.]. We believe that it is common in many large do­
mains. How to exploit it when it arises, how to con­
struct the above ideal representation in the context of
Bayesian networks, and how to guarantee the correct­
ness of inference in the representation is the subject of
this paper.
2

in Bayesian Networks for Large Expert Systems

halter 1988; Jensen, Lauritzen and Olesen 1990a;
Baker and Boult 1990; Suermondt, Cooper and Heck­
erman 1990).
This paper takes the exact approach. For general but
sparse nets, efficient computation has been achieved by
creating a secondary directed [Lauritzen and Spiegel­
halter 1988], or undirected clique tree (junction tree)
!Jensen, Lauritzen and Olesen 1990a) structure, which
also offers the advantage of trading compile time with
running time for expert systems. Both methods and
many others are based on a net representation which
does not consider domain structure and lumps all vari­
ables into a homogeneous network.
Pruning Bayesian nets with respect to each query in­
stance is another exact method with savings in com­
putational cost [Baker and Boult 1990). The method
does not support incremental evidence (i.e. all evi­
dence must be entered at one time).
Heckerman [1990b] partitions Bayesian nets into small
groups of naturally related variables to ease the con­
struction of large networks. But once the construction
is finished, the run time representation is still homo­
geneous.
Suermondt, Cooper and Heckerman (1990] combine
cutset conditioning with the clique tree method and
convert the original net into a set of clique trees to
obtain computational savings. The cutset is chosen
mainly based on net topology. It does not lead to the
exploration of localization in general.
2.2

'OBVIOUS' WAYS TO EXPLORE
LOCALIZATION

Splitting homogeneous nets

One obvious way to explore localization is to split a
homogeneous Bayesian net into a set of subnets ac­
cording to localization. Each subnet can then be used
as a separate computational object. This is not always
workable as is shown by the following example.

EXPLORE LOCALIZATION IN
BAYESIAN NETS

2.1

B ACKGROUND

Cooper (1990] has shown that probabilistic inference in
a general Bayesian net is NP-hard. Several different
approaches have been pursued to avoid combinatorial
explosion for typical cases, and thus to reduce compu­
tational cost. Two classes of approaches can be identi­
fied. One class explores approximation [Hention 1988;
Pearl 1988; Jensen and Andersen 1990]. Another class
explores specificity in computing exact probabilities
[Pearl 1986; Heckerman 1990a; Lauritzen and Spiegel-

Figure 2: Left: a DAG D. Right: A set of subnets
formed by sectioning D.
Suppose the directed acyclic graph (DAG) D in
Figure 2 is split according to localization into
{Dl, D2, D3}. Suppose variable G is instantiated by
evidence. According to d-separation [Pearl 1988], now

345

346

Xiang, Poole, and Beddoes

both paths between E and F are active. Therefore,
in order to pass a new piece of evidence on E to F,
the joint distribution on { B, C} needs to be passed
from {D1,D2} to D3 1• However, this is not possible
because neither D1 nor D2 contains this joint distribu­
tion. This shows that arbitrarily partitioning Bayesian
nets causes loss of information and is incorrect in gen­
eral.

Figure

3:

An unsectioned Bayesian net.

Splitting the junction tree

Another obvious way to explore localization is to pre­
serve localization within subtrees of a junction tree
[Jensen, Lauritzen and Olesen 1990a] by clever choice
in triangulation and junction tree construction. If this
can be done, the junction tree can be split and each
subtree can be used as a separate computational ob­
ject. The following example shows that this is also not
always workable. Consider the DAG 0 in Figure 3.
Suppose variables in the DAG form three naturally
related groups which satisfy localization:
Gr
G2
Ga

=
=

=

{Ar,A2,Aa,Hr,H2,Ha,H4}
{FI,F2,HI,H2}
{E1,E2, £3,H2, H3,Ho�,}

We would like to construct a junction tree which would
preserve the localization within three subtrees. The
graph <pin Figure 4 is the moral graph of 0. Only the
cycle A3 - H3 - E3 E1 - H4 - A3 needs to be trian­
gulated. There are six distinct ways of triangulation
out of which only two do not mix nodes in different
groups. The two triangulations have the link (Ha,H4)
in common but they do ;wt make a significant differ­
ence in the following analysis. The graph A in Figure 4
shows one of the two triangulations. All the cliques in
A appear as nodes of graph r.
-

The junction tree r does not preserve localization since
cliques 3, 4, 5 and 8 correspond to group G1 but are
connected via cliques 6 and 7 which contain Ea from
group Ga. This is unavoidable. When there is evi­
dence for A1 or A2 in A, updating the belief in group
G3 requires passing the joint distribution of H2 and
1 Passing only the marginal distributions on B and on
C is not correct.

r

S-�
Figure 4: .P: the moral graph of 0 in Figure 3. A:
a triangulated graph of ci>. r: the junction tree con­
structed from A.
H3• But updating the belief in Aa only requires pass­
ing the marginal distribution of H3. That is to say,
updating the belief in A3 needs less information than
group G3• In the junction tree representation, this be­
comes a path from cliques 3, 4 and 5 to clique 8 via
cliques 6 and 7.

In general, let X and Y be two sets of variables in the
same natural group, and let Z be a set of variables in
a distinct group. Suppose the information exchange
between pairs of them requires the exchange of distri­
bution on sets lxy, lxz and lyz of variables respec­
tively. Sometime lxy is a subset of both lxz and ]yz.
When this is the case, a junction tree representation
will always indirectly connect cliques corresponding to
X and Y through cliques corresponding to Z if the
method in Jensen, Lauritzen and Olesen [ 1990a] is fol­
lowed.
A brute force method

There is, however, a way around the problem with a
brute force method. In the above example, when there
is evidence for A1 or A2, the brute force method pre­
tends that updating the belief in A3 needs as much
information as Ga. What one does is to add a dummy
link (H2,A3) to the moral graph <I> in Figure 4. Then

Exploring Localization in Bayesian Networks for Large Expert Systems

triangulating the augmented graph gives the graph A'
in Figure 5. The resultant junction tree r' in Fig­
ure 5 does have three subtrees Which correspond to
the three groups desired. However, the largest cliques
now have size four instead of three as before. In the bi­
nary case, the size of the total state space is 84 instead
of 76 as before.
In general, the brute force method preserves natural lo­
calization by congregation of a set of interfacing nodes
(nodes H2, H3, H4 above) between natural groups. In
this way, the joint distribution on interfacing nodes
can be passed between groups, and preservation of lo­
calization and preservation of tree structure can be
compatible. However, in a large application domain
with the original network sparse, this will greatly in­
crease the amount of computation in each group due to
the exponential enlargement of the clique state space.
The required increase of computation could outweigh
the savings gained by exploring localization in general.

3

MULTIPLY SECTIONED
BAYESIAN NETS

This section introduces a knowledge representation
formalism, Multiply Sectioned Bayesian Networks
(MSBNs), as our solution to explore localization.
We want to partition a large domain according to nat­
ural localization into subdomains such that each can
be represented separately by a Bayesian subnet. Each
subnet then stands as a computational object, and dif­
ferent subnets cooperate with each other during atten­
tion shift by exchanging a small amount of information
between them. We call such a set of subnets a MSBN.
The construction of a MSBN can be formulated con­
ceptually in the opposite direction. Suppose the do­
main has been represented with a homogeneous net­
work. We frequently refer to a homogeneous net as an
UnSectioned Bayesian network (USBN). A MSBN is a
set of Bayesian subnets resulted from the sectioning of
the corresponding USBN.
For example the DAG 0 in Figure 3 is sectioned into
{81,82, 03} in Figure 6 according to the localization
described in Section 2.2. A variable shared by 'ad­
jacent' subnets appears in both subnets. The set of
shared variables is subject to a technical constraint,
in addition to localization, as will be discussed in Sec­
tion 4.
,

Figure 5: A' is a triangulated graph. r' is a junction
tree of A'.
The trouble illustrated in the above two situations can
be traced to the tree structure of a junction tree repre­
sentation which requites a single path between any two
cliques in the tree. In the normal triangulation case,
one has small cliques but one loses localization. In
the brute force case, one preserves localization but one
does not have small cliques. To summarize, the preser­
vation of natural localization and small cliques can not
coexist by the method of Andersen et al. [1989J and
Jensen, Lauritzen and Olesen [ 1990a]. It is claimed
here that this is due to a single information channel
between local groups of variables. This paper present
a representation which, by introducing multiple infor­
mation channels between groups and by exploring con­
ditional independence, allows passing the joint distri­
bution on a set of interfacing variables b e tween groups
by passing only marginal distributions on subsets of
the set.

Figure 6: The set {81, 82, 03} forms a MSBN for the
USBN 0 in Figure 3.

3.1

TRANSFORMATION OF MSBNS
INTO JUNCTION FORES TS

The junction tree representation [Andersen et al. 1989;
Jensen, Lauritzen and Olesen 1990a] allows efficient
computation for general but sparse networks in expert
systems. Thus it is desirable to transform each subnet
of a MSBN into a junction tree. The resultant set of
junction trees is called a junction forest.
For example, the MSBN {8\ 82, 03} in Fi�ure 6 is
transformed into the junction forest {r1' r 'r3} in
Figure 7. Omit the ribbed bands for the moment which
will be introduced shortly.
In order to propagate evidence between junction trees
during attention shift, information channeJs need to

be created between them. As discussed in Section 2.2,
multiple channels are required in general to preserve

347

348

Xiang, Poole, and Beddoes

probabilities in this tree are the same as in a globally
consistent junction forest [Xiang, Poole and Beddoes
1992]. It is this feature of MSBNsfjunction forests
that allows the exploitation of localization.

Figure 7: The set {f1, rz, f3} is a junction forest
transformed from the MSBN {81, 82, e3} in Figure 6.
Linkages between junction trees are shown by ribbed
bands.
both localization and a small clique size. These chan­
nels are called linkages between the two junction trees
being connected. Each linkage connects two cliques in
different trees. The two cliques being connected are
called the host cliques of the linkage. A linkage is the
intersection of its two host cliques. Host cliques are
selected such that the union of linkages is the set of in­
terfacing variables between the corresponding subnets,
and each linkage is maximal. With multiple linkages
created between junction trees, we have a linked junc­
tion forest.

For example, in Figure 7, linkages between junction
trees are indicated with ribbed bands connecting the
corresponding host cliques. There is only one linkage
between clique 1 of f1 and clique 2 of f2' namely,
{H1,H2}. The two linkages between f1 and f3 are
{H2, H3} and {H3, H4}.
Up to here, we have only discussed the manipula­
tions of graphical structures of a MSBN. As other
approaches based on secondary structures [Lauritzen
and Spie&elhalter 1988; Jensen, Lauritzen and Ole­
sen 1990aj, there needs to be a corresponding conver­
sion from the probability distribution of the MSBN to
the belief table (potential) of the linked junction for­
est. Readers are refened to Xiang, Poole and Beddoes
[1992] for details regarding the conversion.
3.2

EVIDENTIAL REASONING

Afte1· a l inked junction fores t is created, it becomes the
permanent representation of the corresponding MSBN.
The evidential reasoning during a consultation session
will be performed solely in the junction forest.
Due to localization, only one junction tree in a junc­
tion forest is active during evidential reasoning. When
new evidence becomes available to the currently active
junction tree, it is entered and the tree is made consis­
tent. The operations to enter evidence and to main­
tain consistency within a junction tree are the same as
Jensen, Lauritzen and Olesen [1990a]. We only main­
tain consistency in the currently active tree. All the

When the user shifts attention from the currently ac­
tive tree to a 'destination' tree, all previously acquired
evidence is absorbed through an operation ShiftAt­
tention. The operation swaps in and out sequentially
a chain of 'intermediate' junction trees between the
currently active tree and the destination tree. It has
been shown (ib.} that, with a properly structured junc­
tion forest, the following is true.
Start with any active junction tree in a
globally consistent junction forest. Repeat
the following cycle a finite number of times:
1. Enter evidence to the currently active
tree and make the tree consistent a fi­
nite number of times.
2. Use ShiftAttention to shift attention
to any destination tree.
The marginal distributions obtained in
the final active tree are identical to those of
a globally consistent forest.
The above property shows the most important charac­
terization of MSBNs and junction forests, namely, the
capability of exploiting localization to reduce the com­
putational cost. Note that the above statement only
requires the initial global consistency of the junction
forest.
With localization, the user's interest and new evidence
remain in the sphere of one junction tree for a pe­
riod of time. Thus the time and space requirement,
while reasoning within a junction tree, is bounded
above by what is required by the largest junction tree.
The judgments obtained take into account all the rele­
vant background knowledge and evidence. Compared
to the USBN and the single junction tree representa­
tion where each piece of evidence has to be propagated
through the entire system, this leads to computational
savings.
When the user shifts interest to another set of vari­
ables contained in a different destination tree, only
the intermediate trees need to be updated. The time
required is linear to the number of intermediate trees
and to the number of linkages between each pair of
neighbours [ib.]. No matter how large the entire junc­
tion forest, the time requirement for attention shift
is fixed once the destination tree and mediating trees
are fixed. The space requirement is upper bounded
by what is needed by the largest junction tree. With
localization, the computational cost for attention shift
is incurred only occasionally.
Given the above analysis, the computational complex­
ity of evidential reasoning in a MSBN with jJ subnets
of equal size is about 1//3 of the corresponding USBN
system given localization. The actual time require-

Exploring Localization in Bayesian Networks for Large Expert Systems

ment is a little more than 1/ f3 due to the computation
required for attention shift. The actual space require­
ment is a little more than 1 / f3 due to the repetition of
interfacing nodes.

Section 2.2 has shown that we cannot divide a homo­
geneous Bayesian net or its junction tree arbitrarily
in order to explore localization. This section discusses
major technical issues in the MSBN/junction forest
representation.

For example, the sectioning in Figure 2 satisfies the
d-sepset condition, but the resultant MSBN does not
guarantee correct inference. This is because the sec­
tioning has an unsound overall organization of subnets.
Intuitively, the overall structure of a MSBN should en­
sure that evidence acquired in any subnet be able to
propagate to a different subnet by a unique chain of
subnets. In the example of Figure 2, after a piece of
evidence is available on G, a new piece of evidence on
E has to propagate to F through two different chains
of subnets D2 - D3 and D2 - D1 - D3• This violates
the above requirement and causes the problem. The
issue of overall structure is treated formally in (ib.J.

4.1

4.3

4

TECHNICAL ISSUES

INTERFACE BETWEEN S UBNETS

Localization does not dictate exactly what should be
the boundary between different subnets. The intuitive
criterion is that the interface should allow evidence
acquired to be propagated to adjacent subnets dur­
ing attention shift by a small amount of information
exchange. We define d-sepset as the criterion of inter­
face, which makes use of Pearl's d-separation concept
[Pearll988]. We denote the union D of DAGs D1 and
D2 by D = D1 UD2 = (N1 uN2,E1 u E2).
Definition 4.1 (d-sepset) Let D = D1 U D2 be a
DAG. The set of nodes I = N1 n N2 is a d-sepset
between subDAG D1 and D2 if the following condition
holds.
For every A; E I with its parents 1!"; in
either 1!"; <;:;; N1, or 1!"; <;:;; N2•

D,

Elements of a d-sepset are called d-sepnodes. When
the above condition holds, D is said to be sectioned
into {Dl,D2}.

The following theorem and corollary [Xiang, Poole and
Beddoes 1992)2 say that a d-sepset d-separates subnets
in a MSBN and is a sufficient information channel.

MORAL I-TRIANGULATION BY
LOCAL COMPUTATION

Transformation of a MSBN into a junction forest
requires moralization and triangulation conceptually
the same way as the other approaches based on sec­
ondary structures [Lauritzen and Spiegelhalter 1988;
Andersen et al. 1989; Jensen, Lauritzen and Olesen
1 990a.}. However, in the MSBN context, the transfor­
mation can be performed globally or by local compu­
tation at the level of the subnets. The global compu­
tation performs moralization and triangulation in the
same way as the other approaches with care not to mix
the nodes of distinct subnets into one clique. An addi­
tional mapping of the resultant moralized and triangu­
lated graph into subgraphs corresponding to the sub­
nets is needed. But where space saving is concerned,
local computation is desired.
Since the number of parents for a d-sepnode may
be different for different subnets, the moralization in
MSBN cannot be achieved by 'pure' local computation
in each subnet. Communication between the subnets
is required to ensure that the parents of d-sepnodes
are moralized identically in different subnets.

The d-sepset criterion concerns with the interface be­
tween each pair of subnets. This is not sufficient for a
workable MSBN.

The criterion of triangulation in a MSBN is to en­
sure the 'int ac tness ' of a resulting hypergra.ph f rom
the corresponding homogeneous net. Problems arise
if one insists on triangulation by local computation at
the level of subnets. One problem is that an inter­
subnet cycle will be triangulated in the homogeneous
net, but the cycle cannot be identified by examining
each of the subnets involved individually. Another
problem is that cycles involving d-sepnodes may be
triangulated differently in different subnets. The so­
lution is to let the subnets communicate during tri­
angulation. Since moralization and triangulation both
involve adding links and both require communication
between subnets, the corresponding local operations in
each subnet can be performed together and messages
to other subnets can be sent together. Therefore, oper­
ationally, moralization and triangulation in MSBN are
not separate steps as in the single junction tree repre­
sentation. The corresponding integrated operation is
t ermed morali-triangulation to reflect this fac t.

2They are simplified here to the MSBN of two subnets.

For example, the MSBN in Figure 6 is morali-

Theorem 4.2 Let a DAG
{ D1, D2} and I = N1 nN2 be
N1 \ I from N2 \ I.

a

D be sectioned into
d-sepset. I d-separates

Co r ollary 4.3 Let (D, P) be a Bayesian net, D be
sectioned into { D1, D2}, and I = N1 n N2 be the d­
sepset. When evidence is available at variables in N1,
the propagation of the joint distribution on I from D1
to D 2 is sufficient in order to obtain posterior distri­
bution on N2•
4.2

OVERALL S TRUCTURE OF MSBNS

349

350

Xiang, Poole,

and Beddoes

Figure 8: Morali-triangulated graphs of the MSBN in Figure 6. The meaning of the different line-types is
explained in Section 4.3.
triangulated to the graphs in Figure 8. Thin solid
lines ( e.g. (A1, HI)) are from the original arcs by drop­
ping directions. Thin dotted lines (e .g. ( A1, A2)) are
links added by local moralization. Thick dotted lines
(e.g. (H1,H2)) are 'moral' links added through com­
munication. Thin dashed lines (e.g. (H3,H4)) are
added through communication for triangulation. The
thick solid line ((Ea, H4)) is added by local triangula­
tion. A formal treatment and an algorithm for morali­
triangulation are given in Xiang, Poole and Beddoes
[1992].
4.4

PROPAGATING INFORMATION
THROUGH MULTIPLE L INKAGES

Propagating information between junction trees of a
junction forest is required in two different situations:
belief initialization and evidential reasoning. In both
cases, information needs to be propagated between
junction trees of a junction forest through multiple
linkages. Care is to be taken against potential errors.

Belief initialization serves the same purpose as in other
approaches based on secondary structures [Lauritzen
and S iegelhalter 1988; Jensen, Lauritzen and Olesen
1990a . It establishes the global consistency before any
evidence is available. This requires the propagation
of knowledge stored in each junction tree to the rest
of the forest. When doing so, redundant information
could be passed through multiple linkages. We must
make sure that the information is passed only once.

f

In evidential reasoning, evidence acquired in one junc­
tion tree needs to be propagated to the destination
tree during attention shift. The potential error in this
case takes a different form from the case of initializa­
tion. Passing information through multiple linkages
from one junction tree to another can 'confuse' there­
ceiving tree such that the correct consistency between

the two cannot be established.
Detailed illustrations of these potential problems and
the operations which avoid them are given in Xiang,
Poole and Beddoes [1992].
5

CONCLUSION

This paper overviews MSBNs and junction forests as
a flexible knowledge representation and as an efficient
inference formalism. This formalism is suitable for ex­
pert systems which reason about uncertain knowledge
in large domains where localization exists.
MSBNs allow partitioning of a large domain into
smaller natural subdomains such that each of them can
be represented as a Bayesian subnet, and can be tested
and refined individually. This makes the representa­
tion of a complex domain easier for knowledge engi­
neers and may make the resultant system more natural
and more understandable to system users. The mod­
ularity facilitates implementation of large systems in
an incremental fashion. When partitioning, a knowl­
edge engineer has to take into account the technical
constraints imposed by MSBNs which are not very re­
strictive.
Each subnet in the MSBN is transformed into a junc­
tion tree such that the MSBN is transformed into a
junction forest where evide ntial reasoning takes place.
Each subnet/junction tree in the MSBN /junction for­
est stands as a separate computational object. Since
the representation allows transformation by local com­
putation at the level of subnets, and allows reason­
ing to be conducted with junction trees, the space re­
quirement is governed by the size of the largest sub­
net/junction tree. Hence large applications can be
built and run on relatively small computers wherever
hardware resources are of concern. This was, in fact,

Exploring Localization in Bayesian Networks for Large Expert Systems

our original motivation for developing the MSBN rep·
resentation.
During a consultation session, the MSBN representa·
tion allows only the 'interesting' junction tree to be
loaded while the rest of the forest remains inactive and
uses no computational resources. The judgments made
on variables in the active tree are consistent with all
the knowledge available, including both prior knowl­
edge and all the evidence contained in the entire forest.
When the user's attention shifts, inactive trees can be
made active and previous accumulation of evidence is
preserved. This is achieved by passing the joint beliefs
on d-sepsets. The overall computational resources re­
quired are governed by the size of the largest subnet,
and not by the size of the application domain.
The MSBN has been applied to an expert system
PAINULIM for diagnosis of neuromuscular diseases
characterized
a painful or impaired upper limb [Xi­
ang et al. 1991 .

br.

The MSBN representation makes the localization as­
sumption about the large domain being represented.
Our justification of the generality of localization has
been intuitive and has been based on our experience
in PAINULIM. We are prepared to test its generality
in other large domains.
Acknowledgements

This work is supported by Operating Grants A3290,
OGP0044121 and OGP0090307 from NSERC, and
CRD3474 from the Centre for Systems Science at SFU.
We are grateful to Stefan Joseph and anonymous re­
viewers for helpful comments to an earlier draft.
References

[Andersen et al. 19891 S.K. Andersen, K.G. Olesen,
F.V. Jensen and F. Jensen, BUGIN - a shell for
building Bayesian belief universes for expert sys­
tems. Proc. 11 IJCAI, Detroit, Michigan, Vol. 2,
pp. 1080-1085.
[Baker and Boult 1990) M. Baker and T.E. Boult,
Pruning Bayesian networks for efficient computa·
tion. Proc. 6 Conf. Uncertainty in AI, Cambridge,
Mass., pp. 257-264.
[Cooper 1990] G.F. Cooper, The computational com­
plexity of probabilistic inference using Bayesian be­
lief networks. Artificial Intelligence, 42, pp. 393-405.
[Beckerman 1990a] D. Beckerman, A tractable infer­
ence algorithm for diagnosing multiple diseases. In
Uncertainty in Artificial Intel ligence S. Edited by
M. Henrion, R.D. Shacht er, L.N. Kana! and J.F.
Lemmer. Elsevier Science Publishers, pp. 163·171.
[Beckerman 1990b] D. Heckerman, Probabilistic Sim­
ilarity Networks, Ph.D. Thesis, Stanford U.
[Henrion 1988] M. Henrion, Propagating uncertainty
in Bayesian networks by probabilistic logic sam-

piing. Uncertainty in Artificial Intelligence 2. Edited
by J.F. Lemmer and L.N. Kana!, Elsevier Science
Publishers, pp. 149-163.
[Jensen, Lauritzen and Olesen 1990a] F.V. Jensen,
S.L. Lauritzen and KG. Olesen, Bayesian updat­
ing in causal probabilistic networks by local compu·
tations. Computational Statistics Quarterly. 4, pp.
269·282.
[Jensen and Andersen 1990] F . Jensen and S.K. An­
dersen, Approximations in Bayesian belief universes
for knowledge-based systems. Proc. 6 Conf. Uncer­
tainty in AI, Cambridge, Mass., pp. 162-169.
[Lauritzen and Spiegelhalter 1988] S.L. Lauritzen and
D.J. Spiegelhalter, Local computation with proba­
bilities on graphical structures, and their applica·
tion to expert systems. ]. Royal Statistical Society,
Series B, 50: 157-244.
[Pearl1986j J. Pearl, Fusion, propagation, and struc­
turing in belief networks. Artificial Intelligence, 29:
241-288.
[Pearl 1988] J. Pearl, Probabilistic Reasoning in In­
telligent Systems: Networks of Plausible Inference,

Morgan J{aufmann.

[Suermondt, Cooper and Beckerman 1990]
J. Suermondt, G. Cooper and D. Beckerman, A
combination of cutset conditioning with clique-tree
propagation in the PATHFINDER system. Proc. 6
Conf. Uncertainty in AI, Cambridge, Mass., pp.
273-279.
[Xiang et al. 1991] Y. Xiang, B. Pant, A. Eisen, M.P.
Beddoes and D. Poole, PAINULIM: A neuromuscu­
lar diagnostic aid using multiply sectioned Bayesian
networks. Proc. ISMM Inter. Conf. on Mini and Mi­
crocomputers in Medicine and Healthcare, 64-69.
[Xiang, Poole and Beddoes 1992] Y. Xiang, D. Poole
and M.P. Beddoes, Multiply sect ion ed Bayesian
networks and junction forests for large knowledge­
based systems. CSS-IS TR 92-04, Simon Fraser U.

351

