Edward Herskovits
Medical Computer Science
Stanford University
Stanford, CA 94305

manually by an expert or with an expert. Some important
progress has been made in developing methods to improve
the efficiency of knowledge acquisition from experts
[Beckerman 1990]. These methods are likely to remain
important in domains of small to moderate size in which
there are readily available experts. Some domains,
however, are large. In others, there are few, if any,
available experts. Methods for assisting, or in some cases
replacing, the manual expert-based methods of knowledge
acquisition are needed.
Databases are becoming increasingly abundant in many
areas, including science, engineering, and business. In
each of these areas, there arc many potential opportunities
for using probabilistic networks to provide assistance in
decision making. By using databases to assist in
constructing probabilistic networks, we may be able to
decrease knowledge acquisition time significantly.
Automatically generated networks could be used directly to
provide decision-making assistance, or used as a starting
point for modification by an expert. In the latter case, the
.
editing of a network may require substantially less time
than de novo generation of the network by an expert.
The automated construction of probabilistic networks also
can provide insight into the probabilistic dependencies that
exist among the domain variables. One application IS the
automated discovery of dependency relationships. The
computer program searches for a probabilistic network
structure that has a high posterior probability given the
database, and outputs the structure and its probability. A
related task is computer-assisted hypothesis testing: the
user enters a hypothesized structure of the dependency
relationships among a set of variables and the program
calculates the probability of the structure given a database
of cases on the variables. These applications have the
potential to affect broad areas of scientific discovery and
data evaluation.
As an example, consider the fictitious database of cases
shown in Table 1. Suppose that xi is an experimental
condition and x2 and x3 arc two experimental outcomes.

A Bayesian Method for Constructing Bayesian Belief Networks from Databases

Given the database, what are the qualitative dependency
relationships among the variables? For example, do x1 and
x3 influence each other directly, or do they do so only
through x2? What is the probability that x3 will be
present if x1 is present? Clearly, there are no categorically
correct answers to each of these questions. The answers
depend on a number of factors, including the model that
we use to represent the data, and our prior know ledge
abcut the data in the database and the relationships among
the variables. In this paper, we do not attempt to consider
all such factors in their full generality. Rather, we
specialize the general task by presenting one particular
framework for constructing probabilistic networks from
databases, as for example the database in Table 1, such
that these networks can be used for probabilistic inference,
as for instance in calculating P(x3 = present I x1 = present).
In particular, we focus on using a Bayesian belief network
as a model of probabilistic dependency. Our primary goal
is to construct such a network ( or networks), given a
database and a set of explicit assumptions about our prior
probabilistic knowledge of the domain, and then use that
network (or networks) for inference.
Table 1: A database example. For notational
convenience, in the text we sometimes use 0 to
denote absent and 1 to denote present.
Vari·1hle values for e·1ch case
Case
1
2
3
4
5
6
7
8
9
10

xl

'

present
present
absent
·present
absent
absent
I present
absent
I present
absent

xz

absent
present
absent
present
absent
present
present
absent
present
absent

'

2.1 BAYES IAN BELIEF NETWORKS

A Bayesian belief-network structure Bs is a directed acyclic
graph in which nodes represent domain variables and arcs
between nodes represent probabilistic dependencies
[Cooper 1989, Horvitz, Breese, et al. 1988, Lauritzen and
Spiegelhalter 1988, Neapolitan 1990, Pearl 1986, Pearl
1988, Shachter 1988]. A variable in a Bayesian belief­
network structure may be continuous [Shachter and
Kenley 1989] or discrete. In this paper, we shall focus
our discussion on discrete variables. Figure 1a shows an
example of a belief-network structure, which we shall call
Bs1, containing three variables. The arc from x1 to x2
indicates that these two variables are probabilistically
dependent. Similarly, the arc from x2 to x3 indicates a
probabilistic dependency between these two variables. The
absence of an arc from x 1 to x3 implies that there is no
direct probabilistic dependency between x 1 and x3• In
particular, the probability of each value of x3 is
conditionally independent of the value of x1 given that the
value of x2 is known. Figure 1 b shows an alternative
structure that expresses different dependency relationships
among the three variables. The representation of
conditional dependencies and independencies is the
essential function of belief networks. For a detailed
discussion of the semantics of Bayesian belief networks,
see [Pearl 1988].

' ·

X3

absent
present
present
present
absent
present
present
absent
present
absent

(a) Structure Bs1

2 METHODS

In this section, we first briefly review some key concepts
about Bayesian belief networks. Then, we present the
primary theoretical developments of our work thus far in
developing methods for learning the structure of Bayesian
belief networks from databases; a more detailed discussion
with proofs appears in [Cooper and Herskovits 199 1].
Finally, we discuss the empirical results of an algorithm
that applies this theory to search for the most likely
belief-network structure, given a database.

(b) Structure Bsz
Figure 1: Two alternative belief-network structures
on three variables.
A Bayesian belief-network structure Bs is augmented by
conditional probabilities, Bp, to form a Bayesian belief
network B. Thus, B = (Bs. Bp). For brevity, we call B a

87

88

Cooper and Herskovits

belief network. For each node1 in the network structure,
there is a conditional probability function that relates this
node to its immediate predecessors (parents). We shall use
11:; to denote the parent nodes of variable x ,. If a node has
no parents, then a prior probability function, P(x;), is
specified. A set of probabilities is shown in Table 2 for
the belief-network structure in Figure Ia. We shall use the
term conditional probability to refer to a probability
statement, such as P(x2 = present I x1 = present). We use
the term conditional probability assignment to denote a
numerical assignment to a conditional probability, as, for
example, the assignment P(x2= present I x1=present) =
0.8. The network structure B s1 in Figure Ia and the
probabilities BPI in Table 2 together define a belief
network, which we denote as B1•

=present)=0.6
= absent)= 0.4
P(xz=present I x1 =present) = 0.8
P(xz = absent I x1 = present) = 0.2
P(xz=present I x1 =absent) = 0.3
P(x2 = absent I x1 =absent)= 0.7
P(xJ=present I x2 = present)= 0.9
P(x3 = absent I x2=present) = 0.1
P(xJ=present I x2 = absent)= 0.15
P(x3 = absent I x2=absent) = 0.85
P(x1

P(x1

n

, Xn)

= IT P(X; I
i 1

Therefore, the joint probability of any instantiation of all
the variables in a belief network can be computed as the
product of only n probabilities. In principle, we can
rec?ver the complete joint-probability space from the
behef-network representation by calculating the joint
probab1ht1es that result from every possible instantiation
of the n variables in the network. Thus, we can determine
any probability of the form P(Z I Y), where Z and Y are
sets of variables with known values (instantiated
variables). For example, for our sample three-node belief
networkS!> P(x3=present I x1 =present)= 0. 75.

To be more specific, let D be a database of cases, Z be the
set of variables represented by D, and Bs.' and Bs be two
J

.

behef-network structures containing exactly those
variables that are in Z. In the next section, we develop a
method for computing P(Bs, I D)/P(Bs I D). By computing
1

Belief networks can be used to represent the probabilities
over any discrete sample space: the probability of any
sample point in that space can be computed from the
probabilities in the belief network. The key feature of
belief networks is their explicit representation of the
conditional independence among events. In particular,
investigators have shown [Kiivcri, Speed, ct al. 1984,
Pearl 1988, Shachter 1986] that the joint probability of
any particular instantiation2 of all n variables in a belief
net work can be calculated as
...

Jr.'

represents an instantiation of the parents of X,.

Let us now consider the problem of finding the most
probable belief-network structure, given a database. Once
such a structure is found, we can derive numerical belief­
network probabilities from the database [Cooper and
Herskovits 1991]. We can use the resulting belief network
for probabilistic inference, such as calculating the value of
P(x3=present I x1 = present). In addition, the structure
may lend insight into the dependency relationships among
the vanables m the database, as, for example, possible
causal relationships.

Table 2: The probability assignments associated with
the belief-network structure Bs1 in Figure 1. We shall
denote these probability assignments as BPI·

P(X 1,

where each X; represents an instantiated variable and

(I)

Jr; ) ,

such ratios for pairs of belief-network structures, we can
rank order a set of structures by their posterior
probabilities. To calculate the ratio of posterior
probabilities, we shall calculate P(Bs.,
D) and P(Bs., D)
'
J

and use the following equivalence:

P(Bs, I D)
P(Bs1 I D)

P(Bs,, D)
P(D)

P(Bs1, D)

=

P(Bs,, D)

P(Bs,1 D)

(2)

P(D)

2.2 A FORMULA FOR COMPUTING P(Bs, D)

=

Since there is a one-to-one correspondence between a
node in B5 and a variable in

Hp, we shall

node and variable interchangeably.

use the terms

2 An instantiated variable is a variable with an assigned
value. When we need to designate a particular value
v;k of variable x;, we shall write x;

=

ViJc.

Let B s represent an arbitrary belief-network structure
containing just the variables in Z. In this section, we
present a method for calculating P(B5, D). In doing so, we
shall introduce five assumptions that render this
calculation computationally tractable.
Assumption 1. The process that generated the database is
modeled as a belief network containing just the variables
in Z, which arc discrete.

A Bayesian Method for Constructing Bayesian Belief Networks from Databases

As this assumption states, we shall not consider
continuous variables in this paper.

where
in D.

A belief network (structure plus conditional probabilities)
is sufficient to capture any probability distribution over
the variables in Z [Pearl 1988]. A belief-network structure
alone, containing just the variables in Z, can capture
many-but not all-the qualitative independence
relationships that might exist in an arbitrary probability
distribution over Z [Pearl l 988]. Assumption 1, therefore,
is justified to the extent that the relationships of
independence and dependence, among variables in the
underlying process that is hidden from us, can be
represented by some belief network. In the remainder of
this section, we shall write as though database D was
generated by Monte Carlo sampling of a belief network
with structure B s that is hidden from us. One of our
primary goals will be to usc D to try to discover B 5• In
this section, we assume that Bs contains just the variables
in Z. In [Cooper and Herskovits 1991], we allow Bs to
contain variables beyond those in Z.

Assumption 3. Cases are complete, that is, there are no
cases that have variables with missing values.

The application of Assumption 1 yields

P(Bs, D)

L

=

P(D I Bs , Bp ) j(Bp I Bs) P(Bs) dBp,

(3)

is the number of cases in D and Cj is the jth case

In [Cooper and Herskovits 1991], we relax this
assumption. We now introduce additional notation to
facilitate the application of Assumption 3. Let Dij denote
the value assignment of variable i in case j. Thus, D21 =
0, since x2 = 0 ( i.e., x2 is absent) in case I in Table I.
In B 5, for every variable X;, there is a set of parents Tr;
(possibly the empty set). For each case in D, the variables
in Tr; are each assigned a particular value; call such a case­
specific instantiation of all the variables in

Assumption 2. Cases occur independently, given a belief
network model.
This assumption is equivalent to assuming that the belief
network that is generating the data is static, that is, it docs
not change as cases are being generated. It follows from
the conditional independence of cases expressed in
Assumption 2 that Equation 3 may be rewritten as

P(Bs, D)

f.

Bp

[

JJ P(Cj
1

I Bs, Bp)] j(Bp I Bs) dBp,

(4)

a ;r­

no parents, then we define 1/J; to be the list (0), where 0
represents the empty set of parents. Although the ordering
of the elements of 1/J; is arbitrary, we shall use a list
(vector), rather than a set, so that we can refer to members
of 1/J; using an index. For example, consider variable x2 in

Bsi> which has parent set n2 = {x1}. In this example, ¢2 =
((x1 = 0), (x1 = !)), because there are cases in D where x1
has the value 0 and cases where it has the value I. Let
1/J;[j] be the jth clement of 1/J;. Thus, for example, ¢2[1] is
=

0). Let cr(i, j) be an index function, such

that the instantiation of

Tr; in

case j is the a(i, J)lh clement

of 1/J;. Thus, for example, cr( 2 I) = 2, because in case I
the parent set of variable x 2 - namely {x 1 } - is
,

instantiated as x1 = 1, which is the second clement of ¢2•

Therefore, I/Jz[a(2, l)l is equal to (x1 = 1). Let q, = I 1/J; I.
Note that since there are m cases in D, q, :;:; m. Since,
according to Assumption 3, cases arc complete, we can
apply Equation I to represent the probability of a case as
follows:
n

P(Cj I Bs, Bp) = IT P(x; = D;j I 1/J;[a(i,j)] , Bp)
i= 1

Substituting Equation 5 into Equ ation 4,

P(Bs, D)

we

(5)
obtain

=

(6)
x

=

Tr;

instantiation. Let 1/J; denote a list of the unique Tr­
instantiations for the parents of x; as seen in D. If x; has

equal to (x1

where Bp is a vector whose values denote the conditional­
probability assignments associated with belief-network
structure B5, and f is the conditional probability density
function over B p given B 5. The integral is over all
possible value assignments to B P· Thus, we are
integrating over all possible belief networks that can have
structure B 5. The integral in Equation 3 represents a
multiple integral and the variables of integration are the
conditional probabilities associated with structureB5.

P(Bs)

m

j(Bp I Bs) dBp.

For a given i and j , lctf(P(x ; I 1/J;[jl,Bp)) denote
our probability distribution over the possible values of

89

90

Cooper and Herskovits

P (xi I li'lj(j],B p ).

That is, the density function

f(P(x; I 1/J;[jl,Bp)) represents our belief about the values
to assign the conditional probability function
P (x; I 1/J;[j],Bp). For notational convenience, we shall
leave the term B P implicit. We shall assume that our
belief about the values to assign to a conditional
probability function in a belief network is not influenced
by our belief about the values to assign any other
conditional probability function. More formally, we can
express this assumption as follows:

a set of parents

As an example, Assumption 4 implies that our belief
about the assignment of a value to the conditional
probability P (x3= 0 I x2= 0) is independent of our
assignment of a value to the conditional probability
P (x2 = 0 I x 1= 0), since these two probabilities are
components of different conditional probability
distributions. However, our belief about P (x2= 0 I x1= 0)
must be dependent on our belief about P (x2= 1 I x1= 0),
since they are members of the same conditional
probability distribution; in particular, since x2 is a binary
variable, P(x2 = 0 I x1= 0) = 1 - P (xz= 1 I Xt= 0).
Assumption 5. For 1 $i $ n, 1 $ j $ qi, the distribution
f(P(x; I 1/J;[j])) is uniform.
This assumption states that initially, before we see the
data, we are indifferent regarding giving one assignment of
values to a conditional probability function versus some
other assignment. This probability density function is,
however, just a special case of the Dirichlet distribution
[deGroot 1970]. In [Cooper and Herskovits 1991] we
generalize Assumption 5 by representing f (P (x; I 1/J;[jl))
with a Dirichlet distribution.
Assumptions 4 and 5 permit us to define the joint density
function f (Bp I Bs) using density functions of the form
f(P(x; I cpJj])) which are uniform.
We now use Assumptions 1 through 5 in the following
theorem, proven in [Cooper and Herskovits 199 1], which
solves Equation 3 for P(Bs,D):
Theorem Let Z be a set of n discrete variables, where
a variable x; in Z has r; possible value assignments:
(v;1, ... , v;7).
Let D be a database of m cases, where
'

each case contains a value assignment for each variable
in Z. Let Bs denote a belief-network structure containing
just the variables in Z . Each variable X; in B s has

Let cp;[jl denote the jth unique

instantiation of n:; relative to D. Suppose there are q;
such unique instantiations of n:;. Define aijk to be the
number of cases in D in which variable x; i s
instantiated as v; k and
LetN;i =
hold,then

L:= 1 aiik· If
n

n:;

is instantiated as 1/J;[j].

Assumptions

qi

P(Bs, D) = P(Bs) IT IT

(r;

_

l )I

·

i=tj=l (N;j+ r;-1)!

Assumption 4. For 1 $i,i' $ n, 1 $j $ q;, 1 $}' $ q;·,
if ij toi'j' then the distributionf(P(x; I 1/J;[j])) is marginally
independent of the distributionflP(x;·l 1/J;·[J'] )) .

n:;.

1

through 5

r;

IT a;jk!· (7)

k= l

0

Equation 7 allows us to calculate P (B 5,D) using
knowledge of P (Bs) combined with simple enumeration
over the cases in the database. For example, by applying
Equation 7 to the structures in Figure 1 with the data in
11
Table 1, we find that P (B s1,D)= 8.9 1 x 10- a n d
12
P (Bs2,D)= 8.9 1 x 10· , i f we assume uniform priors on
P (B s). Note that these numbers are small because the
probability of seeing exactly those data that are in D is
small. Given the assumptions in this section, the data
imply that B s1 is 10 times more likely than Bsz· This
result is not surprising, because we used B1 to generateD
by the application of Monte Carlo sampling.
Consider the time complexity of computing Equation 7.
Let r= max;[r;] fori= 1 to n. Define Is to be the time
s
required to compute the prior probability of structure Bs,
P (Bs). In [Cooper and Herskovits 1991] we show that the
2
time complexity of computing Equation 7 is O(m n r +
Iss>·
Using Equation 7, we can calculate posterior probabilities
of belief-network structures as

P(Bs I D) =

P(Bs, D)

(8)

I, P(Bs, D)
Bs

Applying Equation 8 to our previous example, we obtain
P (Bs1 I D)= 0. 109, and P (B 52 ID)= 0.0 1 1. The
remaining probability mass of 0.88 is distributed among
the other 23 possible three-node belief-network structures.
When there are more than a few variables in the model,
the complexity of computing Equation 8 is intractable,
due to the large number of belief-network structures.
Consider, however, the situation in which

�Bs e y P(Bs, D)� P(D), for some set
where

IY I

Y

of structures,

is small. If Y can be efficiently located, then

A Bayesian Method for Constructing Bayesian Belief Networks from Databases

Equation 8 can be efficiently computed to a close
approximation.

n

P(Bs, D)

= c

2.3 FINDING THE MOST PROBABLE
BELIEF-NETWORK STRUCTURE

Consider the problem of determining the belief-network
structure Bs that maximizes P(Bs I D). Knowing such a
structure may lend insight into the causal relationships
among the model variables, particularly if the structure
has a high posterior probability. The structure also may
be augmented with numerical probabilities, as we discuss
in [Cooper and Herskovits 1991], and used to perform
probabilistic inference.
For a given database D,P(Bs,D) oc P(BsiD), and
therefore finding the Bs that maximizes P(Bs I D) is
equivalent to finding the Bs that maximizes P(Bs,D). We
can maximize P(Bs, D) by applying exhaustively Equation
7 for every possible Bs containing just the variables in z.
As a function of the number of variables, the number of
possible structures grows super-exponentially. Thus, an
exhaustive enumeration of all network structures is not
feasible in most domains. In particular, Robinson
[Robinson 1976] has derived an efficiently computable
recursive function for determining the number of possible
belief-network structures that contain n nodes. For n = 2,
the number of possible structures is 3; for n = 3, it is 25;
for n
10, it is
5, it is 29,000; and, for n
18
approximately 4.2 x 10 . Clearly, we need a method that
is more efficient than is exhaustive enumeration for
locating the B s that maximizes P(B s I D). In Section
2.3.1, we introduce additional assumptions that reduce the
time complexity of enumeration. The complexity,
however, remains exponential. Thus, in Section 2.3.2 we
introduce and discuss a heuristic method that is
polynomial time.
=

=

2.3.1 An Exhaustive Search Procedure

Let us assume that we can specify an ordering on all n
variables, such that if x; precedes x1 in the ordering, then
we do not allow structures in which there is an arc from xi
to x;. In some domains, the time precedence of event
variables could be used to establish such an ordering.
('2)
Given an ordering as a constraint, there remain 2
=
2
(
1)
J
2n n· ! poss1"b e be J"1ef-network structures. Let t(n) =
. .
.
An(n·1)/Z
"
. For Jarge n, 1t 1s not feas1"bJe to app Iy Equauon 7
for each of t(n) possible structures. Therefore, in addition
to a node ordering, let us assume equal priors on B5. That
is, initially, before we observe the data D, we believe that
all structures arc equally likely. In that case, we obtain

q;

f1 f1

(r; - 1)!

r;

f1 aiJk! ,

; = 1 i =1 (N;1 + r;- 1) ! k = 1

(9 )

where c = 1/t(n) is our prior probability, P(B5), for each
Bs. To maximize Equation 9, it is sufficient to find the
parent set of each variable that maximizes the second inner
product. Thus, we have
max[ P(Bs, D)]
Bs

c

=
�

n

; =1

1'i

�

!
rr a ;Jk! ] ,
(r;-1)
+
=
J 1 (N;1
r; - 1) ! .< = 1

rr max [ rr

(10)

where the maximization takes place over every possible
set of parents :rr; of x; that is consistent with the ordering
on the nodes. A generalization of Equation 10, which is
discussed in [Cooper and Herskovits 1991], does not
assume that P(Bs) is uniform. Although solving Equation
10 is no longer super-exponential in n, it remains
exponential in n. Thus, further computational
improvements are needed.
2.3.2 A Heuristic Search Procedure

We propose here one polynomial-time heuristic method,
among many possibilities, that attempts to find the Bs
that maximizes (or nearly maximizes) P(Bs I D). We shall
usc Equation 10 as our starting point, with the attendant
assumptions that we have an ordering on the domain
variables and that, a priori, all structures arc considered
equally likely. We shall modify the maximization
operation on the right of Equation 10 to use a greedy­
search method. In particular, we use an algorithm that
begins by assuming that a node has no parents, and that
then adds incrementally that parent whose addition most
increases the probability of the resulting structure. When
the addition of no single parent can increase the
probability, we stop adding parents to the node. We shall
use the following function:

g(i, :rr;)

q;

=

(r;- 1)!
rr' a··yk.l
1
i= (NiJ+ r; - 1)! k=1
rr

'

(11)

where the CX;jk arc computed rclativc to :rr; being the parents
of x; and relative to a database D which we leave implicit.
Let u be the maximum number of parents allowed for any
node. In [Cooper and Herskovits 1991] we show that
g(i, :rr;) can be computed in O( m u r) time. We also shall
use a function Pred(x;) that returns the set of nodes that
precede xi in the node ordering. Figure 2 contains the
heuristic search algorithm, which we call K2. The
algorithm is named K2 because it evolved from a system

91

92

Cooper and Herskovits

named Kutat6 [Herskovits and Cooper 1990) that applies
the same search heuristics to construct belief networks;
Kutat6 uses entropy to score network structures .

method for obtaining some preliminary test results, which
we shall describe in Section 3.

As shown in [Cooper and Herskovits 1991], the time

3 PRELIMINARY RESULTS

complexity of K2 is O(m u2 n2 r). This result assumes
that the factorials we need in order to apply Equation 11
have been precomputed and stored in an array. We can
further improve runtime speed by replacing
g (i , rr;) and g (i , n ; u (z}) in K2 by /og(g (i , rr;)) and

/og(g(i, n; u(z})), respectively. The logarithmic version
of Equation 11 requires only addition and subtraction
rather than multiplication and division. If the logarithmic
version of Equation 11 is used in K2 then the logarithms
of factorials should be precomputed and stored in an array.
1.
2.

procedure K2;
(Input: A set of n nodes, an ordering on the
nodes, an upper bound u on the
number of parents a node may have,
and a database D containing m cases.}
(Output: For each node, a printout of the
parents of the node.}
for i =
: 1 to n do

3.
4.

1li := 0;

Pold := g(i ,

5.

rr;);(This function is computed
using Equation 11.}

OKToProcecd :=true;
while OKToProcecd and lrr;l

6.
7.

<

u do

let z be the node in Prcd( x;)-

8.

maximizes g(i,
9.
10.
11.

n; that

n; u(z});

Pnew:= g(i, 7!i U( Z});
if P new> P old then
Pold =
: Pnew;

12.
: 1li u(z}
1li =
13.
else OKToProcccd :=false;
end(while};
14.
write('Node: ',X;,' Parents of this node: , rr;);
15.
16. end(for};
17. end(K2};

In this section we describe an experiment in which we
generated a database from a belief network by simulation,
and then attempted to reconstruct the belief network from
the database. In particular, we applied the K2 algorithm to
a database of 10,000 cases generated from the ALARM
belief network. Beinlich constructed the ALARM network
as a research prototype to model potential anesthesia
problems in the operating room [Beinlich, S ucrmondt, et
a!. 1989). ALARM contains 46 arcs and 37 nodes, and
each node has from two to five possible values. We
generated cases using a Monte Carlo technique [Henrion
1988). Each case corresponds to a value assignment for
each of the 37 variables. The Monte Carlo technique is an
unbiased generator of cases, in the sense that the
probability that a particular case is generated is equal to
the probability of the case according to the belief network.
We generated 10,000 such cases to create a database that
we used as input to the K2 algorithm. We supplied K2
with an ordering on the 37 nodes that is consistent with
the partial order of the nodes as specified by ALARM.
From the 10,000 cases, the K2 algorithm constructed a
network identical to ALARM, except that one arc was
missing and one arc was added. A subsequent analysis
revealed that the missing arc is not strongly supported by
the 10,000 cases. The extra arc was added due to the greedy
nature of the search algorithm. The total search time for
the reconstruction was approximately 16 minutes and 38
seconds on a Macintosh II running LightSpccd Pascal
version 2.0. We analyzed the performance of K2 when
given the first 100, 200, 500, 1000, 2000 and 3000 cases
from the same 10,000-case database. Using only 3,000
cases, K2 produced in about 5 minutes the same belief
network as when it used the full 10,000 cases.

'

Figure 2: The K2 algorithm heuristically searches for
the most probable belief-network structure, given a
database of cases and a set of assumptions (sec text).

We emphasize that K2 is just one of many possible
methods for searching the space of belief networks to
maximize the probability metric developed in Section 2.2.
Accordingly, the metric developed in Section 2.2 is a
more fundamental result than is the K2 algorithm.
Nonetheless, K2 has proved valuable as an initial search

Although preliminary, these results arc encouraging
because they demonstrate that K2 can reconstruct a
moderate size belief network rapidly from a set of cases
using readily available computer hardware. We currently
arc investigating the extent to which the performance of
K2 is sensitive to the ordering of the nodes. We also are
exploring methods that do not require an ordering.

4

SUMMARY OF THE LEARNING
METHOD AND RELATED WORK

In the preceding sections, we have described a Bayesian
approach to learning the clepenclcncy relationships among a
set of discrete variables. For notational simplicity, we

A Bayesian Method for Constructing Bayesian Belief Networks from Databases

shall call the approach BLN ( .!iayesian learning of belief
networks). BLN can represent arbitrary belief-network
structures and arbitrary probability distributions on
discrete variables. BLN calculates the probability of a
structure of variable relationships given a database. The
probability of multiple s tructures can be computed and
displayed to the user. BLN also can use multiple
structures in performing inference, as we discuss in
[Cooper and Hcrskovits 1991]. When the number of
domain variables is large, the combinatorics of
enumerating all possible belief network structures
becomes prohibitively expensive. Developing better
methods for efficiently locating highly probable structures
remains an open area of research. BLN is able to represent
the prior probabilities of belief-network structures. For
example, an expert could attach a high probability to the
presence of an arc from node x to node y, indicating that­
according to current scientific belief-it is very likely that
x directly influences y. More generally, a prior probability
could be specified for the presence of a set of arcs. If prior
probability distributions on such structures are not
available to the computer, then uniform priors can be
assumed.

References

Agogino, A.M. and Rege, A., IDES: Influence diagram
based expert system, Mathematical Modelling 8 (1987)
227-233.
Andreassen, S., Woldbye, M., Falck, B. and Andersen,
S.K., MUNIN - A causal probabilistic network for
interpretation of elcctromyographic findings, In:
Proceedings of the International Joint Conference on
Artificial Intelligence, Milan, Italy (1987) 366-372.
Bcinlich, I.A., Suermondt, H.J., Chavez, R.M. and
Cooper, G.F., The ALARM monitoring system: A case
study with two probabilistic inference techniques for
belief networks, In: Proceedings of the Conference on
Artificial Intelligence in Medical Care, London (1989)
247-256.
Chow, C.K. and Liu, C.N., Approximating discrete
probability distributions with dependence trees, IEEE
Transactions on Information Theory 14 (1968) 462467.

Previously described methods for learning belief networks
from databases arc non-Bayesian [Chow and Liu 1968,
Fung and Crawford 1990, Geiger, Paz, et a!. 1990,
Hcrskovits and Cooper 1990, Pearl and Verma 1991,
Rcbane and Pearl 1987, Spines and Glymour 1990,
Spines, Glymour, et a!. 1990, Srinivas, Russell, et a!.
1990, Verma and Pearl 1990, Wermuth and Lauritzen
1983]. With non-Bayesian methods, there is no principled
way to attach prior probabilities to individual arcs or sets
of arcs. In addition, all of these methods, except
[Hcrskovits and Cooper 1990], rely on having threshold
values (e.g., p values) for determining when conditional
independence holds among variables. BLN docs not require
the usc of such thresholds. Also, non-Bayesian belief­
network methods, as well as classical statistical methods,
emphasize finding the single most likely structure, which
they then may usc for inference. They do not, however,
quantify the likelihood of that structure. If a single
structure is used for inference, implicitly the probability
of that structure is assumed to be 1.

Cooper, G.F., Current research directions in the
development of expert systems based on belief
networks, Applied Stochastic Models and Data Analysis
5 (1989) 39-52.

Acknowledgements

Hcckcrman, D.E., Probabilistic Similarity Networks,
Ph.D. dissertation, Medical Information Sciences,
Stanford University (1990).

We thank Lyn Dupre and Clark Glyrnour for helpful
comments on an earlier draft. This work was supported in
part by the National Science Foundation under grant
IRI-8703710 and by the U.S. Army Research Office under
grant P-25514-EL. Computing resources were provided in
part by the SUMEX-AlM resource under grant LM-05208
from the National Library of Medicine.

Cooper, G.F. and Hcrskovits, E.H., A Bayesian method
for the induction of probabilistic networks from data,
Report SMI-91-1, Section of Medical Informatics,
University of Pittsburgh, 1991.
deGroot, M.H., Optimal Statistical Decisions
(McGraw-Hill, New York, 1970).
Fung, R.M. and Crawford, S.L., Constructor: A system
for the induction of probabilistic models, In:
Proceedings of AAA! , Boston, Massachusetts (1990)
762-769.
Geiger, D., Paz, A. and Pearl, J., Learning causal trees
from dependence information, In: Proceedings of AAAI ,
Boston, Massachusetts (1990) 770-776.

Hcckcrman, D.E., Horvitz, E.J. and Nathwani, B.N.,
Update on the Pathfinder project, In: Proceedings of the
Symposium on Computer Applications in Medical Care
(1989) 203-207.
Hcnrion, M., Propagating uncertainty in Bayesian
networks by logic sampling. In: Lemmer J.F. and

93

94

Cooper and Herskovits

Kana! L.N. (Eds.), Uncertainty in Artificial Intelligence
2 (North-Holland, Amsterdam, 1988) 149- 163.

Workshop on Uncertainty in Artificial Intelligence,
Seattle, Washington ( 1987) 222-228.

Henrion, M., An introduction to algorithms for
inference in belief nets. In: Henrion M., Shachter R.D.,
Kana! L.N. and Lemmer J.F. (Eds.), Uncertainty in
Artificial Intelligence 5 (North-Holland, Amsterdam,
1990) 129- 138.

Robinson, R.W., Counting unlabeled acyclic digraphs
(Note : This paper also discusses counting labeled
acyclic graphs.), In: Proceedings of the Fifth Australian
Conference on Combinatorial Mathematics, Melbourne,
Australia ( 1976) 28-43.

Henrion, M. and Cooley, D.R., An experimental
comparison of knowledge engineering for expert
systems and for decision analysis, In: Proceedings of
AAAJ, Seattle ( 1987) 47 1-476.

Shachter, R.D., Intelligent probabilistic inference. In:
Kana! L.N. and Lemmer J.F. (Eds.), Uncertainty in
Artificial Intelligence (North-Holland, Amsterdam,
1986) 371-382.

Herskovits, E.H. and Cooper, G.F., Kutat6: An
entropy-driven system for the construction of
probabilistic expert systems from databases, In:
Proceedings of the Conference on Uncertainty in
Artificial Intelligence, Cambridge, Massachusetts
( 1990) 54-62.

Shachter, R.D., Probabilistic inference and influence
diagrams, Operations Research 36 ( 1988) 589-604.

Holtzman, S., Intelligent Decision Systems (Addison­
Wesley, Reading, MA, 1989).
Horvitz, E.J., Breese, J.S. and Henrion, M., Decision
theory in expert systems and artificial intelligence,
International Journal of Approximate Reasoning 2
( 1988) 247-302.
Kiiveri, H., Speed, T.P. and Carlin, J.B., Recursive
causal models, Journal of the Australian Mathematical
Society 36 ( 1984) 30-52.
Lauritzen, S .L. and Spiegelhaltcr, D.J., Local
computations with probabilities on graphical structures
and their application to expert systems, Journal of the
Royal Statistical Society (Series B) 50 ( 1988) 157-224.
Neapolitan, R., Probabilistic Reasoning in Expert
Systems (John Wiley & Sons, New York, 1990).
Pearl, J., Fusion, propagation and structuring in belief
networks, Artificial Intelligence 29 ( 1986) 241-288.
Pearl, J., Probabilistic Reasoning in Intelligent
Systems (Morgan Kaufmann, San Mateo, California,
1988).
Pearl, J. and Verma, T.S., A theory of inferred
causality, In: Proceedings of the Second International
Conference on the Principles of Knowledge
Representation and Reasoning, Boston, MA ( 199 1)
44 1-452.
Rebane, G. and Pearl, J., The recovery of causal poly­
trees from statistical data, In: Proceedings of the

Shachter, R.D. and Kenley, C.R., Gaussian influence
diagrams, Management Science 35 ( 1989) 527-550.
Spirtes, P. and Glymour, C., An algorithm for fast
recovery of sparse causal graphs, Report CMU-LCL-904, Department of Philosophy, Carnegie-Mellon
University, 1990.
Spines, P., Glymour, C. and Scheines, R., Causal
hypotheses, statistical inference, and automated model
specification, Unpublished report, Department of
Philosophy, Carnegie-Mellon University, 1990.
Srinivas, S., Russell, S. and Agogino, A., Automated
construction of sparse Bayesian networks for
unstructured probabilistic models and domain
information. In: Henrion M., Shachter R.D., Kana!
L.N. and Lemmer J.F. (Eds.), Uncertainty in Artificial
Intelligence 5 (North-Holland, Amsterdam, 1990) 295308.
Suermondt, H.J. and Amylon, M.D., Probabilistic
prediction of the outcome of bone-marrow
transplantation, In: Proceedings of the Symposium on
Computer Applications in Medical Care ( 1989) 2082 12.
Verma, T.S. and Pearl, J., Equivalence and synthesis of
causal models, In: Proceedings of the Conference on
Uncertainty in Artificial Intelligence, Cam bridge,
Massachusetts (1990) 220-227.
Wermuth, N. and Lauritzen, S., Graphical and recursive
models for contingency tables, Biometrika 72 ( 1983)
537-552.

