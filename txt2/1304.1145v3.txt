of variables and P is a probability distribution, then
a and b are totally independent wrt P iﬀ
P (a, b|Z = Z) = P (a|Z = Z) · P (b|Z = Z)
where Z is any subset of U not containing a and b,
and Z is any value of Z. In the second formulation,
two variables are said to be totally uncoupled wrt P
if U can be partitioned into two marginally independent sets U1 and U2 containing a and b respectively,
namely,
P (U1 = U , U2 = U ) =
P (U1 = U ) · P (U2 = U )
where U1 ∪U2 = U , U1 ∩U2 = ∅, and U , U are any
values of these variables. In the third formulation,
two variables a and b are totally disconnected if the
corresponding nodes are disconnected in every belief
network representation of P .
Are these three formulations equivalent and if not,
under what conditions do they coincide? Our main
contribution is to identify a class of distributions
called transitive for which all three formulations are
equivalent. Strictly positive binary distributions and
regular Gaussian distributions (defined below) are
examples of transitive distributions. We also show
that “connectedness” in graphical representations
and “dependence” in the domain represented (i.e.,
the converse of total independence) are equivalent
for every transitive distribution and for none other.
These results have several theoretical and practical ramifications. Our analysis uses a qualitative
abstraction of probabilistic independence known as
graphoids (Pearl and Paz, 1989) and it demonstrates
the need for this abstraction in manipulating independence assumptions (which are an integral part of
every probabilistic reasoning engine). Our proof also
demonstrates that belief networks provide a powerful mathematical tool for understanding of probability theory itself. Finally, we demonstrate the rele-

vance of these results to simplifying the process of
acquiring probabilistic knowledge from experts.

Definition (Pearl and Paz, 1989) A graphoid is any
dependency model M which is closed under the following axioms1 :

2

Trivial Independence

Separability and Transitivity

Throughout our discussion we consider a finite set
of variables U = {u1 , ..., un } each of which is associated with a finite set of values d(ui ) and a probability
distribution P with the Cartesian product
Q
ui ∈ U d(ui ) as its sample space. We use lowercase
letters possibly subscripted (e.g a, b, x or ui ) to denote variables, and use uppercase letters (e.g. X, Y ,
or Z) to denote sets of variables. A bold lowercase
or uppercase letter refers to a value of a variable or
set of variables, respectively. A value X of a set of
variables
X is a member in the Cartesian product
Q
d(x)
where d(x) is the set of values of x. The
x∈X
notation X = X stands for x1 = x , ..., xn = xn
where X = {x1 , ..., xn } and xi is a value of xi .

Definition Let U = {u1 , ..., un } be a finite set
of variables with d(ui ) and P as above. If X, Y ,
and Z are three disjoint subsets of U , then X is
probabilistically independent of Y given Z, denoted
IP (X, Y ; Z), iﬀ for every three sets of values X, Y ,
and Z of X, Y , and Z, respectively, the following
equation holds:
P (X = X, Y = Y | Z = Z) =
P (X = X| Z = Z) · P (Y = Y | Z = Z)

(1)

Every probability distribution induces a dependency model:
Definition (Pearl, 1988) A dependency model M
over a finite set of elements U is any set of triplets
(X, Y ; Z) where X, Y and Z are disjoint subsets
of U . The intended interpretation of M is that
(X, Y ; Z) ∈ M iﬀ X is independent of Y given Z,
also denoted by I(X, Y ; Z).
A probability distribution induces a dependency
model when we identify I with IP .
When speaking about dependency models, we
use both set notations and logic notations. If
(X, Y ; Z) ∈ M , we say that the independence statement I(X, Y ; Z) holds for M . Similarly, we either
say that M contains a triplet (X, Y ; Z) or that M
satisfies a statement I(X, Y ; Z). An independence
statement I(X, Y ; Z) is called an independency and
its negation is called a dependency.
Graphoids are special types of dependency models:

I(X, ∅; Z)

(2)

I(X, Y ; Z) ⇒ I(Y, X; Z)

(3)

Symmetry

Decomposition
I(X, Y ∪ W ; Z) ⇒ I(X, Y ; Z)

(4)

Weak union
I(X, Y ∪ W ; Z) ⇒ I(X, Y ; Z ∪ W )

(5)

Contraction
I(X, Y ; Z) & I(X, W ; Z ∪ Y ) ⇒
I(X, Y ∪ W ; Z)

(6)

It can readily be shown that probabilistic independence (IP ) satisfies these axioms, and therefore every probability distribution defines a graphoid. Several additional types of graphoids are discussed in
(Pearl, 1988; Pearl and Paz, 1989). A simple example of graphoids is given below. Consider a graphoid
M1 over U = {a, b, c, d} which consists of the independence statement I({a, b}, {c, d}; ∅) and the ones
derivable from it by the graphoid axioms. Notice
that using weak union, decomposition, and symmetry axioms, I({a, b}, {c, d}; ∅) implies that the following statements are in M1 as well:
{I(a, c; ∅), I(a, c; b), I(a, c; d), I(a, c; {b, d})}
and, therefore, a and c are totally independent. Similarly, b and c are totally independent.
Next we define total independence and total uncoupledness in graphoid terminology.
Definition Let M be a graphoid over a finite set
of elements U . Two elements a and b of U are said
to be totally independent (wrt M ) iﬀ (a, b; Z) ∈ M
for every subset Z of U \ {a, b}. When a and b are
not totally independent, then we say that a and b
interact and denote it by interact(a, b).
Definition Let M be a graphoid over a finite set
of elements U . Two elements a and b of U are said
1 This

definition diﬀers slightly from that given in (Pearl
and Paz, 1989) where axioms (3) through (6) define semigraphoids. Axiom (2) is added for clarity.

to be totally uncoupled (wrt M ) iﬀ there exist two
subsets U1 and U2 of U such that a ∈ U1 , b ∈ U2 ,
U1 ∩ U2 = ∅, U1 ∪ U2 = U , and (U1 , U2 ; ∅) ∈ M .
When a and b are not totally uncoupled, then we
say that a and b are coupled.
Notice that due to symmetry, decomposition, and
weak union axioms total uncoupledness implies total
independence. The converse does not always hold.
For example, if U consists of three variables a, b and
c, then it is possible that a and b are totally independent, namely that a and b are both marginally
independent [i.e. I(a, b; ∅)], and independent conditioned on c, and yet no variable is independent of the
other two. This happens, for example, if a and b are
the outcome of two independent fair coins and c is a
variable whose domain is {head, tail} × {head, tail}
and whose value is (i, j) if and only if the outcome
of a is i and the outcome of b is j.
This example leads to the following definition:
Definition A graphoid M over a finite set of elements U is separable iﬀ every two totally independent elements a and b are totally uncoupled.
The property of separability, as it turns out, can
be cast in another appealing format; it is equivalent
to the requirement that interaction (the converse of
total independence) is transitive, namely, that interact satisfies axiom 7 below.
Definition A graphoid M over a finite set of elements U is transitive iﬀ
interact(a, b) & interact(b, c) ⇒
interact(a, c)

(7)

This axiom is so appealing to our intuition that
we are tempted to speculate that all distributions
not obeying this property are epistemologically inadequate for modeling a human reasoner, and that
distributions that do satisfy this property are natural in the sense that they adequately represent the
conventional properties of the word “interact”.
The following theorem establishes the equivalence
between separability and transitivity. The proofs of
this and subsequent theorems can be found in the
appendix.
Theorem 1 A graphoid M over a finite set of elements is separable iﬀ it is transitive.
Probabilistic independence between sets of variables, as is well known, is not determined by the independencies between their individual elements; two
sets may be dependent although their individual elements are independent. For example, if a and b

represent the outcome of two independent fair coins
(using 0’s and 1’s) and c is their sum modulo 2 then c
is dependent on {a, b} but is independent of each individual variable. The absence of this compositional
property stands in contrast to our intuition because
we normally expect that a proposition unrelated to
the pieces of some body of knowledge is unrelated to
the whole as well.
Lemma 2 below shows that total independence
does not suﬀer from this anomaly.
Definition Let M be a graphoid over a finite set
of elements U . Two disjoint subsets A and B of U
are totally independent wrt M iﬀ (A, B; Z) ∈ M for
every Z that is a subset of U \ A ∪ B.
Lemma 2 Let M be a graphoid over a finite set of
elements U and let A, B, and C be three subsets of
U . If A and B are totally independent and A and C
are totally independent then, A and B ∪C are totally
independent as well.
Proof: Denote the sentence “X is totally independent of Y ” with J(X, Y ). By definition,
J(A, B) implies (A, B; Z) ∈ M and J(A, C) implies
(A, C; Z ∪ B) ∈ M where Z is an arbitrary subset
of U \ A ∪ B ∪ C. Together, these statements imply by contraction that (A, B ∪ C; Z) ∈ M . Hence,
J(A, B ∪ C) holds. ✷
The definition of total uncoupledness can similarly
be extended to sets and it also satisfies the compositional property stated by Lemma 2.

3

Connectedness
Networks

in

Belief

We have introduced two ways of defining totally unrelated elements of a graphoid; total independence,
and total uncoupledness. Here we suggest a third
approach—total disconnectedness: Elements a and
b are totally disconnected in M if they are disconnected in every belief network representation of M .
We shall see below that two elements are totally disconnected in M if and only if they are totally uncoupled in M .
Definition Let M be a graphoid over a finite set
of elements U . A directed acyclic graph D is a belief network representing M iﬀ there exists a one to
one mapping between elements in U and nodes of
D, and D is constructed from M by the following
steps: assign a total ordering d : u1 , u2 , , ..., un to
the elements of U . For each element ui in U , identify a minimal set of predecessors π(ui ) such that

(ui , {u1 , ..., ui−1 } \ π(ui ); π(ui )) ∈ M . Assign a direct link from every node corresponding to an element in π(ui ) to the node corresponding to ui .
For example, a belief network representing M1 ,
our example from the previous section, constructed
in the order a, b, c, and d consists of four nodes
a, b, c and d and edges from a to b and from c to
d. Another belief network of M1 , constructed in the
order d, c, b, and a yields a graph with reversed
edges. In general, diﬀerent orderings yield networks
with diﬀerent sets of edges.
Definition A trail in a belief network is a sequence
of links that form a path in the underlying undirected graph. Two nodes are connected in a belief
network D iﬀ there exists a trail connecting them in
D. Otherwise they are disconnected. A connected
component of a belief network D is a subgraph D0
in which every two nodes are connected, and D0 is
maximal iﬀ there exists no supergraph of it with this
property.
Definition Two elements of a graphoid M are
said to be totally disconnected iﬀ in every belief network representation of M the nodes corresponding
to these elements are disconnected. Otherwise these
elements are connected in M .
For example in M1 , a and c are totally disconnected. However, to verify this fact would have been
quite diﬃcult without the next theorem which shows
that it suﬃces to examine a single belief network representation of M1 in order to determine disconnectedness, rather than to check all such representations.
Theorem 3 Two elements of a graphoid M are disconnected in some belief network representation of
M iﬀ they are disconnected in every belief network
representation of M (i.e., disconnected in M ).
Consequently, we obtain:

Theorem 4 Let M be a graphoid. Two elements
a and b of M are totally disconnected iﬀ they are
totally uncoupled.
We have thus far obtained the relationships between three formulations of unrelatedness: total disconnectedness and total uncoupledness are equivalent, both are stronger than total independence, and
all three definitions are equivalent for transitive (separable) graphoids.

4

Separable Distributions and
Instantiated Graphoids

The notion of separability developed so far would
have remained unrealized unless examples of separable graphoids were provided. This section provides such examples. Our plan is to introduce a new
axiom, propositional transitivity, show that it implies separability and that it holds for regular Gaussian distribution and strictly positive binary distributions. Consequently, these type of distributions
are separable.
Definition A regular Gaussian distribution is a
multivariate normal distribution with finite nonzero
variances and with finite means. A strictly-positive
binary distribution is a probability distribution
where every variable has a domain of two values,
say 0 and 1, and every combination of the variables’
values has a probability greater than zero.
The definition of dependency models and
graphoids of section 2 precludes the representation
of statements of the form “a and b are independent
given c = c1 , yet dependent given c = c2 ” because
graphoids do not distinguish between values of a
variable. Thus in order to represent axioms that refer to specific values of a variable (as propositional
transitivity does), requires a slight modification of
these definitions.
Definition Let U be a finite set of variables each
associated with a finite set of values. An instantiated dependency model M over U is a set of triplets
(X, Y ; Z) where X, Y and Z are disjoint subsets of
U , and X, Y and Z are their values respectively.
Clearly, every instantiated dependency model
MR , defines a dependency model M in the sense of
section 2; a triplet (X, Y ; Z) is in M iﬀ (X, Y ; Z)
is in MR for every value X, Y , Z of X, Y and Z,
respectively. The model MR is said to induce M . In
particular, every probability distribution defines an
instantiated dependency model.
Definition An instantiated graphoid is any instantiated dependency model that induces a graphoid.
Theorem 5 (Geiger and Heckerman, 1989 )
Regular Gaussian distributions and strictly positive binary distributions satisfy the following axiom,
named propositional transitivity:2
I(A1 A2 A3 A4 , B1 B2 B3 B4 ; ∅) &
I(A1 A2 B3 B4 , B1 B2 A3 A4 ; e = e0 ) &
2 In complicated expressions, A A is used as a shorthand
1 2
notation for A1 ∪ A2 and eA1 denotes {e} ∪ A1 .

I(A1 A3 B2 B4 , B1 B3 A2 A4 ; e = e00 ) ⇒
I(A1 , eA2 A3 A4 B1 B2 B3 B4 ; ∅) ∨
I(B1 , eA1 A2 A3 A4 B2 B3 B4 ; ∅)

(8)

Where all sets mentioned are pairwise disjoint and
do not contain e, and e0 and e00 are distinct values
of e.
Theorem 6 Every instantiated graphoid satisfying
propositional transitivity is separable.
Regular Gaussian distributions satisfy axioms
other than propositional transitivity which can be
used to show separability. A particularly interesting
one is unification:
I(X = X, Y = Y ; Z = Z) ⇒ I(X, Y ; Z)
which states that if X and Y are independent given
one arbitrary value of X, Y , and Z, then these sets
are independent given every value of Z. Thus, although, regular Gaussian distributions have infinite
domains, the number of independencies is finite and
can be completely represented assuming finite domains.
We have chosen, however, to focus on propositional transitivity because this choice allows us to
unify the separability proof for two quite diﬀerent classes of distributions, thus, demonstrating the
power of this axiomatic approach.
We conjecture that propositional transitivity
holds also for binary distributions that are not
strictly positive.

5

Probabilistic
Acquisition

Knowledge

The construction of belief networks as faithful representations of a given domain relies on the ease and
confidence by which an expert can describe the relationships between variables in this domain. Explicating these relationships is often straightforward
but may encounter diﬃculties when variables have
many values. For example, in medical diagnosis, a
variable corresponding to “cancer” may have dozens
of possible values, each corresponding to a diﬀerent
type of cancer. An expert wishing to describe the
relationship between the diﬀerent symptoms, tests,
and treatments of cancer may find it rather confusing unless he first partitions the many types of
cancer into several groups sharing common characteristics; in fact, the grouping of related pieces of

information into more or less independent chunks
is an important step in organizing any large body
of knowledge. Below, we shall see how the theory developed in previous sections facilitates this
task, through the construction of similarity networks
(Heckerman, 1990a; Heckerman, 1990b).
Let h be a distinguished variable designated for
the disease “hypothesis”, and let the values of h,
h , . . ., hn , stand for an exhaustive list of possible diseases. First, a connected undirected graph is
constructed where each of the n nodes represents
a diﬀerent value of h and each link represents a
pair of “similar” diseases, namely diseases that are
sometimes hard to discriminate. Then, for each link
hi —hj in the graph, a local belief network is composed, assuming that either h = hi or h = hj must
hold; it consists of a distinguished root node h, additional nodes that are connected to h representing
symptoms, and links representing the dependencies
among these symptoms and their relationship to the
hypothesis node h. Finally, the global network is
formed from the local networks; it consists of the
union of all links and their adjacent nodes in the
local networks.
In (Heckerman, 1990b), it is shown that under the
assumption of strict-positiveness, namely that every
combination of symptoms and diseases is feasible,
the union of the connected components of node h in
each local network generates a belief network that
faithfully represents the domain. That is, the assertions of conditional independence encoded in the
graph union of the local networks are logically implied by the assertions of conditional independence
in each of the local networks. Although when using
this methodology we must construct many local networks instead of one, there are two important advantages to such a composition. First, local networks
for pairs of similar diseases tend to be small. Second, by composing local networks for pairs of similar diseases, the expert can direct his attention on
those diagnostic subproblems with which he is familiar and thereby increase the quality of the knowledge
he provides.
A diﬃculty with this approach is to identify the
set of nodes that are connected to node h in each
local network. In principle, we could consult the expert by asking him directly queries of the form: “is
node s (s connotes symptom) connected to node h,
given that either h = hi or h = hj must hold?”.
This query, however, may be inadequate because it
refers to a graphical representation of the domain, a
language with which the expert might not be familiar. On the other hand, the query “does this symp-

tom in any circumstances help you to discriminate
between the two diseases hi and hj ?” is much more
appealing since it addresses directly the knowledge
of the expert.
The first query asks about total disconnectedness of s and h, while the second query, which is
concentrated on determining whether P (s|hi , Z) 6=
P (s|hj , Z) for some values of some set of variables
Z, corresponds to asking about total independence
of s and h. This paper shows that total disconnectedness and total independence coincide for transitive
distributions and identifies important classes of distributions that are transitive. Consequently, using
the second query in the construction of similarity
networks is a theoretically-justified heuristic; and
indeed its soundness has been empirically verified
(Heckerman, 1990b).

6

Summary

We have examined the notion of unrelatedness of
variables in a probabilistic framework. We have
introduced three formulations for unrelatedness—
total independence, total uncoupledness, and total disconnectedness— and explored their interrelationships. ¿From a practical view point, these results legitimize prevailing decomposition techniques
of knowledge acquisition; it allows an expert to decompose the construction of a complex belief network into a set of belief networks of manageable size.
Our proof technique uses the qualitative notion of independence as captured by the axioms
of graphoids and would have been much more
diﬃcult had we used the probabilistic definitions
of conditional independence. This axiomatic approach enables us to identify a common property—
propositional transitivity—shared by two distinct
classes of probability distributions (regular Gaussian
and strictly-positive binary), and to use this property without attending to the detailed characteristics
of the classes.
In addition, we have shown that useful classes of
probability distributions are transitive, the proof of
which is facilitated greatly by the network formulation. Thus, we see that network representations,
apart of their dominant role in representing experts’
opinions, are also a powerful mathematical tool for
uncovering formal properties of independence relationships.

Appendix
Some preliminary definitions are needed.
Definition (Pearl, 1988) A node b is called a headto-head node wrt a trail t iﬀ there exist two consecutive edges a → b and b ← c on t. A trail t is active
by Z if (1) every head-to-head node wrt t either is
or has a descendent in Z and (2) every other node
along t is outside Z. Otherwise, the trail is said to
be blocked by Z.
Definition (Pearl, 1988) If X, Y , and Z are three
disjoint subsets of nodes in a dag D, then Z is said to
d-separate X from Y , denoted ID (X, Y ; Z), iﬀ there
exists no active trail by Z between a node in X and
a node in Y .
The next theorem states that d -separation is a
sound criteria; in (Geiger et al., 1990) d -separation
is shown to be complete as well. These results are
fundamental to the theory of belief networks.
Theorem 7 (Verma and Pearl, 1988) Let D be a
belief network representing a graphoid M over a finite set of variables U . If X and Y are d-separated
by Z in D then (X, Y ; Z) ∈ M .
Lemma 8 Let D be a belief network representing a
graphoid M over a finite set of variables U . If A and
B are two subsets of U and they correspond to two
disconnected sets of nodes in D, then (A, B; ∅) ∈ M .
Proof: Follows directly from the theorem above (7);
there is no active trail between a node in A and a
node in B which makes A and B d -separated given
the empty set, hence, (A, B; ∅) is in M . ✷
Proof of Theorem 3: It suﬃces to show that
any two belief networks representing M share the
same maximal connected components. Let DA and
DB be two belief networks representing M . Let CA
and CB be maximal connected components of DA
and DB respectively. Let A and B be the nodes of
CA and CB respectively. We show that either A = B
or A ∩ B = ∅. This will complete the proof because
for an arbitrary maximal connected component CA
in DA there must exists a maximal connected component in DB that shares at least one node with
CA and thus, by the above claim, it must have exactly the same nodes as CA . Thus each maximal
connected component of DA shares the same nodes
with exactly one maximal connected component of
DB . Hence DA and DB share the same maximal
connected components.
Since DA is a belief network representing M and
CA is a connected component of DA , by Lemma 8,

(A, U \ A; ∅) ∈ M , where U stands for M ’s elements.
By symmetry (3) and decomposition (4), (A∩B, B \
A; ∅) ∈ M . A subgraph is a connected component
only if its nodes cannot be partitioned into two sets
U1 and U2 such that (U1 , U2 ; ∅) ∈ M . Hence, for CB
to be connected either A∩B or B \A must be empty.
Similarly for CA to be connected, A ∩ B or A \ B
must be empty. Thus, either A = B or A ∩ B = ∅.
✷
Proof of Theorem 4: Assume a and b are disconnected in one belief network. Then (U1 , U2 ; ∅) ∈
M where U1 are the elements corresponding to nodes
connected to a and U2 are the rest of M ’s elements
(Lemma 8). Hence a and b are uncoupled.
Assume a and b are uncoupled, and that U1 and U2
are the two independent sets which partition M ’s elements and which contain a and b respectively. Constructing a belief network for M in an order in which
all elements in U1 are placed before any element in
U2 yields a network where U1 and U2 are disconnected. ✷
Proof of Theorem 1: First we notice that if M
is separable, namely, when total independence implies total uncoupledness, then M is transitive because in this case total uncoupledness coincides with
total independence and coupledness is transitive.
It remains to show the converse; transitivity implies separability. Let U stand for M ’s elements. Let
a and b be two arbitrary elements of U that do not
interact. We will show by induction on |U | that if
interact satisfies transitivity (7) then there exists a
belief network representation D of M where a and b
are disconnected. Consequently, a and b are uncoupled (Theorem 4) and, therefore, M is separable.
We construct D in the ordering u1 ≡ a, u2 ≡
b, u3 , ..., un ≡ e of M ’s elements. Assume n = 2.
Variables a and b do not interact, therefore (a, b; ∅) ∈
M . Thus, a and b are not connected. Otherwise,
n > 2. Let De be a belief network formed from M
by the ordering u1 , ..., un−1 of M ’s elements. let A
be the set of nodes connected to a and let B be the
rest of the nodes in De . The network D is formed
from De by adding the last node e as a sink and
letting its parents be a minimal set that makes e independent of all the rest of the elements in M (see
the definition of belief networks). By the induction
hypothesis, before e was added, A and B are disconnected. After node e is added, a trail through e
might exists that connects a node in A and a node in
B. We will show that there is none; if the parent set
of e is indeed minimal, then either e has no parents
in A or it has no parents in B, rendering a and b
disconnected.

Since a and b do not interact and since M satisfies
transitivity (7), it follows that either a, or b, do not
interact with e. Without loss of generality assume
that a and e do not interact. Let a0 be an arbitrary
node in A. By transitivity it follows that either a
or e do not interact with a0 , for otherwise, a and
e would interact, contrary to our selection of a. If
a and a0 do not interact, then by the induction hypothesis, A can be partitioned into two independent
subsets, thus A would not be connected in the belief
network De , contradicting our selection of A. Thus,
every element a0 ∈ A does not interact with e. It
follows that the entire set A does not interact with
e (Lemma 2). Thus, in particular, (A, e; B̂) ∈ M
where B̂ are the parents of e in B. Consequently,
e has no parents in A because otherwise its set of
parents were not minimal because B̂ would make e
independent of all other elements in U . Hence, a
and b are on two diﬀerent connected components of
D. ✷
The following definition which abstracts the notion of conditional distribution, is needed for the
next proof.
Definition Let M (U ) be an instantiated dependency model over a finite set of variables U =
{u1 , ..., un }. The conditional of M (U ) on un = un ,
denoted M ({u1 , ..., un−1 }|un = un ), is an instantiated dependency model that contains a triplet
(X, Y ; Z) iﬀ (X, Y ; Z ∪ {un }) ∈ M (U )
Proof of Theorem 6: Let M be an instantiated
graphoid and let U be its variables. Let a and b be
two arbitrary variables in U that are totally independent. We will show by induction on |U | that if
M satisfies propositional transitivity then there exists a belief network representation D of M where a
and b are disconnected. Consequently, a and b are
uncoupled (Theorem 4) and, therefore, M is separable.
We construct D in the ordering u1 ≡ a, u2 ≡
b, u3 , ..., un ≡ e of M ’s variables. Assume n = 2.
Variables a and b are totally independent, therefore,
(a, b; ∅) ∈ M . Thus, a and b are disconnected. Otherwise, n > 2. Let De be a belief network formed
from M by the ordering u1 , ..., un−1 of M ’s variables. let A be the set of nodes connected to a
and let B be the rest of the nodes in De . Since
a and b are totally independent, by the induction
hypothesis, (A, B; ∅) ∈ M (≡ I1 ). The network D
is formed from De by adding the last node e as a
sink and letting its parents be a minimal set that
makes e independent of the rest of M ’s variables
(see the definition of belief Networks). Let De0 and

De00 be belief networks of the conditional graphoids
M (A ∪ B|e = e’) (≡ Me’ ) and M (A ∪ B|e =
e”) (≡ Me” ) respectively, formed in the ordering
u1 , ..., un−1 . Since both Me’ and Me” are subsets
of M , a and b are totally independent in both these
dependency models. By the induction hypothesis
Me’ and Me” are separable. Hence there exists a
partitioning Ae’ , Âe’ , Be’ and B̂e’ of A ∪ B where
A = Ae’ Âe’ , B = Be’ B̂e’ , a ∈ Ae’ and b ∈ Be’ ,
such that (Ae’ B̂e’ , Be’ Âe’ ; ∅) ∈ Me’ (≡ I2 ). Similarly, there exists a possibly-diﬀerent partitioning
Ae” , Âe” , Be” and B̂e” of A∪B where A = Ae” Âe” ,
B = Be” B̂e” , a ∈ Ae” and b ∈ Be” , such that
(Ae” B̂e” , Be” Âe” ; ∅) ∈ Me” (≡ I3 ). In other words,
each of the two instances of e induces a partitioning
of A and B into two independent subsets. There
are at most eight disjoint subsets formed by the two
partitioning. These are: A1 ≡ Ae’ ∩ Ae” , A2 ≡
Âe’ ∩ Ae” , A3 ≡ Ae’ ∩ Âe” , A4 ≡ Âe’ ∩ Âe00 , B1 ≡
Be’ ∩Be00 , B2 ≡ B̂e’ ∩Be” , B3 ≡ Be’ ∩B̂e” and B4 ≡
B̂e’ ∩ B̂e00 . These definitions yield the following relationships: A = A1 A2 A3 A4 , Ae’ = A1 A3 , Âe’ =
A2 A4 , Ae” = A1 A2 , Âe” = A3 A4 , B = B1 B2 B3 B4 ,
Be’ = B1 B3 , B̂e’ = B2 B4 , Be” = B1 B2 and B̂e” =
B3 B4 . Rewriting assertions I1 , I2 and I3 using
these notations yields (A1 A2 A3 A4 , B1 B2 B3 B4 ; ∅) ∈
M , (A1 A3 B2 B4 , B1 B3 A2 A4 ; e = e’) ∈ M and
(A1 A2 B3 B4 , B1 B2 A3 A4 ; e = e”) ∈ M which are
the three antecedents of propositional transitivity (8). Since M is closed under this axiom, it
follows that either (A1 , eA2 A3 A4 B1 B2 B3 B4 ; ∅) ∈
M or (B1 , eA1 A2 A3 A4 B2 B3 B4 ; ∅) ∈ M. Since a ∈
A1 and b ∈ B1 , M is separable. ✷

Acknowledgments
This research is based on our dissertations (Geiger,
1990; Heckerman, 1990b). We thank Judea Pearl for
encouraging us to pursue the subject, Dan Hunter
for checking our proofs, and Jeﬀ Barnett, Norman
Dalkey, Ross Shachter, Steve Smith, and Tom Verma
for helpful comments.

References
Geiger, D. (1990). Graphoids: A Qualitative Framework for Probabilistic Inference. PhD thesis,
Computer Science Department, University of
California.

Geiger, D. and Heckerman, D. (1989). Interaction
models. Technical Report R-141, Cognitive Systems Laboratory, University of California, Los
Angeles, CA. In preparation.
Geiger, D., Verma, T., and Pearl, J. (1990). Indentifying independence in Bayesian networks.
Networks, 20.
Heckerman, D. (1990a). Probabilistic similarity networks. Networks, 20.
Heckerman, D. (1990b). Probabilistic Similarity Networks. PhD thesis, Program in Medical Information Sciences, Stanford University, Stanford,
CA.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.
Morgan Kaufmann, San Mateo, CA.
Pearl, J. and Paz, A. (1989). Graphoids: A graphbased logic for reasoning about relevance relations. In Du Boulay, B., editor, Advances in
Artificial Intelligence—II. North-Holland, New
York.
Verma, T. and Pearl, J. (1988). Causal networks:
Semantics and expressiveness. In Proceedings
of Fourth Workshop on Uncertainty in Artificial
Intelligence, Minneapolis, MN, pages 352–359.
Association for Uncertainty in Artificial Intelligence, Mountain View, CA.

