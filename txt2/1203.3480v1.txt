
Game theory has been widely used to model strategic
interactions of agents. A natural question is where do
the game representations come from? Traditionally,
games have usually been constructed by hand, but in
that case they may only be theoretical abstractions of
reality. Alternatively, one might have access to a simulator, for example, in the trading agent competition,
but that is unusual. The problem is similar to that of
developing a probabilistic model, and a common approach, which we take in this paper, is to learn the
model from data.
Consider for example the following situation. A farmer
wants to begin selling fruits in a market. She has to

Figure 1: Our Learning Approach
The naive approach makes the connection between the
strategy and payoff variables with the observed data,
but does not connect the strategy variables to the payoff variables. An alternative approach could learn the
payoffs from data, and then connect the strategies to
the payoffs by solving the game, using some notion of
rationality. This approach, however, ignores the observed data about strategies. Our approach makes all
three connections. We learn both the payoff variables
and strategy variables from data, and use the payoffs
to inform the strategies and vice versa.

A possible notion of rationality is Nash equilibrium.
However, Nash equilibrium is a brittle concept since
it assumes players have completely converged to playing rational strategies and does not allow for noise.
Instead, we use the quantal response equilibrium concept to model the rationality of agents. In quantal
response equilibrium, players do not choose best responses with probability one as in Nash equilibrium.
Instead, they ‚Äúbetter respond‚Äù by choosing responses
with higher expected payoffs with higher probabilities.
Using the quantal response equilibrium allows us to
model bounded rationality of agents as well as being
robust to noise in the data.
Our approach is formulated as a weighted constraint
satisfaction problem (WCSP). Our WCSP model has
variables for both mixed strategy profiles and payoff
values. The two sets of soft constraints assign lower
costs to variable assignments that have higher probability based on the observed data. The third set of
soft constraints expresses the bounded rationality of
the agents using the quantal response equilibrium concept. The advantage of using the WCSP framework is
that we are able to incorporate and balance the goals
of fitting the learned strategy profiles and payoff values to the observed data with the goal of connecting
the learned strategy profiles and payoff values using
the quantal response equilibrium. Furthermore, using
a CSP framework allows us to potentially add additional types of constraints for dealing with more complex game structures. For instance, one possible future
work is to add additional constraints for learning the
structures of graphical games (Kearns et al., 2001).
Related Work. There has been some research in recent years on learning games from data. Ficici et al.
(2008) learned a reduced game form representation
from data by clustering agents with similar strategic views of the game and used this representation to
approximately solve asymmetric games of many players. Vorobeychik et al. (2007) used regression learning
techniques to learn payoff functions of infinite games.
Duong et al. (2009) applied existing learning algorithms such as branch-and-bound, greedy, and local
search algorithms to learning graphical games from
data. These search algorithms were evaluated based
on metrics such as minimizing empirical loss, approximating graphical structure, and approximating Nash
equilibria. All these approaches differ from ours in that
they do not use rationality to inform the learning.
Soni et al. (2007) formulated the problem of computing pure strategy approximate Nash equilibria in oneshot complete-information games as a constraint satisfaction problem. Their PureProp CSP defined variables corresponding to the strategy profiles of players and constraints to solve for -Nash equilibrium of

the resulting game. Their work is similar to our approach since we essentially use rationality constraints
to solving for quantal response equilibrium of the normal form game based on observed data on payoff values. However, their work differs from our approach
because we also use constraints related to learning the
parameters from observed data in our CSP framework.

2
2.1

Preliminaries
Normal Form Games

In this paper, we consider one-shot complete information normal form games. Such a game G = [I, A, U]
is given by a set of players I with |I| = N , a set of
pure strategies A = √ói‚ààI Ai where Ai is the set of
pure strategies for player i ‚àà I, and U = √ói‚ààI ui where
ui : A ‚Üí < is the payoff function for player i ‚àà I. A
mixed strategy
of player i is denoted by œÉi : Ai ‚Üí [0, 1]
P
where
a‚ààAi œÉi (a) = 1 is a probability distribution
over all the pure strategies of player i. We use ‚àíi to
refer to the set of all players other than player i. Thus,
a‚àíi and œÉ‚àíi denote the joint pure strategy and mixed
strategy profile of the set of players ‚àíi.
We focus on finite games for which the set of players
I and the sets of pure strategies Ai for each player
i ‚àà I are finite. The game G is considered complete
information in the sense that each player i has full
knowledge of the payoff functions of the other players.
2.2

Equilibrium Notions

In our learning model, we consider two popular equilibrium notions: the mixed strategy Nash equilibrium
and the quantal response equilibrium.
Nash equilibrium assumes that every player is completely rational. In Nash equilibrium, every player i is
playing a best response to the mixed strategy profile
of all the other players ‚àíi.
Definition: 1 A joint mixed strategy profile œÉ ‚àó is
Nash equilibrium if every player i is playing a mixed
strategy best response to the strategy profiles of its opponents ‚àíi, i.e.
‚àó
‚àó
ui (œÉ‚àíi
, œÉi‚àó ) ‚â• ui (œÉ‚àíi
, œÉi ), ‚àÄœÉi

(1)

This definition implies that no player can benefit in
expectation by deviating unilaterally from the Nash
equilibrium strategy profile œÉ ‚àó .
We use the quantal response equilibrium notion to
model the bounded rationality of agents (McKelvey
and Palfrey, 1996). In a quantal response equilibrium, players fix their strategies, form beliefs about the

strategies of the other players, compute their expected
payoffs based on their beliefs, and choose a strategy
which assigns relatively higher probabilities to actions
with relatively higher expected payoffs. We use the
most common specification for the quantal response
equilibrium called the logit equilibrium (LQRE). The
LQRE uses an exponentiated response function and a
positive real valued parameter Œª to capture how sensitive each player is to the differences in expected payoffs. The parameter Œª can be interpreted as a measure
of the degree of rationality of the particular player
choosing this response function. For instance, when
Œª = 0, the player chooses each pure strategy with equal
probability. As Œª increases, the player becomes more
and more responsive to the differences in expected payoffs of different pure strategies and chooses his mixed
strategy best response accordingly. The strategies chosen by the players in a LQRE converges to Nash equilibrium when Œª approaches infinity.
Definition: 2 A joint mixed strategy profile œÉ ‚àó is a
LQRE for some non-negative real number Œª (referred
to as Œª-LQRE hereafter), if for every player i, œÉ ‚àó (ai )
for every ai ‚àà Ai is a exponentiated best response to
‚àó
the joint mixed strategy profile œÉ‚àíi
, i.e. for all ai ‚àà Ai ,
we have
œÉi‚àó (ai )

2.3

=P

e

(Œª

P

aj ‚ààAi

a‚àíi ‚ààA‚àíi

e

(Œª

‚àó
œÉ‚àíi
(a‚àíi )ui (ai ,a‚àíi ))

P

a‚àíi ‚ààA‚àíi

‚àó (a
œÉ‚àíi
‚àíi )ui (aj ,a‚àíi ))

(2)

Weighted Constraint Satisfaction
Problems

A classic constraint satisfaction problem (CSP) is defined by a set of variables, each with a finite domain,
and a set of constraints. Each constraint consists of
a subset of the variables, and a relation over the variables in the constraint. A solution to a CSP is an
assignment of values to the variables such that every
constraint is satisfied, i.e., the projection of the assignment onto the constraint variables is in the constraint
relation.
A weighted CSP (WCSP) is similar to a CSP except
that it has two kinds of constraints. Hard constraints
are like those in an ordinary CSP. Soft constraints consist of a set of variables and a cost function from those
variables to non-negative numbers. The goal of solving
a CSP is to find an assignment such that all the hard
constraints are satisfied, while the sum of the costs of
the soft constraints for the assignment is minimized.

3

Problem Definition

We consider generating M samples of the play of a
one-shot simultaneous-move game G. G is a com-

plete information game, so the players have complete
knowledge of the payoff matrix of the game. We assume that player i chooses a fixed mixed strategy œÉi
chosen in advance and plays it for every sample generated. This is a simplifying assumption that allows us
to treat the training instances as independent; relaxing it is a topic for future work. For the k-th sample,
player i chooses to play some pure strategy ai ‚àà Ai
and incurs some real valued payoff ui (A) where A is
the joint pure strategy profile chosen by all the players.
For example, in our fruit vendor example, the players
are the fruit vendors, and the pure strategies of each
player are the location of the stall, the types of fruit
sold, and the prices. This game is played for many
times by the fruit vendors.
In this paper, we take the perspective of an outside
observer (referred to as O hereafter). O is not a participant of the game but might be someone who potentially wants to join the game, just like the fruit
seller who wants to join the market. We assume that
the number of players and the set of pure strategies
available to each player are known to O. We also assume that the set of players playing the game is fixed.
Given these assumptions, O observes the game being
played for M times. Whenever, the players choose a
joint pure strategy profile a for the k-th sample, O
can observe this joint pure strategy profile a perfectly.
This assumption fits our domain, where the observer
can observe the fruit, prices and location of each vendor, but can easily be relaxed if necessary. However,
O cannot observe the payoffs directly, but only has a
noisy estimate of them. In the fruit example, the observer can observe the number of customers at each
stall but not the exact transactions. We model this by
having O observe a payoff that is Gaussian distributed
around its expected value ui (A) using a fixed variance
R2 ; other noise models can easily be accommodated.
Our goal is, given these observations, to learn a representation of the game that balances the fit of the
learned game to the data with the assumption that
the players are playing a Œª-LQRE for some fixed Œª.
This balancing involves three factors: (1) the fit of the
estimated payoffs to the observed payoffs; (2) the fit of
the estimated strategies to the observed strategies; and
(3) the fit of the estimated strategies to the estimated
payoffs. The Œª-LQRE assumption may or may not be
correct. Therefore, we do not enforce this assumption
in a hard manner, but rather introduce slack, penalizing estimates that deviate far from it. The assumption
serves as a force with which to enforce some sense on
the data. It behaves rather like a Bayesian prior. With
a large amount of data, the LQRE assumption will be
overcome, but with small amounts of data it will help
prevent over-fitting. For the LQRE assumption, we

assume that the outside observer knows all the Œª values for the players although the players do not need to
know each other‚Äôs Œª values.

to be the true value of œÉi (ak ) in the true mixed strategy œÉi (ak ) chosen by player i.

4

This consistency constraint ensures that the values of
œÉi (ak ) form a valid mixed strategy probability distribution since our strategy maximum likelihood constraints were defined for each pure strategy separately
rather than for the set of pure strategies for a particular player.

Our WCSP Learning Model

We formulate our learning model using the weighted
constraint satisfaction framework. Our WCSP model
has one variable corresponding to each œÉi (ai ) which
is the probability of the pure strategy ai in the mixed
strategy œÉi of each player i. Moreover, our WCSP also
defines one variable for each entry in the ui (A) which
is the true payoff vector of player i when the joint pure
strategy profile chosen by all the players is A. Since
the domains of the strategy and payoff variables are
continuous, we discretize them to be multiples of real
valued discretization parameters  and Œ¥ respectively.
We also restrict the payoff values ui (A) to be within a
pre-defined range [umin , umax ] where umin , umax ‚àà <
are the minimum and the maximum of the observed
payoff values. Now we define the four types of constraints in our WCSP learning model.
Strategy Maximum Likelihood Constraints
Intuitively, the value of œÉi (ak ) for mixed strategy œÉi
for player i and the pure strategy ak ‚àà Ai for player
i gives the probability of observing the pure strategy
ak being played by player i for a particular instance
of game play. Therefore, given a sequence of observed
pure strategies a1k , ..., aM
k chosen by player i, we can
calculate the log-likelihood of a particular œÉi (ak ) value
given the observed sequence of pure strategies as

log œÉi (a1k ) √ó ... √ó œÉi (aM
k ) =

M
X

log œÉi (ajk ) (3)

j=1;ajk =ak

Given a sequence of observed pure strategies, for each
player i, we would like to choose a value for œÉi (ak ) in
order to maximize the log-likelihood of the observations. Maximizing the log-likelihood of observations is
equivalent to minimizing the negative log-likelihood of
the observations.
Therefore, for our strategy maximum likelihood constraints, we associate a cost with each possible discretized value of œÉi (ak ) for each pure strategy ak ‚àà Ai
of player i, which is equal to the negative log-likelihood
of the observed sequence of pure strategies chosen by
player i for the M samples of game play. For every
player i, and for each pure strategy ak ‚àà Ai for player
i, the cost can be calculated as follows:
cost(œÉi (ak )) = ‚àí log P (a1i , ..., aM
i |œÉi (ak ))

(4)

Based on the above equation, the value of œÉi (ak ) associated with minimum cost has the highest probability

Strategy Consistency Constraints

The strategy consistency constraint is a unary hard
constraint for the variables œÉi (ak ) of player i. This
constraint specifies that the probabilities œÉi (ai ) assigned to the pure strategies ai ‚àà Ai must sum to
1, i.e.
X
œÉi (ak ) = 1
(5)
‚àÄi ‚àà I,
ak ‚ààAi

Payoff Maximum Likelihood Constraints
We defined the observed payoff of a player as a Gaussian distribution N (ui (A), R2 ). Therefore, the loglikelihood of observing a sequence of payoff values
vi1 , ..., viM with the corresponding pure strategy profiles a1 , ..., aM for player i can be calculated as
M
X

cost(ui (a)) =

log N (vij , aj |ui (a), R2 )

(6)

j=1:aj =a

Given a sequence of observed payoff values, for each
player i, we would like to choose a value for ui (a) in
order to maximize the log-likelihood of the observations, which is equivalent to minimizing the negative
log-likelihood of the observations.
For our payoff maximum likelihood constraints, we define the cost for ui (a) as the negative log-likelihood of
the observed sequence of payoffs of player i when the
observed pure strategy profile aj is the same as a. For
every player i, the cost can be calculated as follows:
M
X

cost(ui (a)) = ‚àí

log N (vij , aj |ui (a), R2 )

(7)

j=1:aj =a

Rationality Constraints
There is a rationality constraint for every pure strategy
of every player. This constraint only involves variables
and does not involve any of the observed data. For
the rationality constraint, the cost for player i for a
pure strategy measures how far off the probability of
the strategy is from its quantal response probability.
From Equation 2, we obtain that in a Œª-LQRE,
X
| exp(Œª
œÉ‚àíi (a‚àíi )ui (ak , a‚àíi ))‚àí
œÉi (ak )

a‚àíi
X

ak ‚ààAi

exp(Œª

X
a‚àíi

œÉ‚àíi (a‚àíi )ui (ak , a‚àíi ))|

= 0 (8)

We therefore obtain the following constraint for all i ‚àà
I and ak ‚àà Ai :
cost(œÉi (ak ), œÉ‚àíi (¬∑), ui (ak , ¬∑)) =
X
Œ±| exp(Œª
œÉ‚àíi (a‚àíi )ui (ak , a‚àíi ))

(9)

a‚àíi

‚àíœÉi (ak )

X
ak ‚ààAi

exp(Œª

X

œÉ‚àíi (a‚àíi )ui (ak , a‚àíi ))|

a‚àíi

The œÉ‚àíi (¬∑) indicates that the constraint has a variable
for every pure strategy of every other player. Similarly,
ui (ak , ¬∑) indicates that it has one for every action profile consistent with player i playing ak . Clearly, this is
a lot of variables, so the constraint will be huge. We
will discuss how to deal with this in the next section.
Note that our LQRE constraints only include strategy
profiles observed in the data.
The parameter Œ± determines the strength of the rationality constraints compared to the data-driven strategy and payoff constraints. A larger Œ± is analogous to
having a stronger prior, and will make it more difficult to overcome the rationality constraint, whereas a
smaller Œ± will allow the rationality constraints to be
overcome more easily by the data. One method to
choose Œ± is to use cross-validation.

5

Complexity and Constraint
Decomposition

Clearly, the size of the rationality constraints is exponential in the number of variables. If there are N
players and K pure strategies per player, there will be
on the order of N K variables, so the size of the constraint will be exponential in N K. Exponential cost in
N is acceptable, because the size of the normal form
representation is itself exponential in the number of
players. Furthermore, representations like graphical
games provide a handle on how to deal with games
with large numbers of players. However, an exponential cost in K would severely limit the practicality of
the approach. Fortunately, we can avoid this cost using constraint decomposition, which we now describe.
We go through all four types of constraints in turn.
5.1

Strategy Maximum Likelihood
Constraints

The strategy maximum likelihood constraints are
unary constraints. In particular, there is one constraint for each pure strategy of each player. With
N players and K pure strategies for each player, there
are N K strategy maximum likelihood constraints in
our WCSP. If we use  as the discretization parameter for strategy variables, then the complexity of the
strategy maximum likelihood constraints is NK .

5.2

Strategy Consistency Constraints

Each strategy consistency constraint involves K strategy variables, and there are N such constraints. If we
use  as the discretization parameter for the strategy
variables, then there are on the order of 1 choices of
values for each variable. For each player i, if we implement this consistency constraint directly, then there
are 1K values to search from where  is a small positive
real number. The complexity of the strategy consistency constraints is NK which is exponential in K.
However, for each constraint for each player i, we define K extra variables t1 , ..., tk and decompose the
multi-variable hard constraint into K ternary hard
constraints as follows.
Ô£±
Ô£¥
Ô£¥t1 = œÉi (a1 )
Ô£¥
Ô£≤t = t + œÉ (a )
2
1
i 2
(10)
Ô£¥ ...
Ô£¥
Ô£¥
Ô£≥
tk = tk‚àí1 + œÉi (an ) = 1
Given this decomposition, we only need to search from
K
3 possible tuples of values. This is a significant reduction in complexity of this constraint as long as K > 3.
The complexity of this constraint is N3K which is polynomial in K.
5.3

Payoff Maximum Likelihood Constraints

The payoff maximum likelihood constraints are unary
constraints. There is one constraint for each pure
strategy for each player i, for a total of N K N constraints. If we use Œ¥ as the discretization parameter
for payoff variables, then the complexity of the payoff
N
maximum likelihood constraint is N K
Œ¥ .
5.4

Rationality Constraints

The rationality constraints are multi-variable soft constraints. There is one constraint for each pure strategy
of each player i. With N players and K pure strategies
for each player, we have N K rationality constraints.
For each player i and each pure strategy ak of player i,
we can decompose the rationality constraints in several
steps. First of all, the term œÉ‚àíi (a‚àíi )ui (ak , a‚àíi ) can be
decomposed into (N +1) ternary constraints as follows:
Ô£±
Ô£¥
Ô£¥t1 = œÉ1 (a1 )
Ô£¥
Ô£¥
Ô£¥
Ô£¥...
Ô£¥
Ô£¥
Ô£¥
Ô£¥ti‚àí1 = ti‚àí2 œÉi‚àí1 (ai‚àí1 )
Ô£≤
ti+1 = ti‚àí1 œÉi+1 (ai+1 )
Ô£¥
Ô£¥
Ô£¥
...
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
tn = tn‚àí1 œÉn (an )
Ô£¥
Ô£¥
Ô£≥
tn+1 = tn ui (ak , a‚àíi )

(11)

Use xi to denote the last equation in the decomposition
of the ith product œÉ‚àíi (a‚àíi )ui (ak , a‚àíi ). Then the term
X
exp(Œª
œÉ‚àíi (a‚àíi )ui (ak , a‚àíi ))
(12)
a‚àíi

can be decomposed as follows:
Ô£±
Ô£¥
y1 = x1
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤y2 = y1 + x2
...
Ô£¥
Ô£¥
Ô£¥ykn‚àí1 = ykn‚àí1 ‚àí1 + xkn‚àí1
Ô£¥
Ô£¥
Ô£¥
Ô£≥y n‚àí1 = eŒªykn‚àí1
k
+1

(13)

For decomposing Equation 12, a set of constraints of
in the form of equation 11 is constructed for every
joint action profile a‚àíi . Since there are K N ‚àí1 action
profiles for a‚àíi , Equation 12 can be decomposed into
K N ‚àí1 (N + 1) constraints. Notice that the decomposition of Equation 12 can be reused since it appears in
two terms in the rationality constraint.
Finally, we decompose the following summation
X
X
exp(Œª
œÉ‚àíi (a‚àíi )ui (ak , a‚àíi ))
(14)
ak ‚ààAi

a‚àíi

This requires computing Equation 12 K times for different values of ak , plus another K + 1 constraints for
the sum. With one more constraint to express Equation 9, the total number of constraints is K N (N + 1) +
K + 2. Thus, the total number of constraints is on the
order of O(N K N ). If we use discretization parameter
 for strategy variables and Œ¥ for payoff variables, then
the complexity of the rationality constraints is on the
N KN
order of O( min{,Œ¥}
3 ).
As a result of these decompositions, the size of the
WCSP is polynomial in the size of the normal form
game representation. Of course, WCSP is an NP-hard
problem, so the complexity of our approach is probably
exponential in the worst case. Nevertheless, WCSP
is often tractable for real problems, so there is good
reason to hope that it will be for us too.

6

Evaluation

In this section, we first discuss several baseline methods that we compare our learning method with. Then
we describe our implementation, present our experimental results and discuss their implications.
6.1

Baseline Methods

A completely naive approach is to learn the strategies and payoffs of the game separately from the data
given. We call this method the Naive method. This is

a maximum likelihood approach for which the strategies and payoffs with highest log probabilities given
the observations will be chosen. This approach respects the data completely. In our setting, the payoffs
are noisily observed. Moreover, if the number of samples is small, then the observed frequency of each pure
strategy might not reflect the underlying mixed strategy being played. For these reasons, we expect that
the Naive method will perform poorly when the standard deviations of the payoffs are high and or when
the number of samples is small.
A more sophisticated approach is to learn the payoff values from the observed data and infer the mixed
strategies being played by solving for an equilibrium
based on a certain solution concept. We use NaiveNash
to refer to the method using the Nash equilibrium concept and NaiveLQRE to refer to the method using the
LQRE concept. This approach ignores the observed
pure strategy profiles.
6.2

Experimental Setup

We evaluate our learning methods with computational experiments on randomly generated normal
form games with 2 players and 2 pure strategies for
each player (referred to as 2-player game hereafter).
Our implementation consists of the following steps:
1. Generate a random 2-player normal form game.
2. Use Gambit (McKelvey et al., 2007) to compute
LQRE of the game with different Œª values.
3. Given specific LQRE strategies, generate data on
game play, assuming players are playing according to
the specified strategies, and payoffs are generated from
a Gaussian distribution centered around the payoffs
specified in the game.
4. Generate and store a WCSP problem in a predefined XML format. This XML file defines the variables, the domains of the variables, and the constraints
in the form of costs associated with value assignment
tuples.
5. Solve the WCSP specified in the XML file using Toolbar2 (Bouveret et al., 2004) which is a stateof-the-art C++ solver for WCSP, Max-SAT, and
Bayesian Networks.
We derived our results by averaging the data over 10
random 2-player normal form games. The payoff values are sampled uniformly from [1, 2]. We chose the
strategy discretization parameter  to be 5%, the payoff discretization parameter Œ¥ to be 0.1, the LQRE
multiplier Œ± to be 100, and the standard deviation R
of payoff values to be 0.7. These are parameters of our
model that need to be set manually and finding ways
to set them automatically will be future work. For
our preliminary results, the implementation does not
include the constraint decomposition described. We

had to truncate the LQRE constraint written out into
the XML file to only contain 15 of all the generated tuples with the smallest costs due to the enormous size
of the resulting XML file (70mb). Although this does
not affect the validity of our results, we believe that
incorporating the constraint decomposition will result
in an immediate improvement for the efficiency of our
implementation.
For the Naive method, the XML file includes only the
strategy and payoff maximum likelihood constraints.
For the NaiveNash method, we take the payoff values
learned from the Naive method, use Gambit to solve
for Nash equilibrium. For games with multiple Nash
equilibria, we pick the one minimizing our error measure. For the NaiveLQRE method, we again take the
payoff values learned from the Naive method and use
Gambit to solve for the LQRE using the same Œª value
used to generate the data.
6.3

Results and Discussion

We evaluated our learning method in three different
settings. First, we varied the training set sizes and
compare our approach with the Naive, NaiveLQRE,
and NaiveNash methods. Second, we varied the Œª values and do the same comparison. Finally, we examined
the performance of our algorithm using wrong Œª. The
error of each learning instance is measured by computing the Euclidean distance between the combined
vectors of actual versus learned strategy and payoff
values.
Varying training set sizes
We first consider varying the training set size, i.e. the
number of samples of the game plays observed. When
the training set size is small, learning an accurate representation of the game is expected to be difficult since
the limited amount of data does not reflect the mixed
strategy chosen by the players accurately. Also, with
limited amount of data, the payoff values sampled with
noise may be too biased to reflect the true mean and
standard deviation of the observed payoff values.
In this setting, we found that our approach with rationality constraints performs better than the naive approach when the training set size is small. Table 1
summarizes our results for this setting. When the
number of samples is 10, the average error obtained by
our approach is 8% less than that of the Naive method.
Also, we found that the NaiveLQRE and NaiveNash
perform considerably worse than the Naive method
and our LQRE method. This observation is expected,
especially for NaiveNash, since the limited amount of
noisy data causes the maximum likelihood methods to
produce inaccurate payoff values. As a result, solving
for equilibrium strategies using these payoff values re-

sult in equilibrium strategies that tend to be far from
the actual strategies being played.
However, as the number of samples increases, we observe that the Naive method gradually outperforms
our LQRE approach since there is enough data to
for the Naive method to learn the payoff values accurately. By learning accurate payoff values, the errors
of NaiveLQRE and the NaiveNash methods also improve. In particular, when the number of samples is
50, the average error of our LQRE approach is still
close to that of the Naive method. However, for 100
samples, the Naive method outperforms our LQRE
approach by a larger amount.
Table 1: Errors for Varying Training Set Sizes, Œª ‚âà 3
LQRE
Naive
NaiveLQRE
NaiveNash

M = 10
0.994
1.086
1.117
1.266

M = 50
0.684
0.668
0.729
0.990

M = 100
0.642
0.473
0.516
0.812

Varying Œª values
Next, we tested how well our LQRE approach performs
as we vary the value of Œª. For this set of results, we fix
the number of samples to be 10 and vary the Œª values
used to solve for the LQRE of the games. The concern of this setting is whether the correct Œª value will
guide the WCSP with rationality constraints to find
the correct mixed strategy and payoff combinations.
We ran experiments for 3 Œª values and the results are
shown in Table 2. The lambda values are approximate since Gambit does not allow us to specify the
lambda values with which to solve the LQRE of the
game. Our findings indicate that our approach performs better than the naive approach for small and
moderate values of Œª, but not for very large Œª. We believe that this is because the LQRE approaches Nash
equilibrium as Œª approaches infinity. For the games we
considered, we observed that when Œª is around 10, the
mixed strategies corresponding to a LQRE are already
extremely close to the Nash equilibrium strategies of
the game.
Table 2: Errors for Varying Œª Values, M = 10
LQRE
Naive
NaiveLQRE
NaiveNash
Using the wrong Œª

Œª‚âà1
0.954
1.069
1.037
1.325

Œª‚âà3
0.994
1.086
1.117
1.266

Œª ‚âà 10
1.202
1.078
1.130
1.184

A potential criticism of our approach is that it requires
the knowledge of Œª or how approximately rational the
players are. It might be unrealistic to expect this in
real world examples. To counter this criticism, we investigated how well our approach works when we have
the wrong Œª value, i.e., when the Œª used for generating
the data is different from the Œª used for learning. In
other words, does it help to assume that the agents are
somewhat rational, even if we are wrong about to the
degree of their rationality? To test this question, we
ran experiments fixing M = 10 and Œª = 1 for learning
but varying the actual Œª used to generate data.

this. Moreover, in addition to using wrong Œª values, it
would be worthwhile to investigate the case when our
LQRE assumption is incorrect. Alternatively, a possible extension is to combine the rationality constraints,
which as we have argued are like Bayesian priors, with
Bayesian priors of the traditional kind.
Acknowledgements
This work was supported by the Air Force Office of
Scientific Research under MURI contract 5710002613.
References

The results are shown in Table 3. The column headers
are the actual values of Œª used to generate the data.
We were pleasantly surprised to find that our algorithm performs quite well even when Œª is significantly
wrong, and the performance does not degrade quickly.
When the true Œª is 1, which means that we are learning with the correct value, our method improves over
the naive method by 13%. When the true Œª is 2, our
method still improves over the naive method by 9%.
This indicates that imposing rationality constraints is
beneficial, even when it is not exactly correct.
Table 3: Errors for Using Wrong Œª Values, M = 10

LQRE
Naive
% Improvement

7

Œª ‚âà 0.5
0.856
0.986
13.18

Œª‚âà1
0.877
1.010
13.17

Œª ‚âà 1.5
0.927
1.036
10.52

Œª‚âà2
0.916
1.008
9.13

Conclusion and Future Work

This paper has introduced the idea that connecting
strategies to payoffs using rationality constraints can
improve learning games from data, particularly with
limited data. While we have only demonstrated this
for 2-by-2 normal form games, the paper serves as a
proof by demonstration that the idea can be beneficial.
Although our experimental results are preliminary, we
hope that this paper opens the door to research on efficient and scalable implementations and approximations of the idea. For future work, we first would like
to generate data for games with more players, more actions for each player, and greater number of samples.
We also plan to experiment with games with interesting structures. There are several other specific future
directions. It would be preferable to make Œª a variable
that is learned rather than treating it as a fixed parameter that must be set. Making Œª a variable can be accommodated in our WCSP framework. Also, we might
not want to assume that all agents use the same Œª, and
our LQRE formulation can be generalized to capture

Bistarelli, S., Montanari, U., and Rossi, F. (1997).
Semiring-based constraint satisfaction and optimization. Journal of the ACM, 44:201‚Äì236.
Bouveret, S., Heras, F., de Givry, S., Larrosa, J., Sanchez, M., and Schiex, T. (2004).
Toolbar: a state-of-the-art platform for WCSP.
http://www.inra.fr/bia/t/degivry/toolbar.pdf.
Camerer, C. F. (2003).
Princeton Univ. Press.

Behavioral game theory.

Duong, Q., Vorobeychik, Y., Singh, S., and Wellman,
M. P. (2009). Learning graphical game models. In
Proceedings of the 21st International Joint Conference
on Artificial Intelligence (IJCAI‚Äô09), pages 116‚Äì121.
Ficici, S., Parkes, D. C., and Pfeffer, A. (2008). Learning and solving many-player games through a clusterbased representation.
In Proceedings of the 24th
Conference in Uncertainty in Artificial Intelligence
(UAI‚Äô08), pages 187‚Äì195.
Kearns, M. J., Littman, M. L., and Singh, S. P. (2001).
Graphical models for game theory. In Proceedings of
the 17th Conference in Uncertainty in Artificial Intelligence (UAI ‚Äô01), pages 253‚Äì260.
McKelvey, R. D., McLennan, A. M., and Turocy, T. L.
(2007). Gambit: Software tools for game theory, version 0.2007.01.30. http://www.gambit-project.org.
McKelvey, R. D. and Palfrey, T. R. (1996). Quantal
response equilibria for normal form games. In Normal
Form Games, Games and Economic Behavior, pages
6‚Äì38.
Soni, V., Singh, S., and Wellman, M. P. (2007). Constraint satisfaction algorithms for graphical games. In
Proceedings of the 6th International Joint Conference
on Autonomous Agents and Multiagent Systems (AAMAS‚Äô07), pages 1‚Äì8.
Vorobeychik, Y., Wellman, M. P., and Singh, S.
(2007). Learning payoff functions in infinite games.
Mach. Learn., 67(1-2):145‚Äì168.

