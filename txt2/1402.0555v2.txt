
with probability at least 1 âˆ’ Î´, relative to a set of policies Î  âŠ† AX mapping contexts x âˆˆ X to actions
a âˆˆ A (where K is the number of actions). The regret is the difference between the cumulative reward of
the best policy in Î  and the cumulative reward collected by the algorithm. Because the bound has a mild
logarithmic dependence on |Î |, the algorithm can compete with very large policy classes that are likely
1

to yield high rewards, in which case the algorithm also earns high rewards. However, the computational
complexity of the above algorithms is linear in |Î |, making them tractable for only simple policy classes.
A sub-linear in |Î | running time is possible for policy classes that can be efficiently searched. In
this work, we use the abstraction of an optimization oracle to capture this property: given a set of context/reward vector pairs, the oracle returns a policy in Î  with maximum total reward. Using such an oracle in an i.i.d. setting (formally defined in Section 2.1), it is possible to create Ç«-greedy (Sutton and Barto,
1998) or epoch-greedy (Langford and Zhang, 2007) algorithms that run in time O(log |Î |) with only
a single call to the oracle per round. However, these algorithms have suboptimal regret bounds of
O((K log |Î |)1/3 T 2/3 ) because the algorithms randomize uniformly over actions when they choose to
explore.
The Randomized UCB algorithm of DudÄ±Ìk et al. (2011a) achieves the optimal regret bound (up to logarithmic factors) in the i.i.d. setting, and runs in time poly(T, log |Î |) with OÌƒ(T 5 ) calls to the optimization
oracle per round. Naively this would amount to OÌƒ(T 6 ) calls to the oracle over T rounds, although a doubling trick from our analysis can be adapted to ensure only OÌƒ(T 5 ) calls to the oracle are needed over all
T rounds in the Randomized UCB algorithm. This is a fascinating result because it shows that the oracle
can provide an exponential speed-up over previous algorithms with optimal regret bounds. However, the
running time of this algorithm is still prohibitive for most natural problems owing to the OÌƒ(T 5 ) scaling.
In this work, we prove the following1 :
Theorem 1.
an algorithm for the i.i.d. contextual bandit problem with an optimal regret bound
qThere is 
KT
requiring OÌƒ
calls to the optimization oracle over T rounds, with probability at least 1 âˆ’ Î´.
ln(|Î |/Î´)

p
p
Concretely, we make OÌƒ( KT / ln(|Î |/Î´)) calls to the oracle with a net running time of OÌƒ(T 1.5 K log |Î |),
vastly improving over the complexity of Randomized UCB. The major components of the new algorithm
are (i) a new coordinate descent procedure for computing a very sparse distribution over policies which
can be efficiently sampled from, and (ii) a new epoch structure which allows the distribution over policies to be updated very infrequently. We consider variants of the epoch structure that make different
computational trade-offs;
on one extreme we concentrate the entire computational burden on O(log T )
p
KT
/
ln(|Î |/Î´))
oracle calls each time, while on the other we spread our computation
rounds
with
OÌƒ(
p
âˆš
over T rounds with OÌƒ( K/ ln(|Î |/Î´)) oracle calls for each of these rounds. We stress that in either
case, the total number of calls to the oracle is only sublinear in T . Finally, we develop a more efficient
online variant, and conduct a proof-of-concept experiment showing low computational complexity and
high reward relative to several natural baselines.
Motivation and related work. The EXP4-family of algorithms (Auer et al., 2002; McMahan and Streeter,
2009; Beygelzimer et al., 2011) solve the contextual bandit problem with optimal regret by updating
weights (multiplicatively) over all policies in every round. Except for a few special cases (Helmbold and Schapire,
1997; Beygelzimer et al., 2011), the running time of such measure-based algorithms is generally linear in
the number of policies.
In contrast, the Randomized UCB algorithm of DudÄ±Ìk et al. (2011a) is based on a natural abstraction
from supervised learningâ€”the ability to efficiently find a function in a rich function class that minimizes
the loss on a training set. This abstraction is encapsulated in the notion of an optimization oracle,
which is also useful for Ç«-greedy (Sutton and Barto, 1998) and epoch-greedy (Langford and Zhang, 2007)
algorithms. However, these latter algorithms have only suboptimal regret bounds.
Another class of approaches based on Bayesian updating is Thompson sampling (Thompson, 1933; Li,
2013), which often enjoys strong theoretical guarantees in expectation over the prior and good empirical
performance (Chapelle and Li, 2011). Such algorithms, as well as the closely related upper-confidence
bound algorithms (Auer, 2002; Chu et al., 2011), are computationally tractable in cases where the posterior distribution over policies can be efficiently maintained or approximated. In our experiments, we
compare to a strong baseline algorithm that uses this approach (Chu et al., 2011).
1 Throughout this paper, we use the OÌƒ notation to suppress dependence on logarithmic factors in T and K, as well as
log(|Î |/Î´) (i.e. terms which are O(log log(|Î |/Î´)).

2

To circumvent the â„¦(|Î |) running time barrier, we restrict attention to algorithms that only access the
policy class via the optimization oracle. Specifically, we use a cost-sensitive classification oracle, and a key
challenge is to design good supervised learning problems for querying this oracle. The Randomized UCB
algorithm of DudÄ±Ìk et al. (2011a) uses a similar oracle to construct a distribution over policies that solves
a certain convex program. However, the number of oracle calls in their work is prohibitively large, and
the statistical analysis is also rather complex.2
Main contributions. In this work, we present a new and simple algorithm for solving a similar convex
program as that used by Randomized UCB. The new algorithm is based on coordinate descent: in each
iteration, the algorithm calls the optimization oracle to obtain a policy; the output is a sparse distribution
over
p these policies. The number of iterations required to compute the distribution is smallâ€”at most
OÌƒ( Kt/ ln(|Î |/Î´)) in any round t. In fact, we present a more general scheme based on p
epochs and warm
start in which the total number of calls to the oracle is, with high probability, just OÌƒ( KT / ln(|Î |/Î´))
over all T rounds; we prove that this is nearly optimal for a certain class of optimization-based algorithms.
The algorithm is natural and simple to implement, and we provide an arguably simpler analysis than that
for Randomized UCB. Finally, we report proof-of-concept experimental results using a variant algorithm
showing strong empirical performance.

2

Preliminaries

In this section, we recall the i.i.d. contextual bandit setting and some basic techniques used in previous
works (Auer et al., 2002; Beygelzimer et al., 2011; DudÄ±Ìk et al., 2011a).

2.1

Learning Setting

Let A be a finite set of K actions, X be a space of possible contexts (e.g., a feature space), and Î  âŠ† AX
be a finite P
set of policies that map contexts x âˆˆ X to actions a âˆˆ A.3 Let âˆ†Î  := {Q âˆˆ RÎ  : Q(Ï€) â‰¥
0 âˆ€Ï€ âˆˆ Î , Ï€âˆˆÎ  Q(Ï€) â‰¤ 1} be the set of non-negative weights over policies with total weight at most
A
one, and let RA
+ := {r âˆˆ R : r(a) â‰¥ 0 âˆ€a âˆˆ A} be the set of non-negative reward vectors.
Let D be a probability distribution over X Ã— [0, 1]A , the joint space of contexts and reward vectors; we
assume actionsâ€™ rewards from D are always in the interval [0, 1]. Let DX denote the marginal distribution
of D over X.
In the i.i.d. contextual bandit setting, the context/reward vector pairs (xt , rt ) âˆˆ X Ã— [0, 1]A over all
rounds t = 1, 2, . . . are randomly drawn independently from D. In round t, the agent first observes the
context xt , then (randomly) chooses an action at âˆˆ A, and finally receives the reward rt (at ) âˆˆ [0, 1]
for the chosen action. The (observable) record of interaction resulting from round t is the quadruple
(xt , at , rt (at ), pt (at )) âˆˆ X Ã— A Ã— [0, 1] Ã— [0, 1]; here, pt (at ) âˆˆ [0, 1] is the probability that the agent chose
action at âˆˆ A. We let Ht âŠ† X Ã— A Ã— [0, 1] Ã— [0, 1] denote the history (set) of interaction records in the
b xâˆ¼H [Â·] to denote expectation when a context x is chosen
first t rounds. We use the shorthand notation E
t
from the t contexts in Ht uniformly at random.
Let R(Ï€) := E(x,r)âˆ¼D [r(Ï€(x))] denote the expected (instantaneous) reward of a policy Ï€ âˆˆ Î , and
let Ï€â‹† := arg maxÏ€âˆˆÎ  R(Ï€) be a policy that maximizes the expected reward (the optimal policy). Let
Reg(Ï€) := R(Ï€â‹† ) âˆ’ R(Ï€) denote the expected (instantaneous) regret of a policy Ï€ âˆˆ Î  relative to the
optimal policy. Finally, the (empirical cumulative) regret of the agent after T rounds4 is defined as
T
X
t=1

2 The


rt (Ï€â‹† (xt )) âˆ’ rt (at ) .

paper of DudÄ±Ìk et al. (2011a) is colloquially referred to, by its authors, as the â€œmonster paperâ€ (Langford, 2014).
to VC classes is simple using standard arguments.
4 We have defined empirical cumulative regret as being relative to Ï€ , rather than to the empirical reward maximizer
â‹†
p
PT
arg maxÏ€âˆˆÎ  t=1 rt (Ï€(xt )). However, in the i.i.d. setting, the two do not differ by more than O( T ln(|Î |/Î´)) with
probability at least 1 âˆ’ Î´.
3 Extension

3

2.2

Inverse Propensity Scoring

An unbiased estimate of a policyâ€™s reward may be obtained from a history of interaction records Ht using
inverse propensity scoring (IPS; also called inverse probability weighting): the expected reward of policy
Ï€ âˆˆ Î  is estimated as
t
X
ri (ai ) Â· 1{Ï€(xi ) = ai }
b t (Ï€) := 1
R
.
(1)
t i=1
pi (ai )

This technique can be viewed as mapping Ht 7â†’ IPS(Ht ) of interaction records (x, a, r(a), p(a)) to context/reward vector pairs (x, rÌ‚), where rÌ‚ âˆˆ RA
+ is a fictitious reward vector that assigns to the chosen
action a a scaled reward r(a)/p(a) (possibly greater than one), and assigns to all other actions zero
rewards. This transformation IPS(Ht ) is detailed in Algorithm 3 (in Appendix A); we may equivalently
b t by R
b t (Ï€) := tâˆ’1 P
define R
(x,rÌ‚)âˆˆIPS(Ht ) rÌ‚(Ï€(x)). It is easy to verify that E[rÌ‚(Ï€(x))|(x, r)] = r(Ï€(x)), as
b t (Ï€) is
p(a) is indeed the agentâ€™s probability (conditioned on (x, r)) of picking action a. This implies R
an unbiased estimator for any history Ht .
b t (Ï€) denote a policy that maximizes the expected reward estimate based
Let Ï€t := arg maxÏ€âˆˆÎ  R
d t (Ï€) := R
b t (Ï€t ) âˆ’ R
b t (Ï€)
on inverse propensity scoring with history Ht (Ï€0 can be arbitrary), and let Reg
d
denote estimated regret relative to Ï€t . Note that Regt (Ï€) is generally not an unbiased estimate of Reg(Ï€),
because Ï€t is not always Ï€â‹† .

2.3

Optimization Oracle

One natural mode for accessing the set of policies Î  is enumeration, but this is impractical in general.
In this work, we instead only access Î  via an optimization oracle which corresponds to a cost-sensitive
learner. Following DudÄ±Ìk et al. (2011a), we call this oracle AMO5 .
Definition 1. For a set of policies Î , the arg max oracle (AMO) is an algorithm, which for any sequence
of context and reward vectors, (x1 , r1 ), (x2 , r2 ), . . . , (xt , rt ) âˆˆ X Ã— RA
+ , returns
arg max
Ï€âˆˆÎ 

2.4

t
X

rÏ„ (Ï€(xÏ„ )).

Ï„ =1

Projections and Smoothing

In each round, our algorithm chooses an action by randomly drawing a policy Ï€ from a distribution over
Î , and then picking the action Ï€(x) recommended
by Ï€ on the current context x. This is equivalent
P
to drawing an action according to Q(a|x) := Ï€âˆˆÎ :Ï€(x)=a Q(Ï€), âˆ€a âˆˆ A. For keeping the variance of
reward estimates from IPS in check, it is desirable to prevent the probability of any action from being
Âµ
too small. Thus, as in
P previous work, we also use a smoothed projection Q (Â·|x) for Âµ âˆˆ [0, 1/K],
Âµ
Q (a|x) := (1 âˆ’ KÂµ) Ï€âˆˆÎ :Ï€(x)=a Q(Ï€) + Âµ, âˆ€a âˆˆ A. Every action has probability at least Âµ under
QÂµ (Â·|x).
For technical reasons, our algorithm maintains non-negative weights Q âˆˆ âˆ†Î  over policies that sum
to at most one, but not necessarily equal to one; hence, we put any remaining mass
 policy
Pon a default
Ï€Ì„ âˆˆ Î  to obtain a legitimate probability distribution over policies QÌƒ = Q + 1 âˆ’ Ï€âˆˆÎ  Q(Ï€) 1Ï€Ì„ . We
then pick an action from the smoothed projection QÌƒÂµ (Â·|x) of QÌƒ as above. This sampling procedure
Sample(x, Q, Ï€Ì„, Âµ) is detailed in Algorithm 4 (in Appendix A).

3

Algorithm and Main Results

Our algorithm (ILOVETOCONBANDITS) is an epoch-based variant of the Randomized UCB algorithm of
DudÄ±Ìk et al. (2011a) and is given in Algorithm 1. Like Randomized UCB, ILOVETOCONBANDITS solves
5 Cost-sensitive

learners often need a cost instead of reward, in which case we use ct = 1 âˆ’ rt .

4

an optimization problem (OP) to obtain a distribution over policies to sample from (Step 7), but does
so on an epoch schedule, i.e., only on certain pre-specified rounds Ï„1 , Ï„2 , . . .. The only requirement of
the epoch schedule is that the length of epoch m is bounded as Ï„m+1 âˆ’ Ï„m = O(Ï„m ). For simplicity, we
assume Ï„m+1 â‰¤ 2Ï„m for m â‰¥ 1, and Ï„1 = O(1).
The crucial step here is solving (OP). Before stating the main result, let us get some intuition about
this problem. The first constraint, Eq. (2), requires the average estimated regret of the distribution Q
over policies to be small, since bÏ€ is a rescaled version of the estimated regret of policy Ï€. This constraint
skews our distribution to put more mass on â€œgood policiesâ€ (as judged by our current information), and
can be seen as the exploitation component of our algorithm. The second set of constraints, Eq. (3),
requires the distribution Q to place sufficient mass on the actions chosen by each policy Ï€, in expectation
over contexts. This can be thought of as the exploration constraint, since it requires the distribution to
be sufficiently diverse for most contexts. As we will see later, the left hand side of the constraint is a
bound on the variance of our reward estimates for policy Ï€, and the constraint requires the variance to
be controlled at the level of the estimated regret of Ï€. That is, we require the reward estimates to be
more accurate for good policies than we do for bad ones, allowing for much more adaptive exploration
than the uniform exploration of Ç«-greedy style algorithms.
This problem is very similar to the one in DudÄ±Ìk et al. (2011a), and our coordinate descent algorithm
in Section 3.1 gives a constructive proof that the problem is feasible. As in DudÄ±Ìk et al. (2011a), we have
the following regret bound:
Theorem 2. Assume the optimization problem ( OP) can be solved whenever required in Algorithm 1.
With probability at least 1 âˆ’ Î´, the regret of Algorithm 1 (ILOVETOCONBANDITS) after T rounds is

p
KT ln(T |Î |/Î´) + K ln(T |Î |/Î´) .
O
Algorithm 1 Importance-weighted LOw-Variance Epoch-Timed Oracleized CONtextual BANDITS algorithm (ILOVETOCONBANDITS)
input Epoch schedule 0 = Ï„0 < Ï„1 < Ï„2 < Â· Â· Â· , allowed failure probability Î´ âˆˆ (0, 1).
1: Initial weights Q0 := 0 âˆˆpâˆ†Î  , initial epoch m := 1.
2 |Î |/Î´)/(KÏ„ )} for all m â‰¥ 0.
Define Âµm := min{1/2K , ln(16Ï„m
m
2: for round t = 1, 2, . . . do
3:
Observe context xt âˆˆ X.
4:
(at , pt (at )) := Sample(xt , Qmâˆ’1 , Ï€Ï„m âˆ’1 , Âµmâˆ’1 ).
5:
Select action at and observe reward rt (at ) âˆˆ [0, 1].
6:
if t = Ï„m then
7:
Let Qm be a solution to (OP) with history Ht and minimum probability Âµm .
8:
m := m + 1.
9:
end if
10: end for

Optimization Problem (OP)
d

Î 
t (Ï€)
Given a history Ht and minimum probability Âµm , define bÏ€ := Reg
ÏˆÂµm for Ïˆ := 100, and find Q âˆˆ âˆ†
such that
X
Q(Ï€)bÏ€ â‰¤ 2K
(2)

b xâˆ¼H
âˆ€Ï€ âˆˆ Î  : E
t



Ï€âˆˆÎ 

1
Âµ
m
Q (Ï€(x)|x)

5



â‰¤ 2K + bÏ€ .

(3)

3.1

Solving (OP) via Coordinate Descent

We now present a coordinate descent algorithm to solve (OP). The pseudocode is given in Algorithm 2.
Our analysis, as well as the algorithm itself, are based on a potential function which we use to measure
progress. The algorithm can be viewed as a form of coordinate descent applied to this same potential
function. The main idea of our analysis is to show that this function decreases substantially on every
iteration of this algorithm; since the function is nonnegative, this gives an upper bound on the total
number of iterations as expressed in the following theorem.
Theorem 3. Algorithm 2 (with Qinit := 0) halts in at most
solution Q to ( OP).

4 ln(1/(KÂµm ))
Âµm

iterations, and outputs a

Algorithm 2 Coordinate Descent Algorithm
Require: History Ht , minimum probability Âµ, initial weights Qinit âˆˆ âˆ†Î  .
1: Set Q := Qinit .
2: loop
3:
Define, for all Ï€ âˆˆ Î ,
VÏ€ (Q)
SÏ€ (Q)
DÏ€ (Q)
if

4:
5:

6:
7:
8:

9:
10:
11:
12:

P

b xâˆ¼H [1/QÂµ (Ï€(x)|x)]
= E
t


b xâˆ¼Ht 1/(QÂµ (Ï€(x)|x))2
= E
= VÏ€ (Q) âˆ’ (2K + bÏ€ ).

Ï€ Q(Ï€)(2K + bÏ€ ) > 2K then
Replace Q by cQ, where

c := P

2K
< 1.
Q(Ï€)(2K
+ bÏ€ )
Ï€

(4)

end if
if there is a policy Ï€ for which DÏ€ (Q) > 0 then
Add the (positive) quantity
VÏ€ (Q) + DÏ€ (Q)
Î±Ï€ (Q) =
2(1 âˆ’ KÂµ)SÏ€ (Q)

to Q(Ï€) and leave all other weights unchanged.
else
Halt and output the current set of weights Q.
end if
end loop

3.2

Using an Optimization Oracle

We now show how to implement Algorithm 2 via AMO (c.f. Section 2.3).
Lemma 1. Algorithm 2 can be implemented using one call to AMO before the loop is started, and one
call for each iteration of the loop thereafter.
Proof. At the very beginning, before the loop is started, we compute the best empirical policy so far, Ï€t ,
by calling AMO on the sequence of historical contexts and estimated reward vectors; i.e., on (xÏ„ , rÌ‚Ï„ ), for
Ï„ = 1, 2, . . . , t.
Next, we show that each iteration in the loop of Algorithm 2 can be implemented via one call to AMO.
Going over the pseudocode, first note that operations involving Q in Step 4 can be performed efficiently
since Q has sparse support. Note that the definitions in Step 3 donâ€™t actually need to be computed for
all policies Ï€ âˆˆ Î , as long as we can identify a policy Ï€ for which DÏ€ (Q) > 0. We can identify such a
policy using one call to AMO as follows.
6

First, note that for any policy Ï€, we have
b xâˆ¼Ht
VÏ€ (Q) = E

and

bÏ€ =




t
1
1
1X
=
,
QÂµ (Ï€(x)|x)
t Ï„ =1 QÂµ (Ï€(xÏ„ )|xÏ„ )

t
d t (Ï€)
b t (Ï€t )
Reg
R
1 X
=
âˆ’
rÌ‚Ï„ (Ï€(xÏ„ )).
ÏˆÂµ
ÏˆÂµ
ÏˆÂµt Ï„ =1

Now consider the sequence of historical contexts and reward vectors, (xÏ„ , rÌƒÏ„ ) for Ï„ = 1, 2, . . . , t, where
for any action a we define


ÏˆÂµ
1
+ rÌ‚Ï„ (a) .
(5)
rÌƒÏ„ (a) :=
t QÂµ (a|xÏ„ )
It is easy to check that
t

Since 2K +

b t (Ï€t )
R
ÏˆÂµ

1 X
DÏ€ (Q) =
rÌƒÏ„ (Ï€(xÏ„ )) âˆ’
ÏˆÂµ Ï„ =1

b t (Ï€t )
R
2K +
ÏˆÂµ

!

.

is a constant independent of Ï€, we have
arg max DÏ€ (Q) = arg max
Ï€âˆˆÎ 

Ï€âˆˆÎ 

t
X

rÌƒÏ„ (Ï€(xÏ„ )),

Ï„ =1

and hence, calling AMO once on the sequence (xÏ„ , rÌƒÏ„ ) for Ï„ = 1, 2, . . . , t, we obtain a policy that maximizes
DÏ€ (Q), and thereby identify a policy for which DÏ€ (Q) > 0 whenever one exists.

3.3

Epoch Schedule

Recalling
the setting of Âµm in Algorithm 1, Theorem 3 shows that Algorithm 2 solves (OP) with
p
if we use the epoch schedule Ï„m = m (i.e., run
OÌƒ( Kt/ ln(|Î |/Î´)) calls to AMO in round t. Thus, p
Algorithm 2 in every round), then we get a total of OÌƒ( KT 3/ ln(|Î |/Î´)) calls to AMO over all T rounds.
This number can be dramatically reduced using a more carefully chosen epoch schedule.
p
Lemma 2. For the epoch schedule Ï„m := 2mâˆ’1 , the total number of calls to AMO is OÌƒ( KT / ln(|Î |/Î´)).
Proof. The epoch schedule satisfies the requirement Ï„m+1 p
â‰¤ 2Ï„m . With this epoch schedule, Algorithm 2
is run only O(log T ) times over T rounds, leading to OÌƒ( KT / ln(|Î |/Î´)) total calls to AMO over the
entire period.

3.4

Warm Start

We now present a different technique to reduce the number of calls to AMO. This is based on the
observation that practically speaking, it seems terribly wasteful, at the start of a new epoch, to throw out
the results of all of the preceding computations and to begin yet again from nothing. Instead, intuitively,
we expect computations to be more moderate if we begin again where we left off last, i.e., a â€œwarm-startâ€
approach. Here, when Algorithm 2 is called at the end of epoch m, we use Qinit := Qmâˆ’1 (the previously
computed weights) rather than 0.
p
We can combine warm-startâˆšwith a different epoch schedule to guarantee OÌƒ( KT / ln(|Î |/Î´)) total
calls to AMO, spread across O( T ) calls to Algorithm 2.
Lemma 3. Define the epoch schedule (Ï„1 , Ï„2 ) := (3, 5) and Ï„m := m2 for m â‰¥p3 (this satisfies Ï„m+1 â‰¤
2Ï„m ). With high probability,âˆšthe warm-start variant of Algorithm 1 makes OÌƒ( KT / ln(|Î |/Î´)) calls to
AMO over T rounds and O( T ) calls to Algorithm 2.

7

3.5

Computational Complexity

So far, we have only considered computational complexity in terms of the number of oracle calls. However,
the reduction also involves the creation of cost-sensitive classification examples, which must be accounted
for in the net computational cost. As observed in the proof of Lemma 1 (specifically Eq. (5)), this requires
the computation of the probabilities QÂµ (a|xÏ„ ) for Ï„ = 1, 2, . . . , t when the oracle has to be invoked at
round
p t. According to Lemma 3, the support of the distribution Q at time t can be over at most
OÌƒ( Kt/ ln(|Î |/Î´))
p policies (same as the number of calls to AMO). This would suggest a computational
complexity of OÌƒ( Kt3 / ln(|Î |/Î´)) for querying the oracle at time t, resulting in an overall computation
cost scaling with T 2 .
We can, however, do better with some natural bookkeeping. Observe that at the start of round t, the
conditional distributions Q(a|xi ) for i = 1, 2, . . . , t âˆ’ 1 can be represented as a table of size K Ã— (t âˆ’ 1),
where rows and columns correspond to actions and contexts. Upon receiving the new example
in round
p
t, the corresponding t-th column can be added to this table in time K Â· |supp(Q)| = OÌƒ(K Kt/ ln(|Î |/Î´))
(where supp(Q) âŠ† Î  denotes the support of Q), using the projection operation described in Section 2.4.
Hence the net cost of these updates, as a function of K and T , scales with as (KT )3/2 . Furthermore,
the cost-sensitive examples needed for the AMO can be obtained by a simple table lookup now, since the
action probabilities are directly available. This involves O(Kt) table lookups when the oracle is invoked
at time t, and again results in an overall cost scaling as (KT )3/2 . Finally, we have to update the table
when the distribution Q is updated in Algorithm 2. If we find ourselves in the rescaling step 4, we can
simply store the constant c. When we enter step 8 of the algorithm, we can do a linear scan over the table,
rescaling and incrementing the entries. This also resutls in a cost of O(Kt) when the update happens at
time t, resulting in a net scaling as (KT )3/2 . Overall,
p we find that the computational complexity of our
algorithm, modulo the oracle running time, is OÌƒ( (KT )3 / ln(|Î |/Î´)).

3.6

A Lower Bound on the Support Size

An attractive feature of the coordinate descent algorithm, Algorithm 2, is that the number of oracle calls
is directly related to the number of policies in the support of Qm . Specifically, for the doubling schedule
m ))
of Section 3.3, Theorem 3 implies that we never have non-zero weights for more than 4 ln(1/(KÂµ
policies
Âµm
in epoch m. Similarly, the total number of oracle calls for the warm-start approach in Section 3.4 bounds
the total number of policies which ever have non-zero weight over all T rounds. The support size of the
distributions Qm in Algorithm 1 is crucial to the computational complexity of sampling an action (Step 4
of Algorithm 1).
In this section, we demonstrate a lower bound showing that it is not possible to construct substantially
sparser distributions that also satisfy the low-variance constraint (3) in the optimization problem (OP).
To formally define the lower bound, fix an epoch schedule 0 = Ï„0 < Ï„1 < Ï„2 < Â· Â· Â· and consider the
following set of non-negative vectors over policies:
Qm :={Q âˆˆ âˆ†Î  : Q satisfies Eq. (3) in round Ï„m }.
(The distribution Qm computed by Algorithm 1 is in Qm .) Recall that supp(Q) denotes the support of
Q (the set of policies where Q puts non-zero entries). We have the following lower bound on |supp(Q)|.
Theorem 4. For any epoch schedule 0 = Ï„0 < Ï„1 < Ï„2 < Â· Â· Â· and any M âˆˆ N sufficiently large, there
exists a distribution D over X Ã— [0, 1]A and a policy class Î  such that, with probability at least 1 âˆ’ Î´,
s
!
KÏ„M
inf
inf |supp(Q)| = â„¦
.
mâˆˆN: QâˆˆQm
ln(|Î |Ï„M /Î´)
Ï„m â‰¥Ï„M /2

The proof of the theorem is deferred to Appendix E. In the context of our problem, this lower bound
shows that the bounds in Lemma 2 and Lemma 3 are unimprovable, since the number of calls to AMO
is at least the size of the support, given our mode of access to Î .
8

4

Regret Analysis

In this section, we outline the regret analysis for our algorithm ILOVETOCONBANDITS, with details
deferred to Appendix B and Appendix C.
b t (Ï€) are controlled by (a bound on) the variance of
The deviations of the policy reward estimates R
b xâˆ¼H [Â·] replaced by
each term in Eq. (1): essentially the left-hand side of Eq. (3) from (OP), except with E
t
Exâˆ¼DX [Â·]. Resolving this discrepancy is handled using deviation bounds, so Eq. (3) holds with Exâˆ¼DX [Â·],
with worse right-hand side constants.
The rest of the analysis, which deviates from that of Randomized UCB, compares the expected regret
d t (Ï€) using the variance constraints Eq. (3):
Reg(Ï€) of any policy Ï€ with the estimated regret Reg
Lemma 4 (Informally). With high probability, for each m such that Ï„m â‰¥ OÌƒ(K log |Î |), each round t in
d t (Ï€) + O(KÂµm ).
epoch m, and each Ï€ âˆˆ Î , Reg(Ï€) â‰¤ 2Reg

This lemma can easily be combined with the constraint Eq. (2) from (OP): since the weights Qmâˆ’1
P
d
used in any round t in epoch m satisfy Ï€âˆˆÎ  Qmâˆ’1 (Ï€)Reg
Ï„m âˆ’1 (Ï€) â‰¤ Ïˆ Â· 2KÂµÏ„m âˆ’1 , we obtain a bound
on the (conditionally) expected regret in round t using the above lemma: with high probability,
X
e mâˆ’1 Reg(Ï€) â‰¤ O(KÂµmâˆ’1 ).
Q
Ï€âˆˆÎ 

Summing these terms up over all T rounds and applying martingale concentration gives the final regret
bound in Theorem 2.

5

Analysis of the Optimization Algorithm

In this section, we give a sketch of the analysis of our main optimization algorithm for computing weights
Qm on each epoch as in Algorithm 2. As mentioned in Section 3.1, this analysis is based on a potential
function.
Since our attention for now is on a single epoch m, here and in what follows, when clear from context,
we drop m from our notation and write simply Ï„ = Ï„m , Âµ = Âµm , etc. Let UA be the uniform distribution
over the action set A. We define the following potential function for use on epoch m:
!
P
b x [RE (UA kQÂµ (Â· | x))]
E
Ï€âˆˆÎ  Q(Ï€)bÏ€
Î¦m (Q) = Ï„ Âµ
.
(6)
+
1 âˆ’ KÂµ
2K
The function in Eq. (6) is defined for all vectors Q âˆˆ âˆ†Î  . Also, RE (pkq) denotes the unnormalized
relative entropy between two nonnegative vectors p and q over the action space (or any set) A:
X
RE (pkq) =
(pa ln(pa /qa ) + qa âˆ’ pa ).
aâˆˆA

This number is always nonnegative. Here, QÂµ (Â·|x) denotes the â€œdistributionâ€ (which might not sum to
1) over A induced by QÂµ for context x as given in Section 2.4. Thus, ignoring constants, this potential
function is a combination of two terms: The first measures how far from uniform are the distributions
induced by QÂµ , and the second is an estimate of expected regret under Q since bÏ€ is proportional to the
empirical regret of Ï€. Making Î¦m small thus encourages Q to choose actions as uniformly as possible
while also incurring low regret â€” exactly the aims of our algorithm. The constants that appear in this
definition are for later mathematical convenience.
For further intuition, note that, by straightforward calculus, the partial derivative âˆ‚Î¦m /âˆ‚Q(Ï€) is
roughly proportional to the variance constraint for Ï€ given in Eq. (3) (up to a slight mismatch of constants). This shows that if this constraint is not satisfied, then âˆ‚Î¦m /âˆ‚Q(Ï€) is likely to be negative,
meaning that Î¦m can be decreased by increasing Q(Ï€). Thus, the weight vector Q that minimizes Î¦m
satisfies the variance constraint for every policy Ï€. It turns out that this minimizing Q also satisfies the
9

low regret constraint in Eq. (2), and also must sum to at most 1; in other words, it provides a complete
solution to our optimization problem. Algorithm 2 does not fully minimize Î¦m , but it is based roughly
on coordinate descent. This is because in each iteration one of the weights (coordinate directions) Q(Ï€)
is increased. This weight is one whose corresponding partial derivative is large and negative.
To analyze the algorithm, we first argue that it is correct in the sense of satisfying the required
constraints, provided that it halts.
Lemma 5. If Algorithm 2 halts and outputs a weight vector Q, then the constraints Eq. (3) and Eq. (2)
must hold, and furthermore the sum of the weights Q(Ï€) is at most 1.
The proof is rather straightforward: Following Step 4, Eq. (2) must hold, and also the weights must
sum to 1. And if the algorithm halts, then DÏ€ (Q) â‰¤ 0 for all Ï€, which is equivalent to Eq. (3).
What remains is the more challenging task of bounding the number of iterations until the algorithm
does halt. We do this by showing that significant progress is made in reducing Î¦m on every iteration. To
begin, we show that scaling Q as in Step 4 cannot cause Î¦m to increase.
P
Lemma 6. Let Q be a weight vector such that Ï€ Q(Ï€)(2K + bÏ€ ) > 2K, and let c be as in Eq. (4). Then
Î¦m (cQ) â‰¤ Î¦m (Q).
Proof sketch. We consider Î¦m (cQ) as a function of c, and argue that its derivative (with respect to
c) at the value of c given in the lemma statement is always nonnegative. Therefore, by convexity, it is
nondecreasing for all values exceeding c. Since c < 1, this proves the lemma.
Next, we show that substantial progress will be made in reducing Î¦m each time that Step 8 is executed.
Lemma 7. Let Q denote a set of weights and suppose, for some policy Ï€, that DÏ€ (Q) > 0. Let Qâ€² be
a new set of weights which is an exact copy of Q except that Qâ€² (Ï€) = Q(Ï€) + Î± where Î± = Î±Ï€ (Q) > 0.
Then
Ï„ Âµ2
.
(7)
Î¦m (Q) âˆ’ Î¦m (Qâ€² ) â‰¥
4(1 âˆ’ KÂµ)
Proof sketch. We first compute exactly the change in potential for general Î±. Next, we apply a secondorder Taylor approximation, which is maximized by the Î± used in the algorithm. The Taylor approximation, for this Î±, yields a lower bound which can be further simplified using the fact that QÂµ (a|x) â‰¥ Âµ
always, and our assumption that DÏ€ (Q) > 0. This gives the bound stated in the lemma.
So Step 4 does not cause Î¦m to increase, and Step 8 causes Î¦m to decrease by at least the amount
given in Lemma 7. This immediately implies Theorem 3: for Qinit = 0, the initial potential is bounded by
Ï„ Âµ ln(1/(KÂµ))/(1 âˆ’ KÂµ), and it is never negative, so the number of times Step 8 is executed is bounded
by 4 ln(1/(KÂµ))/Âµ as required.

5.1

Epoching and Warm Start

As shown in Section 2.3, the bound on the number of iterations of the algorithm from Theorem 3 also
gives a bound on the number of times the oracle is called. To reduce the number of oracle calls, one
approach is the â€œdoubling trickâ€ of Section 3.3, which enables
p us to bound the total combined number of
iterations of Algorithm 2 in the first T rounds is p
only OÌƒ( KT / ln(|Î |/Î´)). This means that the average
number of calls to the arg-max oracle is only OÌƒ( K/(T ln(|Î |/Î´))) per round, meaning that the oracle
is called far less than once per round, and in fact, at a vanishingly low rate.
We now turn to warm-start approach of Section 3.4, where in each epoch m + 1 we initialize the
coordinate descent algorithm with Qinit = Qm , i.e. the weights computed in the previous epoch m.
To analyze this, we bound how much the potential changes from Î¦m (Qm ) at the end of epoch m to
Î¦m+1 (Qm ) at the very start of epoch m + 1. This, combined with our earlier results regarding how
quickly Algorithm 2 drives down the potential, we are able to get an overall bound on the total number
of updates across T rounds.

10

Table 1: Progressive validation loss, best hyperparameter values, and running times of various algorithm
on RCV1.
Algorithm
P.V. Loss
Searched
Seconds

Ç«-greedy
0.148
0.1 = Ç«
17

Explore-first
0.081
2 Ã— 105 first
2.6

Bagging
0.059
16 bags
275

LinUCB
0.128
103 dim, minibatch-10
212 Ã— 103

Online Cover
0.053
cover n = 1
12

Supervised
0.051
nothing
5.3

Lemma 8. Let M be the largest integer for which Ï„M+1 â‰¤ T . With probability at least 1 âˆ’ 2Î´, for all T ,
the total epoch-to-epoch increase in potential is
!
r
M
X
T ln(|Î |/Î´)
(Î¦m+1 (Qm ) âˆ’ Î¦m (Qm )) â‰¤ OÌƒ
,
K
m=1
where M is the largest integer for which Ï„M+1 â‰¤ T .
Proof sketch. The potential function, as written in Eq. (6), naturally breaks into two pieces whose
epoch-to-epoch changes can be bounded separately. Changes affecting the relative entropy term on the
left can be bounded, regardless of Qm , by taking advantage of the manner in which these distributions
are smoothed. For the other term on the right, it turns out that these epoch-to-epoch changes are related
to statistical quantities which can be bounded with high probability. Specifically, the total change in this
term is related first to how the estimated reward of the empirically best policy compares to the expected
reward of the optimal policy; and second, to how the reward received by our algorithm compares to that
of the optimal reward. From our regret analysis, we are able to show that both of these quantities will
be small with high probability.
This lemma, along with Lemma 7 can be used to further establish Lemma 3. We only provide an
intuitive sketch here, with the details deferred to the appendix. As we
p observe in Lemma 8, the total
amount that the potential increases across T rounds is at most OÌƒ( T ln(|Î |/Î´)/K). On the other
hand, Lemma 7 shows that each time Q is updated by Algorithm 2 the potential decreases by at least
â„¦Ìƒ(ln(|Î |/Î´)/K) (using our choice
p of Âµ). Therefore, the total number of updates of the algorithm totaled2
over all T rounds is at most OÌƒ( KT / ln(|Î |/Î´)). For instance,
âˆš if we use (Ï„1 , Ï„2 ) := (3, 5) and Ï„m := m
T times in T rounds, and on each of those
for m â‰¥ 3, then the weight vector
Q
is
only
updated
about
p
rounds, Algorithm 2 requires OÌƒ( K/ ln(|Î |/Î´)) iterations, on average, giving the claim in Lemma 3.

6

Experimental Evaluation

In this section we evaluate a variant of Algorithm 1 against several baselines. While Algorithm 1 is
significantly more efficient than many previous approaches, the overall computational complexity is still
at least OÌƒ((KT )1.5 ) plus the total cost of the oracle calls, as discussed in Section 3.5. This is markedly
larger than the complexity of an ordinary supervised learning problem where it is typically possible to
perform an O(1)-complexity update upon receiving a fresh example using online algorithms.
A natural solution is to use an online oracle that is stateful and accepts examples one by one. An
online cost-sensitive classification (CSC) oracle takes as input a weighted example and returns a predicted
class (corresponding to one of K actions in our setting). Since the oracle is stateful, it remembers and
uses examples from all previous calls in answering questions, thereby reducing the complexity of each
oracle invocation to O(1) as in supervised learning. Using several such oracles, we can efficiently track a
distribution over good policies and sample from it. We detail this approach (which we call Online Cover)
in the full version of the paper. The algorithm maintains a uniform distribution over a fixed number
n of policies where n is a parameter of the algorithm. Upon receiving a fresh example, it updates all
n policies with the suitable CSC examples (Eq. (5)). The specific CSC oracle we use is a reduction to
11

squared-loss regression (Algorithms 4 and 5 of Beygelzimer and Langford (2009)) which is amenable to
online updates. Our implementation is included in Vowpal Wabbit.6
Due to lack of public datasets for contextual bandit problems, we use a simple supervised-to-contextualbandit transformation (DudÄ±Ìk et al., 2011b) on the CCAT document classification problem in RCV1 (Lewis et al.,
2004). This dataset has 781265 examples and 47152 TF-IDF features. We treated the class labels as actions, and one minus 0/1-loss as the reward. Our evaluation criteria is progressive validation (Blum et al.,
1999) on 0/1 loss. We compare several baseline algorithms to Online Cover; all algorithms take advantage
of linear representations which are known to work well on this dataset. For each algorithm, we report
the result for the best parameter settings (shown in Table 6).
1. Ç«-greedy (Sutton and Barto, 1998) explores randomly with probability Ç« and otherwise exploits.
2. Explore-first is a variant that begins with uniform exploration, then switches to an exploit-only
phase.
3. A less common but powerful baseline is based on bagging: multiple predictors (policies) are trained
with examples sampled with replacement. Given a context, these predictors yield a distribution
over actions from which we can sample.
4. LinUCB (Auer, 2002; Chu et al., 2011) has been quite effective in past evaluations (Li et al., 2010;
Chapelle and Li, 2011). It is impractical to run â€œas isâ€ due to high-dimensional matrix inversions,
so we report results for this algorithm after reducing to 1000 dimensions via random projections.
Still, the algorithm required 59 hours7 . An alternative is to use diagonal approximation to the
covariance, which runs substantially faster (â‰ˆ1 hour), but gives a worse error of 0.137.
5. Finally, our algorithm achieves the best loss of 0.0530. Somewhat surprisingly, the minimum occurs
for us with a cover set of size 1â€”apparently for this problem the small decaying amount of uniform
random sampling imposed is adequate exploration. Prediction performance is similar with a larger
cover set.
All baselines except for LinUCB are implemented as a simple modification of Vowpal Wabbit. All
reported results use default parameters where not otherwise specified. The contextual bandit learning
algorithms all use a doubly robust reward estimator instead of the importance weighted estimators used
in our analysis DudÄ±Ìk et al. (2011b).
Because RCV1 is actually a fully supervised dataset, we can apply a fully supervised online multiclass
algorithm to solve it. We use a simple one-against-all implementation to reduce this to binary classification, yielding an error rate of 0.051 which is competitive with the best previously reported results.
This is effectively a lower bound on the loss we can hope to achieve with algorithms using only partial
information. Our algorithm is less than 2.3 times slower and nearly achieves the bound. Hence on this
dataset, very little further algorithmic improvement is possible.

7

Conclusions

In this paper we have presented the first practical algorithm to our knowledge that attains the statistically
optimal regret guarantee and is computationally efficient in the setting of general policy classes. A
remarkable feature of the algorithm is that the total number of oracle calls over all T rounds is sublinearâ€”
a remarkable improvement over previous works in this setting. We believe that the online variant of the
approach which we implemented in our experiments has the right practical flavor for a scalable solution
to the contextual bandit problem. In future work, it would be interesting to directly analyze the Online
Cover algorithm.
6 http://hunch.net/
7 The

~ vw. The implementation is in the file cbify.cc and is enabled using --cover.
linear algebra routines are based on Intel MKL package.

12

Acknowledgements
We thank Dean Foster and Matus Telgarsky for helpful discussions. Part of this work was completed
while DH and RES were visiting Microsoft Research.

References
Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning
Research, 3:397â€“422, 2002.
Peter Auer, NicoloÌ€ Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed
bandit problem. SIAM Journal of Computing, 32(1):48â€“77, 2002.
Alina Beygelzimer and John Langford. The offset tree for learning with partial labels. In KDD, 2009.
Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual bandit
algorithms with supervised learning guarantees. In AISTATS, 2011.
Avrim Blum, Adam Kalai, and John Langford. Beating the holdout: Bounds for k-fold and progressive
cross-validation. In COLT, 1999.
Olivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. In NIPS, 2011.
Wei Chu, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual bandits with linear payoff functions.
In AISTATS, 2011.
Miroslav DudÄ±Ìk, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong
Zhang. Efficient optimal learning for contextual bandits. In UAI, 2011a.
Miroslav DudÄ±Ìk, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. In ICML,
2011b.
David P. Helmbold and Robert E. Schapire. Predicting nearly as well as the best pruning of a decision
tree. Machine Learning, 27(1):51â€“68, 1997.
John
Langford.
Interactive
machine
learning,
http://hunch.net/~jl/projects/interactive/index.html.

January

2014.

URL

John Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. In
NIPS, 2007.
David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. Rcv1: A new benchmark collection for text
categorization research. The Journal of Machine Learning Research, 5:361â€“397, 2004.
Lihong Li. Generalized Thompson sampling for contextual bandits. CoRR, abs/1310.7163, 2013.
Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to personalized news article recommendation. In WWW, 2010.
H. Brendan McMahan and Matthew Streeter. Tighter bounds for multi-armed bandits with expert advice.
In COLT, 2009.
Richard S. Sutton and Andrew G. Barto. Reinforcement learning, an introduction. MIT Press, 1998.
William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika, 25(3â€“4):285â€“294, 1933.

13

Algorithm 3 IPS(H)
input History H âŠ† X Ã— A Ã— [0, 1] Ã— [0, 1].
output Data set S âŠ† X Ã— RA
+.
1: Initialize data set S := âˆ….
2: for each (x, a, r(a), p(a)) âˆˆ H do
â€²
â€²
3:
Create fictitious rewards rÌ‚ âˆˆ RA
+ with rÌ‚(a) = r(a)/p(a) and rÌ‚(a ) = 0 for all a âˆˆ A \ {a}.
4:
S := S âˆª {(x, rÌ‚)}.
5: end for
6: return S.
Algorithm 4 Sample(x, Q, Ï€Ì„, Âµ)
input Context x âˆˆ X, weights Q âˆˆ âˆ†Î  , default policy Ï€Ì„ âˆˆ Î , minimum probability Âµ âˆˆ [0, 1/K].
output Selected actionPaÌ„ âˆˆ A and probability pÌ„ âˆˆ [Âµ, 1].
1: Let QÌƒ := Q + (1 âˆ’
Ï€âˆˆÎ  Q(Ï€))1Ï€Ì„
P
(so Ï€âˆˆÎ  QÌƒ(Ï€) = 1).
2: Randomly draw action aÌ„ âˆˆ A using the distribution
X
QÌƒÂµ (a|x) := (1 âˆ’ KÂµ)
QÌƒ(Ï€) + Âµ, âˆ€a âˆˆ A.
Ï€âˆˆÎ :
Ï€(x)=a

3:
4:

Let pÌ„(aÌ„) := QÌƒÂµ (aÌ„|x).
return (aÌ„, pÌ„(aÌ„)).

A

Omitted Algorithm Details

Algorithm 3 and Algorithm 4 give the details of the inverse propensity scoring transformation IPS and
the action sampling procedure Sample.

B
B.1

Deviation Inequalities
Freedmanâ€™s Inequality

The following form of Freedmanâ€™s inequality for martingales is from Beygelzimer et al. (2011).
Lemma 9. Let X1 , X2 , . . . , XT be a sequence of real-valued P
random variables. PAssume for all t âˆˆ
{1, 2, . . . , T }, Xt â‰¤ R and E[Xt |X1 , . . . , Xtâˆ’1 ] = 0. Define S := Tt=1 Xt and V := Tt=1 E[Xt2 |X1 , . . . , Xtâˆ’1 ].
For any Î´ âˆˆ (0, 1) and Î» âˆˆ [0, 1/R], with probability at least 1 âˆ’ Î´,
S â‰¤ (e âˆ’ 2)Î»V +

B.2

ln(1/Î´)
.
Î»

Variance Bounds

Fix the epoch schedule 0 = Ï„0 < Ï„1 < Ï„2 < Â· Â· Â· .
Define the following for any probability distribution P over Î , Ï€ âˆˆ Î , and Âµ âˆˆ [0, 1/K]:


1
V (P, Ï€, Âµ) := Exâˆ¼DX
,
P Âµ (Ï€(x)|x)


1
b xâˆ¼H
.
Vbm (P, Ï€, Âµ) := E
Ï„m
P Âµ (Ï€(x)|x)

(8)
(9)

The proof of the following lemma is essentially the same as that of Theorem 6 from DudÄ±Ìk et al.
(2011a).
14

Lemma 10. Fix any Âµm âˆˆ [0, 1/K] for m âˆˆ N. For any Î´ âˆˆ (0, 1), with probability at least 1 âˆ’ Î´,
75(1 âˆ’ KÂµm ) ln |Î | 6.3 ln(2|Î |2 m2 /Î´)
V (P, Ï€, Âµm ) â‰¤ 6.4Vbm (P, Ï€, Âµm ) +
+
Âµ2m Ï„m
Âµm Ï„m

for all probability distributions P over Î , all Ï€ âˆˆ Î , and all m âˆˆ N. In particular, if
s
ln(2|Î |m2 /Î´)
,
Ï„m â‰¥ 4K ln(2|Î |m2 /Î´),
Âµm â‰¥
KÏ„m
then

V (P, Ï€, Âµm ) â‰¤ 6.4Vbm (P, Ï€, Âµm ) + 81.3K.

Proof sketch. By Bernsteinâ€™s (or Freedmanâ€™s) inequality and union bounds, for any choice of Nm âˆˆ N
and Î»m âˆˆ [0, Âµm ] for m âˆˆ N, the following holds with probability at least 1 âˆ’ Î´:
(e âˆ’ 2)Î»m V (P, Ï€, Âµm ) ln(|Î |Nm +1 2m2 /Î´)
+
V (P, Ï€, Âµm ) âˆ’ Vbm (P, Ï€, Âµm ) â‰¤
Âµm
Î»m Ï„m

all Nm -point distributions P P
over Î , all Ï€ âˆˆ Î , and all m âˆˆ N. Here, an N -point distribution over Î  is
N
a distribution of the form N1 i=1 1Ï€i for Ï€1 , Ï€2 , . . . , Ï€N âˆˆ Î . We henceforth condition on this â‰¥ 1 âˆ’ Î´
probability event (for choices of Nm and Î»m to be determined).
Using the probabilistic method (for more details, we refer the reader to the proof of Theorem 6 from
DudÄ±Ìk et al. (2011a)), it can be shown that for any probability distribution P over Î , any Ï€ âˆˆ Î , any
Âµm âˆˆ [0, 1/K], and any cm > 0, there exists an Nm -point distribution Pe over Î  such that


V (P, Ï€, Âµm ) âˆ’ V (Pe , Ï€, Âµm ) + cm Vbm (Pe, Ï€, Âµm ) âˆ’ Vbm (P, Ï€, Âµm )


â‰¤ Î³Nm ,Âµm V (P, Ï€, Âµm ) + cm Vbm (P, Ï€, Âµm )

p
where Î³N,Âµ := (1 âˆ’ KÂµ)/(N Âµ) + 3(1 âˆ’ KÂµ)/(N Âµ).
Combining the displayed inequalities (using cm := 1/(1 âˆ’ (e âˆ’ 2)Î»m /Âµm )) and rearranging gives
V (P, Ï€, Âµm ) â‰¤

1 + Î³Nm ,Âµm Vbm (P, Ï€, Âµm )
1
1
ln(|Î |Nm +1 2m2 /Î´)
.
Â·
Â·
+
Â·
Î»
Î»
m
m
1 âˆ’ Î³Nm ,Âµm 1 âˆ’ (e âˆ’ 2) Âµ
1 âˆ’ Î³Nm ,Âµm 1 âˆ’ (e âˆ’ 2) Âµ
Î»m Ï„m
m
m

Using Nm :=pâŒˆ12(1 âˆ’ KÂµm )/Âµm âŒ‰ and Î»m := 0.66Âµm for all m âˆˆ N gives the claimed inequalities.
If Âµm â‰¥ ln(2|Î |m2 /Î´)/(KÏ„m ) and Ï„m â‰¥ 4K ln(2|Î |m2 /Î´), then Âµ2m Ï„m â‰¥ ln(|Î |)/K and Âµm Ï„m â‰¥
ln(2|Î |2 m2 /Î´), and hence
75(1 âˆ’ KÂµm ) ln |Î | 6.3 ln(2|Î |2 m2 /Î´)
+
â‰¤ (75 + 6.3)K = 81.3K.
Âµ2m Ï„m
Âµm Ï„m

B.3

Reward Estimates

Again, fix the epoch schedule 0 = Ï„0 < Ï„1 < Ï„2 < Â· Â· Â· . Recall that for any epoch m âˆˆ N and round t in
epoch m,
â€¢ Qmâˆ’1 âˆˆ âˆ†Î  are the non-negative weights computed at the end of epoch m âˆ’ 1;
e mâˆ’1 is the probability distribution over Î  obtained from Qmâˆ’1 and the policy Ï€mâˆ’1 with the
â€¢ Q
highest reward estimate through epoch m âˆ’ 1;

e Âµmâˆ’1 (Â·|xt ) is the probability distribution used to choose at .
â€¢ Q
mâˆ’1
15

Let
m(t) := min{m âˆˆ N : t â‰¤ Ï„m }

(10)

be the index of the epoch containing round t âˆˆ N, and define
Vt (Ï€) :=

e m , Ï€, Âµm )}
{V (Q

max

0â‰¤mâ‰¤m(t)âˆ’1

for all t âˆˆ N and Ï€ âˆˆ Î .

(11)

Lemma 11. For any Î´ âˆˆ (0, 1) and any choices of Î»mâˆ’1 âˆˆ [0, Âµmâˆ’1 ] for m âˆˆ N, with probability at least
1 âˆ’ Î´,
2
b t (Ï€) âˆ’ R(Ï€)| â‰¤ Vt (Ï€)Î»mâˆ’1 + ln(4t |Î |/Î´)
|R
tÎ»mâˆ’1
for all policies Ï€ âˆˆ Î , all epochs m âˆˆ N, and all rounds t in epoch m.

Proof. Fix any policy Ï€ âˆˆ Î , epoch m âˆˆ N, and round t in epoch m. Then
t

X
b t (Ï€) âˆ’ R(Ï€) = 1
R
Zi
t i=1

where Zi := rÌ‚i (Ï€(xi )) âˆ’ ri (Ï€(xi )). Round i is in epoch m(i) â‰¤ m, so
|Zi | â‰¤

1
1
â‰¤
Âµm(i)âˆ’1
e
Âµm(i)âˆ’1
Qm(i)âˆ’1 (Ï€(xi )|xi )

by the definition of the fictitious rewards. Because the sequences Âµ1 â‰¥ Âµ2 â‰¥ Â· Â· Â· and m(1) â‰¤ m(2) â‰¤ Â· Â· Â·
are monotone, it follows that Zi â‰¤ 1/Âµmâˆ’1 for all 1 â‰¤ i â‰¤ t. Furthermore, E[Zi |Hiâˆ’1 ] = 0 and
E[Zi2 |Hiâˆ’1 ] â‰¤ E[rÌ‚i (Ï€(xi ))2 |Hiâˆ’1 ]
e m(i)âˆ’1 , Ï€, Âµm(i)âˆ’1 ) â‰¤ Vt (Ï€)
â‰¤ V (Q

for all 1 â‰¤ i â‰¤ t. The first inequality follows because for var(X) â‰¤ E(X 2 ) for any random variable
X; and the other inequalities follow from the definitions of the fictitious rewards, V (Â·, Â·, Â·)Pin Eq. (8),
and Vt (Â·) in Eq. (11). Applying Freedmanâ€™s inequality and a union bound to the sums (1/t) ti=1 Zi and
Pt
(1/t) i=1 (âˆ’Zi ) implies the following: for all Î»mâˆ’1 âˆˆ [0, Âµmâˆ’1 ], with probability at least 1âˆ’2Â·Î´/(4t2|Î |),
t

1X
ln(4t2 |Î |/Î´)
Zi â‰¤ (e âˆ’ 2)Vt (Ï€)Î»mâˆ’1 +
.
t i=1
tÎ»mâˆ’1

The lemma now follows by applying a union bound for all choices of Ï€ âˆˆ Î  and t âˆˆ N, since
XX

Ï€âˆˆÎ  tâˆˆN

C

Î´
â‰¤ Î´.
2t2 |Î |

Regret Analysis

Throughout this section, we fix the allowed probability of failure Î´ âˆˆ (0, 1) provided as input to the
algorithm, as well as the epoch schedule 0 = Ï„0 < Ï„1 < Ï„2 < Â· Â· Â· .

16

C.1

Definitions

Define, for all t âˆˆ N,
dt := ln(16t2 |Î |/Î´),

(12)

and recall that,
Âµm = min

(

1
,
2K

r

dÏ„m
KÏ„m

)

.

Observe that dt /t is non-increasing with t âˆˆ N, and Âµm is non-increasing with m âˆˆ N.
Let


1
dÏ„m
.
â‰¤
m0 := min m âˆˆ N :
Ï„m
4K
Observe that Ï„m0 â‰¥ 2.
Define
Ï := sup
mâ‰¥m0

Recall that we assume Ï„m+1 â‰¤ 2Ï„m ; thus Ï â‰¤

C.2

âˆš
2.

r

Ï„m
Ï„mâˆ’1



.

Deviation Control and Optimization Constraints

Let E be the event in which the following statements hold:
V (P, Ï€, Âµm ) â‰¤ 6.4Vbm (P, Ï€, Âµm ) + 81.3K

(13)

for
p all probability distributions P over Î , all Ï€ âˆˆ Î , and all m âˆˆ N such that Ï„m â‰¥ 4KdÏ„m (so Âµm =
dÏ„m /(KÏ„m )); and
)
(r
ï£±
3V
d
2V
d
ï£´
t
t
t
t
ï£´
if m â‰¤ m0 ,
,
ï£²max
t
t
b
(14)
|Rt (Ï€) âˆ’ R(Ï€)| â‰¤
ï£´
ï£´
ï£³Vt (Ï€)Âµmâˆ’1 + dt
if m > m0 .
tÂµmâˆ’1

for all all policies Ï€ âˆˆ Î , all epochs m âˆˆ N, and all rounds t in epoch m. By Lemma 10, Lemma 11, and
a union bound, Pr(E) â‰¥ 1 âˆ’ Î´/2.
For every epoch m âˆˆ N, the weights Qm computed at the end of the epoch (in round Ï„m ) as the
solution to (OP) satisfy the constraints Eq. (2) and Eq. (3): they are, respectively:
X
d Ï„ (Ï€) â‰¤ Ïˆ Â· 2KÂµm
(15)
Qm (Ï€)Reg
m
Ï€âˆˆÎ 

and, for all Ï€ âˆˆ Î ,

d Ï„ (Ï€)
Reg
m
Vbm (Qm , Ï€, Âµm ) â‰¤ 2K +
.
(16)
Ïˆ Â· Âµm
âˆš
Recall that Ïˆ = 100 (as defined in (OP), assuming Ï â‰¤ 2). Define Î¸1 := 94.1 and Î¸2 := Ïˆ/6.4 (needed
for the next Lemma 12). With these settings, the proof ofâˆšLemma 13 will require that Î¸2 â‰¥ 8Ï, and hence
Ïˆ â‰¥ 6.4 Â· 8Ï; this is true with our setting of Ïˆ since Ï â‰¤ 2.

17

C.3

Proof of Theorem 2

We now give the proof of Theorem 2, following the outline in Section 4.
The following lemma shows that if Vt (Ï€) is largeâ€”specifically, much larger than Kâ€”then the estimated
regret of Ï€ was large in some previous round.
Lemma 12. Assume event E holds. Pick any round t âˆˆ N and any policy Ï€ âˆˆ Î , and let m âˆˆ N be the
epoch achieving the max in the definition of Vt (Ï€). Then
ï£±
ï£´
if Âµm = 1/(2K),
ï£²2K
d
Vt (Ï€) â‰¤
RegÏ„m (Ï€)
ï£´
if Âµm < 1/(2K).
ï£³ Î¸1 K +
Î¸ 2 Âµm

Proof. Fix a round t âˆˆ N and policy Ï€ âˆˆ Î . Let m â‰¤ m(t) âˆ’ 1 be the epoch achieving the max in the
e m , Ï€, Âµm ). If Âµm = 1/(2K), then V (Q
e m , Ï€, Âµm ) â‰¤ 2K.
definition of Vt (Ï€) from Eq. (11), so Vt (Ï€)p
= V (Q
So assume instead that 1/(2K) > Âµm = dÏ„m /(KÏ„m ). This implies that Ï„m > 4KdÏ„m . By Eq. (13),
which holds in event E,
e m , Ï€, Âµm ) â‰¤ 6.4Vbm (Q
e m , Ï€, Âµm ) + 81.3K.
V (Q
e m satisfies the inequalities
The probability distribution Q
d
e m , Ï€, Âµm ) â‰¤ Vbm (Qm , Ï€, Âµm ) â‰¤ 2K + RegÏ„m (Ï€) .
Vbm (Q
ÏˆÂµm

Above, the first inequality follows because the value of Vbm (Qm , Ï€, Âµm ) decreases as the value of Qm (Ï€Ï„m )
e m ; the second inequality is the constraint Eq. (16) satisfied
increases, as it does when going from Qm to Q
by Qm . Combining the displayed inequalities from above proves the claim.

d (Ï€) for any policy Ï€ by using the deviation bounds
In the next lemma, we compare Reg(Ï€) and Reg
t
for estimated rewards together with the variance bounds from Lemma 12. Define t0 := min{t âˆˆ N :
dt /t â‰¤ 1/(4K)}.
Lemma 13. Assume event E holds. Let c0 := 4Ï(1 + Î¸1 ). For all epochs m â‰¥ m0 , all rounds t â‰¥ t0 in
epoch m, and all policies Ï€ âˆˆ Î ,
d t (Ï€) + c0 KÂµm ;
Reg(Ï€) â‰¤ 2Reg

d t (Ï€) â‰¤ 2 Reg(Ï€) + c0 KÂµm .
Reg

Proof. The proof is by induction on m. As the base case, consider m = m0 and t â‰¥ t0 in epoch m. By
definition of m0 , Âµmâ€² = 1/(2K) for all mâ€² < m0 , so Vt (Ï€) â‰¤ 2K for all Ï€ âˆˆ Î  by Lemma 12. By Eq. (14),
which holds in event E, for all Ï€ âˆˆ Î ,
(r
) r
6Kd
6Kdt
4Kd
t
t
b t (Ï€) âˆ’ R(Ï€)| â‰¤ max
|R
â‰¤
,
t
t
t
where we use the fact that 4Kdt /t â‰¤ 1 for t â‰¥ t0 . This implies
r
6Kdt
d
|Regt (Ï€) âˆ’ Reg(Ï€)| â‰¤ 2
.
t

âˆš
by the triangle inequality and optimality of Ï€t and Ï€â‹† . Since t > Ï„m0 âˆ’1 and c0 â‰¥ 2 6Ï, it follows that
âˆš
d (Ï€) âˆ’ Reg(Ï€)| â‰¤ 2 6ÏKÂµm0 â‰¤ c0 KÂµm0 .
|Reg
t
For the inductive step, fix some epoch m > m0 . We assume as the inductive hypothesis that for all
epochs mâ€² < m, all rounds tâ€² in epoch mâ€² , and all Ï€ âˆˆ Î ,
d tâ€² (Ï€) + c0 KÂµmâ€² ;
Reg(Ï€) â‰¤ 2Reg

d tâ€² (Ï€) â‰¤ 2 Reg(Ï€) + c0 KÂµmâ€² .
Reg
18

We first show that

d t (Ï€) + c0 KÂµm
Reg(Ï€) â‰¤ 2Reg

(17)

for all rounds t in epoch m and all Ï€ âˆˆ Î . So fix such a round t and policy Ï€; by Eq. (14) (which holds
in event E),


d t (Ï€) = R(Ï€â‹† ) âˆ’ R(Ï€) âˆ’ R
b t (Ï€t ) âˆ’ R
b t (Ï€)
Reg(Ï€) âˆ’ Reg


b t (Ï€â‹† ) âˆ’ R
b t (Ï€)
â‰¤ R(Ï€â‹† ) âˆ’ R(Ï€) âˆ’ R

2dt
â‰¤ Vt (Ï€) + Vt (Ï€â‹† ) Âµmâˆ’1 +
.
(18)
tÂµmâˆ’1
Above, the first inequality follows from the optimality of Ï€t . By Lemma 12, there exist epochs i, j < m
such that
d Ï„ (Ï€)
Reg
i
Â· 1{Âµi < 1/(2K)},
Î¸ 2 Âµi
d Ï„ (Ï€â‹† )
Reg
j
Vt (Ï€â‹† ) â‰¤ Î¸1 K +
Â· 1{Âµj < 1/(2K)}.
Î¸ 2 Âµj
Vt (Ï€) â‰¤ Î¸1 K +

Suppose Âµi < 1/(2K), so m0 â‰¤ i < m: in this case, the inductive hypothesis implies
d Ï„ (Ï€)
Reg
2 Reg(Ï€) + c0 KÂµi
c0 K
2 Reg(Ï€)
i
â‰¤
â‰¤
+
Î¸ 2 Âµi
Î¸ 2 Âµi
Î¸2
Î¸2 Âµmâˆ’1

where the second inequality uses the fact that i â‰¤ m âˆ’ 1. Therefore,


2
c0
KÂµmâˆ’1 +
Reg(Ï€).
Vt (Ï€)Âµmâˆ’1 â‰¤ Î¸1 +
Î¸2
Î¸2

(19)

Now suppose Âµj < 1/(2K), so m0 â‰¤ j < m: as above, the inductive hypothesis implies
d Ï„ (Ï€â‹† )
Reg
j
Î¸ 2 Âµj

since Reg(Ï€â‹† ) = 0. Therefore,

â‰¤

2 Reg(Ï€â‹† ) + c0 KÂµj
c0
= K
Î¸ 2 Âµj
Î¸2

Vt (Ï€â‹† )Âµmâˆ’1 â‰¤



c0
KÂµmâˆ’1 .
Î¸1 +
Î¸2

(20)

Combining Eq. (18), Eq. (19), and Eq. (20), and rearranging gives




1
d t (Ï€) + 2 Î¸1 + c0 KÂµmâˆ’1 + 2dt
Reg(Ï€) â‰¤
.
Reg
Î¸2
tÂµmâˆ’1
1 âˆ’ Î¸22

Since m â‰¥ m0 +1, it follows that Âµmâˆ’1 â‰¤ ÏÂµm by definition of Ï. Moreover, since t > Ï„mâˆ’1 , (dt /t)/Âµmâˆ’1 â‰¤
KÂµ2mâˆ’1 /Âµmâˆ’1 â‰¤ ÏKÂµm Applying these inequalities to the above display, and simplifying, yields Eq. (17)
because c0 â‰¥ 4Ï(1 + Î¸1 ) and Î¸2 â‰¥ 8Ï.
We now show that
d t (Ï€) â‰¤ 2 Reg(Ï€) + c0 KÂµm
Reg
(21)
for all Ï€ âˆˆ Î . Again, fix an arbitrary Ï€ âˆˆ Î , and by Eq. (14),


b t (Ï€t ) âˆ’ R
b t (Ï€) âˆ’ R(Ï€â‹† ) âˆ’ R(Ï€)
d t (Ï€) âˆ’ Reg(Ï€) = R
Reg


b t (Ï€t ) âˆ’ R
b t (Ï€) âˆ’ R(Ï€t ) âˆ’ R(Ï€)
â‰¤ R

2dt
â‰¤ Vt (Ï€) + Vt (Ï€t ) Âµmâˆ’1 +
tÂµmâˆ’1
19

(22)

where the first inequality follows from the optimality of Ï€â‹† . By Lemma 12, there exists an epoch j < m
such
Vt (Ï€t ) â‰¤ Î¸1 K +

d Ï„ (Ï€t )
Reg
j
Î¸ 2 Âµj

Â· 1{Âµj < 1/(2K)}.

Suppose Âµj < 1/(2K), so m0 â‰¤ j < m: in this case the inductive hypothesis and Eq. (17) imply


d
d
+ c0 KÂµj
2
2
Reg
(Ï€
)
+
c
KÂµ
t
0
m
RegÏ„j (Ï€t )
t
2 Reg(Ï€t ) + c0 KÂµj
3c0
â‰¤
â‰¤
=
K
Î¸ 2 Âµj
Î¸ 2 Âµj
Î¸ 2 Âµj
Î¸2
d t (Ï€t ) = 0). Thus
(the last equality follows because Reg


3c0
Vt (Ï€t )ÂµÏ„ (t)âˆ’1 â‰¤ Î¸1 +
KÂµmâˆ’1 .
Î¸2

(23)

Combining Eq. (22), Eq. (23), and Eq. (19) gives




d t (Ï€) â‰¤ 1 + 2 Reg(Ï€) + 2Î¸1 + 4c0 KÂµmâˆ’1 + 2dt .
Reg
Î¸2
Î¸2
tÂµmâˆ’1

Again, applying the inequalities Âµmâˆ’1 â‰¤ ÏÂµm and (dt /t)/Âµmâˆ’1 â‰¤ KÂµm to the above display, and simplifying, yields Eq. (21) because c0 â‰¥ 4Ï(1 + Î¸1 ) and Î¸2 â‰¥ 8Ï. This completes the inductive step, and thus
proves the overall claim.

The next lemma shows that the â€œlow estimated regret guaranteeâ€ of Qtâˆ’1 (optimization constraint
d t (Â·) to Reg(Â·) from Lemma 13.
Eq. (15)) also implies a â€œlow regret guaranteeâ€, via the comparison of Reg

Lemma 14. Assume event E holds. For every epoch m âˆˆ N,
X
e mâˆ’1 (Ï€) Reg(Ï€) â‰¤ (4Ïˆ + c0 )KÂµmâˆ’1
Q
Ï€âˆˆÎ 

where c0 is defined in Lemma 13.

Proof. Fix any epoch m âˆˆ N. If m â‰¤ m0 , then Âµmâˆ’1 = 1/(2K), in which case the claim is trivial.
Therefore assume m â‰¥ m0 + 1. Then
X
X

e mâˆ’1 (Ï€) Reg(Ï€) â‰¤
e mâˆ’1 (Ï€) 2Reg
dÏ„
Q
(Ï€) + c0 KÂµmâˆ’1
Q
mâˆ’1
Ï€âˆˆÎ 

Ï€âˆˆÎ 

 X

d
= 2
(Ï€)
+ c0 KÂµmâˆ’1
Qmâˆ’1 (Ï€)Reg
Ï„mâˆ’1
Ï€âˆˆÎ 

â‰¤ Ïˆ Â· 4KÂµmâˆ’1 + c0 KÂµmâˆ’1 .

The first step follows from Lemma 13, as all rounds in an epoch m â‰¥ m0 + 1 satisfy t â‰¥ t0 ; the second
e mâˆ’1 is a probability distribution, that Q
e mâˆ’1 = Qmâˆ’1 + Î±1Ï€Ï„
for
step follows from the fact that Q
mâˆ’1
d
some Î± â‰¥ 0, and that RegÏ„mâˆ’1 (Ï€Ï„mâˆ’1 ) = 0; and the last step follows from the constraint Eq. (15) satisfied
by Qmâˆ’1 .
Finally, we straightforwardly translate the â€œlow regret guaranteeâ€ from Lemma 14 to a bound on the
cumulative regret of the algorithm. This involves summing the bound in Lemma 14 over all rounds t
(Lemma 15 and Lemma 16) and applying a martingale concentration argument (Lemma 17).
Lemma 15. For any T âˆˆ N,

T
X
t=1

Âµm(t) â‰¤ 2

r

20

dÏ„m(T ) Ï„m(T )
.
K

Proof. We break the sum over rounds into the epochs, and bound the sum within each epoch:
T
X
t=1

m(T )

Âµm(t) â‰¤

X

Ï„m
X

X

Ï„m
X

m(T )

â‰¤
â‰¤
â‰¤

Âµm

m=1 t=Ï„mâˆ’1 +1

m=1 t=Ï„mâˆ’1 +1

r

r

r

dÏ„m
KÏ„m

m(T )
dÏ„m(T ) X Ï„m âˆ’ Ï„mâˆ’1
âˆš
K m=1
Ï„m

m(T ) Z
dÏ„m(T ) X Ï„m dx
âˆš =
K m=1 Ï„mâˆ’1 x

r

dÏ„m(T )
K

Z

Ï„m(T )

Ï„0

dx
âˆš =2
x

r

dÏ„m(T ) âˆš
Ï„m(T ) .
K

Above, the first step uses the fact that m(1) = 1 and Ï„m(t)âˆ’1 + 1 â‰¤ t â‰¤ Ï„m(t) . The second step uses
the definition of Âµm . The third step simplifies the sum over t and uses the bound dÏ„mâˆ’1 â‰¤ dÏ„m(T ) . The
remaining steps use an integral bound which is then directly evaluated (recalling that Ï„0 = 0).
Lemma 16. For any T âˆˆ N,
T
X

Âµm(t)âˆ’1

t=1

Ï„m0
+
â‰¤
2K

r

8dÏ„m(T ) Ï„m(T )
.
K

âˆš
Proof. Under the epoch schedule condition Ï„m+1 â‰¤ 2Ï„m , we have Âµm(t)âˆ’1 â‰¤ 2Âµm(t) whenever m(t) >
m0 ; also, Âµm(t)âˆ’1 â‰¤ 1/(2K) whenever m(t) â‰¤ m0 . The conclusion follows by applying Lemma 15.
Lemma 17. For any T âˆˆ N, with probability at least 1 âˆ’ Î´, the regret after T rounds is at most


q
p
C0 4KdÏ„m0 âˆ’1 + 8KdÏ„m(T ) Ï„m(T ) + 8T log(2/Î´)

where C0 := (4Ïˆ + c0 ) and c0 is defined in Lemma 13.

P
e m(t)âˆ’1 Reg(Ï€). Since
Proof. Fix T âˆˆ N. For each round t âˆˆ N, let Zt := rt (Ï€â‹† (xt )) âˆ’ rt (at ) âˆ’ Ï€âˆˆÎ  Q
X
X
e m(t)âˆ’1 (Ï€)R(Ï€) =
e m(t)âˆ’1 Reg(Ï€),
E[rt (Ï€â‹† (xt )) âˆ’ rt (at )|Htâˆ’1 ] = R(Ï€â‹† ) âˆ’
Q
Q
Ï€âˆˆÎ 

Ï€âˆˆÎ 

it follows that E[Zt |Htâˆ’1 ] = 0. Since |Zt | â‰¤ 2, it follows by Azumaâ€™s inequality that
T
X
t=1

p
Zt â‰¤ 2 2T ln(2/Î´)

with probability at least 1 âˆ’ Î´/2. By Lemma 10, Lemma 11, and a union bound, the event E holds with
probability at least 1 âˆ’ Î´/2. Hence, by another union bound, with probability at least 1 âˆ’ Î´, event E
holds and the regret of the algorithm is bounded by
T X
X

t=1 Ï€âˆˆÎ 

e m(t)âˆ’1 (Ï€) Reg(Ï€) + 2
Q

p
2T ln(2/Î´).

The double summation above is bounded by Lemma 14 and Lemma 16:


T X
T
X
X
Ï„m0 q
e
+ 8KdÏ„m(T ) Ï„m(T ) .
Qm(t)âˆ’1 (Ï€) Reg(Ï€) â‰¤ (4Ïˆ + c0 )K
Âµm(t)âˆ’1 â‰¤ (4Ïˆ + c0 )
2
t=1
t=1
Ï€âˆˆÎ 

By the definition of m0 , Ï„m0 âˆ’1 â‰¤ 4KdÏ„m0 âˆ’1 . Since Ï„m0 â‰¤ 2Ï„m0 âˆ’1 by assumption, it follows that Ï„m0 â‰¤
8KdÏ„m0 âˆ’1 .
21

Theorem 2 follows from Lemma 17 and the fact that Ï„m(T ) â‰¤ 2(T âˆ’ 1) whenever Ï„m(T )âˆ’1 â‰¥ 1.
There is one last result implied by Lemma 12 and Lemma 13 that is used elsewhere.
Lemma 18. Assume event E holds, and t is such that dÏ„m(t)âˆ’1 /Ï„m(t)âˆ’1 â‰¤ 1/(4K). Then


b t (Ï€t ) â‰¤ R(Ï€â‹† ) + Î¸1 + c0 + c0 + 1 KÂµm(t)âˆ’1 .
R
Î¸2

Proof. Let mâ€² < m(t) achieve the max in the definition of Vt (Ï€â‹† ). If Âµmâ€² < 1/(2K), then mâ€² â‰¥ m0 , and
Vt (Ï€â‹† ) â‰¤ Î¸1 K +

d (Ï€â‹† )
Reg
Ï„ mâ€²

Î¸ 2 Âµm â€²
2 Reg(Ï€â‹† ) + c0 KÂµmâ€²
â‰¤ Î¸1 K +
= cK
Î¸ 2 Âµm â€²

for c := Î¸1 + c0 /Î¸2 . Above, the second inequality follows by Lemma 13. If Âµmâ€² = 1/(2K), then the same
bound also holds. Using this bound, we obtain from Eq. (14),

To conclude,

b t (Ï€â‹† ) âˆ’ R(Ï€â‹† ) â‰¤ cKÂµm(t)âˆ’1 +
R

dt
.
tÂµm(t)âˆ’1



b t (Ï€â‹† ) âˆ’ R(Ï€â‹† ) + Reg
d t (Ï€â‹† )
b t (Ï€Ï„m ) = R(Ï€â‹† ) + R
R

dt
d t (Ï€â‹† )
+ Reg
tÂµm(t)âˆ’1
dt
â‰¤ R(Ï€â‹† ) + cKÂµm(t)âˆ’1 +
+ c0 KÂµm(t)
tÂµm(t)âˆ’1
â‰¤ R(Ï€â‹† ) + cKÂµm(t)âˆ’1 +

where the last inequality follows from Lemma 13. The claim follows because dt /t â‰¤ dÏ„m(t)âˆ’1 /Ï„m(t)âˆ’1 and
Âµm(t) â‰¤ Âµm(t)âˆ’1 .

D
D.1

Details of Optimization Analysis
Proof of Lemma 5

Following the execution of Step 4, we must have
X
Q(Ï€)(2K + bÏ€ ) â‰¤ 2K.

(24)

Ï€

This is because, if the condition in Step 7 does not hold, then Eq. (24) is already true. Otherwise, Q is
replaced by Qâ€² = cQ, and for this set of weights, Eq. (24) in fact holds with equality.
P Note that, since all
quantities are nonnegative, Eq. (24) immediately implies both Eq. (2), and that Ï€ Q(Ï€) â‰¤ 1.
Furthermore, at the point where the algorithm halts at Step 10, it must be that for all policies Ï€,
DÏ€ (Q) â‰¤ 0. However, unraveling definitions, we can see that this is exactly equivalent to Eq. (3).

D.2

Proof of Lemma 6

Consider the function
g(c) = B0 Î¦m (cQ),

22

where, in this proof, B0 = 2K/(Ï„ Âµ), where we recall that we drop the subscripts on Ï„m and Âµm . Let
QÂµc (a|x) = (1 âˆ’ KÂµ)cQ(a|x) + Âµ. By the chain rule, the first derivative of g is:
g â€² (c)

= B0

X

Q(Ï€)

Ï€

X

=

Ï€

To handle the second term, note that
X
Ï€

b xâˆ¼Ht
Q(Ï€)E



âˆ‚g(cQ)
âˆ‚Q(Ï€)



b xâˆ¼H
Q(Ï€) (2K + bÏ€ ) âˆ’ 2E
t

1
QÂµc (Ï€(x)|x)



=

X
Ï€

b xâˆ¼Ht
Q(Ï€)E

b xâˆ¼Ht
= E

b xâˆ¼H
= E
t
=

"

"

"

1
QÂµc (Ï€(x)|x)



X 1{Ï€(x) = a}
QÂµc (a|x)

aâˆˆA

X X Q(Ï€)1{Ï€(x) = a}

aâˆˆA Ï€

QÂµc (a|x)
#

X Q(a|x)
QÂµc (a|x)
aâˆˆA
"
X
cQ(a|x)

1b
Exâˆ¼Ht
c

aâˆˆA

(25)

#

#

(1 âˆ’ KÂµ)cQ(a|x) + Âµ

#

â‰¤

K
.
c

(26)

P
P
To see the inequality in Eq. (26), let us fix x and define qa = cQ(a|x). Then a qa = c Ï€ Q(Ï€) â‰¤ 1 by
Eq. (4). Further, the expression inside the expectation in Eq. (26) is equal to
X
a

qa
(1 âˆ’ KÂµ)qa + Âµ

= KÂ·

1 X
1
K a (1 âˆ’ KÂµ) + Âµ/qa

1
P
(1 âˆ’ KÂµ) + KÂµ/ a qa
1
â‰¤ KÂ·
= K.
(1 âˆ’ KÂµ) + KÂµ
â‰¤ KÂ·

(27)
(28)

Eq. (27) uses Jensenâ€™s inequality, combined with the fact that the function 1/(1 âˆ’ KÂµ + Âµ/x) is concave
(as a function of x). Eq. (28) uses the fact that the function 1/(1 âˆ’ KÂµ + KÂµ/x) is nondecreasing (in x),
and that the qa â€™s sum to at most 1.
Thus, plugging Eq. (26) into Eq. (25) yields
g â€² (c) â‰¥

X
Ï€

Q(Ï€)(2K + bÏ€ ) âˆ’

2K
=0
c

by our definition of c. Since g is convex, this means that g is nondecreasing for all values exceeding c. In
particular, since c < 1, this gives
B0 Î¦m (Q) = g(1) â‰¥ g(c) = B0 Î¦m (cQ),
implying the lemma since B0 > 0.

D.3

Proof of Lemma 7
Âµ

We first compute the change in potential for general Î±. Note that Qâ€² (a|x) = QÂµ (a|x) if a 6= Ï€(x), and
otherwise
Âµ
Qâ€² (Ï€(x)|x) = QÂµ (Ï€(x)|x) + (1 âˆ’ KÂµ)Î±.

23

Thus, most of the terms defining Î¦m (Q) are left unchanged by the update. In particular, by a direct
calculation:

 
2K
2
b xâˆ¼Ht ln 1 + Î±(1 âˆ’ KÂµ)
âˆ’ Î±(2K + bÏ€ )
E
(Î¦m (Q) âˆ’ Î¦m (Qâ€² )) =
Ï„Âµ
1 âˆ’ KÂµ
QÂµ (Ï€(x)|x)
"

2 #
1 Î±(1 âˆ’ KÂµ)
Î±(1 âˆ’ KÂµ)
2
b
Exâˆ¼Ht
âˆ’
â‰¥
1 âˆ’ KÂµ
QÂµ (Ï€(x)|x) 2 QÂµ (Ï€(x)|x)
=
=
=

âˆ’Î±(2K + bÏ€ )

(29)

2

2Î±VÏ€ (Q) âˆ’ (1 âˆ’ KÂµ)Î± SÏ€ (Q) âˆ’ Î±(2K + bÏ€ )
Î±(VÏ€ (Q) + DÏ€ (Q)) âˆ’ (1 âˆ’ KÂµ)Î±2 SÏ€ (Q)
(VÏ€ (Q) + DÏ€ (Q))2
.
4(1 âˆ’ KÂµ)SÏ€ (Q)

(30)
(31)

Eq. (29) uses the bound ln(1 + x) â‰¥ x âˆ’ x2 /2 which holds for x â‰¥ 0 (by Taylorâ€™s theorem). Eq. (31) holds
by our choice of Î± = Î±Ï€ (Q), which was chosen to maximize Eq. (30). By assumption, DÏ€ (Q) > 0, which
implies VÏ€ (Q) > 2K. Further, since QÂµ (a|x) â‰¥ Âµ always, we have


1
b
SÏ€ (Q) = Exâˆ¼Ht
QÂµ (Ï€(x) | x)2


1
VÏ€ (Q)
1 b
Â· Exâˆ¼Ht
.
=
â‰¤
Âµ
QÂµ (Ï€(x) | x)
Âµ
Thus,

2

VÏ€ (Q)
VÏ€ (Q)
(VÏ€ (Q) + DÏ€ (Q))2
â‰¥
= VÏ€ (Q) Â·
â‰¥ 2KÂµ.
SÏ€ (Q)
SÏ€ (Q)
SÏ€ (Q)
Plugging into Eq. (31) completes the lemma.

D.4

Proof of Lemma 8

We break the potential of Eq. (6) into pieces and bound the total change in each separately. Specifically,
by straightforward algebra, we can write
Î¦m (Q) = Ï†am (Q) + Ï†bm + Ï†cm (Q) + Ï†dm (Q)
where
Ï†am (Q)

=

Ï†bm

=

Ï†cm (Q) =

"
#
X
Ï„m Âµm
Âµ
b
Exâˆ¼Ht âˆ’
ln Q (a|x)
K(1 âˆ’ KÂµm )
a

Ï„m Âµm ln K
1 âˆ’ KÂµm
!
X
Ï„m Âµm
Q(Ï€) âˆ’ 1
Ï€

Ï†dm (Q)

=

Ï„m Âµm X
Q(Ï€)bÏ€ .
2K Ï€

P
We assume throughout that Ï€ Q(Ï€) â‰¤ 1 as will always be the case for the vectors produced by
Algorithm 2. For such a vector Q,
!
X
c
c
Ï†m+1 (Q) âˆ’ Ï†m (Q) = (Ï„m+1 Âµm+1 âˆ’ Ï„m Âµm )
Q(Ï€) âˆ’ 1 â‰¤ 0
Ï€

24

since Ï„m Âµm is nondecreasing. This means we can essentially disregard the change in this term.
Also, note that Ï†bm does not depend on Q. Therefore, for this term, we get a telescoping sum:
r
M
X
T dT
b
b
b
b
b
(Ï†m+1 âˆ’ Ï†m ) = Ï†M+1 âˆ’ Ï†1 â‰¤ Ï†M+1 â‰¤ 2
ln K
K
m=1
since KÂµM+1 â‰¤ 1/2, and where dT , used in the definition of Âµm , is defined in Eq. (12).
Next, we tackle Ï†am :
Lemma 19.

M
X

(Ï†am+1 (Qm )
m=1

âˆ’

Ï†am (Qm ))

â‰¤6

r

T dT
ln(1/ÂµM+1 ).
K

Proof. For the purposes of this proof, let
Cm =
Then we can write
Ï†am (Q) = âˆ’

Âµm
.
1 âˆ’ KÂµm

Ï„m X
Cm X
ln QÂµm (a|xt ).
K t=1 a

Note that Cm â‰¥ Cm+1 since Âµm â‰¥ Âµm+1 and âˆ’ ln QÂµm (a|xt ) â‰¥ 0. Thus,
 Ï„m X
Cm+1 X
a
a
Ï†m+1 (Q) âˆ’ Ï†m (Q) â‰¤
ln QÂµm (a|xt )
K
a
t=1

Ï„m+1
XX
âˆ’
ln QÂµm+1 (a|xt )
t=1

=

a

X
Ï„m X



QÂµm (a|xt )
QÂµm+1 (a|xt )
t=1 a

Ï„m+1
X X
Âµm+1
âˆ’
(a|xt ).
ln Q
Cm+1
K

ln



t=Ï„m +1 a

â‰¤ Cm+1 [Ï„m ln(Âµm /Âµm+1 ) âˆ’ (Ï„m+1 âˆ’ Ï„m ) ln Âµm+1 ].

(32)

Eq. (32) uses QÂµm+1 (a|x) â‰¥ Âµm+1 , and also
Âµm
(1 âˆ’ KÂµm )Q(a|x) + Âµm
QÂµm (a|x)
â‰¤
,
=
QÂµm+1 (a|x)
(1 âˆ’ KÂµm+1 )Q(a|x) + Âµm+1
Âµm+1
using Âµm+1 â‰¤ Âµm . A sum over the two terms appearing in Eq. (32) can now be bounded separately.
Starting with the one on the left, since Ï„m < Ï„m+1 â‰¤ T and KÂµm â‰¤ 1/2, we have
r
T dT
Cm+1 Ï„m â‰¤ 2Ï„m Âµm+1 â‰¤ 2Ï„m+1 Âµm+1 â‰¤ 2
.
K
Thus,
M
X

m=1

Cm+1 Ï„m ln(Âµm /Âµm+1 )

â‰¤ 2

r

r

M
T dT X
ln(Âµm /Âµm+1 )
K m=1

T dT
ln(Âµ1 /ÂµM+1 ))
K
r
T dT
â‰¤ 2
(âˆ’ ln(ÂµM+1 )).
K
= 2

25

(33)

For the second term in Eq. (32), using Âµm+1 â‰¥ ÂµM+1 for m â‰¤ M , and definition of Cm , we have
M
X

m=1

âˆ’Cm+1 (Ï„m+1 âˆ’ Ï„m ) ln Âµm+1

â‰¤ âˆ’2(ln ÂµM+1 )
â‰¤ âˆ’2(ln ÂµM+1 )
â‰¤ âˆ’4

r

M
X

(Ï„m+1 âˆ’ Ï„m )Âµm+1

m=1
T
X

Âµm(t)

t=1

T dT
(ln ÂµM+1 )
K

(34)

by Lemma 15. Combining Eqs. (32), (33) and (34) gives the statement of the lemma.
Finally, we come to Ï†dm (Q), which, by definition of bÏ€ , can be rewritten as
X
d Ï„ (Ï€)
Ï†dm (Q) = B1 Ï„m
Q(Ï€)Reg
m

Ï€

where B1 = 1/(2KÏˆ) and Ïˆ is the same as appears in optimization problem (OP). Note that, conveniently,
d (Ï€) = Sbm (Ï€m ) âˆ’ Sbm (Ï€),
Ï„m Reg
Ï„m

where Sbm (Ï€) is the cumulative empirical importance-weighted reward through round Ï„m :
Sbm (Ï€) =

Ï„m
X
t=1

From the definition of QÌƒ, we have that

b Ï„m (Ï€).
rÌ‚t (Ï€(xt )) = Ï„m R

Ï†dm (QÌƒ) = Ï†dm (Q)
+B1 1 âˆ’
= Ï†dm (Q)

X
Ï€

!

d (Ï€m )
Q(Ï€) Ï„m Reg
Ï„m

dÏ„
d Ï„ (Ï€m ) = 0. And by a similar computation, Ï†d (QÌƒ) â‰¥ Ï†d (Q) since Reg
(Ï€) is always
since Reg
m+1
m+1
m+1
m
nonnegative.
Therefore,
Ï†dm+1 (Qm ) âˆ’ Ï†dm (Qm )

â‰¤ Ï†dm+1 (QÌƒm ) âˆ’ Ï†dm (QÌƒm )

 

X
= B1
QÌƒm (Ï€) Sbm+1 (Ï€m+1 ) âˆ’ Sbm+1 (Ï€) âˆ’ Sbm (Ï€m ) âˆ’ Sbm (Ï€)
Ï€



= B1 Sbm+1 (Ï€m+1 ) âˆ’ Sbm (Ï€m )
Ï„m+1

âˆ’B1

X X

!

QÌƒm (Ï€)rÌ‚t (Ï€(xt )) .

t=Ï„m +1 Ï€

(35)

We separately bound the two parenthesized expressions in Eq. (35) when summed over all epochs.
Beginning with the first one, we have
M 

X
Sbm+1 (Ï€m+1 ) âˆ’ Sbm (Ï€m )
=

m=1

SbM+1 (Ï€M+1 ) âˆ’ Sb1 (Ï€1 ) â‰¤ SbM+1 (Ï€M+1 ).
26

But by Lemma 18 (and under the same assumptions),
b Ï„M +1 (Ï€M+1 )
= Ï„M+1 R
â‰¤ Ï„M+1 (R(Ï€â‹† ) + D0 KÂµM )
p
â‰¤ Ï„M+1 R(Ï€â‹† ) + D0 KT dT ,

SbM+1 (Ï€M+1 )

(36)

where D0 is the constant appearing in Lemma 18.
For the second parenthesized expression of Eq. (35), let us define random variables
X
Zt =
QÌƒÏ„ (t) (Ï€)rÌ‚t (Ï€(xt )).
Ï€

Note that Zt is nonnegative, and if m = Ï„ (t), then
X
Zt =
QÌƒm (Ï€)rÌ‚t (Ï€(xt ))
Ï€

=

X

QÌƒm (a|xt )rÌ‚t (a)

a

=

X

rt (a)1{a = at }
QÌƒÂµm (a|xt )

QÌƒm (a|xt )

a

â‰¤

rt (at )
â‰¤2
1 âˆ’ KÂµm

since QÌƒÂµm (a|x) â‰¥ (1 âˆ’ KÂµm )QÌƒm (a|x), and since rt (at ) â‰¤ 1 and KÂµm â‰¤ 1/2. Therefore, by Azumaâ€™s
inequality, with probability at least 1 âˆ’ Î´,
Ï„M +1

X
t=1

Ï„M +1

Zt â‰¥

X
t=1

E[Zt |Htâˆ’1 ] âˆ’

p
2Ï„M+1 ln(1/Î´).

The expectation that appears here can be computed to be
X
E[Zt |Htâˆ’1 ] =
QÌƒm (Ï€)R(Ï€)
Ï€

so

R(Ï€â‹† ) âˆ’ E[Zt |Htâˆ’1 ] =
=

X
Ï€

X

QÌƒm (Ï€)(R(Ï€â‹† ) âˆ’ R(Ï€))
QÌƒm (Ï€) Reg(Ï€)

Ï€

â‰¤

(4Ïˆ + c0 )KÂµm

by Lemma 14 (under the same assumptions, and using the same constants). Thus, with high probability,
Ï„M +1

X
t=1

Ï„M +1

(R(Ï€â‹† ) âˆ’ Zt )

â‰¤ (4Ïˆ + c0 )K

X
t=1

Âµm(t) +

p
2Ï„M+1 ln(1/Î´)

p
p
â‰¤ (4Ïˆ + c0 ) 8KT dT + 2T ln(1/Î´)

by Lemma 16.
Combining the above bound with our earlier inequality Eq. (36), and applying the union bound, we
find that with probability at least 1 âˆ’ 2Î´, for all T (and corresponding M ),
!
r
M
X
T
d
d
(Ï†m (Qm ) âˆ’ Ï†m+1 (Qm )) â‰¤ O
ln(T |Î |/Î´) .
K
m=1
Combining the bounds on the separate pieces, we get the bound stated in the lemma.
27

D.5

Proof of Lemma 3

We finally have all the pieces to establish our main bound on the oracle complexity with warm-start
presented in Lemma 3. The proof is almost immediate, and largely follows the sketch in Section 3.4
apart from one missing bit of detail. Notice that we start Algorithm 1 with Q0 = 0, at which point
the objective Î¦0 (Q0 ) = 0 since Ï„0 = 0. Initially, owing to the small values of Ï„m , we might be in the
regime where Âµ = 1/2K, where the decrease in the potential guaranteed by Lemma 7 is just OÌƒ(Ï„ /K 2 ).
However, in this regime, it is easy to check that Q0 = 0 remains a feasible solution to (OP). It clearly
satisfies the regret constraint Eq. (2), and Âµ = 1/(2K) ensures that the variance constraints Eq. (3) are
also met. Hence, we make no
p calls to the oracle in this initial regime and can focus our attention to Ï„m
large enough so that Âµm = dÏ„m /(KÏ„m ).
2
In this regime, we observe that Ï„m
Âµm = dÏ„m /K, so that Lemma 7 guarantees that we decreaes the
objective by at least dÏ„m /(4K) (recalling KÂµm â‰¥ 0). Hence, the total decrease in our
p objective after
N calls to the oracle is at least N dÏ„m /(4K), while the net increase is boundedpby OÌƒ( T dT /K. Since
the potential is always positive, the number of oracle calls can be at most OÌƒ( T K/ ln(|Î |/Î´)), which
completes the proof.

E

Proof of Theorem 4

Recall the earlier definition of the low-variance distribution set
Qm = {Q âˆˆ âˆ†Î  : Q satisfies Eq. (3) in round Ï„m }.

p
2 |Î |/Î´)/Ï„
Fix Î´ âˆˆ (0, 1) and the epoch sequence, and assume M is large enough so Âµm = ln(16Ï„m
m for
all m âˆˆ N with Ï„m â‰¥ Ï„M /2. The low-variance constraint Eq. (3) gives, in round t = Ï„m ,
b xâˆ¼Ht
E




d Ï„ (Ï€)
Reg
1
m
â‰¤ 2K +
,
Âµ
m
Q (Ï€(x)|x)
ÏˆÂµm

âˆ€Ï€ âˆˆ Î .

Below, we use a policy class Î  where every policy Ï€ âˆˆ Î  has no regret (Reg(Ï€) = 0), in which case
Lemma 13 implies




c0 KÂµm
1
c0
b xâˆ¼Ht
E
â‰¤
2K
+
, âˆ€Ï€ âˆˆ Î .
=
K
2
+
QÂµm (Ï€(x)|x)
ÏˆÂµm
Ïˆ

Applying Lemma 10 (and using our choice of Âµm ) gives the following constraints: with probability at
least 1 âˆ’ Î´, for all m âˆˆ N with Ï„m â‰¥ Ï„M /2, for all Ï€ âˆˆ Î ,
#
"


c0
1
â‰¤ 81.3K + 6.4K 2 +
Exâˆ¼DX
=: cK
(37)
e Âµm (Ï€(x)|x)
Ïˆ
Q

e the leftover mass can be put on any policy, say, already in
(to make Q into a probability distribution Q,
the support of Q). That is, with high probability, for every relevant epoch m, every Q âˆˆ Qm satisfies
Eq. (37) for all Ï€ âˆˆ Î .
Next, we construct an instance with the property that these inequalities cannot be satisfied by a very
sparse Q. An instance is drawn uniformly at
âˆšrandom from N different contexts denoted as {1, 2, . . . , N }
(where we set, with foresight, N := 1/(2 2cKÂµM )). The reward structure in the problem will be
extremely simple, with action K always obtaining a reward of 1, while all the other actions obtain a
reward of 0, independent of the context. The distribution D will be uniform over the contexts (with these
deterministic rewards). Our policy set Î  will consist of (K âˆ’ 1)N separate policies, indexed by 1 â‰¤ i â‰¤ N
and 1 â‰¤ j â‰¤ K âˆ’ 1. Policy Ï€ij has the property that
(
j
if x = i,
Ï€ij (x) =
K otherwise.
28

In words, policy Ï€ij takes action j on context i, and action K on all other contexts. Given the uniform
distribution over contexts and our reward structure, each policy obtains an identical reward


1
1
1
R(Ï€) = 1 âˆ’
Â·0=1âˆ’ .
Â·1+
N
N
N
In particular, each policy has a zero expected regret as required.
Finally, observe that on context i, Ï€ij is the unique policy taking action j. Hence we have that
e ij ) + Âµm . Now, let us consider the constraint Eq. (37) for
e
e ij ) and Q
e Âµm (j|i) = (1 âˆ’ KÂµm )Q(Ï€
Q(j|i) = Q(Ï€
the policy Ï€ij . The left-hand side of this constraint can be simplified as
Exâˆ¼DX

"

1
e Âµm (Ï€(x)|x)
Q

#

=
=

N
1
1 X
Âµ
e
m
N x=1 Q (Ï€ij (x)|x)
1 X
1

N

x6=i

e Âµm (Ï€ij (x)|x)
Q

1
1
.
Â·
â‰¥
e Âµm (j|i)
N Q

+

1
1
Â·
e Âµm (j|i)
N Q

e does not put any support on the policy Ï€ij , then Q
e Âµm (j|i) = Âµm , and thus
If the distribution Q
#
"
1
1
1
1
1
â‰¥
> cK
Exâˆ¼DX
=
â‰¥ âˆš
Â·
Âµ
Âµ
e
e
N Q m (j|i)
N Âµm
2N ÂµM
Q m (Ï€(x)|x)

âˆš
e violates Eq. (37), which means that every Q âˆˆ Qm
(since N < 1/( 2cKÂµM )). Such a distribution Q
e ij ) > 0. Since this is true for each policy Ï€ij , we see that every Q âˆˆ Qm has
must have Q(Ï€
s
!
K âˆ’1
KÏ„M
=â„¦
|supp(Q)| â‰¥ (K âˆ’ 1)N = âˆš
ln(Ï„M |Î |/Î´)
2 2cKÂµM
which completes the proof.

F

Online Cover algorithm

This section describes the pseudocode of the precise algorithm use âˆš
in our experiments (Algorithm 5).
The minimum exploration probability Âµ was set as 0.05 min(1/K, 1/ tK) for our evaluation.
Two additional details are important in Step 9:
1. We pass a cost vector rather than a reward vector to the oracle since we have a loss minimization
rather than a reward maximization oracle.
2. We actually used a doubly robust estimate DudÄ±Ìk et al. (2011b) with a linear reward function that
was trained in an online fashion.

29

Algorithm 5 Online Cover
input Cover size n, minimum sampling probability Âµ.
1: Initialize online cost-sensitive minimization oracles O1 , O2 , . . . , On , each of which controls a policy
Ï€(1) , Ï€(2) , . . . , Ï€(n) ; U := uniform probability distribution over these policies.
2: for round t = 1, 2, . . . do
3:
Observe context xt âˆˆ X.
4:
(at , pt (at )) := Sample(xt , U, âˆ…, Âµ).
5:
Select action at and observe reward rt (at ) âˆˆ [0, 1].
6:
for each i = 1, 2, P
. . . , n do
7:
Qi := (i âˆ’ 1)âˆ’1 j<i 1Ï€(j) .
8:
pi (a) := QÂµ (a|xt ).
Âµ
t)
9:
Create cost-sensitive example (xt , c) where c(a) = 1 âˆ’ prtt (a
(at ) 1{a = at } âˆ’ pi (a) .
10:
Update Ï€(i) = Oi (x, c)
11:
end for
12: end for

30

