We also provide a simple but efficient procedure
for nonuniform partition. To represent a nonuni­

ing in Bayesian networks with

discrete

variables (for

example, [Pearl, 1988, Lauritzen and Spiegelhalter, 1988,
Li and D'Ambrosio, 1994,

Dechter, 1996]),

few

algo­

rithms support efficient inference in hybrid Bayesian net­
works, Bayesian networks where continuous and discrete

variables are intermixed.

Exact probabilistic inference

in hybrid networks can be reduced to taking multidi­
mension al

integrals in the same way that exact infer­

ence in discrete networks can be reduced to computing
sum s [Li and D'Ambrosio, 1994, Dechter, 1996].

How­

ever, computing integrals exactly is possible only for a re­
stricted class of continuous functions.

form discretization in the computer memory, we

For

introduce a new data structure, which we call a Bi­

classes where exact probabilistic inference is possible

example,

one

of

the

hybrid Bayesian

network

nary Split Partition (BSP) tree. We show that BSP

are networks with Conditional Gaussian (CG) density

trees can be an exponential factor smaller than the

functions [Lauritzen and Wermuth, 1989, Lauritzen, 1992,

data structures in the standard uniform discretiza­

Olesen, 1993]. Probabilistic inference in these networks is

tion in multiple dimensions and show how the BSP

polynomial in the number of continuous variables. How­

trees can be used in the standard join tree algo­

ever, the CG limitations on the dependencies between vari­

rithm. We show that the accuracy of the inference

ables obstruct the application of hybrid networks in many

process can be significantly improved by adjusting

domains. In many practical problems we have dependen­

discretization with evidence. We construct an it­

cies substantially different from those encompassed by the

erative anytime algorithm that gradually improves

CG model.

the quality of the discretization and the accuracy

sor of a continuous variable, say a fire alarm activated by

of the answer on a query. We prov id e empiric al
evidence that the algorithm converges.

For example,

we

c annot

model a discrete sen­

smoke concentration, since continuous variables are not al­
lowed to have discrete descendants in CG hybrid networks.
Given that we want to reason with a more general class

1

of networks and distributions than CG, we need to design

INTRODUCTION

approximate methods for inference in hybrid networks. A

A Bayesian network is an efficient representation of a joint
probabi lity distribution over domain variables.

Bayesian

networks allow intuitive causal interpretation of dependen­
cies as well as efficient algorithms for probabilistic infer­
ence. In particular, we can obtain answers for queries about
the probabilities of some events given information about
others. Bayesian networks are becoming a popular tool for
reasoning under uncertainty and have been used in a num­
ber of practical systems.

useful extension of the previous technique is to decompose
an arbitrary conditional probability distribution into sev­
eral CG distributions, and to represent continuous functions
as the sums of CG functions [Driver and Morrel, 1995,

Alag and Agogino, 1996].

The price one pays is a fast

growth of the number of terms in the sums during prob­
abilistic inference. In a join tree, for example, each time a
clique potential is multiplied by a message, the number of
terms in the resulting sum is the product of the number of

Nonuniform dynamic discretization in hybrid networks

315

terms in each of the factors before the multiplication, thus

inference steps can be ap propriately interleaved with dis­

making the number of terms in the s ums grow exponen­

cretization steps.

tially with the path length. In addition to this probl em, the
initial approximation of an arbitrary continuous function
by a sum of

CGs can also present a

computational chal­

l enge. The number of terms in such a decomposition can

be p rohibitivel y large, and probabilistic inference compu­
tationally intractable.

Like most other approximation techniques, our approach
targets the discretization to do well on the most likely sce­
narios.

As a consequence, it is likely to incur large er­

rors in the case of unlikely evidence. We could refine our

ally, we would discretize each variable separately and rep­

discretization so that it remains accurate under all circum­
stances, but the resulting discretization is likely to be in­
feasibly large. Rathe r, we propose an ap proach that adjusts
the discretization to reflect the observed evidence and the
needs of the query. An optimal implementation of this pro­

resent the conditional probabili ties of the nodes in a net­

cess requires that we base our discretization on the pos­

work and the clique potentials as multidimensional tables.

terior pro bability distributions. Since we do not have ac­
cess to this posterior, we execute an iterative proced ure , in
which the accuracy of our predicti ons and the quality of our

The other approach, the one most c ommon ly used in prac­
tice, is to discretize all variables in

a

network. Tradition­

In this appr oach , the size of a clique potential table is the
product of the number of discretized values for the vari­

ables participating in the clique. Since the number of vari­

discretization increases with each iteration. We show em­

ables in a clique can be quite large, we usually cannot af­

pirically that the results of this procedur e converge quickly

ford to discretize the variables as finely as we would like.

to the exact results.

As a consequence, we incur significant error relative to the
exact solution.
In this paper, we propose an alternative approach. Rather
than discretizing each variable separately, we discretize a
continuous function on its entire multidimensional domain
at once. Thus, we

can adjust our discretization to the shape

of the function, providing a fi ner partition in places where

2

NONUNIFORM DISCRETIZATION

A discretization is conventionally understood as a subdivi­

sion of the range of a continuous variable into a set of sub­

ranges. If each of th e variables is discretized separately, the
computational complexity of probabilistic inference grows

the function changes rapidly, while leaving relatively "flat"

as the number of discretization subranges, or the number

regions at a very ro ugh level of granularity.

We show t ha t

of states per variable, to the power of the induced width of

the nonuniform discretization allows us to provide much

the graph [Dechter, 1996]. Since the induced width can be

greater accuracy using the same number of partitions of the

as large as 20 for practical networks, we want to keep the

function domain.

number of discretization subregions low while preserving

To represent one of such nonuniform discretization, we

mo st of the information in the discretized function.

wou ld like to discretize only the regions of a function

introduce a new data structure, a Binary Split Partition

We

(BSP) tree. A BSP tree represents a recursive binary par­

that contribute to the structure of the joint probability distri­

tition of a function domain and is similar to the quadtrees
or octrees used in graphics for representing space objects
[Samet and Webber, 1988]. 1 In a BSP tree, we restrict the
partitions of the multidimensional domains to binary splits
by a plane orthogonal to one of the coordinate axes. We

bution. By providing more detail about the more relevant

show that, for a given number of partitions, BSP trees come
very close to the optimal nonuniform discretization of a
multidimensional probability function. In particular, th ey

significantly more compact than the traditional repre­
sentation of continuous functions by multidimensional ta­

are

bles.
Of course, we don't want to discretize the entire joint den­
sity function at once. We therefore propose an approach

parts of the space, we can provide

able separately. In this section, we generalize the notion
of a discretization. We begin by considering the problem
of di scretizing a sing l e function on a multidimensional do­

main. In later sections, we will apply these ideas to de­
compo sed probability distributions, such as those found in

a Bayesian network.
treatment in this part is based on the rela ­
tive entropy or Kullback-Leibler (KL) dista nc e be­
tween two pr obabi lity densit y functions J( x) and g( x)
[Cover and Thomas, 1991 ]:
All

where the density function in each clique in a join tree is

D(fllg)

discretized using our nonuniform discretization. We show
how the traditional join tree algorithm can b e adapted to
do i n fe re nce with the BSP trees, and how the probabilistic
1 Recursive partition of multivariate domains have been
also used in multivariate regression and machine learning

[Breiman et al., 1984, Moore, 1991].

a much more accurate

picture of the distribution than if we discretize each vari­

=is f(x) log �i:�

as a metric for the error introduced

tion.

There are many

relative e ntro py

as

dx

( 1)

by the discretiza­

justifications for the use of

a distance metric between distribu­

tions [Cover and Thomas, 1991], including several of use­
ful properties that we will use throughout the paper.

Kozlov and Koller

316

Our first task is to find the optimal discretization and op­

spatial respectively are often used for a compressed repre­

timal values for the discretized function. Without loss of

sentation of space objects. Both of these data structures

generality, we consider discretizing a continuous function

have proved to be very efficient computationally.

defined on a hypercube

n

[0, l)n. To compare the results

=

for different discretizations, we need a formal definition.

Definition 2.1: A discretization 'D of a hypercube

[0, l]n

from

is a piecew ise constant function

n

to a finite set of integers from 1 to

iD(x1,

m.

n

ture that represents a hierarchical binary decomposition of
=

... ,xn)

The function

defines mutually exclusive and collectively exhaustive set

{

of subregions w;, i

=

1, .. . , m}

in

f.!. I

stant in each of the subregions w;, we call it a dis­
cretized function on the discretization 'D.

The follow­

ing theorem proves that the optimal value for the dis­
.

..

, Xn

)

is the mean of the func­

tionf(x1, ... ,xn) in each of the subregionsw;.
Theorem 2.1:

a multidimensional function. Each node in the tree repre­
sents a subregion of the function domain and can have two
children. Each of the children represents half of the par­
ent's space. The splitting continues from the root of the
tree, representing the whole function domain, to the leaves,
that carry information about the function in a particular

If a probability density function fD(x1, ... ,xn) is con­

cretized function fD( x 1,

A Binary Split Partition (BSP) tree is a recursive data struc­

subregion.
We restrict the splits to subdivisions of the space into two
halves defined by a plane orthogonal to one of the axes.
For

ex amp l e,

one of the possible BSP trees for a function

of two variables

x

and y is shown in Fig. 1. On the first

level, we split the function domain by a line orthogonal to

Given a discretization 'D of a region n.

the minimum KL distance between a probability den­
sity function f(x1, ... , xn) and a discretized function
fD(Xt, ... , xn) is achieved by the discretized function
D(Xt, ... , xn) which is piecewise constant and in each
of the subregions w; is equal to the mean of the function

y. On the second level, we leave the left node as a leaf
representing the lower half of the xy plane. We split the
right one, representing the upper half of the xy plane, by a
line orthogonal to x. Each of the children on the third level

/

is split even further.

f(xt, ... ,xn) in the corresponding subregion w;. The
KL distance to any other piecewise constant on 'D func­

Fig. I is constant in the lower half of the xy plane. The

As a result, the function represented by the BSP tree in
discretization has higher granularity in the upper half of

tionfD(Xt, ... , xn) which is also piecewise constant and

the xy plane, where we continue splitting. The BSP tree

in each of the

in Fig. 1 might

subregions w;

is

g iven by the sum of KL dis­

/
..
xn)tof(x
,
.
x
, n)·
,
t

tancesfrom fD(x 1 , ... ,xn)to D(xl, ... ,xn)andfrom

/v(xt, ...
This

ts

a

position
tance

consequence

properties

of

[Cover and Thomas,

of

relative

decom­

entropy

dis­

Due to the space

restrictions we do not show the proof of the theorem.

to build up a BSP tree:
which direction.

to the relative entropy error is computationally expensive
for a general function, we use a bound on the
the function mean
function minimum

discretization that minimizes the relative entropy error. Be­
a

discretization which is very close to optimal.

To find an optimal discretization of a hypercube n into
m

subregions we should search through all possible

partitions of n into

m

gives us the minimal
nal function f(x1 ,

fv (x 1,

...

, Xn

• • •

KL

distance between the origi­
its discretized function

) with optimally assigned values in the sub­

regions. This procedure is computationally intensive even
in one dimension and becomes computationally intractable
in multiple dimensions. Instead, we propose a simple re­
cursive technique and show that it generates results very
close to optimal.

/,the function maximum !max, and the
!min in the given subregion w;:

[

f
{ flog£ dO <.:;, !max -J fmin log mi;n
J
fmax - fmin
J
lw,
f
+ J- fmin
max
I
fma.x

subregions for a partition that

, Xn) and

KL distance

between the function f and its discretization fv based on

cretized function given the discretization, we need to find a
low, we provide a simple divide and conquer technique for

which leaf to split next and in

Since computing the exact contribution

Now that we have a procedure to assign values to a dis­

finding

useful for representing a function with

To discretize a function, we need heuristic that tells us how
well-known

1 991].

be

some structure in the upper half of the xy plane.

where

lw; I

_

. Jmax

fm-tn

og

1-

]1

·I

w, ,

denotes the volume of a discretization subre­

gion w;. The parameters

f. ! max. !min

are estimated by

randomly sampling f at several points.
During the discretization process, all leaves are kept in a
priority queue. The estimates of the relative entropy error
are used to take the leaves out of the queue. A leaf with
the largest error estimate is then split first, and the two re­
sulting leaves are put back into the queue. To control the

We borrow the idea of a recursive space decomposition

accuracy of our discretization, we also maintain the sum of

from graphics [Samet and Webber,

1988], where quadtree

all estimates for all the leaves in the queue. We stop the

and octree decomposition of two- and three-dimensional

discretization process when either the estimate of the error

317

Nonuniform dynamic discretization in hybrid networks

EJ
.

a

8 b
d.· cb c6 ·,6J

G:f--Cb cb·--o
0.7

0.>

Figure 1: An example of a two dimensional hierarchical space
decomposition. Internal nodes of the tree store the axis of a
split Leaves of the tree store the average of the continuous
density function over the subregion represented by the leaf.
,., .------�---.

Figure 2: BSP tree (solid line) and optimal (dashed line) dis­
cretization of a normal distribution
(dotted
line)_ The number of discretization intervals is 16 in both cases.

N(x; 0.5, 0.0025)

The optimal discretization was found by the gradient descent
method.
10',------.

10'

,.�

0

0.02

)(

0.'0!

ill

0,1

,•

10'

Figure 3: The relative entropy error of discretization as a func­
tion of the number of discretization subregions for equidistant
(dashed line), BSP tree (solid line), and optimal (crosses) dis­
cretization. The optimal discretization was found by the gradi­

ent descent method. The error of the BSP tree and optimal dis­
cretizations are almost identical for large number of discretiza­

tion intervals. The error of uniform discretization is about a
factor of ten larger.

becomes smaller than some fixed parameter {J or the num­
ber of leaves exceeds some fixed number N.
Finding the direction of the optimal split presents a chal­
lenging problem. In an ideal si t uation we would estimate
the decrease in the relative entropy distance due to all pos­
sible splits and choose the optimal one. However, to do this
exactly, we would need to estimate multidimensional inte­
grals. Instead, we sample several points around the cen­
ter of the subregions w; and pick the direction in which
the function changes most, i.e., the coordinate axis along
which the ratio !max/ /min is the largest around the center
of the subregion w;.
,

The result of the one-dimensional discretization of a nor­
maldistributionN(x;t-t,a-2) = �
1
., exp(-(x-t-t)2/2u2)
with J.l = 0.5 and

o.e

V�1UJ

u =

0.05 is shown in Fig.

2.

The algo-

Figure 4: Number of discretization subregions as a function of
the number of dimensions for BSP tree (solid line) and uniform

(dashed line) discretization. The discretization was performed

to approximate a multivariate normal distribution proportional

N(L,�-! x;/(n -1) -xn; 0.0025)

to
0,
with relative entropy
error 0.02, 0.05, and 0. 1 . For a large number of dimensions,
the BSP discretization performs much better (notice the loga­
rithmic scale for the number of discretization subregions).

rithm correctly chooses to discretize the regions that have
higher derivative and/or are high in probability density. In
fact, the discretization obtained with BSP tree is very close
to the optimal discretization obtained by the gradient de­
scent method. This is confirmed by Fig. 3, which shows
the relative entropy error as a function of the number of dis­
cretization subregions. The error of the BSP tree discretiza­
tion and the optimal discretization is virtually the same for
the number of discretization intervals larger than 16. On
the other hand, an eq uidistan t discretization requires about
a factor of 5 more discretization intervals to reach the same
accuracy.
BSP trees are even more promising for representing mul­
tidimensional density functions. If a function has sharp
ridges, the savings are exponential in the number of

318

Kozlov and Koller

Q
_

�G:t-���6=
cO
�-

GJ p ci:J cn
o· o o b
_
_ _

____

G6 0 ciJ 'b
8. 6J
cb -bJ
o----Cb eb----b

Figure 5: Adjusting the structure of the BSP tree in Fig. 1 to another BSP tree that has a root split on variable x.

dimensions-we save a constant factor along each of

the dimensions. The results of discretizing a multivari­
ate normal distribution proportional to N(L�-l x;/(n1) - Xn; 0, 0.0025) with different relative entropy error
are shown in Fig. 4. Given the accuracy, the number of
discretization subregions grows much slower for a BSP
discretization than for a standard uniform equidistant dis­
cretization of each of the variables separately. We save
about a factor of 10 in 5 dimensions.

3

OPERATIONS ON BSP TREES

In this section, we briefly consider summation, multiplica­
tion, and integration of functions represented by BSP trees.
We will show in the next section how the BSP trees can
be used in the standard join tree algorithm for probabilistic

trees respectively.
: input: two nodes of a tree representi ng the same subregionw,
2: output: a node of a tree representing the result of summation in
the subregionw,

3: if both nodes are leaves then
4:
return a leaf with the sum of the values stored in the nodes
5: e�ifthesecondnodeis a leaf then
6:
add the constant from the second node to all leaves of the first

7:
8:

9:

1 0:
11:
12:
13:
14:

subtree

return the first node

else
if split on different variables

then

adjust the strucrure of the first tree

end if

add left subtrees of both operands

of both operands
return a node with the result of the previous two operations
as its chi ldren
5: end if
add right subtrees

Figure 6: Algorithm for the summation of two BSP trees.

out of the scope of this paper. We refer the reader to
[Samet and Webber, 1 988] for a comprehensive review on
this subject

The algorithm for summing two nodes of a BSP tree is
shown in Fig. 6. Summation takes O(N1 + N2) opera­
tions in the best and O(N1 x N2) operations in the worst
case. Intuitively, if all the splits in both operands are on the
same variable, the computational complexity is linear. If
the trees are completely misaligned, for example if all the
splits in the first tree are on variable x and in the second
tree are on variable y, then the computational complexity

Let us start with the summation of two BSP trees. We will

is quadratic.

inference.

We assume that the BSP trees are encoded as a tree struc­
ture that uses pointers. Other implementations, that might
be more computationally efficient, are possible, but are

use summation later in our integration algorithm. If the
structure of the trees is aligned, i.e., they have exactly the
same splits on the same levels, the summation is reduced
to a tree traversal. The values at the leaves are summed
during the traversal. Tree traversal takes linear time and the
computational complexity for the aligned trees is O(N),
where N is the number of leaves in a tree.
If the trees are not aligned, we need to adjust the structure
of one of them by inserting additional nodes. For example,
if we want to sum the tree in Fig. I with another tree that
has a root split on variable x, we adjust the structure of the
first tree as shown in Fig. 5 by moving the x split in the
right branch up and by making two additional leaves in the
corresponding branches. Complete alignment of the trees
takes O(N1 x N2) operations in the worst case, where N1
and N2 are the number of leaves in the first and the second

: input: two nodes of a tree representing the s ame subregion w,
2: output: a node of a tree representing the result of multiplication
3:

in the subregionw;

then
return a leaf with the product of the values stored in the nodes
5: else if the seco nd node is a leaf then
multiply all leaves in the first subtree by a const ant from the
6:

4:

if both nodes are leaves

second node

7:
retnrn the first node
8: else
if split on different variables then
9:
10:
adjust the structure of the first tree
11:
endif
12:
multiply left subtrees of both operands
13:
multiply right subtrees of both operan ds
14:
return a node with the res u lt of the previous two operations
as its children

5: end if
Figure 7: Algorithm for multiplying two BSP trees.

The algorithm for multiplying two nodes of a BSP tree is

Nonuniform dynamic discretization in hybrid networks

shown in Fig. 7. As in the summation algorithm, we have
to align the trees when they have different structure. Anal­
ogously,multiplication takes O(N1 + N2) operations in the
best and O(N1 x N2) operations in the worst case w!len the
trees are completely misaligned.

the robot coordinate. The third observation is a discrete
noisy observation of the robot in the left halfspace x <
0.5. If the robot position is x, the sensor is likely to give a
1
reading of true with probability (1 + exp( 40(x- 0.5))r .

: input: a node of a tree and the i-th variable to be integrated ove
2: output: a node of the tree representing the result of the integration
3 : if the node is a leafthen
4:
return the node itself
5: else if split on i-th v ari able then
6:
integrate left subtree
7:
integrate right subtree
8:
return the sum of the left and the right subtrees divided by two
9: else
10:
return the node itself
1: end if
Figure 8: Algorithm for integrating a

function

represented as a

BSP tree over a variable.

Finally, we consider the integration of a function repre­
sented as a BSP tree over some variable. This is the op­
eration that, in the discrete case, corresponds to variable
elimination by summation. Since a leaf represents a con­
stant value of a function over a region, integration of a leaf
is reduced to multiplication of the value stored in the leaf by
the corresponding multidimensional volume. It is p ossib le
to compute this volume during the process of tree traver­
sal, since the volume of a subregion represented by a child
is al ways half the size of the subregion represented by its
parent. The integration algorithm is presented in Fig. 8 . In­
tegration also takes O(N1 + N2) operations in the best and
O(N1 x N2) operations in the worst case.
Many other algorithms, for such tasks as computing the ex­
pected value of a function, the cross entropy, or the differ­
ential entropy, can be expressed as a simple traversal ofthe
tree, thus taking linear time with respect to the size of the
tree.
4

...

0

1

Figure 9: A simple hybrid Bayesian network used as an example
and its join tree.

The dependence of the robot coordinate at the next obser­
vation on the robot position at the current observation is
given by the conditional probabilities p(x3lx2) = N(x3xz; 0, 0.01) and p(x2lx1) = N (x2- x1; 0, 0.01 ). Our prior
beliefs about the robot position are uniform: p(xl) = 1.
We would like to know the probability distribution of the
robot coordinate after three observations.
The join tree corresponding to this net work is shown in
Fig. 9. Probabilistic inference with continuous variables
can be analyzed in the same optimal factoring approach
used in[Li and D'Ambrosio, 1994]. The posterior proba­
bility of the robot coordinate after observations o1, o2, and
o3 is:

p(x3lo1,oz,o3)

p(x3,o1,o2,o3)/p(ol,o2,o3)
1
p(o3lx3)p(x3lx2)p(o2 lx2)p(x2lxt)
1
,....,
=

11

BASIC PROBABILISTIC INFERENCE

p(o1lx1)p(x1) dx1dx2

ALGORITHM
=

We no w show ho w we can integrate our nonuniform dis­
cretization using BSP trees into standard Bayesian net­
work inference algorithms. Our approach will be based on
the optimalfactoring approach to probabilistic inference
[Li and D'Ambrosio, 1994].
We illustrate our algorithm using an example. Let us as­
sume that we can observe a one-dimensional robot on the
interval 0 ::; x ::; 1. Although we do not kno w the robot's
position exactly, we kno w the readings ofa number ofsen­
sors and kno w that the robot can walk randomly bet ween
the observations. The Bayesian net work corresponding to
this situation is sho wn in Fig. 9.
If the robot's position x, the first and the second sensor
reading is o with probability p(olx) = N(o- x; 0, 0.01).
Thus, the first t wo observations are noisy observations of

319

p(o3lx3)

(1

1

11 p(x31xz)p(ozlx2)

)

p(x2lxJ)p(o1lxr)p(xl)dx1 dx2.
(2 )

Computing integrals in the above decomposition corre­
sponds to computing messages in the standard join tree al­
gorithm. Partial sums or integrated conditional probabili­
ties represent messages passed from one clique to another.
This net work was chosen so as to allo w us to compare the
performance of our algorithm to the true ans wers. There­
fore, the net work is exactly solvable up to a normalizing
factor given the observations o1, o2, o3 = true:
(3)

320

Kozlov and Koller

where the answer was obtained by integrating the joint
probability over x1 and x2.
The reformulated join tree algorithm is shown in Fig. 10.
For our network, we would first multiply the continuous
conditional probabilities p( o1 lx1 ) and p(xt) in clique C1,
discretize it using our BSP tree construction algorithm, and
pass a discretized message to clique C2. The clique C2 will
multiply the message by p(x21xt), discretize it, then inte­
grate over x1, and pass it to the next clique C3. The basic
operations used in the inference process reduce to the oper­
ations described in Section 3. The process continues until
the last clique, C5, receives the discretized message, multi­
plies it by the continuous function p( o3 lx3 ), and discretizes
it. Note that the function at a clique is only discretized af­
ter it received the messages from its subtrees, allowing its
choice of discretization to be much more informed.
: build a join tree through moralization and triangulation

2:
3:

their argument s
find a clique that contains the query node and make it the root of

the tree

4: for each clique starting from the leaves and up to the root do
5:
multiply all messages from descendants
6:
multiply the previous result by the assigned functions
7:
discretize the previous result with some fixed precision 6
form a message up by integrating over variables that are not in
8:
the parent clique

: end for

I 0: A

D(p(xle)llfv(xle))
=

reformulated join tree algorithm.

In many cases, this process does very well. For example,
as we see in Fig. 11, if our first two observations are the
same-o1 = o2 = 0.2-the results of the inference are
very close to the exact solution, computed analytically in
(3). However, as our observations become more and more
unlikely, the accuracy of the results of this basic inference
algorithm begins to deteriorate. If, for example, the sec­
ond observation is changed to o2 = 0.65, the results of the
inference contain only a single bump (see Fig. 11) and are
very different from the exact answer. In the next section we
describe how to improve the performance of this algorithm
in the case of unlikely evidence.

j p(e) j p(xle) Iogp(xle)jf:v(xle) dx de.

The second integral essentially represents a KL distance
between the desired answer to a query p( xI e) and the an­
swer obtained with our discretized network fv(xle). How­
ever, this relative entropy integral is multiplied by p( e).
Thus, even if we bound the relative entropy error of the
discretization for the whole network byE, the bound on the
error of our answer is t /p( e).
Rather than minimizing the KL distance of the full joint
probability, as we do by discretizing partial sums, we need
to minimize the KL distance:

j p(qle) logp(qle)//:v(qle) dq

of tli

Bayesian network graph
assign continuous functions to cliques that completely contain all

Figure

where D(p(xle)llfv
: (xle)) is the conditional relative en­
tropy or conditional Kullback-Leibler distance:

of the query node q conditioned on the evidence e. Given
the evidence, we might want to change the discretization
and rediscretize the regions that become probable given the
evidence. Thus, we propose a more general metric that
will reflect our preferences to discretize some regions more
finely than the others.
Definition 5.1: The weighted relative entropy or Weighted
Kullback-Leibler (WKL) distance W (f(x)llg(x); w(x))
between the density functions f(x) and g(x) with a strictly
positive weight w(x) is defined by:

W (f(x)llg(x); w (x))

=is w(x)f(x) log �i:� dx

(4)

where we assume that the integral exists. I
The WKL distance reduces to the KL distance if the weight
function is constant. Although in general the WKL distance
is not even sign definite, the following inequalities hold if
the weight w(x) and the function f(x) are discretized using
the same discretization V:

D(f(x)lifv(x)) min w:v(x)
X

5

::; W (f(x)llfv(x); w:v(x))

DYNAMIC DISCRETIZATION

To understand why the results of probabilistic inference de­
pend on the probability of evidence p(e), let us consider the
decomposition of the KL distance between the true joint
probability distribution given by the product of all con­
tinuous functions p(x, e) = Tii p(x;IPa(xi)) and the dis­
cretized joint probability distribution fv(x, e):

D(p(x, e)llfv(x, e))
= D(p(e )ll/:v(e)) + D(p(x le)ll/:v(xle)),

::; D(f(x)llf:v(x)) maxw:v(x).
X

The proof follows from considering the contributions from
each of the subregions Wj. These inequalities follow from
bounding each contribution by maximum/minimum weight
times the corresponding contribution to the relative en­
tropy.

5.1

WEIGHT ASSIGNMENT

Now, consider our initial discretization procedure. At each
clique C;, we discretize some function which combines the

Nonuniform dynamic discretization in hybrid networks

0.6

(a)

OJ

=

o2

=

\

0.1

�

0.2

U

U

U

(b) OJ

=

M
.

0.2, 02

M

W

=

M

M

321

1

0.65

Figure

11: Posterior probability p(x3) for a network shown in Fig. 9 for similar (a) and contradictory (b) evidence. The results of the
inference are shown by a solid line, the exact result is shown by a dotted line. Evidence o3 is true in both cases.

incoming messages with the clique's own assigned func­
tions. The problem is that this function only takes into con­
sideration the evidence which is "down the tree."

If the

other evidence results in a very different posterior distribu­
tion, our discretization procedure may be placing the em­
phasis on the wrong part of the space.

distance for the parent.
We begin with the root clique. Our goal at this clique is to
minimize the K.L distance of the query given the evidence,
the weight at that clique should be uniformly 1. Now, let
us look at how the weights for a child clique can be de­
rived given the weights for a parent. Consider two cliques

This is not a problem at the root clique, since the root clique
gets messages which already combine the information at
all the other cliques. When we combine it with the clique's

x,y

Ct = {
} andCz
C2 the child. Let

=

{y,z},whereC1

w(x, y)

is the parent and

be the weight for clique C1. We

want to find the best weight on the message

s(y)

coming

own assigned functions, the result is the best approximation

from the clique c2 that minimizes the relative entropy error

r(x,y)s(y),

we have for the clique's correct posterior density function.

of the true potentialf(x,y)

Therefore, a constant weight at the root clique is the right

stands for the product of all assigned to the clique functions

one. An intermediate clique, on the other hand, receives

and

messages from all but the parent clique, the one that con­

discretized potential

nects it with the root. Had it had this message, the prod­

compose the WKL distance between f and

uct of all the messages and the assigned clique functions
would have been the best guess about the posterior distribu­
tion. Again, no weights would have been necessary. Since
the clique lacks this additional message from its parent, the
function it uses for the discretization is an incomplete esti­
mate of the posterior. In order to get this function as close
as possible to the posterior, we must choose a weight

w

that

best approximates this "missing" message from the parent
clique.

wherer(x,y)

for the message coming from C2, relative to the

fv(x,y) rv(x, y)sv(y).
fv
W y)llfv(x,y); w(x,y))
J w(x,y)f(x, y)log ffv((xx,, y)dx dy
J w(x,y)1·(x,y)s(y)log rvr((x, y)s(
)svy)( ) dxdy
jw(x,y)r(x,y)s(y)log rvr(�x,,yy\ dxdy
J y)r(x, y)s(y) log s;r�) dx dy
(r(x, y)ll rv(x, y); w(x, y)s(y))
+W(s(y)llsv(y);j w(x,y)r(x,y)dx).

We de­

=

into a sum

of WKL distances:
(f(x,

y)

=

=

x, y

y

=

To make this intuition precise, let us track the errors in the
network more rigorously, starting from the root clique con­
taining the query node, and prove that the best weight for
a clique is essentially the message that it should have got­
ten from its parent. Formally, our goal is to assign weights
w

s(y)

=

to the various join tree cliques so that minimizing the

WK.L distance between the correct and discretized function

+

=

w(x,

W

within each clique will minimize our error for the probabil­

ity of the query node q given evidence

e.

We assign weights

by going down the tree, in the direction opposite to the ini­

In our discretization procedure (see Fig. 10), we first get the

discretized message

r(x,y)sv(y).

sv(y),
rv(x,y) fv(x,y)/sv(x,y),

and then discretize the prod­

tial message propagation (as in (2)). We assume that the

uct

a weight for the child clique to minimize the resulting WKL

ble for the first term in the sum and is controlled by the pre-

weight for a parent clique are already known, and then pick

Sin ce

=

the clique's potential discretization procedure is responsi­

Kozlov and Koller

322

cis ion parameter {j. The minimization of the second term:
W

(s(y) l l sv ( y) ; j w ( x , y )r( x , y) dx)
(s(y) l l sv (y) ; f w( x , ����x , y) dx )
=

(5 )

=

W (f(y1 z ) l l fn ( y , z) ; w (y , z ) )

f(y 1 z )
w (y1 z )f(y , z ) l og fn z dy dz
(y , )

J
J w (y, z )f(y, z) s;r;) dy dz
f(y, z )js(y)
w(y, z)f(y, z) log
( )/
dy dz,
v
! y , z sv ( y )
J
l og

+

we can derive that by reducing the WKL distance
fn
W (f(y, z ) l l (y, z) ; w (y, z)) we reduce the WKL dis­
tance:
W

(s(y) l lsv (y) ; J w (y, ��jy, z ) dz ) ,

(6)

of the true message s(y) to the discretized one sv(y) with
the weight J w(y, z ) f(y, z)/ s(y) dz.
5.2

WEIGHT PROPAGATION

Comparing equations (5) and (6), we conclude that the
weights of the neighboring cliques should satisfy the fol­
lowing condition:

J w (x , y)f(x , y) dx
s(y)

_

j w (y1 z) f(y, z ) dz
s (y)

•

•

1

•

W

Similarly, considering the clique C2 which has to integrate
its potentials to form the message s(y) J f(y, z) dz:

=

similar result can be achieved by considering the de­
composition of the KL distance of the true joint probability
/(x1 1
xn ) of a network, which is the product of all
clique potentials fc, (X; ), to the discretized joint probabil­
ity fv (x 1 ,
xn ) . which is the product of all discretized
potentials J!i,• (X; ):
.

is implicitly done when we discretize c2 's potential and is
independent of the discretization of C 1 's potentials. Thus,
by reducing the WKL distance ( 5) we reduce the WKL dis­
tance of the true potential to the discretized potential of the
clique C1 which will be used to propagate message further
to the root. This result can be easily extended to several
child cliques; in this case r( x , y) is the product of all as­
signed to the clique functions and messages from all the
other children.

=

A very

( 7)

This equation essentially says that the products of weights
and clique potentials of the two neighboring cliques have
to be calibrated. Given that the weight and the clique po­
tential of clique C1 is fixed, equation (7) tells us to choose
w (y, z ) to guarantee calibration of clique c2 to the clique
CI · But this is exactly the process for propagating the pos­
terior evidence back to the leaves in the join tree algorithm,
except that we update the weights so that the product of the
clique's weight and potential is calibrated, not the clique
potential itself.

.

.

,

(8)
where we denoted the set of variables in the clique
C; as X; .
Equation (8) is a consequence of the
decomposition properties of the KL distance ( l) and
says that the weight for the clique C; should be
fx. ex, f(x t , . . . 1 Xn) drlj fc' (X; ) . Calibration (7) is dif­
ferent because we discretize the product of the clique po­
tential and messages from the child cliques, not the clique
potential itself.
We note that we made several assumptions during our
derivation of (7). For instance, we approximated fv( x , y),
which is a discretized product f(x, y)
r(x, y)s(y), by a
product rv(x , y)sv (y) of discretized functions. But a dis­
cretized function assigns each of its discrete values a value
which is the average of the function in the corresponding
region; and it is not generally true that the average of a
product r(x, y)s(y) is the product of the averages. More
careful analysis shows that for continuous functions the er­
ror made by this approximation contributes o( l/ N), where
N is the average number of splits along a variable, com­
pared to the magnitude of the WKL distance itself. Thus,
this error becomes negligible when the BSP trees grow
larger.
=

Finally, we observe that (7) extends to a more general prob­
lem than minimizing the standard KL distance of the an­
swer to a query. Consider, for example, a user who is inter­
ested in minimizing the WKL distance with weights deter­
mined by sensitivity analysis or utility considerations. In
this case, the above propagation rule applies naturally, and
can be used to provide a good discretization with respect to
this particular WKL distance as our error metric.
5.3

ITERATIVE ALGORITHM

To apply the idea of discretizing each clique based on the
appropriate WKL distance, we need to know the weights.
Before any propagation takes place, we clearly do not have
this information. However, in order to do any propagation,
we need some initial discretization. We avoid the circular­
ity i n this definition by using an iterative algorithm. We
start out by assigning constant weights (one) to all cliques,
and doing an initial round of propagation. In that round, we
propagate partial sums up the tree and weights down the
tree. When the initial propagation is finished, the cliques

323

Nonuniform dynamic discretization in hybrid networks

have weights, so we can do another round of propagating

one leaf, reflecting a very poor initial discretization which

messages up the tree.

was done with uniform weights.
Already on the second iteration, after only one phase of

1 : repeat
2:
execute the basic inference algorithm
3:
propagate weights down the join tree
4: until the posterior probability density converges

weight propagation, the cliques had a very good estimate
of the posterior distribution and therefore the weights. The
BSP tree on the second iteration had

11

Figure 1 2 : An iterative join tree algorithm.

ability distribution by a KL distance of

We repeat this procedure i teratively2 to get better and bet­
ter estimates of the weights and more precise answers to
our query until the posterior probability of the query node
converges (see Fig. 1 2). The weights are stored in the same
B SP tree as the discretized clique potential, thus ensuring

1 3).

The prior probability discretizations before a n d after the
first weight update are shown in Fig.

that the trees in our algorithm are not pruned, the dis­
cretization can become only finer and we can not effec­
tively change algorithm focus even if the contribution of

WKL

distance

leaves,

true probability distribution by a KL distance of 0.001 (see
Fig.

tial propagation the N (x1 ;

A B SP tree stores discretization V at each clique. Given

The BSP

18

and the posterior probability distribution differed from the

the non-negativity of the

WKL distances.

0.03.

tree after the third round o f propagation had

the same discretization for the weights and potentials and

some discretization subregions to the total

leaves, and the pos­

terior probability di stribution differed from the true prob­

ing to the product

1 4 . While at the ini­

0 . 2 , 0.01) Gaussian correspond­
p(o1 l x 1 )p(x i ) i s di scretized with the

weight one, the weight i s nonuniform for the second round
of propagation as shown in Fig. 1 4(b ) . The new di scretiza­
tion takes into account much larger weights on the right
slope and rediscretizes it more finely.

becomes very small. To avoid the uncontrolled growth of
the B SP trees, we prune them on each iteration by remov­
ing the leaves that contribute to the total relative entropy
error less that an average leave in the tree.

The error i n

the leaves is estimated by Monte Carlo integration during
clique discretization.

6.2

CONVERGENCE

The algorithm converges by the second iteration in most
cases. Figure 15 shows the relative entropy error as a func­
tion of the i teration number for several precision param­

6. The relative entropy error dropped very abruptly

After pruning, the clique potentials are rediscretized again

eters

on the next propagation i teration.

after the first iteration and experienced small oscillations

A removed leave can

reappear again as a result of the clique rediscretization.
However, it is less likely to appear i f the assigned to the cor­
responding subregion weight is small. On the other hand,
the subregions with the l arge weights are more likely to be
the first in the discretization queue and to be rediscretized
more finely. Let us look how this scheme works in practice.

around the final answer after that.
Pruning allows to compare the efficiency of our approach to
the uniform discretization. S i nce we can effectively change
discretization focus with evidence, we get a smaller error
with our approach than with a uniform approach for the
equivalent number of discretization subregions per clique.
Figure 16 shows the relative entropy error of a dynamically

EXPERIMENTS

6

discretized network compared to a uniformly discretized
network given the same number of partitions per clique

All our results are based o n the simple problem described i n
Fig. 9 , which has exact solution (3). Errors were evaluated
by n umerical i ntegration.

(so that if we have had N partitions per variable in a uni­

form discretization, we would have N 2 partitions per two­
dimensional clique).
For the nonuniform discreti zation, we get a factor of 4 bet­

6.1

REDISCRETIZATION

First, we tested how the di screti zation adapts to the weights
and posterior distributions. We used an unlikely evidence
set 0 1 = 0.2, 0 2 = 0.8, and o3 = true (the probabil­
3
ity o f evidence i s only 10- ) and the preci sion parameter

{)

=

0.02. On the first round of propagation, we could not

resolve any of the structure; the BSP tree contained only

2 A similar idea of executing inference on a simplified network
and then refining the approximation based on the results was also
used by [Wellman and Liu, 1 994] for the related problem of state­
space abstraction.

ter precision as compared to the case of uniform di scretiza­
tion with the same number of discretization subregions.
In practice, the savings can be much bigger since our al­
gorithm can focus the discretization effort on the cliques
that have potentials most i mportant for a particular evi­
dence, not potentials of all cliques at the same time. How­
ever, comparison with the uniform discretization i s not so
straightforward i n this case.

Although in our simple ex­

periments we did not get any computation time advantage
over a uniform discretization-table representation of con­
ditional probabilities i s very efficient computationally-we
had evidence in Fig. 4 that the nonuniform dynamic dis-

324

Kozlov and Koller

(a) Second iteration

(b) Third iteration

Figure 1 3 : Posterior probability p( x 3 ) for a network shown in Fig. 9 with dynamic discretization for two successive iterations. The
result of the inference is shown by a solid line, the exact result is shown by a dotted line. Evidence is o 1
0 . 2 , o2
0 . 8 , and o3
true.
=

0.5
.

(a) Prior probability p( x1 )

-o.G

0.7

=

0.8

=

09

(b) Rediscretized prior p( x1 )

Figure 14: Original and rediscretized prior probabilities p ( x 1 ) . The dashed line shows the estimate of posterior that the clique has after
the first weight propagation. Notice the change in the granularity of the discretization. Evidence is 0 1
0 . 2 , 02
0 . 8, and 03 = true.
=

cretization should be more efficient for more complex do­

=

probabilistic inference which show that nonuniform dis­

mains requiring precision and thus a very fine discretization

cretization can be substantially more compact than the tra­

of multidimensional domains.

ditional uniform discretization.

7

CONCLUSIONS

Any fixed discretization, however, cannot account well for
all possible configurations of evidence. Therefore, we are
likely to get l arge errors for unlikely evidence. We develop

In this paper, we provide an effective algorithm for prob­

a new metric based on the relative entropy that allows to

abilistic inference in hybrid networks with an arbitrary

emphasize discretization of some regions as opposed to

topology and arbitrary functional dependence between

others and show how to use it in a self-adj usting anytime

variables based on nonuniform dynamic discretization .

rediscretization algorithm. This algorithm constantly up­

While our approach is based on discretizing the function,

dates the discretization in accordance with the evidence. It

it is derived from the key insight that, within the domain

can be run to provide answers of any desired accuracy. Our

of the function , not all of the regions should be accorded

preliminary empirical results suggest that convergence to

equal importance. We suggest the i dea of nonuniform dis­

the right solution is very rapid in practice.

cretization, which discretizes multidimensional domains as
a whole, rather than discretizing each vari able separately.
Nonuniform discretization has been successfully used in
multivariate regression and machine learning. We provide
results for the discretization of probability distributions for

Given the recent emphasi s on building and using hybrid
systems, we believe that probabilistic inference algorithms

for the corresponding models will become more and more
necessary and precision more and more important. As such

Nonuniform dy namic discretization in hybrid networks

325

,,. .----�-�--�----,
0

0 05

...

0.01

Ill

0.001

't ,
'

,,

'

"
"

- - 0.2, 0.8

0.6, O.t

•

02. 0.8
0.2, O.S

a

---:�:::::,
'<:::::::�

- 0.6.0.1�'

o

- - unif011'1'1
- SSP tcoo

' .., ... �� � ...
... ..._ � ..
''

· : :o..
' .. .... "'t, � '

.

"

,,

.

,o

--!.

,,-•,L
-----,.---: - --:--�---:------:--

,
i'larabon

Figure 1 5 : The relative entropy error as a function of the it­
eration number and the precision parameter 6. Evidence is
0 1 = 0.6, o2
0 . 9 (solid line) and 0 1
0.2, o2
0.8
(dashed line). 0 3 is always true.
=

=

=

Figure 1 6: The relative entropy error as a function of the num­
ber of discretization subregions and evidence. Evidence is
01
0.6, o 2
0 . 9 (circles), o 1
0.2, 0 2
0.8 (pluses),
and 0 1
0 . 2 , 02
0 . 5 (stars). 03 is always true.
=

=

=

=

=

=

systems are typically fairly complex, and involve tightly

Lauritzen, S. L. ( 1 992).

coupled discrete and continuous elements, exact algorithms

means, and variances in mixed graphical association mod­

are unlikely to be available. Our algorithm deals effectively

els. JASA , 87( 420): 1 089 - 1 1 08.

with arbitrary hybrid systems, so that we can hope that it
will be applicable to many of these applications. In partic­
ular, we believe that the ability of our algorithm to adjust

itself to the evidence i t sees will prove very useful in appli­
cations, e.g., real-time monitoring of hybrid systems, that

Propagation of probabilities,

Lauritzen, S. L. and Spiegelhalter, D. J. ( 1 988).

Local

computations with probabilities on graphical structures and
their application to expert systems. Jou rnal of the Royal
Statistical Society, B 50:253 - 25 8 .

require fast and effic ient change of focus.

Lauritzen, S . L . and Wermuth, N. ( 1 989). Graphical mod­

Acknowledgments

qualitative and some quantitative. The Annals of Statistics,

els for association between variables, some of which are
17( 1 ): 3 1 -57.

This research was supported through the generosity of the
Powell foundation, by ONR grant N000 1 4-96- l -07 1 8. and
by ARO under the MURI program "Integrated Approach to

Li ,

Z. and D ' Ambrosio, B . ( 1 994). Efficient inference in

Bayes networks as a combinatorial optimization problem.

Intelligent Systems", grant number DAAH04-96- 1 -034 l _

International Journal of Approximate Reasoning, l l ( 1 ):55

References

Moore, A W. ( 1 99 1 ). Variable resolution dynamic pro­

Alag, S. and Ago g i no , A. M . ( 1 996). Inference using mes­

ate real-valued state-spaces.

sian continuous networks. Proceedings of the Twelfth VA/

- 337.

sage

propagation and topology transformation vector gaus­

Conference, pages 20 - 27. Morgan Kaufmann .
Breiman,

L.,

Friedman,

Stone, C. J. ( 1 984).

J . H.,

Olshen,

and

Classification and regression trees.

Cover, T. and Thomas, J. ( 1 99 1 ) . Elements of lnfonnation
Theory. John Wiley & Sons, Chichester, UK.
Dechter, R. ( 1 996). Bucket elimination: A unifying frame­
In Proceedings of the

Twelfth VAl Conference, pages 2 1 1 - 2 1 9. Morgan Kauf­
man n .
Driver, E. and Morrel, D. ( 1 995). Implementation

of con­

tinuous bayesian networks using sums of weighted gaus­
sians.

I n Proceedings of the Eleventh VA/ Conference,

pages 1 34 - 1 40. Morgan Kaufmann.

gramming: Efficiently learning action maps in multivari­
In Machine Learning: Pro­

ceedings of the Eighth International Workshop, pages 333

Olesen,

R. A.,

Wadsworth International Group, Belmont, CA.

work for probabil istic inference.

- 81 .

K.

G. ( 1 993).

Causal probabilistic networks

with both discrete and continuous varibales. IEEE PAM/,

1 5(3):275 - 279.
Pearl , J. ( 1 988). Probabilistic Reasoning in Intelligent Sys­
tems: Networks of Plausible Inference. Morgan Kaufmann.
Samet,

H. and Webber,

R. ( 1 988). Hierarchical data struc­

tures and algorithms for computer graphics. 1. Fundamen­
tals. IEEE Computer Graphics and Applications, 8(3):48 -

68 .
Wellman, M. P. and Liu. C.-L. ( 1 994). State-space abstrac­
tion for anytime evaluation

of probabi l i stic networks. In

Proceedings of the Tenth VA/ Conference, pages 567 - 574.

Morgan Kaufmann.

