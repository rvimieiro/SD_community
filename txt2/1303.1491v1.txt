ing the expected discounted cumulative reward with
respect to the underlying stochastic process and the
specially formulated reward function. In our formula­
tion, a policy is nothing more than a conditional plan
for achieving goals quickly on average.

Introduction

We are interested in solving sequential decision making
problems given a model of the underlying dynamical
system specified as a stochastic automaton (i.e., a set
of states, actions, and a transition matrix which we
assume is sparse ) . In the following, we refer to the
specified automaton as the system automaton. Our
approach builds on the theoretical work in operations
research and the decision sciences for posing and solv­
ing sequential decision making problems, but it draws
its power from the goal-directed perspective of artifi­
cial intelligence. Achieving a goal corresponds to per­
forming a sequence of actions in order to reach a state
satisfying a given proposition. In general, the shorter
the sequence of actions the better. Because the state
transitions are governed by a stochastic proeess, we
cannot guarantee the length of a sequenee achieving a
given goal. Instead, we are interested in minimizing

Instead of generating an optimal policy for the sys­
tem automaton, which would be impractical for an
automaton with a large state space, we formulate a
simpler or restrictert stochastic automaton and then
search for an optimal policy in this restricted automa­
ton. At all times, the system maintains a restricted au­
tomaton. The restricted automaton and correspond­
ing policy are improved as time permits by successive
refinement. This approach was inspired by the work
of Drummond and Bresina [Drummond and Bresina,
1990] on anytime synthetic projeC-tion.
The state space for the restricted automaton corre­
sponds to a subset ·of the states of the system au­
tomaton (this subset is called the envelope of the re­
stricted automaton) and a special state OUT that rep­
resents being in some state outside of the envelope.
For states in the envelope, the transition funetion of
the restricted automaton is the same as in the system
automaton. The pseudo state OUT is a sink (i.e., all
actions result in transitions back to OUT) and, for a
given action and state in the envelope, the probability
of making a transition to OUT is one minus the sum
of the probabilities of making a transition to the same
or some other state in the envelope.
There are two basic types of operations on the re­
stricted automaton. The first is called envelope al­
teration and serves to increase or decrease the num­
ber of states in the restricted automaton. The second
is called policy generation and determines a policy for

310

Dean et al.

b

i.

ii.

Figure 1: Stochastic process and a restricted version

the system automaton using the restricted automaton.
Note that, while the policy is constructed using the re­
stricted automaton, it is a complete policy and applies
to all of the states in the system automaton. For states
outside of the envelope, the policy is defined by a set of
reflexes that implement some default behavior for the
agent. In this paper, deliberation scheduling refers to
the problem of allocating processor ·time to envelope
alteration and policy generation.
There are several different methods for envelope al­
teration. In the first method, we simply search for
a (new) path or trajectory from the initial state to a
state satisfying the goal and add the states traversed in
this path to the state space for the restricted automa­
ton. This method need not make use of the current
restricted automaton. A second class of methods op­
erates by finding the first state outside the envelope
that the agent is most likely to transition to using its
current policy, given that it leaves the set of states
corresponding the current envelope. There are several
variations on this: add the state, add the state and the
n next most likely states, add all of the states in a path
from the state to a state satisfying the goal, add all of
the states in a path from the state to a state back in
the current envelope. Finally, there are methods that
prune states from the current envelope on the grounds
that the agent is unlikely to end up in those states
and therefore need not consider them in formulating a
policy.
Figure 1.i shows an example system automaton con­
sisting of five states. Suppose that the initial state is
1, and state 4 satisfies the goal. The path 1 � 2 � 4
goes from the initial state to a state satisfying the
goal and the corresponding envelope is {1, 2, 4}. Fig­
ure 1.ii shows the restricted automaton for that en­
velope. Let 1r( x) be the action specified by the pol­
icy 1r to be taken in state x; the optimal policy for
the restricted automaton shown in Figure l.ii is de­
fined by 1r ( 1) = 1r (2) = 1r (4) = a on the states of
the envelope and the reflexes by 1r(OUT)
b (i.e.,
'ifX (/_ {1 1 2 1 4} 1 1r( X) = b)·
=

All of our current policy generation techniques are
based on iterative algorithms such as value iteration
[Bellman, 1957] and policy iteration [Howard, 1960].
In this paper, we use the latter. These techniques can
be interrupted at any point to return a policy whose

value improves in expectation on each iteration. Each
iteration of policy iteration takes 0( IE13) where E is
the envelope or set of states for the restricted automa­
ton. The total number of iterations until no further
improvement is possible varies but is guaranteed to be
polynomial in lEI. This paper is primarily concerned
with how to allocate computational resources to enve­
lope alteration and policy generation. In the following,
we consider several different models.
In the simpler models called precursor-deliberation
models, we assume that the agent has one opportu­
nity to generate a policy and that, having generated
a policy, the agent must use that policy thereafter.
Precursor-deliberation models include
1. a deadline is given in advance, specifying when
to stop deliberating and start acting according to
the generated policy
2. the agent is given an unlimited amount of time to
respond, with a _linear cost of delay
There are also more complicated precursor­
deliberation models, which we do not address in this
paper, such as the following two models, in which a
trigger event occurs, indicating that the agent must
begin following its policy immediately with no further
refinement.
3. the trigger event can occur at any time in a fixed
interval with a uniform distribution
4. the trigger event is governed by a more compli­
cated distribution, e.g., a normal distribution cen­
tered on an expected time
In more complicated models, called recurrent­
deliberation models, we assume that the agent period­
ically replans. Recurrent-deliberation models include
1. the agent performs further envelope alteration
and policy generation if and only if it 'falls out'
of the envelope _defined by the current restricted
automaton
2. the agent performs further envelope alteration
and policy generation periodically, tailoring the
restricted automaton and its corresponding pol­
icy to states expected to occur in the near future
The rest of this paper assumes some familiarity
with basic methods for sequential decision making in
stochastic domains. A companion paper [Dean et
al., 1993] provides additional details regarding algo­
rithms for precursor-deliberation models. In this pa­
per, we dispense with the mathematical preliminaries,
and concentrate on conveying basic ideas and empir­
ical results. A complete description of our approach
including relevant background material is available in
a forthcoming technical report.
2

Deliberation Scheduling

In the previous section, we sketched an algorithm that
generates policies. Each policy 1r has some value with

Deliberation Scheduling for Time-Critical Sequential Decision Making

311

respect to an initial state x0; this value is denoted
V,.(x0) and corresponds to the expected cumulative
reward that results from executing the policy starting
in x0• Given a stochastic process and reward function,
V,.(x0) is well defined for any policy 1r and state x0.
We are assuming that, in time critical applications,
it is impractical to compute V,. (x0) for a given policy
and initial state and, more importantly, that it is im­
practical to compute the optimal policy for the entire
system automaton.
In order to control complexity, in generating a pol­
icy, our algorithm considers only a subset of the state
space of the stochastic process. The algorithm starts
with an initial policy and a restricted state space (or
envelope), extends that envelope, and then computes
a new policy. We would like it to be the case that the
new policy 1r1 is an improvement over (or at the very
least no worse than ) the old policy 1r in the sense that
V1r' (xo)- V ,.(xo) 2: 0 .
In general, however, we cannot guarantee that the pol­
icy will improve without extending the state space to
be the entire space of the system automaton, which
results in computational problems. The best that we
can hope for is that the algorithm improves in expecta­
tion. Suppose that the initial envelope is just the ini­
tial state and the initial policy is determined entirely
by the reflexes. The difference Vrr'(xo)- V1r(xo) is a
random variable, where 1r is the reflex policy and 1r' is
the computed policy. We would like it to be the case
that E[V1r'(x0)- V1r(x0)] > 0, where the expectation
is taken over start states and goals drawn from some
fixed distribution. Although it is possible to construct
system automata for which even this improvement in
expectation is impossible, we believe most moderately
benign navigational environments, for instance, are
well-behaved in this respect.
Our algorithm computes its own estimate of the value
of policies by using a smaller and computationally
more tractable stochastic process. Ideally, we wo'uld
like to show that there is a strong correllation be­
tween the estimate that our algorithm uses and the
value of the policy as defined above with respect to
the complete stochastic process, but for the time be­
ing we show empirically that our algorithm provides
policies whose values increase over time.
Our basic algorithm consists of two stages: envelope
alteration (EA) followed by policy generation (PG).
The algorithm takes as input an envelope and a policy
and generates as output a new envelope and policy.
We also assume that the algorithm has access to the
state transition matrix for the stochastic process. In
general, we assume that the algorithm is applied in
the manner of iterative refinement, with more than
one invocation of the algorithm. We will also treat en­
velope alteration and policy generation as separate, so
we east the overall process of poliey formation in terms
of some number of rounds of envelope alteration fol­
lowed by poliey generation, resulting in a sequenee of

Figure 2: Sequenee of restrieted automata and associ­
ated paths through state space
polieies. Figure 2 depicts a sequenee of automata gen­
erated by iterative refinement along with the associ­
ated paths through state spaee traversed in extending
the envelope.
Envelope alteration can be further classified in terms
of three basic operations on the envelope: trajectory
planning, envelope extension, and envelope pruning.
Trajectory planning eonsists of searching for some path
from an initial state to a state satisfying the goal. En­
velope extension consists of adding states to the enve­
lope. Envelope pruning involves removing states from
the envelope and is generally used only in recurrent­
deliberation models.
Let

1r;

represent the policy after the ith round and let

tEA; be the time spent in the ith round of envelope
alteration. We say that poliey generation is inflexi­
ble if the ith round of poliey generation is always run
to completion on IEil . Policy generation is itself an

iterative algorithm that improves an initial policy by
estimating the value of policies with respect to the re­
stricted stochastic. process mentioned earlier. W hen
run to eompletion, policy generation continues to iter­
ate until it finds a policy that it cannot improve with
respect to its estimate of value. The time spent on the
ith round of policy generation tpa, depends on the
size of the state space IEil .
In the following, we present a number of deeision mod­
els. Note that for each instance of the problems that
we eonsider, there is a large number of possible deci­
sion models. Our seleetion of which decision models to
investigate is guided by our interest in providing some
insight into the problems of time-critical deeision mak­
ing and our antieipation of the combinatorial problems
involved in deliberation scheduling.
3

Precursor Deliberation

In this section we consider the first precursor­
deliberation model, in which there is a fixed deadline
known in advance. It is straightforward to extend this
to model 2, where the agent is given an unlimited re­
sponse time with a Linear eost of delay; models :3 and
4 are more eomplicated and and are not eonsidered in
this paper.

312

3.1

Dean et al.

The Model

Let troT be the total amount of time from the current
time until the deadline. If there are k rounds of enve­
lope alteration and policy generation, then we have
tEA1 + tpa, +···+ tEAk+ tpak =trOT·
Case 1: Single round; inflexible policy genera­
tion In the simplest case, policy generation does not

inform envelope alteration and so we might as well do
all of the envelope alteration before policy generation,
and tEA, + tpa, = troT· Here is what we need in
order to schedule time for EA1 and PG1:
1. the expected value, taken over randomly-chosen
pairs of initial states and goals, of the improve­
ment of the value of the initial state, given a fixed
amount of time allocated to envelope alteration,
E[V1r, (xo)- V1ro(xo)itEA.Ji
2. the expected size of the envelope given the time
allocated to the first round of envelope alteration,
E[IE1IItEA,]; and
3. the expected time required for policy generation,
given the size of the envelope after the first round
of envelope alteration, E [tpa,IIE1I].
Note that, because policy generation is itself an
iterative refinement algorithm, we can interrupt
it at any point and obtain a policy, for instance,
when policy generation takes longer than pre­
dicted by the above expectation.
Each of (1), (2) and (3) can be determined empm­
cally, and, at least in principle, the optimal allocation
to envelope alteration and policy generation can be
determined.
Case II: Multiple rounds; inflexible policy gen­
eration Assume that policy generation can prof­

itably inform envelope alteration, i.e., the policy after
round i provides guidance in extending the environ­
ment during round i +1. In this case, we also have k
rounds and tEA, +tpa, +···+tEAk+ tpak =troT·

Informally, let the fringe states for a given envelope
and policy correspond to those states outside the enve­
lope- that can be reached with some probability greater
than zero in a single step by following the policy start­
ing from some state within the envelope. Let the most
likely falling-out state with respect to a given envelope
and policy correspond to that fringe state that is most
likely to be reached by following the policy starting
in the initial state. We might consider a very simple
method of envelope alteration in which we just add the
most likely falling-out state and then the next most
likely and so on. Suppose that adding each additional
state takes a fixed amount of time. Let

E[V1r; (xo)- V1r;_, (xo)IIE;-11 = m, IE;I = m + n]
denote the expected improvement after the ith round
of envelope alteration and policy generation given that

there are n states added to the m states already in the
envelope after round i - 1.
Again, the expectations described above can be ob­
tained empirically. Coupled with the sort of expecta­
tions described for Case I (e.g., E [tPa;IIE;I] ) , one
could (in principle) determine the optimal number
of rounds k and the allocation to tEA; and tpa; for
1 � j � k . In practice, we use slightly different statis­
tics and heuristic methods for deliberation scheduling
to avoid the combinq.torics.
Case III: Single round: flexible policy genera­
tion Actually, this case is simpler in concept than

Case I, assuming that we can compile the following
statistics.
Case IV: Multiple round: flexible policy gener­
ation Again, with ;tdditional statistics, e.g.,

E[V1r;(xo)-V1r;_, (xo)IIE;-11 = m, IE;I = m+n, tpa;_.],
this case is not much more difficult than Case II.
3.2

Algorithms and Experimental Results

Our initial.experiments are based on stochastic au­
tomata with up to several thousand states; automata
were chosen to be small enough that we can still
compute the optimal policy using exact techniques
for comparison, but large enough to exercise our ap­
proach. The domain, mobile-robot path planning, was
chosen so that it would be easy to understand the poli­
cies generated by our algorithms. For the experiments
reported here, there were 166 locations that the robot
might find itself in and four possible orientations re­
sulting in 664 states. These locations are arranged on
a grid representing the layout of the fourth floor of the
Brown University Computer Science department. The
robot is given a tasK to navigate from some starting
location to some target location. The robot has five ac­
tions: stay, go forward, turn right, turn left, and turn
about. The stay action succeeds with probability one,
the other actions succeed with probability 0.8, except
in the case of sinks corresponding to locations that
are difficult or impossible to get out of. In the mobile­
robot domain, a sink might correspond to a stairwell
that the robot could fall into. The reward function
for the sequential des_:ision problem associated with a
given initial and target location assigns 0 to the four
states corresponding to the target location and -1 to
all other states.
We gathered a variety of statistics on how extend­
ing the envelope increases value. The statistics that
proved most useful corresponded to the expected im­
provement in value for different numbers of states
added t"o the envelope. Instead of conditioning just on
the size of the envelope prior to alteration we found it
necessary to condition on both the size of the envelope
and the estimated value of the current policy (i.e., the

Deliberation Scheduling for Time-Critical Sequential Decision Making

value of the optimal policy computed by policy itera­
tion on the restricted automaton). At run time, we use
the size of the automaton and the estimated value of
the current policy to index into a table of performance
profiles giving expected improvement as a function of
number of states added to the envelope. Figure 3 de­
picts some representative functions for different ranges
of the value of the current policy.

313

Value
10.00
s.oo
6.00

-+--r
--·--=s=·-:t""'"'--+-...,---+---r - hCXi6ie-----·
=--··
-+--+---n------t----1---t---1
---t
..

.J
'

-+-1--i+----+----l--t-- -----t--

r'
-+----t----'j-+-,1---++---+1
2.00 -+-1---i-+---+--+----t----'j­
..�
/
0,00 -fl--L--1----+--'--t------t--

4.00

0.00

2.00

4.00

6.00

8.00

Time (secoo.ds}

Figure 4: Comparison of the greedy algorithm with
standard (inflexible) policy iteration and interruptable
(flexible) policy iteration

10.00

20.00

30.00

40.00

Figure 3: Expected improvement as a function of the
number of states n added to initial envelope of size m
In general, computing the optimal deliberation sched­
ule for the multiple-round precursor-deliberation mod­
els described above is computationally complex. We
have experimented with a number of simple, greedy
and myopic scheduling strategies; we report on one
such strategy here.
Using the mobile-robot domain, we generated 380,000
data points to compute statistics of the sort shown in
Figure 3 plus estimates of the time required for one
round of envelope alteration followed by policy gen­
eration given the size of the envelope, the number of
states added, and value of the current policy. We use
the following simple greedy strategy for choosing the
number of states to add to the envelope on each round.
For each round of envelope alteration followed by pol­
icy generation, we use the statistics to determine the
number of states which, added to the envelope, max­
imizes the ratio of performance improvement to the
time required for computation. Figure 4 compares the
greedy algorithm with the standard (inflexible) pol­
icy iteration on the complete automaton and with an
interruptable (flexible) version of policy iteration on
the complete automaton. The data for Figure 4 was
determined from one representative run of the three
algorithms on a particular initial state and goal. In
another paper [ Dean et al., 1993] we present results
for the average improvement of the start state under
the policy available at time t as a function of time.
4
4.1

Recurrent Deliberation
The Model

In recurrent-deliberation models, the agent has to re­
peatedly decide how to allocate time to deliberation,
taking into account new information obtained during
execution. In this section, we consider a particular

model for recurrent deliberation in which the agent al­
locates time to deliberation only at prescribed times.
We assume that the agent has separate deliberation
and execution modules that run in parallel and com­
municate by message passing; the deliberation module
sends policies to the execution module and the execu­
tion module sends observed states to the deliberation
module. We also assume that the agent correctly iden­
tifies its current state; in the extended version of this
paper, we consider the case in which there is uncer­
tainty in observation.
We call the model considered in this section the dis­
crete, weakly-coupled, recurrent deliberation model. It
is discrete because each tick of the clock corresponds to
exactly one state transition; recurrent because the exe­
cution module gets a new policy from the deliberation
module periodically; weakly coupled in that the two
modules communicate by having the execution mod­
ule send the deliberation module the current state and
the deliberation module send the execution module the
latest policy. In this section, we consider the case in
which communication between the two modules occurs
exactly once every n ticks; at times n, 2n, 3n, . . ., the
deliberation module sends off the policy generated in
the last n ticks, recei�es the current state from the ex­
ecution module, and begins deliberating on the next
policy. In the next section, we present an algorithm for
the case where the interval between communications is
allowed to vary.
In the recurrent models, it is often necessary to remove
states from the envelope in order to lower the compu­
tational costs of generating policies from the restricted
automata. For instance, in the mobile-robot domain,
it may be appropriafe to remove states corresponding
to portions of a path the robot has already traversed
if there is little chance of returning to those states. In
general, there are many more possible strategies for
deploying envelope alteration and policy generation in
recurrent models than in the case of precursor mod­
els. Figure 5 shows a typical sequence of changes to
the envelope corresponding to the state space for the
restricted automaton. The current state is indicated

314

Dean et al.

Find path to the goal

�
Extend the envelope

Extend and then prune the envelope

�·
0

Find path back to the

interval, the execution module is given a new policy 11"1,
and the deliberation module is given the current state
x'. It is possible that x' is not included in the enve­
lope for 11"1; if the reflexes do not drive the robot inside
the envelope then the agent's behavior throughout the
next n-tick interval will be determined entirely by the
reflexes. Figure 6 shows a possible run depicting inter­
vals in which the system is executing reflexively and
intervals in which it is using the c.urrent policy; for this
example, we assume_reflexes that enable an agent to
remain in the same state indefinitely.

Let 8n (x,1r, x') be the probability of ending up in x'
starting from x and following 1r for n steps. Suppose
that we are given a set of strategies {F1,F2, }. As
Extend and then prune the envelope
is usual in such combinatorial problems with indefi­
nite horizons, we adopt a myopic decision model. In
particular, we assume that, at the beginning of each
n-tick interval, we are planning to follow the current
policy 1r for n steps, .follow the policy F(1r) generated
Figure 5: Typical sequence of changes to the envelope
by some strategy F attempting to improve on 1r for the
next n steps, and thereafter follow the optimal policy
intervals during which the system is executing reflexively
7r*. If we assume that it is impossible to get to a goal
0
3n
4n
n
2n
state in the next 2n steps, the expected value of using
strategy F is given by
falls out�ofthe envelo
2n-l
n
curre nt state happens tt be contaied in the new envelo
Z:-l+i2 L 8n (x,7r,x1) L8n (x',F (7r),x")V.,.. (x") ,
.

�

I

falls out of the envelope again

+

r

ctuTent state is not in the new envelope

'

current state is in the new envelope

Figure 6: Recurrent-deliberation
by + and the goal state is indicated by D.
To cope with the attendant combinatorics, we raise
the level of abstraction and assume that we are given
a small set of strategies that have been determined
empirically to improve policies significantly in vari­
ous circumstances. Each strategy corresponds to some
fixed schedule for allocating processor time to envelope
alteration and policy generation routines. Strategies
would be tuned to a particular n-tick deliberation cy­
cle. One strategy might be to use a particular pruning
algorithm to remove a specified number of states and
then use whatever remains of the n ticks to generate
a new policy. In this regime, deliberation scheduling
consists of choosing which strategy to use at the begin­
ning of each n-tick interval. In this section, we ignore
the time spent in deliberation scheduling; in the next
section, we will arrange it so that the time spent in
deliberation scheduling is negligible.
Before we get into the details of our decision model,
consider some complications that arise in recurrent­
deliberation problems. At any given moment, the
agent is exec.uting a polic.y, call it 1r, defined on the cur­
rent envelope and augmented with a set of reflexes for
states falling outside the envelope. The agent begins
exec.uting 1r in state x. At the end of the c.urrent n-tick

x'EX

i=O

.

•

[

x"EX

l

where 0 <= 1 < 1 i& a discounting factor, controlling
the degree of influence of future results on the current
decision.
Extending the above model to account for the possi­
bility of getting to the goal state in the next 2n steps
is straightforward; computing a good estimate of v.,..
is not, however. We might use the value of some pol­
icy other than 7r*, but then we risk choosing strategies
that are optimized to support a particular suboptimal
policy when in fact. the agent should be able to do
much better. In general, it is difficult to estimate the
value of prospects beyond any given limited horizon
for sequential decision problems of indefinite duration.
In the next section, we consider one possible practical
expedient that appears to have heuristic merit.
4.2

Algorithms and Experimental Results

In this section, we present a method for solving
recurrent-deliberation problems of indefinite duration
using statistical estimates of the value of a variety of
deliberation strategies. We deviate from the decision
model described in the previous sec.tion in one addi­
tional important way; we allow variable-length inter­
vals for deliberation. Although fixed-length facilitate
exposition, it is much easier to collect useful statistical
estimates of the utility of deliberation strategies if the
deliberation interval is allowed to vary.
For the remainder of-this section, a deliberation strat­
egy is just a particular sequence of invocations of enve­
lope alteration and policy generation routines. Delib-

Deliberation Scheduling for Time-Critical Sequential Decision Making

eration strategies are parameterized according to at­
tributes of the policy such as the estimated value of
policies and the size of the envelopes. The function
EIV (F,V11, IE11l) provides an estimate of the expected
improvement from using the strategy F assuming that
the estimated value of the current policy and the size
of the corresponding envelope fall within the speci­
fied ranges. This function is implemented as a table in
which each entry is indexed by a strategy F and a set of
ranges, e.g., { [minV11,maxV,.], [miniE,.I,maxiE111]}.

315

online deliberation scheduling.

We determine the EIV function off line by gathering
statistics for F running on a wide variety of policies.
The ranges are established so that, for values within
the specified ranges the expected improvements have
low variance. At run time, the deliberation scheduler
computes an estimate of the current policy V,., deter­
mines the size IE,.. I of the corresponding envelope and
chooses the strategy F maximizing EIV (F, V,., IE111).

To avoid complicating the online decision making, we
have adopted the following expedient which allows us
to keep our one-step-lookahead model. We modify the
transition probabilities for the restricted automaton so
that there is always a non-zero probability of getting
back into the envelope having fallen out of it. Exactly
what this probability should be is somewhat eompli­
cated. The particular value chosen will determine just
how concerned the agent will be with the prospect of
falling out of the envelope. In fact, the value is depen­
dent on the actual strategies chosen by deliberation
scheduling which, in our particular case, depends on
EIV and this value of falling back in. We might pos­
sibly resolve the circularity by solving a large and very
complicated set of simultaneous equations; instead, we
have found that in practice it is not difficult to find a
value that works reasonably well.

To build a table of estimates of function EIV off line,
we begin by gathering data on the performance of
strategies ranging over possible initial states, goals,
and policies. For a particular strategy F , initial state
x, and policy 1r, we run F on 1r, determine the elapsed
number of steps k, and compute estimated improve­
ment in value,

The experimental results for the recurrent model were
obtained on the mobile-robot domain with 1422 possi­
ble locations and hence 5688 states. The actions avail­
able to the agent were the same as those used to obtain
the precursor-model-results. The transition probabil­
ities were also the same, except that the domain no
longer contained sinks.

[-�

'Yi

+ 'Yk

x� 8k(x,

71",

]

x')VF(1r)(x') - V,.(x),

where the first term corresponds to the value of using
1r for the first k steps and F (1r) there after and the
second term corresponds to the case in which we do
no deliberation whatsoever and use 1r forever. As in
the model described in the previous section, we assume
that the goal cannot be reached in the next k steps;
again it is simple to extend the analysis to the case in
which the goal may be reached in less than k steps.
Given data of the sort described above, we build the
table for EIV (F,V,., IE,..I ) by appropriately dividing
the data into subsets with low variance.
One unresolved problem with this approach is exactly
how we are to compute V11 ( x). Recall that 1r is only
a partial policy defined on a subset of X augmented
with a set of reflexes to handle states outside the cur­
rent envelope. In estimating the value of a policy, we
are really interested in estimating the value of the aug­
mented partial policy. If the reflexes kept the agent in
the same place indefinitely, then as long as there was
some nonzero probability of falling out of the envelope
with a given policy starting in a given state the actual
value of the policy in that state would be 1/ ( 1 1).
Of course, this is an extremely pessimistic estimate for
the long term value of a particular policy since in the
recurrent model the agent will periodically compute a
new policy based on where it is in the state space. The
problem is that we cannot directly account for these
subsequent policies without extending the horizon of
the myopic decision model and absorbing the associ­
ated computational costs in offline data gathering and
-

-

We used a set of 24 hand-crafted strategies, which were
combinations of envelope optimization (a) and the
following types of envelope alteration;
1. findpath (FP): if the agent's current state X cur
is not in the envelope, find a path from Xcur to a
goal state, and add this path to the envelope
2. robustify (R[N]): we used the following heuris­
tic to extend the envelope: find the N most l�kely
fringe states that the agent would fall out of the
envelope into, and add them to the envelope
3. prune (P[N]): of the states that have a worse
value than the current state, remove the N least
likely to be reached using the current policy.
Each of the strategies used began with findpath and
ended with optimization. Between the first and last
phases, robustification, pruning and optimization were
used in different combinations, with the number of
states to be added or deleted E {10, 20, 50, 100}; exam­
ples of the strategies we used are {FP R[10] a}, {FP
P[20] a}, {FP P[20] R[50] o}, {FP R[100] P[50]
0}, {FP R[50] a P[50] 0}.
We collected statistics over about 4000 runs generat­
ing 100,000 data points for strategy execution. The
start/goal pairs were, chosen uniformly at random and
we ran the simulated robot in parallel with the plan­
ner until the goal was reached. The planner executed
the following loop: choose one of the 24 strategies uni­
formly .at random, execute that strategy, and then pass
the new policy to the simulated robot. We found the
following conditioning variables to be significant: the
envelope size, lEI, the value of the current state V,.,
the "fatness" of the envelope (the ratio of envelope

316

Dean et al.

size to fringe size), and the Manhattan distance, M,
between the start and goal locations. We then build
a lookup table of expected improvement in value over
the time the strategy takes to compute, 8V11' / k, as a
function of E, V11', the fatness, M and the strategy s.
To test our algorithm, we took 25 pairs of start and
goal states, chosen uniformly at random from pairs of
Manhattan distance less than one third of the diameter
of the world. For each pair we ran the simulated robot
in parallel with the following deliberation mechanisms:
• recurrent-deliberation with strategies chosen us­
ing statistical estimates of EIV (LOOKUP)
• dynamic programming policy iteration over the
entire domain, with a new policy given to the
robot after each iteration (ITER) and only after
it has been optimized ( wHOLE )
The average number of steps taken by LOOKUP,
and WHOLE were 71, 87 and 246 respectively

ITER

W hile the improvement obtained using the recurrent­
deliberation algorithm is only small it is statistically
significant. These preliminary results were obtained
when there were still bugs in the implementation, how­
ever, since we have determined that the strategies are
in fact being pessimistic, we expect to obtain further
performance improvement using LOOKUP. Recall also
that we are still working in the comparatively small
domain necessary to be able to compute the optimal
policy over the whole domain; for larger domains, ITER
and WHOLE are computationally infeasible.
5

Related Work and Conclusions

Our primary interest is in applying the sequential de­
cision making techniques of Bellman [Bellman, 1957]
and Howard [Howard, 1960] in time-critical applica­
tions. Our initial motivation for this research arose
in attempting to put the anytime synthetic projec' ­
tion work of Drummond and Bresina [Drummond and
Bresina, 1990] on more secure theoretical foundations.
The approach described in this paper represents a
particular instance of time-dependent planning [Dean
and Boddy, 1988] and borrows from, among others,
Horvitz' [Horvitz, 1988] approach to flexible compu�
tation. Hansson and Mayer's BPS (Bayesian Problem
Solver) [Hansson and Mayer, 1989] supports general
state space search with decision theoretic control of in­
ference; it may be that BPS could be used as the basis
for envelope alteration. Boddy [Boddy, 1991] describes
solutions to related problems involving dynamic pro­
gramming. For an overview of resource-bounded de­
cision making methods, see chapter 8 of the text by
Dean and Wellman [Dean and Wellman, 1991].
We have presented an approach to coping with un­
certainty and time pressure in decision making. The
approach lends itself to a variety of online computa­
tional strategies, a few of which are described in this
paper. Our algorithms exploit both the goal-directed,

state-space search methods of artificial intelligence and
the dynamic programming, stochastic decision making
methods of operations research. Our empirical results
demonstrate that it is possible to obtain high perfor­
mance policies for large stochastic processes in a man­
ner suitable for time critical decision making.
Acknowledgements

Tom Dean's work was supported in part by a Na­
tional Science Foundation Presidential Young Investi­
gator Award IRI-8957601, by the Advanced Research
Projects Agency of the DoD monitored by the Air
Force under Contract No. F30602-91-C-0041, and by
the National Science foundation in conjunction with
the Advanced Research Projects Agency of the DoD
under Contract No. IRI-8905436. Leslie Kaelbling's
work was supported· in part by a National Science
Foundation National Young Investigator Award IRI9257592 and in part by ONR Contract N00014-914052, ARPA Order 8225. Thanks also to Moises Lejter
for his input during the development and implementa­
tion of the recurrent deliberation model.
References

[Bellman, 1957] Bellman, Richard 1957. Dynamic
Programming. Princeton University Press.
[Boddy, 1991] Boddy, Mark 1991. Anytime problem
solving using dynamic programming. In Proceedings
AAAI-91. AAAI. 738-743.
[Dean and Boddy, 1988] Dean, Thomas and Boddy,
Mark 1988. An analysis of time-dependent plan­
ning. In Proceedings AAAI-88. AAAI. 49-54.
[Dean and Wellman, 1991] Dean, Thomas and Well­
man, Michael 199!. Planning and Control. Morgan
Kaufmann, San Mateo, California.
[Dean et al., 1993] Dean, Thomas; Kaelbling, Leslie;
Kirman, Jak; and Nicholson, Ann 1993. Planning
with deadlines in stochastic domains. In Proceedings
AAAI-93. AAAI.
[Drummond and Bresina, 1990] Drummond,
Mark and Bresina, John 1990. Anytime synthetic
projection: Maximizing the probability of goal sat­
isfaction. In Proceedings AAAI-90. AAAI. 138-144.
[Hansson and Mayer, 1989] Hansson,
Othar and Mayer, Andrew 1989. Heuristic search
as evidential reasoning. In Proceedings of the Fifth
Workshop on Uncertainty in AI. 152-161.
[Horvitz, 1988] Horvitz, Eric J. 1988. Reasoning un­
der varying and uncertain resource constraints. In
Proceedings AAAI-88. AAAI. 111-116.
[Howard, 1960] Howard, Ronald A. 1960. Dynamic
Programming and Markov Processes. MIT Press,
Cambridge, Massachusetts.

