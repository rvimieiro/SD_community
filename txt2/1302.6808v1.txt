metric. These methods use the two components to
identify a network or set of networks with high rel­
ative posterior probabilities, and these networks are
then used to predict future events.
•

Author's primary affiliation:

Computer Science De­

partment, Technion, Haifa 32000, Israel.

Previous work has concentrated on domains contain­
ing only discrete variables, under the assumption that
data is sampled from a multivariate discrete distribu­
tion. In this paper, we develop metrics for domains
containing only continuous variables, under the as­
sumption that continuous data is sampled from a mul­
tivariate normal ( Gaussian ) distribution. Previously,
when working with continuous variables, the standard
solution had been to transform each such variable Xi to
a discrete one by splitting its domain into several mu­
tually exclusive and exhaustive regions. Our metrics
eliminate the need for this transformation. In addi­
tion, our metrics have the advantage that they use the
low polynomial dimentionality of the parameter space
of a mulitivariate normal distribution, whereas their
discrete counterparts often require a parameter space
that is exponential in the number of domain variables.
Our work can be viewed as an extension of traditional
statistical approaches for identifying vanishing regres­
sion coefficients, such as those described in DeGroot
(1970, Chapter 11). In particular, we translate two
assumptions that we identified in HGC for domains
containing only discrete variables, called parameter
modularity and event equivalence, to domains contain­
ing continuous variables. The assumption of parame­
ter modularity, addresses the relationship among prior
distributions of parameters for different Bayesian­
network structures. The property of event equivalence
says that two Bayesian-network structures that repre­
sent the same set of independence assertions should
correspond to the same event and thus receive the
same score. We show that, when combined, these as­
sumptions allow the construction of reasonable prior
distributions for multivariate normal parameters from
a single prior Bayesian network specified by a user.
Our identification of event equivalence arises from a
subtle distinction between two types of Bayesian net­
works. The first type, called belief networks, repre­
sents only assertions of conditional independence and
dependence. The second type, called causal networks,
represents assertions of cause and effect as well as as­
sertions of independence and dependence. In this pa­
per, we argue that metrics for belief networks should
satisfy event equivalence, whereas metrics for causal

236

Geiger and Heckerman

networks need not.

tribution. In this case, we write

Our score-equivalent metrics for belief networks are
similar to the metrics described by Dawid and Lau­
ritzen (1993), except that our metrics score directed
networks, whereas their metrics score undirected net­
works. In this paper, we concentrate on directed mod­
els rather than on undirected models, because we be­
lieve that users find the former easier to build and
interpret.
We note that much of the mathematics involved in our
derivations is borrowed from DeGroot's book, "Opti­
mal Statistical Decisions ," (1970).

,

p(xl()= n ( m E-1)
(2rr)-n/21EI-l/2e-1/2(x-m)'E-l (£-m)
where m is an n-dimensional mean vector, and E =
( O"ij) is an n x n covariance matrix, both of which are
itr:plicitly functions of e' and where 1�1 is the deter­
mmant of E. We shall often find it convenient to refer
to the precision matrix W= E-1, whose elements are
denoted by W;j.
This distribution can be written as a product of condi­
tional distributions each being an independent normal
distribution. Namely,
Tl

2

Gaussian Belief Networks

(3)

Throughout this discussion , we consider a domain x
of n continuous variables x1, . . , Xn. We use p( xl�)
to denote the joint probability density function (pdf)
over x of a person with background knowledge �. We
use p(ele) to denote the probability of a discrete event

.

e.

belief network for x represents a joint pdf over x
by encoding assertions of conditional independence as
well as a collection of pdfs. From the chain rule of
probability, we know

A

"

p(x1, ... ,xniO=

IJ p(x;lx1 , . . . ,x;-1,0

i=1

For each variable x;, let II; � { x1, . .
of variables that renders x; and {x1,
tionally independent. That is,

..
,

, x; _I}
. , x;_

p(x;lx1, ...,x;-1,e) =p(x;III;,e)

(1)

be a set
condi-

I}

(2)

A belief network is a pair (Bs, Bp ) where Bs is a
belief-network structure that encodes the assertions of

conditional independence in Equation 2, and Bp is a
set of pdfs corresponding to that structure. In partic­
ular, Bs is a directed acyclic graph such that (1) each
variable in U corresponds to a node in Bs, and (2)
the parepts of the node corresponding to x; are the
nodes corresponding to the variables in II;. (In the
remainder of this paper, we use x; to refer to both the
variable and its corresponding node in a graph.) As­
sociated with node x; in Bs are the pdfs p(x;III;,().
Bp is the union of these pdfs. Combining Equations 1
and 2, we see that any belief network for x uniquely
determines a joint pdf for x. That is,

n

p (x1 ,... ,xnl()=

IJ p(x;III;,e)

i=l

A minimal belief network

is a belief network where
Equation 2 is violated if any arc is removed . Thus,
a minimal belief network represents both assertions of
independence and assertions of dependence.
Let us suppose that the joint probability density func­
tion for x is a multivariate (nonsingular) normal dis-

i=1

i-1

p(x;lx1, ... ,x;-1,�) = n(m; + L b;j(Xj
j=1

�

mj), 1/v;)

(4)

where m; is the unconditional mean of x;, v; is the
conditional variance of x; given values for x1, . . . , x;_r,
and b;j is a linear coefficient reflecting the strength of
the relationship between x; and Xj (e.g . , DeGroot, p.

1
55) .
Thus, we may interpret a multivariate normal
distribution as a belief network , where b;j
0 (j < i)
implies that xj is not a parent of x;. We call this spe­
cial form of a belief network a Gaussian belief network.
The name is adopted from Shachter and Kenley (1989)
who first described Gaussian influence diagrams.

::::

More formally, a Gaussian belief network is a pair
(Bs, Bp), where (1) Bs is a belief-network structure
containing nodes x1,
. , X11 and no arc from xj to x;
whenever b;j = 0, j < i, (2) Bp is the collection of
parameters m = (ml,····mn), v= {v 1, ...,vn}, and
{ b;j I j < i}, and (3) the joint distribution over i
is determined by Equations 3 and 4. Due to special
properties of nonsingular normal distributions, a min­
imal Gaussian belief network is one were there is an
arc from Xj to x; if and only if b;j ::j; 0 .

.

Given a multivariate normal density, we can generate a
Gaussian belief network, and vice versa. The uncondi­
tional means m are the same in both representations.
Shachter and Kenley (1989) describe the general trans­
formation from iJ and {b;j I i < j} of a given Gaus­
sian belief network G to the precision m atrix W of the
normal distribution represented by G. They use the
following recursive formula in which W(i) denotes the

i x i upper left submatrix of W, b; denotes the column
vector (b1,;, . . , b;-1,;) and bi denotes the transposed
vector b; (i.e. , the line vector (b1,1, . . , b;-t,i)):
W(i) + b;+1b:+1 �k
Vi+l
'-'i+l
(5)
-,
W(i + 1) =

.

(

.

��
Vi+l

1

1
__

)

Vi+l

The coefficients b;1 can be thought of as regression co­
efficients or expressed in terms of Yule's (1907) partial re­
gression coefficient f3.

Learning Gaussian Networks

237

Table

1: An complete database for the domain associ­
ated with the network shown in Figure 1.
Ca.t:�e

-1.55

0.11

2

0.18

-3.04

-2.35

4
5

fori> 0, and

W(l)

v\.

=

in this paper.
For

example,

n(m1,l/v1),x2
x1
n(m3 +b13(x1- ml) + b23(x2-

suppose

n(m2, ljv2), and X3 =
m2), 1/v3)- The belief-network

structure defined by
these equations is shown in Figure 1. The precision
matrix is given by

W=

(l+�
VI

V3

�
V3

_!/.u.
v,

�
V3

_l_+�
V3
t!2
_£..u_
'-'3

_

v,

-�

(6)

v,

_l_
v,

b.

)

The Gaussian-belief-network representation of a mul­
tivariate normal distribution is better suited to model
elicitation and understanding than is the standard rep­
resentation [Shachter and Kenley, 1 989]. To assess a
Gaussian belief network, the user needs to specify
(1) the unconditional mean of each variable x; (m; ) ,
(2) the relative importance of each parent Xj in de­
termining the values of its child x; ( b;j), and (3 ) a
conditional variance for x; given that its parents are
fixed ( v;). Equation 5 then det ermine s W. In con­
trast, when assessing a normal distribution directly,

one needs to guarantee that the assessed covariance
matrix is positive-definite-a task done by altering in
some ad hoc manner the correlations stated by the
user.

3

A Metric for
Networks

Gaussian Belief

We are interested in computing a score for a Gaus­
sian belief-network structure, given a set of cases D =
{i1, ..., im}- Each ca se i; is the observation of one
or more variables in
We sometimes refer to D as

i.

a database. Table 1 is an example of a database for
the three-node domain of the Gaussian belief network
shown in Figure 1.
Our scoring metrics are based on five assumptions, the

first of which is the following:

Assumption 1 The database D is a random sample

from a multivariate normal distribution with unknown

means m and unknown precision matrix w.

Because every Gaussian belief network is equivalent
to a multivariate normal distribution, Assumption 1 is

-

0. 42

!.23

.5

1.04

0.48

0.27

! . 52

-0.68
0.31

-0.22

-0.13

-0.60

0.44
0.57

-1.82

-2.76

9

0.64

0.47

0.74

10

II

1.05
0.43

2.13

0.63

12

0.16

-0.94

-1.96

13

1.64

1.25

1.03

14

-0.52

-2.18

-2.31

15

-0.37

7

Equation 5 plays a key role

! .87

8

6

A belief-network structure for three van­

"'

-0.78

3

Figure 1:
abies.

Variable values for ea.ch ca:��e
:r:
X")

I

0

1

0.!5

0.13

0.20

-1.30

!.35

0.87

-0.70

16
17

18

1.44

-0.83

-

-0.55

-1.33

19

0.79

-0.62

-1.67

20

0.53

-0.93

.

0.23

1 61
.

-2.00

-2 92

equivalent to stating that the database D is a random
sample from a Gaussian belief network with unknown
parameters, v, B = {b;j li < i}, m.

A

Bayesian measure of the goodness of a network
structure is its posterior probability given a database:

p(BsiD,�)

=

c p(BsiO p(DIBs,�)

where c = 1/p(DI�) = 1/I:;B p(Bsl�) p(DIBs,�) is
a normalization constant. Fo� even small domains,

however, there are too many network structures to sum
over in order to determine the constant. Therefore we

use

p(Bsl�) p(DIBs,e) = p(D,Bsl�)

as

our score.

Also problematic is our use of the term Bs as an ar­
gument of a probability. In particular, Bs is a belief­
network structure, not an event. Thus, we need a def­
inition of an event B'S that corresponds to structure
Bs (the superscript "e" stands for event). A natural
definition for this event is that B'} holds true iff the

database is a random sample from a minimal Gaussian
belief network with structure Bs -that is, iff for all
j < i, b;j 1- 0 if and only if there is an arc from xj to
x; in Bs. For example the event B'S corresponding to
the Gaussian belief network of Figure 1, is the event

{ h2

=

0,

b1a# 0, b2s# 0}.

This definition has the following desirable property.
When two belief-network structures represent the
same assertions of conditional independence, we say
that they are isomorphic. For example, in the three
variable domain {x1,x2,x3}, the network structures
x -+ x2 -+ x3 and x1 f- x2 -+ x3 represent the same
assertion: x1 and x3 are independent given x2. Given
the definition of B�, it can be shown that events B'}1
and

Bh

are equivalent if and only if the structures
and Bs2 are isomorphic. That is, the relation of
isomorphism induces an equivalence class on the set of
events B'g. We call this property event equivalence.

Bs1

There is a problem with the definition, however.
In particular, events corresponding to some non­
isomorphic network structures are not mutually ex-

238

Geiger and Heckerman

elusive.

For example, in the four-variable domain
consider the structures x1 => B ¢::: X-t
x4, where B is the subnetwork struc­
ture x2 � xs, and x => B means that there is an
arc from x to both variables in B. The events corre­
sponding to these structures both include the situation
where x1 and x4 are marginally independent . Arbi­
trary overlaps between events can make scores difficult
to interpret and use. For example, the prediction of fu­
ture events by averaging over multiple models cannot
be justified. In our case, however, we can repair the
definition of B5 so as to make non-equivalent events
mutually exclusive, without affecting our mathemati­
cal results or the intuitive understanding of events by
the user. In particular , all overlaps will be of mea­
sure zero with respect to the events that create the
overlap . Thus, given a set of overlapping events, we
simply exclude the intersection from all but one of the
events. We note that this revised definition retains the
property of event equivalence.

{ x1, x2, x3, x4 } ,
and x1 => B =>

Proposition 1 (Event Equivalence)

Belief-network structures Bs1 and Bs2 are isomorphic
if and only if B51 = Bs2.
the score for network structure Bs is
an immediate consequence of the property
of event equivalence is score equivalence.
Because

The next assumption leads to such a conjugate distri­
bution. If all variables in a case are observed , we say
that the case is complete. If all cases in a database are
complete, we say that the database is complete.
Assumption 2

All databases are complete.2

Given this assumption, the following distribution 1s
conjugate for multivariate-normal sampling .

Suppose
that xl_, . . . , xl is a random sample from a multivari­
ate normal distribution with an unknown value of the
mean vector iii and an unknown value of the precision
matrix W. Suppose that the prior joint distribution of
iii and W is the normal- Wishart distribution: the con­
ditional distribution of m given W is n(j10, vW) such
that v > 0, and the marginal distribution of W is a
Wishart distribution with a: > n- 1 degrees of freedom
and precision matrix To, denoted by w(a:, T0). Then
the posterior joint distribution of iii and W given xi,
i = 1, ... , l, is as follows: The conditional distribution
of m given w is a multivariate normal distribution
with mean vector j11 and a precision matrix ( v+ l) W,
where
Theorem 3 (DeGroot, 1 970, p. 1 78)

p( D, B'}; Jc:),

Proposition 2 (Score Equivalence) The scores of
two isomorphic belief-network structures must be equal.

Given the property of event equivalence, we techni­
cally should score each belief- network-structure equiv­
alence class, rather than each belief-network struc­
ture. Nonetheless, users find it intuitive to work with
( i.e., construct and interpret ) belief networks. Conse­
quently, we continue our presentation in terms of belief
networks, keeping Proposition 2 in mind.
3.1

Complete Gaussian Belief Networks

We first derive p(D,B'};j�), assuming Bs is the struc­
ture of a complete Gaussian belief network. A com­
plete Gaussian belief network is one with no missing
edges. Applying the property of event equivalence, we
know that the event associated with any complete be­
lief network is the same; and we use B'!sc to denote
this event.
To motivate the derivation, consider the following ex­
pansion of p(DIBsc

'�):

m

p(D IBsc ' �) = Ilp(CIICI, ... ,Cz-l,Bsc'�) =
1=1

IT j
1=1

p(Crlm, W,B$0,{) p(m, WIC,, .. ,c,_1,B$0,0 dm dW

Thus, we can derive the metric if we find a conjugate
distribution for the parameters iii and W such that
the integral above has a closed form solution.

(7)
and the marginal of W is

71 are given by

w

(a: + l, 11), where

St

I

and

(8)

Sz = 2._)i;- Xt)(i;- Xt)'
i=l

and
71

vl

-

o -,
= To+ St + -v+ 1 (flo- Xi)(fl - Xz)

(9 )

In this theorem , X1 and S1 are the sample mean and
sample variance of the database, respectively. Also,
an n dimensional Wishart distribution with a: degrees
of freedom and precision matrix
is given by

To

p(WI�) = w(a:, To)
}
c(n, a:) JTo la/21W I (a-n-1)/2e-1/2tr{ToW

(10)

where tr{ToW} is the sum of the diagonal elements of
and

To W

c(n, a )

�

[

2""''·•(•->)/<

fi rt+�- r'
;)

The terms a and
are implicit functions of the user's
background knowledge�·

To

2SDLC present a survey of approximation methods for
handling missing data in the context of discrete variables.
Some of these methods in modified form can be applied to
Gaussian networks.

Learning Gaussian Networks

From Equation 7, we see that v can be thought of
as being an equivalent sampl e size for m-that is, the
equivalent number of cases the user has seen, since he
was ignorant about m . When l new cases are seen, the
posterior mean is updated as a weighted average of the
prior mean computed based on v cases and the sample
mean based on l cases. Furthermore, if xi, ... , x-;. is a
random sample of n-dimensional random vectors from
a multivariate normal distribution for which the mean
vector is 0 and the n x n precision matrix is
then
xixi' has the Wishart distribution given in
W=
Equation 10 (DeGroot, p. 56). Thus, we may interpret
a as the user's equivalent sample size for the precision
matrix To. Note that a must be at least the number
of variables in the domain. We address the assessment
of ilo and To in Section 3.4.

T0,

2::;=l

Summarizing our discussion so far, we make the fol­
lowing assumption:
Assumption 3

The
prwr
distribu­
tion p( m, w IB�c 1 ,; ) is a normal- Wishart distribution
as given in Theorem 3.

From Equation 5, this assumption fixes the distribu­
tion p(1n, v,BIE.Sc,�). Nonetheless, we shall some­
times find it easier to specify the prior density in the
space of W, rather then in the space of parameters
describing a Gaussian belief network.
It is well know that, if
and if

p(m , WIEse'�)

p(ilm , W,Esc'�)=

n(m, W)

is a normal-Wishart distribu­

tion as specified by Theorem 3, then

p(iiEL,�) ,

de­

fined by

p(xiBsc' 0

=

j p(xlm,

w, Bsc,�)

p(m, w, Bsc, �) dm aw

is an n dimensional multivariate t distribution with
1 = a - n + 1 degrees of freedom, location vector
ilo, and a precision matrix T�
p.

180) .

=

Also, the t distribution

written in a less traditional form,
Tiao, 1973, p. 440):

�T0-1

(DeGroot ,

p(iiEsc, 0 can be
as follows ( B ox and
(11)

=

(2rr)-n/2(�v-t/2 c(n,a) ITolo/2ITtl-(atl)/2
v+l

c(n,a+l)

where T1 is defined by Equation

9 (l

=

1) .

Combining these facts with T heorem 3, we know that
is a multivariate t distribu­

p( CdC 1, . . . , C1-1 , Esc,�)

tion with parameters v + l - 1, a+ l 11-1. Consequently, we obtain

p(D I B�c'O ==II p(C, I Ct,
!=l

. ,C,_t,B�c' 0

1,

i11-1, and

239

Multiplying Equation 12 by the prior probability
yields a metric for scoring complete Gaus­
sian 6 elief networks.

p(E'S I�)
3.2

General Gaussian Belief Networks

We now consider an arbitrary Gaussian belief network
of

Es. To form a prior distribution for the parameters
Es, we make two additional assumptions:
Assumption 4 (Parameter Independence)

For every Gaussian belief network Bs' p(v, BIBS.,e)=

I17=1 p(v;,b�IB5,�).

We note that this assumption is consistent with As­
sumption 3, because if p(WIBsc, e) is a W ishart

distribution, then p( v, EIEsc,�), obtained from
p( WIEse' e) by using Equation 5 and the Jaco­
bian 8Wj8vB of this transformation , is equal to
rr=l p( Vj' �lEse e). The derivation of this claim is
1

given in the Appendix (Theorem 7).

x;
If
has the same parents in two Gaussian belief networks
Es1 and Es2, then p(v;,b:IEh,e) = p(v;,b:IB52,�).
Assumption 5 (Parameter Modularity)

Assumption 4 has been made in discrete contexts
by many researchers (e.g., CH, Buntine, SDLC, and
HGC) . Assumption 5 has also been made by these
same researchers, but HGC were the first researchers
to make the assumption explicit and to emphasize its
importance for generating prior distributions. Param­
eter modularity plays a similar important role in the
current development. In particular, this assumption ,
in conjunction with the property of event equivalence
and our previous assumptions allows us to determine
the joint prior distribution of the parameters m, ii', B
associated with any Gaussian network Es from the
joint density p(m , WIEse).
To see this fact , first note that, by the definition of the
event B5, p(m lv,E,B 5,�) = p(m iv,B,Bsc'�). The

latter distribution is determined by p(m iW,Esc,e),
which is given. Second , from Assumption 4, we ob-

p(v;,b:IB,S,�) fo r
each i. By Assumption 5, however, p(vi,b�\B8,()
is equal to p( Vj 1 b: I Bs,c'e) for any complete network
structure Es' where the parents of x; are the same as
c
are those in Es. By event equivalence and Assumption 4, we obtain p(v;, ��E�,c , �) from the given density
p(WIEsc'�).
tain

p(v,BIB�,e)

by determining

From Assumptions 1 through 5, we derive p(DIB5,e).
To do so, we need the following theorem whose proof
is provided in the Appendix.
Theorem 4 If p(ilm , W,

D, �) is a multivariate nor­
mal distribution, and p( m I w, D, B.S e) is a multi­
variate normal distribution with a precision matrix
1

240

Geiger and Heckerman

vW, v > 0, then p(xdxt, ... , Xi-1, v, B, D, B�, �) =
p(x;III;, v;, b�, nx,n,, B�,, {), where Bs' is any network
where X; h as the same parents as in Bs I and nx;IT; is
the database D restricted to the variables in { x;} U II;.

By applying Theorem 4 to the first term of the right­
hand-side of Equation 15, and posterior parameter in­
dependence and posterior parameter modularity to the
second term, we obtain

p(x;Jxt, ... ,x;_t,D1, B5,0

In particul ar, this claim holds for any complete Gaus­
sian belief network Esc == Bs' in which II; and x;
appear before any other variables, and II; appears be­
fore x;.

Let Dt = { C1, . . . , Ct-1} and Ct be an instance of
x1, . .. , Xn· In the following derivation, we use x; and
II; to represent the instance of x; and II; in the lth
case. Theorem 4 yields,

J [p(x;JII;,
·

Vj,

( nr;TI;, B5c,()

J

p(v;, b� IDt;IT;, B5c, e) dv;b�

p(x; JII;, Dt'n', Esc,�)
Therefore,

p(Div, B,B�,�)
m

n

II IT p(x; lx1, . .. , Xi-1. v, B, Dt, Bf,, �)
1=1 i=1
p( Xj' II;iv;�b�' nr;IT; B5 , ()

ft IT

Furthermore, because p(II; JDt'rr', Bs , �) is a multi­
variate t distribution, we know that c

I

p(II;jv;,b;,DF'n',B.S,{)

1=1i=1

p(ll; JDf'n;,Bsc' �) = p(II; JDP', Bsc• {)

and

p(II;Iv;, b�, nr;IT;) B�,�)

=

p(II;Iv;, �, np· B.S,{)
I

(DeGroot, p. 60).
16, we have

€
:::::: rrn p(Dx;IT; lEse,()
(DIES' �)
.
p(DIT; JBeSc ,<.,C )
•=1

By combining these equations, we obtain the following

P

likelihood separab ility property:

p(Div,B,Bf,,�)

x,n,lv;,!�,B.S,{)
=IT p(D
p(DIT•Iv;,b;,B},�)
i=t

( 3)
1

rule, p(v, BID,B.S, ( ) IS proportional
Thus, because
to p(Div, B, B },�)p (v , BI B�,().
p(Div, B, Bf,,�) factors as shown by Equation 13, and
p(v,BIBJ,,{) factors as given by Assumption 4, we

By

Bayes

obtain the following

property:

posterior parameter independence

n
p(v, BID, B5,() =II p(v;, b�IDx,n,, Bf,,�)
i=l

In a similar manner, whenever x; has the same par­
ents in two Gaussian belief networks Bs and Bs•, by
using Equation 13 where B.S in the right hand side is
replaced by B'f,, and using Assumption 5, we obtain
the posterior parameter modularity property:

�)
- p( Vj) b-I
e �)
i D x;IT; ,Be51)<.,
p ( Vj b-iIDx;IT; , Bs)
<., -

p(Ct!D1, B5,()

=

n

IIp(CdDt, B5,�),

1=1

(14)

IIP(�;Ixt, ..., x;-t,Dt, B'5,{)

i=1

p(x;lx1, . .. , x;-1, D1,B5,�)

f (p(x;Jx1, ... ,x;-1,DI,v,B,B'j;,()
·p(v,B 1 Dt, B5,()] dvB

where each term in 17 is of the form given in Equa­

tion 12. Multiplying Equation 17 by p (B5 IE.), we ob­
tain a metric for an arbitrary Gaussian belief net­

work Bs. We call this metric BGe which stands for
Bayesian metric for Gaussian networks having score
equivalence.
3.3

Score Equivalence

In making the assumptions of parameter indepen­
dence and parameter modularity, we have-in effect­
specified the prior densities for the multinomial param­
eters in terms of the structure of a belief network. Con­
sequently, there is the possibility that this specification
violates the property of score equivalence. The follow­
ing theorem, however, demonstrates that our specifi­
cation implies score equivalence.

struc­
tures, then p(DJBh, E.) and p(DJB52, E.) as computed
by Equation 17 are equal.

Now, we have
=

( 17)

Theorem 5 (Score Equivalence)
If Bst and B52 are isomorphic bel ief-network

I

p(DIB5,�)

Thus, combining Equations 14 and

( 1 5)

Proof: In Heckerman et a!. (1994, Theorem 10), we
show that a belief network structure can be trans­
formed into an isomorphic structure by a series of arc
reversals, such that, whenever an arc from x; to Xj is
reversed, II; = IIj \ { x;}. Thus, our claim follows if we
can prove it for the case where Bs1 and Bs2 differ by
a single arc reversal with this restriction.
So, let Bs1 and Bs2 be two isomorphic network struc­
tures that differ only in the direction of the arc between
x; and Xj (say x;-+ Xj in B5t). Let R be the parents

241

Learning Gaussian Networks

of x; in Bs1- By the cited theorem, R U { x; } is the
parents of Xj in Bs1, R is the parents of Xj i n Bs2,
and

RU

{xj }

is the parents of

x;

in

Bs2.

Because the

two structures differ only in the reversal of a single arc,

the only terms in the product of

Equation 17 that can
x; and Xj. For B51 these

differ are those i nvolving
terms are

Bs2,

p(Dx;x;RIBsc'�)
p(DRIBsc'�)

they are

p(Dx,xJRIBsc'�)
p(Dx;RIBsc'�) p(Dx;x;RIBsc'�)
�
�)
p(
p(DRIBsc'
Dx;RIBsc' ) - p(D RIBsc'�)
Thus, p(DIBh,�) = p(DIBh,�)- D
3.4

which are not precise anyway-are being reasonably

interpreted.

,

p(Dx•RIBsc '�) p(Dx;x;RIBsc '�)
p(D RIBsc'�) p(Dx•RiB5c,0
whereas for

normal and t distributions are similar in that both
have a single maximum and symmetric tails around
their maximum.3 Therefore, the users' assessments­

3.5

Simple Example

Suppose the user's prior-network structure
shown

in

1

Figure

(0.1, -0.3, 0.2), v = (1, 1, 1), b� = (0),

a

is straightfor­

T0.

In addition, a

this section, we concentrate on the assessment of
and To.

a multivariate normal distribution is valid, recall that,

in ou r approach, the user actually specifies a family of
rather than a single normal

over, we h ave seen that if

and

distribution. More­

p(m, WIEse'�) is a normal­
p(xiBsc, �) is actually a

Wishart distribution, then

multivariate t distribution given by Equation 11 with
paramet e rs v, a , j1 0, and To. Thus, the direct assess­
ment of ilo and To are difficult. Nonetheless, we can

and

E(xl�) = ilo

and

cov (xl�)

=

1 r�-1
--y-2

where E ( i l�) and
covariance of

x,

=

cov(il�)

(v + 1) To
v(a- n -1)

( 18 )

0 1.7
0 1.7 1.7
1.7 1.7 5.1

(

T2o =

13.8 11.3 6.7
11.3 35.8 27.7
6.7 27.7 41.2

)

T20, yielding

Finally, using Equation 12 with c ( n

::: 3, a = 6) =
26) ::: 2.6 x 1 0 13 , we obtain
p(DIBsc, �) = 1.5 x 10-88. To compute

0.029 and c( n
the density

=

3, a+ m

=

the density for an incomplete network structure-say
x1 � x2 � x 3 we use Equation 17:
-

p(DIB�,--+x2-+x3'�)
p(D{x, ,x2} lEse'�) p(D{x2,xs} JBsc, �)
p(D{x2} lEse'�)
1.3

X

10

-

59

X

·

1Q-62

X
---:
:-:--

1.9

-----

6.8

lQ-34

= 3.5

X

10-88

where we compute each term in the previous equation
by eliminating the appropriate rows and columns of To
and T20 and again using Equation 1 2.

(19)

There are eleven distinct (i.e ., no nisomorphi c) belief­

are the expectation and

ing that these structures are equally likely, we obtain
the BGe score for each structure B5 by multiplying the

respectively (e.g, DeGroot, pp.

60-

61).
Therefore, to assess ji0 and T0, we first ask
the user to build a prior Gaussian belief network for
i = {xi, . . . , Xn}- Then, we use Equation 5 to gener­
ate a covariance matrix cov ( il�) . Finally, we use the
means and covariance matrix from this prior Gaussian
belief network to determine fio and To.
Although this procedure is heuristic in the sense that
is assessed as if it came from a normal d istri­
bution rather then from a multivariate t distribution,

cov(xl�)

)

1.7

8, and use Equation 9 to compute

use

a heuristic method t h at is based on the following
equations for ilo and To known to hold for t distribu­
tions:

(

Then, we compute the sample mean and sample vari­
ance of the database (l = 20) according to Equations 7

ilo

Whereas using a Gaussian belief network for assessing

W,

and

6 to compute E = cov(xl�).
19 with v= a = 6 and n = 3

We obtain

To=

user can assess the equivalent sample sizes directly. In

m

1.1

First, we use the parameters of the prior network i n

the prior

multivariate normal distributions indexed by

and

having observed the database shown in Table 1.

to compute

ward . Buntine and HGC, for example, describe meth­
ods that facilitate these assessments.

that

J1o
f:; = (1, 1).

Let us apply the B Ge metric

Next, we apply Equation

probabilities p(Bs I�), (2) the equivalent sample sizes a
and v, an d (3) the parameters {10 and To. The assess­

p(Bs I�)

6.

are both equal to

From the previous discussion , we see that there are

ment of the prior probabilities

IS

parameters

Also, suppose the user's equivalent sample sizes

three components of a user's prior knowledge that are

( 1)

has

conjunction with Equation

Encoding Prior Knowledge: The Prior
Gaussian Belief Network

relevant to learning Gaussian networks:

and

network structures for

density

{ x1, x2, x 3 } .

p(DIB.S,�) by 1/ 11.

Therefore, assum­

After renormalizaticm, we

find that the network structure
highest posterior probability:

x1 -+ x2 -+ X3 has the
0.60. Not surprising,

v = (1, 1, 1), f:;

(0, 1)).

the dat abase in Table 1 was generated from this net­
work structure (with parameters {10 = (0.5, 0.2, -0.5),
3 Also,

as

=

(1), and 'b;

=

the number of degrees of freedom becomes

arbitrarily large, the multivariate t distribution converges
to the multivariate normal distribution

( DeGroot,

p.

255).

242

4

Geiger and Heckerman

Metrics for Gaussian Causal
Networks

People often have knowledge about the causal relation­
ships among variables in addition to knowledge about
conditional independence. Such causal knowledge is
stronger than is conditional-independence knowledge,
because it allows us to derive beliefs about a domain
after we intervene. Causal networks, described-for
example-by Spirtes et al. (1993), Pearl and Verma
(1991), and Beckerman and Shachter (1994) represent
such causal relationships among variables. In partic­
ular, a causal network for U is a belief network for
U, wherein it is asserted that each nonroot node x is
caused by its parents. The precise meaning of cause
and effect is not important for our discussion. The in­
terested reader should consult the previous references.
The event C.S is the same as that for a belief-network
structure, except that we also include in the event the
assertion that each nonroot node is caused by its par­
ents. Thus, in contrast to the case for belief networks,
it is not appropriate to require the properties of event
equivalence or score equivalence. For example, con­
sider a domain containing two variables x and y. Both
the causal network Cs1 where x points to y and the
causal network Cs2 where y points to x represent the
assertion that x and y are dependent. The network
Cs1 , however, in addition represents the assertion that
x causes y, whereas the network Cs2 represents the as­
sertion that y causes x. Thus, the events C.h are C.S2
are not equal. Indeed, it is reasonable to assume that
these events-and the events associated with any two
different causal-network structures-are mutually ex­
clusive.
In principle, then, a user may assign a (possibly dif­
ferent) prior distribution to the parameters m, v, and
B to every complete Gaussian causal network, con­
strained only by the assumption of parameter mod­
ularity. The prior distributions for parameters of in­
complete networks would then be determined by pa­
rameter modularity. We call this general metric BG,
as it is a superset of the BGe metric. For practical rea­
sons, however, the assessment process should be con­
strained. One alternative is to use the BGe metric. A
more general alternative is to continue to use the prior
network to compute iio and To, but to allow equivalent
sample size to vary for different variables and different
parent sets of each variable. We call this metric the
BGp metric, where "p" stands for prior network.
5

Summary and Future Work

We have described metrics for learning belief networks
and causal networks from a combination of user knowl­
edge and statistical data for domains containing only
continuous variables. An important contribution has
been our elucidation of the property of event equiv­
alence and the assumption of parameter modularity.

We have shown that these properties, when combined,
allow a statistician to compute a reasonable prior dis­
tribution for the parameters of any Gaussian belief
network, given a single prior Gaussian belief network
provided by a user.

A legitimate concern with our approach is that the
multivariate model is too restrictive. In practice, when
this model is inappropriate, statisticians will typically
turn to a more general model where each continuous
variable conditioned on its parents is assumed to be
a mixture of multivariate normal distributions. In
Geiger and Beckerman (1994), we derive metrics for
domains containing both discrete and continuous vari­
ables, subject to the restriction that a domain can be
decomposed into disjoint sets of continuous variables
where each such set is conditioned by a set of dis­
crete variables. We note that this work, when com­
bined with approximation methods that handle miss­
ing data, provides a method for learning with multi­
variate mixtures.
In the discrete case, a complete network has one pa­
rameter for each instance of i. Consequently, it is easy
to overfit such a structure with data; and the met­
rics developed for discrete domains provide a means
by which we can avoid such overfitting. In the contin­
uous case, a complete network has only n + n( n- 1) /2
parameters. Thus, it is possible that the errors intro­
duced by our methods, arising from heuristic search in
an exponential space to find one or a handful of struc­
tures with high scores outweigh the benefits associated
with decreasing the degree of overfitting. We leave this
concern for future experimentation.

Acknowledgments

We thank Wray Buntine and anonymous reviewers for
useful suggestions.
References

[Box and Tiao, 1973] Box, G. and Tiao, G. (1973).
McGraw-Hill, Addison Wesley.
[Cooper and Herskovits, 1991] Cooper, G. and Her­
skovits, E. (January, 1991). Technical Report SMI91-1, Section of Medical Informatics, University of
Pittsburgh.
[Cooper and Herskovits, 1992] Cooper, G. and Her­
skovits, E. (1992). Machine Learning, 9:309-347.
[Dawid and Lauritzen, 1993] Dawid, A. and Lau­
ritzen, S. (1993). Annals of Statistics, 21:1272-1317.
[DeGroot, 1970] DeGroot, M. ( 1970). McGraw-Hill,
New York.
[Geiger and Beckerman, 1994] Geiger, D. and Hecker­
man, D. (March, 1994). Technical Report MSR-TR94-10, Microsoft.
(Beckerman et al., 1994] Beckerman, D., Geiger, D.,
and Chickering, D. (1994b). In this proceedings.

Learning Gaussian Networks

Shachter, 1994] Heckerman , D. and
( 1994) . In this proceedings.
Verma, 1991] Pearl , J . and Ve rma , T.

[Heckerman and
Shachter, R.

[Pearl and

( 1 99 1 ) .

In Allen, J . , Fikes, R . , and Sandewa.ll , E . ,

Knowledge Representation and Reasoning:
Proceedings of the Second International Conference,
pages 441-452. Morgan Kaufman n , New Yor k .
[Shachter and Kenley, 1989] Shachter, R. and Kenley,
C. ( 1989) . Management Science, 35:527-550.
[S pie gelhalter e t al . , 1 993] Spiegelhalter, D., Dawid,
A . , Lauritzen , S . , and Cowel l , R. ( 1993) . Statistical
Science, 8:219-282 .
editors,

[Spirtes

1993] Spirtes, P . , Glymour, C., and
(1993) . Springer-Verlag, New York .
[Yule, 1907} Yule , G. ( 1 907) . Proceedings of the Royal
Society of London, Series A, 79: 1 82-193.
et al. ,

243

so that the determinant i n E qu ati o n 23 factors as a
function of i. Also, Equation 5 implies (by in d uc tion )
that each element Wij in W is a sum of terms each being a function of b� and v; . Co ns e qu ent ly, the exponent
in Eq u at ion 23 factors as a function of i. 0

If p(ilm, W, D , B'}; ,�) is a multivari­
ate normal distribution, and p (mi W, D, B'};,e) is a
multivariate normal distribution with precision matrix
vW, 11 > 0, then p(x; l x1, . . . , x;_ 1 , v, B , D, B'}; , �) =
p(x; I II; , v; , ( D" ' n ' , B'J;, , 0 where Bs' is any network
where x; has the same parents as in Bs , and Dx , n ; is
the database D restricted to the variables in { x;} U II; .
Theorem 4

Scheines, R.

p(i'I W, D, B� , O

Appendix
Theorem 6 The Jacobian J for the change of vari­

ables from W to { v,B } is given by
n
J = EJW/EJvB = II v; (i+l)
i= l

Proof: Let
variables in
form :

(20)

J ( i ) denote the J acobian for the first z
W . Then J ( i) has the following matrix
0
_ .l..
J; _ l ' i - 1
v,

(21)

0

h,k i s t h e ident i ty matrix o f size k
the absolute value of J(i) is given by,

where

I J(i) l

=

which gi ves Equation

x

�

i l · I J (i - 1)1

V;

20.

k. Thus ,

(22)

0

Theorem 7 If p(W iO has an n-dimensional Wishart

distribution, then

p(v, B i�) = II p(v ; , b� i� )
i =l

a }
I W I ( a- n- l )/2e - 1 / 2 lr{T W

( 23)

Thus , we m u st express Equation 23 in terms of { v, B } ,
multiply by the Jacobian given by T heorem 6, and
show that the resulting function factors as a function
of i. From Equation 5, we ge t

1
I W ( i) l = - I W(i - 1 ) 1
v;

J p (i'lm, W, D, B� , () p(mi W, D , B� , �) dm

and Assumptions

1 and 3 ,

we obtain

p(ii W, D, B'}; , <)

(24)

where JlD is the posterior mean after seeing D, given
by Equation 7 of Theorem 3 .
The marginal distribution

p(x1 , . . . , x; J�) of

n(m, W) is a normal
n(m; , W; ) , where m; and W; are the
and W that correspond to x1 , . . . , x; .
I W I = 117= 1 v; 1 , Equation 24 becomes

m al

distribution

By e xpr ess in g
w e obtain

W in terms of v and B

D
p(xt , . . . , x;-t lv,B , D , B,5 ,�)

x ; lv, B , , ----"'
p(x
B5 , �)
. , ---'--'-'---1- , . .-:--- -'--'-.:....,- = C

•

a nor­
distribution
terms in m
T h u s , using

using Equation
- 1 /2

V,
•

, €

5,

_ .!. _v_ A
2 v+ I

( 26)

where

where (i - JlD ) ; is t he column vector of the i elements
of (i - JlD ) that correspond to x 1 , . . . , x;. Starting

Proof: By assumption , we have
= c

=

( 27)

n

p( W ie)

Proof: Using

n

= II v; 1
i=l

with any network Bs' . such that the parents of x; are
the same as in Bs, we obtain exactly Equations 26 and
27. Furthermore , because Jln depends only on Dx;ll; ,
the t heorem is established. 0

