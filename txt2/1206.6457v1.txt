
The space D might be the set of free parameters that
one could feed into a time-consuming algorithm or
the locations where a sensor could be deployed, and
the function f might be a measure of the performance of the algorithm (e.g. how long it takes to
run). We refer the reader to (MocÃåkus, 1982; Schonlau et al., 1998; Gramacy et al., 2004; Brochu et al.,
2007; Lizotte, 2008; Martinez‚ÄìCantin et al., 2009;
Garnett et al., 2010) for practical examples. In this
paper, our assumption is that once the function has
been probed at point x ‚àà D, then the value f (x)
can be observed with very high precision. This is
the case when the deployed sensors are very accurate or if the algorithm is deterministic. An example of this is the configuration of CPLEX parameters
in mixed-integer programming (Hutter et al., 2010).
More ambitiously, we might be interested in the simultaneous automatic configuration of an entire system (algorithms, architectures and hardware) whose
performance is deterministic in terms of several free
parameters and design choices.
Global optimization is a difficult problem without
any assumptions on the objective function f . The
main complicating factor is the uncertainty over the
extent of the variations of f , e.g. one could consider the characteristic function, which is equal to
1 at xM and 0 elsewhere, and none of the methods
we mention here can optimize this function without
exhaustively searching through every point in D.

The way a large number of global optimization
methods address this problem is by imposing some
th
Appearing in Proceedings of the 29 International Confer- prior assumption on how fast the objective function
ence on Machine Learning, Edinburgh, Scotland, UK, 2012. f can vary. The most explicit manifestation of this
remedy is the imposition of a Lipschitz assumption
Copyright 2012 by the author(s)/owner(s).

Regret Bounds for Gaussian Process Bandits with Deterministic Observations

on f , which requires the change in the value of f (x),
as the point x moves around, to be smaller than
a constant multiple of the distance traveled by x
(Hansen et al., 1992). As pointed out in (Bubeck
et al., 2011, Figure 3), it is only important to have
this kind of tight control over the function near its
optimum: elsewhere in the space, we can have what
they have dubbed a ‚Äúweak Lipschitz‚Äù condition.
One way to relax these hard Lipschitz constraints
is by putting a Gaussian Process (GP) prior on the
function. Instead of restricting the function from
oscillating too fast, a GP prior requires those fast
oscillations to have low probability, cf. (Ghosal &
Roy, 2006, Theorem 5).
The main point of these bounds (be they hard or
soft) is to assist with the exploration-exploitation
trade-off that global optimization algorithms have
to grapple with. In the absence of any assumptions
of convexity on the objective function, a global algorithm is forced to explore enough until it reaches
a point in the process when with some degree of certainty it can localize its search space and perform
local optimization (exploitation). Derivative bounds
such as the ones discussed here together with the
boundedness of the search space, guaranteed by the
compactness assumption on D, provide us with such
certainty by producing a useful upper bound that allows us to shrink the search space. This is illustrated
in Figure 1. Suppose we know that our function is
Lipschitz with constant L, then given sample points
as shown in the figure, we can use the Lipschitz property to discard pieces of the search space. This is
done by finding points in the search space where the
function could not possibly be higher than the maximum value already encountered. Such points are
found by placing cones at the sampled points with
slope equal to L and checking where those cones lie
below the maximum observed value.
This crude approach is wasteful because very often
the slope of the function is much smaller than L. As
shown in Figure 2), GPs do a better job of providing
lower and upper bounds that can be used to limit
the search space, by essentially choosing Lipschitz
constants that vary over the search space and the
algorithm run time.
We also assume that the objective function f is
costly to evaluate. We would like to avoid probing
f as much as possible and to get close to the optimum as quickly as possible. A solution to this problem is to approximate f with a surrogate function
that provides a good upper bound for f and which
is easier to calculate and optimize (Brochu et al.,

2009). Surrogate functions also aid with global optimization by restricting the domain of interest. The
surrogate that we will make extensive use of here is
called the Upper Confidence Bound (UCB). It is defined to be ¬µ + BœÉ, where ¬µ and œÉ are the posterior
predictive mean and standard deviation of the GP
and B is a constant to be chosen by the algorithm.
This surrogate function has been studied extensively
in the literature and this paper relies heavily on the
ideas put forth in the paper by Srinivas et al (Srinivas et al., 2010), in which the algorithm consists of
repeated optimization of the UCB surrogate function after each sample. It must be noted however
that our algorithm is distinctly different from their
UCB algorithm.
One key difference between our setting and that of
(Srinivas et al., 2010) is that, whereas we assume
that the value of the function can be observed exactly, for the analysis presented in (Srinivas et al.,
2010) it is necessary for the noise to be non-trivial
(and Gaussian) because the main quantity that is
used in the estimates, namely information gain, cf.
(Srinivas et al., 2010, Equation 3), becomes undefined when the variance of the observation noise (œÉ 2
in their notation) is set to 0, cf. the expression for
I(yA ; fA ) that was given in the paragraph following
Equation (3). So, our analysis is complementary to
theirs. Of course, one could still use their algorithm
in the noiseless setting, but their analytical results
are inapplicable to that case. Moreover, we show
that the regret, r(xt ) = max
 D f ‚àí f (xt ), decreases
‚àí

œÑt

according to O e (ln t)d/4 , implying that the cumulative regret is bounded from above.

The paper whose results are most similar to ours
is (Munos, 2011), but there are some key differences in the methodology, analysis and obtained
rates. For instance, we are interested in cumulative regret, whereas the results of (Munos, 2011)
are proven for finite stop-time regret. In our case,
the ideal application is the optimization of a function that is C 2 -smooth and has an unknown nonsingular Hessian
 at the maximum. We obtain a
regret rate O e

‚àí

œÑt
(ln t)d/4

, whereas the DOO algo-

rithm in (Munos, 2011) has regret rate O(e‚àít ) if
the Hessian is known
and the SOO algorithm has
‚àö
regret rate O(e‚àí t ) if the Hessian is unknown. In
addition, the algorithms in (Munos, 2011) can handle functions that behave like ‚àíckx ‚àí xM kŒ± near
the maximum (cf. Example 2 therein). Moreover,
the hierarchical decomposition of the search space
utilized by DOO and SOO makes them much more
efficient in practice than the algorithm presented in

Regret Bounds for Gaussian Process Bandits with Deterministic Observations

Figure 1. An example of the Lipschitz hypothesis being used to discard pieces of the search space when finding the
maximum of a function f . Although f is only known at the red sample points, if the derivative upper bounds (dashed
lines) are below the best attained value thus far, f (x+ ), the corresponding areas of the search space (shaded regions)
may be discarded.

Figure 2. An example of our branch and bound maximization algorithm with UCB surrogate ¬µ + BœÉ, where ¬µ and œÉ
are the mean and standard deviation of the GP respectively. The region consisting of the points x for which the upper
confidence bound ¬µ(x) + BœÉ(x) is lower that the maximum value of the lower confidence bound ¬µ(x) ‚àí BœÉ(x) does
not need to be sampled anymore. Note that the UCB surrogate function bounds f from above.

this paper: this is a shortcoming of our algorithm
that we would like to remedy in the future.
This problem was also studied by (Vazquez & Bect,
2010) and (Bull, 2011), but using the Expected Improvement surrogate instead of UCB. Our methodology and results are different, but complementary
to theirs.

2. Gaussian process bandits
2.1. Gaussian processes
As in (Srinivas et al., 2010), the objective function
is distributed according to a Gaussian process prior:
f (x) ‚àº GP(m(¬∑), Œ∫(¬∑, ¬∑)).

(1)

For convenience, and without loss of generality, we
assume that the prior mean vanishes, i.e., m(¬∑) = 0.
There are many possible choices for the covariance
kernel. One obvious choice is the anisotropic kernel Œ∫ with a vector of known hyperparameters (Ras-

mussen & Williams, 2006):
Œ∫(xi , xj )


= Œ∫
e ‚àí(xi ‚àí xj )> D(xi ‚àí xj ) , (2)

where Œ∫
e is an isotropic kernel and D is a diagonal
matrix with positive hyperparameters along the diagonal and zeros elsewhere. Our results apply to
squared exponential kernels and MateÃÅrn kernels with
parameter ŒΩ ‚â• 2. In this paper, we assume that the
hyperparameters are fixed and known in advance.
We can sample the GP at t points by choosing
points x1:t := {x1 , . . . , xt } and sampling the values
of the function at these points to produce the vector f1:t = [f (x1 ) ¬∑ ¬∑ ¬∑ f (xt )]> . The function values are
distributed according to a multivariate Gaussian distribution N (0, K), with covariance entries Œ∫(xi , xj ).
Assume that we already have several observations
from previous steps, and that we want to decide
what action xt+1 should be considered next. Let
us denote the value of the function at this arbitrary
new point as ft+1 . Then, by the properties of GPs,

Regret Bounds for Gaussian Process Bandits with Deterministic Observations

f1:t and ft+1 are jointly Gaussian:


 

f1:t
K
k>
‚àº N 0,
,
ft+1
k Œ∫(xt+1 , xt+1 )
where k = [Œ∫(xt+1 , x1 ) ¬∑ ¬∑ ¬∑ Œ∫(xt+1 , xt )]> . Using the
Schur complement, one arrives at an expression for
the posterior predictive distribution:
P (ft+1 |x1:t+1 , f1:t ) = N (¬µt (xt+1 ), œÉt2 (xt+1 )),
where
¬µt (xt+1 ) = k> K‚àí1 f1:t ,
œÉt2 (xt+1 ) = Œ∫(xt+1 , xt+1 ) ‚àí k> K‚àí1 k

(3)

and f1:t = [f (x1 ) ¬∑ ¬∑ ¬∑ f (xt )]> .
2.2. Surrogates for optimization
When it is assumed that the objective function f
is sampled from a GP, one can use a combination
of the posterior predictive mean and variance given
by Equations (3) to construct surrogate functions,
which tell us where to sample next. Here we use the
UCB combination, which is given by
¬µt (x) + Bt œÉt (x),
where {Bt }‚àû
t=1 is a sequence of numbers specified by
the algorithm. This surrogate trades-off exploration
and exploitation since it is optimized by choosing
points where the mean is high (exploitation) and
where the variance is large (exploration). Since the
surrogate has an analytical expression that is easy
to evaluate, it is much easier to optimize than the
original objective function. Other popular surrogate
functions constructed using the sufficient statistics of
the GP include the Probability of Improvement, Expected Improvement and Thompson sampling. We
refer the reader to (Brochu et al., 2009; May et al.,
2010; Hoffman et al., 2011) for details on these.
2.3. Our algorithm
The main idea of our algorithm (Algorithm 1) is to
tighten the bound on f given by the UCB surrogate function by sampling the search space more and
more densely and shrinking this space as more and
more of the UCB surrogate function is ‚Äúsubmerged‚Äù
under the maximum of the Lower Confidence Bound
(LCB). Figure 2 illustrates this intuition.
More specifically, the algorithm consists of two iterative stages. During the first stage, the function
is sampled at enough points in L (the red crosses
in Figure 3) until every point in the search space is

Figure 3. Branch and Bound algorithm for a 2D function. The colored region is the search space and the colormap, with red high and blue low, illustrates the value of
the UCB. Four steps of the algorithm are shown; progressing from left to right and top to bottom. The green
dots designate the points where the function was sampled in the previous steps, while the red crosses denote
the freshly sampled points.

contained inside a simplex of diameter Œ¥, where by
the diameter of a set we mean the maximum length
between any pair of points in the set. In the second stage, the search space is shrunk to discard regions where the maximum is very unlikely to reside.
Such regions are obtained by finding points where
the UCB is lower than the LCB (the complement of
the colored region in the same panel as before). The
e To
remaining set of relevant points is denoted by R.
simplify the task of shrinking the search space, we
simply find an enclosing ball, which is denoted by R
in Algorithm 1. Back to the first stage, we consider
a lattice that is twice as dense as in the first stage of
the previous iteration, but we only sample at points
that lie within our new smaller search space.
In the second stage, the auxiliary step of approxie with the ball R introduces
mating the relevant set R
inefficiencies in the algorithm, since we only need to
e This can be easily remedied in
sample inside R.
practice to obtain an efficient algorithm. Our analysis will show that even without these improvements
it is already possible to obtain very strong exponential convergence rates. Of course, practical improvement will result in better constants and ought to be
considered seriously.
Note that Algorithm 1 terminates once the relevant
region becomes too small to intersect the lattice L.
Our analysis requires for the algorithm to sample

Regret Bounds for Gaussian Process Bandits with Deterministic Observations

Algorithm 1 Branch and Bound
Input: A compact subset D ‚äÜ Rd , a function f : D ‚Üí R and a discrete lattice L ‚äÜ D that is divisible by
powers of 2. Set R ‚Üê D and Œ¥ ‚Üê 1.
repeat
Sample Twice as Densely:
‚Ä¢ Œ¥ ‚Üê Œ¥/2.
‚Ä¢ Sample f at enough points in L so that every point in R is contained in a simplex of diameter Œ¥.
Shrink the Relevant Region:
‚Ä¢ Set


p
p
e
R := x ‚àà R ¬µT (x) + Œ≤T œÉT (x) > sup ¬µT (x) ‚àí Œ≤T œÉT (x) .
R

T is the number points sampled so far and Œ≤T = 2 ln





|L|T 2
= 4 ln T + 2 ln |L|
Œ±
Œ± with Œ± ‚àà (0, 1).
(x‚àó1 , x‚àó2 ) = argsup(x1 ,x2 )‚ààR√ó
e R
e kx1 ‚àí x2 k.

‚Ä¢ Solve the following constrained optimization problem:
 ‚àó

x1 + x‚àó2
‚Ä¢R‚ÜêB
, kx‚àó1 ‚àí x‚àó2 k , where B(p, r) is the ball of radius r centred around p.
2
until R ‚à© L = ‚àÖ
points from a fixed finite set of points, although we
can pick L to be the set of all points in D with
floating point coordinates.

GP GP (0, Œ∫(¬∑, ¬∑)) on D. Moreover, in what follows we will denote the global maximum by xM :=
argmax f (x) and the regret by r(xt ) = f (xM ) ‚àí
x‚ààD

3. Analysis
We begin our analysis by showing that, given sufficient explored locations, the posterior predictive
variance is small. Specifically, the following approximation result is proved in the supplementary material:
Proposition 1 (Variance Bound) Let Œ∫ : Rd √ó
Rd ‚Üí R be a kernel that is four times differentiable
along the diagonal {(x, x) | x ‚àà Rd }, with Q defined
as in part 2 of Lemma 5, and f ‚àº GP (0, Œ∫(¬∑, ¬∑)) a
sample from the corresponding GP. If f is sampled
at points x1:T = {x1 , . . . , xT } that form a Œ¥-cover of
a subset D ‚äÜ Rd , then the resulting posterior predictive standard deviation œÉT satisfies
sup œÉT ‚â§
D

QŒ¥ 2
.
4

3.1. Finiteness of regret
Having shown that the variance vanishes according
to the square of the resolution of the lattice of sampled points, we now move on to show that this estimate implies an exponential asymptotic vanishing of
the regret encountered by our Branch and Bound algorithm. This is laid out in our main theorem stated
below and proven in the supplementary material.
Recall that D ‚äÜ Rd is assumed to be a nonempty compact subset and f a sample from the

f (xt ). Also, by convention, for any set S, we will
denote its interior by S ‚ó¶ , its boundary by ‚àÇS and
if S is a subset of Rd , then conv(S) will denote its
convex hull. The following holds true:
Theorem 2 Suppose we are given:
1. Œ± > 0, a compact subset D ‚äÜ Rd , and Œ∫ a kernel
on Rd that is four times differentiable along the
diagonal;
2. f ‚àº GP(0, Œ∫) a continuous sample on D that
has a unique global maximum xM , which satisfies one of the following two conditions:
(‚Ä†) xM ‚àà D‚ó¶ and f (xM ) ‚àí c1 kx ‚àí xM k2 <
f (x) ‚â§ f (xM ) ‚àí c2 kx ‚àí xM k2 for all x satisfying x ‚àà B(xM , œÅ0 ) for some œÅ0 > 0;
(‚Ä°) xM ‚àà ‚àÇD and both f and ‚àÇD are smooth
at xM , with ‚àáf (xM ) 6= 0;
3. any lattice L ‚äÜ D satisfying the following two
conditions
‚Ä¢
‚Ä¢

2L ‚à© conv(L) ‚äÜ L

(4)

œÅ0
‚àí log2 diam(D)

e+1 L ‚à© L 6= ‚àÖ
2d
if f satisfies (‚Ä†)

(5)

Then, there exist positive numbers A and œÑ and an
integer T such that the points specified by the Branch
and Bound algorithm, {xt }, will satisfy the following
asymptotic bound: For all t > T , with probability
1 ‚àí Œ± we have
‚àí

r(xt ) < Ae

œÑt
(ln t)d/4

.

Regret Bounds for Gaussian Process Bandits with Deterministic Observations

Given the exponential rate of convergence we obtain in Theorem 2, we have the following finiteness
conclusion for the cumulative regret accrued by our
Branch and Bound algorithm:
Corollary 3 Given Œ∫, f ‚àº GP(0, Œ∫) and L ‚äÜ D as
in Theorem 2, the cumulative regret is bounded from
above.
Remark 4 It is worth pointing out the trivial observation that using a simple UCB algorithm ‚àö
with
monotonically increasing and unbounded factor Œ≤t ,
without any shrinking of the search space as we
do here, necessarily leads
‚àö to unbounded cumulative
regret since eventually Œ≤t becomes large enough
so that at points x0 far away from the maximum,
‚àö
Œ≤t œÉt (x0 ) becomes larger than f (xM ) ‚àí f (x). In
fact, eventually the UCB algorithm will sample every
point in the lattice L.
3.2. Remarks on the main theorem
This section includes a discussion of the assumptions
placed on the objective function in Theorem 2 as well
as an outline of the proof, the full details of which
are included in the appendix.
3.2.1. On the statement of Theorem 2
A few remarks on the assumptions and the conclusion of the main theorem are in order:
A. Relationship between the local and global
assumptions on f : The theorem has two seemingly unrelated restrictions on the function f : the
global GP prior and the local behaviour near the
global maximum. However, in many circumstances
of interest, the local condition follows almost surely
from the global condition. Two such circumstances
are if Œ∫ is a MateÃÅrn kernel with ŒΩ > 2 (including
the squared exponential kernel) or if Œ∫ is six times
differentiable. In either case, the sample f is twice
differentiable almost surely, in the former case by
(Adler & Taylor, 2007, Theorem 1.4.2) and (Stein,
1999, ¬ß2.6)) and in the latter situation by (Ghosal &
Roy, 2006, Theorem 5). If the global maximum xM
lies in the interior of D, the Hessian of f at xM will
almost surely be non-singular since the vanishing of
at least one of the eigenvalues of the Hessian is a codimension 1 condition in the space of all functions
that are smooth at a given point, hence justifying
condition (‚Ä†).
On the other hand, if xM lies on the boundary
of D, then condition (‚Ä°) will be satisfied almost
surely, since the additional event of the vanishing

of ‚àáf (xM ) is a codimension d phenomenon in the
space of functions with global maximum at xM .
B. Uniqueness of the global maximum: A randomly drawn continuous sample from a GP on a
compact domain will almost surely have a unique
global maximum: this is because the space of continuous functions on a compact domain that attain
their global maximum at more than one point have
codimension one in the space of all continuous functions on that domain.
C. Assumptions on L: The two conditions (4) and
(5) simply require that the lattice be ‚Äúdivisible by 2‚Äù
and that it be fine enough so that the algorithm can
sample inside the ball B(xM , œÅ0 ) when the maximum
of the function is located in the interior of the search
space D. One can simply choose L to be the set of
points in D that have floating point coordinates: it‚Äôs
just the points at which the algorithm is allowed to
sample the function.
D. On œÑ ‚Äôs dependence: Finally, it is important to
point out that the decay rate œÑ does not depend on
the choice of the lattice L, even though as stated,
the statement of the theorem chooses œÑ only after
L is specified. The theorem was written this way
simply for the sake of readability.
3.2.2. Outline of the proof of Theorem 2
The starting point for the proof is the observation
that one can use the posterior predictive mean and
standard deviation of the GP to obtain a high probability envelope around the objective function (cf.
Lemma 8 in the appendix). Given the fact that the
thickness of this envelope is determined by the height
of the posterior predictive standard deviation, œÉ, we
can use the bound given by Proposition 1 to show
that asymptotically one can rapidly dispense with
large portions of the search space, as illustrated in
Figure 2.
One disconcerting component of Algorithm 1 is the
step that requires sampling twice as densely in each
iteration, since the number of samples can start to
grow exponentially, hence killing any hope of obtaining exponentially decreasing regret. However,
this is where the assumption on the local behaviour
near the global maximum becomes relevant. Since
Proposition 1 tells us that every time the function is
sampled twice as densely, œÉ decreases by a factor of
4, and given our assumption that the function has
quadratic behaviour near the global maximum, we
can conclude that the radius of the search space is
halved after each iteration and so the number of sam-

Regret Bounds for Gaussian Process Bandits with Deterministic Observations

pled points added in each iteration roughly remains
constant. Of course,
this assumes that the multi‚àö
plicative factor Œ≤t remains constant
‚àö in this process.
However, the algorithm requires Œ≤t to grow logarithmically, and‚àöso to fill this gap we need to bound
the growth of Œ≤t , which is tied to the number of
samples needed in each iteration of the algorithm,
which in turn is linked to the resolution of the lattice of sampled points Œ¥ and the size of the relevant
‚àö
set R, which in turn depends on the size of Œ≤t œÉt .
This circular dependence gives rise to a difference
equation, whose solutions we bound by solving the
corresponding differential equation.
3.2.3. Further remarks on the GP prior
Let us step back for a moment and pose the question
of whether it would be possible to carry out a similar
line of reasoning under other circumstances. To answer this, one needs to identify the key ingredients
of the proof, which are the following:
A. A mechanism for calculating a high probability envelope around the objective function (cf.
Lemma 8);
B. An estimate showing that the thickness of the
envelope diminishes rapidly as the function is
sampled more and more densely (cf. Proposition 1), so that the search space can be shrunk
under reasonable assumptions on the behaviour
of the function near the peak.
The reason for our imposing a GP prior on f is that
it gives us property A, while our smoothness assumption on the kernel guarantees property B. However,
GPs are but one way one could obtain these properties and they do this essentially by coming up with
local estimates of the Lipschitz constant based on
the observed values of the objective function nearby.
Perhaps one could explicitly incorporate similar local estimates on the Lipschitz constant into tree
based approaches like HOO and SOO, cf. (Bubeck
et al., 2011) and (Munos, 2011), in which case one
would be able to dispense with the GP assumption
and get similar performance. But, that is beyond
the scope of this paper and will be left for future
work.

4. Discussion
In this paper we proposed a modification of the UCB
algorithm of (Srinivas et al., 2010) which addresses
the noise free case. The key difference is that while
1
the original algorithm achieves an O(t‚àí 2 ) rate of

convergence to the regret minimizer, we obtain an
exponential rate in the number of function evaluations. In other words, the noise free problem is
significantly easier, statistically speaking, than the
noisy case. The key difference is that we need not
invest any samples in noise reduction to determine
whether our observations deviate far from their expectation.
This allows us to discard pieces of the search space
where the maximum is very unlikely to be, when
compared to (Srinivas et al., 2010). We show that
this additional step leads to a considerable improvement of the regret accrued by the algorithm.
In particular, the cumulative regret obtained by
our Branch and Bound algorithm is bounded from
above, whereas the cumulative regret bound obtained in the noisy bandit algorithm is unbounded.
The possibility of dispensing with chunks of the
search space can also be seen in the works involving
hierarchical partitioning, e.g. (Munos, 2011), where
regions of the space are deemed as less worthy of
probing as time goes on.
Our results mirror the observation in active learning that noise free and large margin learning of half
spaces can be achieved much more rapidly than identifying a linear separator in the noisy case (Bshouty
& Wattad, 2006; Dasgupta et al., 2009). This is
also reflected in classical uniform convergence results
for supervised learning (Audibert & Tsybakov, 2007;
Vapnik, 1998) where the achievable rate depends on
the decay of probability mass near the margin.
This suggests that the ability to extend our results
to the noisy case is somewhat limited. An indication of what might be possible can be found in (Balcan et al., 2009), where regions of the version space
are eliminated once they can be excluded with sufficiently high probability. One could model a corresponding Branch and Bound algorithm, which dispenses with points that lie outside the current (or
perhaps the previous) relevant set when calculating
the covariance matrix K in the posterior equations
(3). Analysis of how much of an effect such a computational cost-cutting measure would have on the
regret encountered by the algorithm is a subject of
future research.

Acknowledgements
We are very grateful to the anonymous reviewers for
outstanding feedback. This research was supported
by NSERC and the Institute for Computing, Information and Cognitive Systems (ICICS) at UBC.

Regret Bounds for Gaussian Process Bandits with Deterministic Observations

References
Adler, R. J. and Taylor, J. E. Random Fields and
Geometry. Springer, 2007.

Hoffman, M., Brochu, E., and de Freitas, N. Portfolio allocation for Bayesian optimization. In UAI,
pp. 327‚Äì336, 2011.

Audibert, J.-Y. and Tsybakov, A. B. Fast learning
rates for plug-in classifiers. Annals of Statistics,
35(2):608‚Äì633, 2007.

Hutter, F., Hoos, H. H., and Leyton-Brown, K. Automated configuration of mixed integer programming solvers. In Proceedings of CPAIOR-10, pp.
186‚Äì202, 2010.

Balcan, M.-F., Beygelzimer, A., and Langford, J.
Agnostic active learning. J. Comput. Syst. Sci, 75
(1):78‚Äì89, 2009.

Lizotte, D. Practical Bayesian Optimization. PhD
thesis, University of Alberta, Canada, 2008.

Brochu, E., de Freitas, N., and Ghosh, A. Active
preference learning with discrete choice data. In
NIPS, pp. 409‚Äì416, 2007.
Brochu, E., Cora, V. M., and de Freitas, N. A tutorial on Bayesian optimization of expensive cost
functions, with application to active user modeling and hierarchical reinforcement learning. Technical Report TR-2009-023, arXiv:1012.2599v1,
UBC CS department, 2009.
Bshouty, N. H. and Wattad, E. On exact learning
halfspaces with random consistent hypothesis oracle. In International Conference on Algorithmic
Learning Theory, pp. 48‚Äì62, 2006.
Bubeck, S., Munos, R., Stoltz, G., and Szepesvari,
C. X-armed bandits. Journal of Machine Learning
Research, 12:1655‚Äì1695, 2011.
Bull, A. D. Convergence rates of efficient global optimization algorithms. Journal of Machine Learning
Research, 12:2879‚Äì2904, 2011.
Dasgupta, S., Kalai, A. T., and Monteleoni, C. Analysis of perceptron-based active learning. Journal
of Machine Learning Research, 10:281‚Äì299, 2009.
Garnett, R., Osborne, M. A., and Roberts, S. J.
Bayesian optimization for sensor set selection. In
ACM/IEEE International Conference on Information Processing in Sensor Networks, pp. 209‚Äì
219. ACM, 2010.
Ghosal, S. and Roy, A. Posterior consistency of
Gaussian process prior for nonparametric binary
regression. Ann. Stat., 34:2413‚Äì2429, 2006.
Gramacy, R. B., Lee, H. K. H., and MacReady, W.
Parameter space exploration with Gaussian process trees. In ICML, pp. 353‚Äì360, 2004.
Hansen, P., Jaumard, B., and Lu, S. Global optimization of univariate Lipschitz functions: I. survey and properties. Mathematical Programming,
55:251‚Äì272, 1992.

Martinez‚ÄìCantin, R., de Freitas, N., Brochu, E.,
Castellanos, J., and Doucet, A. A Bayesian
exploration-exploitation approach for optimal online sensing and planning with a visually guided
mobile robot. Autonomous Robots, 27(2):93‚Äì103,
2009.
May, B., Korda, N., Lee, A., and Leslie, D. Optimistic Bayesian sampling in contextual-bandit
problems. 2010.
MocÃåkus, J. The Bayesian approach to global optimization. In System Modeling and Optimization,
volume 38, pp. 473‚Äì481. Springer, 1982.
Munos, R. Optimistic optimization of a deterministic function without the knowledge of its smoothness. In NIPS, 2011.
Rasmussen, C. E. and Williams, C. K. I. Gaussian
Processes for Machine Learning. The MIT Press,
2006.
Schonlau, M., Welch, W. J., and Jones, D. R. Global
versus local search in constrained optimization of
computer models. Lecture Notes-Monograph Series, 34:11‚Äì25, 1998.
Srinivas, N., Krause, A., Kakade, S. M., and Seeger,
M. Gaussian process optimization in the bandit
setting: No regret and experimental design. In
ICML, 2010.
Stein, M. L. Interpolation of Spatial Data: Some
Theory for Kriging. Springer, 1999.
Steinwart, I. and Christmann, A. Support Vector
Machines. Springer, 2008.
Vapnik, V. Statistical Learning Theory. John Wiley
and Sons, New York, 1998.
Vazquez, E. and Bect, J. Convergence properties of
the expected improvement algorithm with fixed
mean and covariance functions. J. of Statistical
Planning and Inference, 140:3088‚Äì3095, 2010.

