subsets. The joint distribution over all the variables is
then approximated by the product of marginal distributions over the factors. This approach relies on the
intuition that a complex system can be decomposed
into weakly interacting subsystems. But what exactly
does weak interaction mean? A standard way to frame

In an earlier paper [Pfeffer, 2001], we presented a new
notion of weak interaction based on a different question. The new question is, when are the factor distributions obtained using the BK approach correct?
This may occur even though the joint distribution is
far from being a product of the factor distributions.
We defined a criterion called separability, saying that
the conditional probability distribution for a factor is
separable if it can be decomposed additively into terms
depending on each of the factors individually. We
showed that if the factors are separable, marginal distributions can correctly be propagated from one time
step to the next. However, observations break this
property, so the BK approach with separable models
only produces exactly correct marginals for prediction,
not monitoring.

That paper left open two important questions. The
first is whether separable models actually perform well
at the monitoring task. The reason to believe that
might be the case is that they correctly propagate
marginal distributions through the dynamics, and only
introduce error in conditioning on observations, unlike
non-separable models which have both kinds of error.
The second question asks whether approximating the
separability property can lead to more natural models
that still have good performance. This paper addresses
both these questions.
In this paper we define approximate separability, and
show that it is a natural property that can hold for real
models. We show experimentally, on some simple examples, that separability and approximate separability
indeed lead to good performance on both the prediction and monitoring tasks. Thus approximate separability, at least on these examples, appears to be a
sufficient condition for making BK work well. We also
show that separability can be used to produce a better factorization of a system than the “obvious” one.
We analyze the structure of approximately separable
models, presenting a method for automatically determining the degree of separability of a conditional probability distribution. Finally, we address the question
of why separable and approximately separable models perform so well on the monitoring task. First we
present a bound on the monitoring error of a separable model for a simple special case. We then define
two different possible sources of error that may arise
using the BK approach, and show that one of them is
much less important than the other. As it turns out,
separable models only have the less important kind of
error in the simple examples studied, which goes some
way towards explaining why they perform well in these
examples.

2

Preliminaries

We will assume that the reader is familiar with
dynamic Bayesian networks (DBNs) [Dean and
Kanazawa, 1989; Murphy, 2002]. DBNs provide a
compact representation of a dynamic model. However, monitoring in DBNs is difficult, because even
though variables are conditionally independent in the
short run they generally become dependent in the long
run. As a result, in order to do exact monitoring one
needs to maintain a joint distribution over all the state
variables, which is infeasible. Therefore approximate
monitoring algorithms are needed. One approach to
approximate monitoring was introduced by Boyen and
Koller [1998]. In their approach, the set of variables
X is divided into factors X1 , ..., Xm , each of which

consists of a subset of the variables.1 Instead of maintaining a complete joint belief state µ(X), marginal
distributions µ̂Xi (Xi ) are maintained. The marginals
are propagated from one time step to the next using a junction tree constructed from the two-time-slice
model of the DBN, in which every factor Xi and X−
i
is contained in a clique. (The notation X−
i means the
variable Xi at the previous time point.)QThe joint dism
tribution is approximated as µ̂(X) = i=1 µ̂Xi (Xi ).
BK provide a bound on the expected KL distance between µ and µ̂, and show that it does not grow over
time.
In a previous paper [Pfeffer, 2001], we asked under
what circumstances does the BK approach work well.
Rather than focus on the difference between µ and µ̂,
however, we asked when is the marginal µ̂Xi equal to
the true marginal µXi . To answer this question, we
introduced notions of sufficiency and separability.
Definition 2.1: Let X,Y and Z be three variables,
and let P (Z|XY ) be given. P (Z|XY
) defines a mapP
ping Φ : ∆XY → ∆Z : Φ(π) = xy π(xy)P (Z|xy). X
and Y are sufficient for Z if Φ depends only on the
marginals of π over X and Y .
In other words, sufficiency says that if we want to know
P (Z), we only need to know π(X) and π(Y ) and do
not need to know the joint π(XY ). Separability is
defined as follows:
Definition 2.2 : P (Z|XY ) is separable if there exist γ and conditional probability distributions (CPDs)
PX and PY , such that P (Z|XY ) = γPX (Z|X) + (1 −
γ)PY (Z|Y ).
These definitions can naturally be generalized to more
than two parents. It turns out that these two properties are equivalent: X and Y are sufficient for Z if
and only if P (Z|XY ) is separable. Note that there is
no requirement that γ be between 0 and 1. This is in
contrast to the previous paper, which required that γ
be between 0 and 1. The statement that separability
and sufficiency are equivalent definitely holds for the
definition given here. The proof in [Pfeffer, 2001] implies that it also holds for the stricter definition. The
two statements together imply that whenever a CPD
has a separable decomposition with gamma not between 0 and 1, it also has one with gamma between 0
and 1. Unfortunately we have not been able to prove
1

In this paper, for the sake of ease of presentation and
analysis, we assume that the factors are disjoint. However the ideas in this paper can easily be extended to nondisjoint factors. Indeed in the previous paper we defined
a notion of conditional separability that can be applied to
non-disjoint factors. This can naturally be extended to
approximate conditional separability.

this directly, which leaves open some doubt about the
correctness of the proof in the previous paper.
Separability is closely related to context-specific independence (CSI) [Boutilier et al., 1996]. It can be
viewed as CSI in which the context variable does not
explicitly appear in the model. Separable models are
also closely related to the Influence Model [Asavathiratham, 2000].

will not be fully separable. Thus one may ask whether
an approximate notion of separability leads to similar
levels of performance to full separability.
Definition 3.1: P (Z|XY ) is α-separable if there exist
0 ≤ α ≤ 1, γ, and CPDs PX , PY and PXY , such that
P (Z|XY )

=

+(1 − α)PXY (Z|XY )

These ideas apply to the BK approach to monitoring
in DBNs as follows:
Definition 2.3: The set of factors X1 , ..., Xm is self−
−
sufficient if ∪m
i=1 Xi = X, and for each i, X1 , ..., Xm
are sufficient for Xi .
Thus if the CPD for each factor is separable in terms
of the factors at the previous time step, the factors are
self-sufficient. Separability can be understood as characterizing the way information flows about a dynamic
system. Intuitively, it says that information may flow
from any of the factors at the previous time step to
a factor at the current time step, but at any point in
time only one of the factors is selected and information
only flows from that one.
Given a self-sufficient set of factors, we can correctly
propagate marginal probabilities from one time step
to the next. Unfortunately, however, this is only true
for propagating the marginals through the dynamics.
Conditioning on observations breaks the property of
sufficiency. The reason is that if we have factors X1
and X2 and have an observation about X2 , we need to
know what we can infer about X1 . To know this, it is
not enough to know the marginals over X1 and X2 ; we
also need to know their joint distribution. In general
we do not have the correct joint probability distribution between the factors using the BK approach, even
for separable models. Thus propagating marginals
with separable models is exact for the prediction task,
where there are no observations about the future, but
not for the monitoring task where we do have observations.

3

Approximate separability

These ideas lead naturally to two questions. The first
question is, given the fact that separable models lead
to correct propagation of marginals through the dynamics, do they also lead to more accurate monitoring? One may hope that because they are only subject
to error when conditioning on observations, the total
monitoring error will be lower. The second question
addresses the problem of defining separable models in
practice. While the previous paper gave some examples of separable models, in general real-world models

α(γPX (Z|X) + (1 − γ)PY (Z|Y ))

Trivially every CPD is 0-separable, and if it is αseparable then it is also β-separable for β < α.
Definition 3.2 : The degree of separability of
P (Z|XY ) is the maximum α for which P (Z|XY ) is
α-separable.
Similar to separability, we can understand approximate separability intuitively as characterizing the flow
of information about the system. If P (Z|XY ) is highly
separable, it says that while information can flow from
both X and Y to Z, most of the time it only flows from
one of them or the other. While it may just as often
flow from X or from Y (e.g. if γ = 1/2), on only rare
occasions does information flow from both of them together. Approximate separability can also be viewed
as an approximate form of CSI.
Example 3.3: The following is an example of an approximately separable decomposition:
X
X−
F
F
T
T

F
.9
.99
.1
.01

Y−
F
T
F
T

T
.1 =
.01
.9
.99
X

X
.91

X−
F
T

F
.989
.011

T
.011
.989

+

.09

X−
F
F
T
T

Y−
F
T
F
T

F
0
1
1
0

T
1
0
0
1

In this example, X tends to take on the same value as
X − with high probability. However, Y − also has an
influence, and the influence of Y − is flipped depending
on X − . If X − is F, Y − being T makes X more likely
to be F, but if X − is T, Y − being T makes X more
likely to be T. The fact that in the decomposition there
is no PY is a coincidence. However the fact that PXY
is a XOR model is not.
In fact, approximate separability is quite a natural
property that holds for many models.

Definition 3.4 : P (X|X − Y − ) is κ-persistent if
P (x|x− y − ) ≥ κ whenever x = x− , for all values y −
of Y − .
Persistence is the property that things tend to stay the
same as before. This is a natural property of dynamic
systems. It does not say that a different factor has only
a small influence on a given factor in relative terms —
X may be many times more likely to change given
some values of Y − than others.
Proposition 3.5: If P (X|X − Y − ) is κ-persistent, it
is κ-separable.
Proof:
P (X|X − Y − ) can be decomposed into
−
κI(X|X ) + (1 − κ)PXY (X|X − Y − ) where I is the
identity CPD, and
(
P (x|x− y − )
if x 6= x−
− −
1−κ
PXY (x|x y ) =
P (x|x− y − )−κ
if x = x−
1−κ
By the definition of κ-persistence,
0P≤ PXY (x|x− y − ) ≤ 1, and
1−κ
− −
x PXY (x|x y ) = 1−κ = 1.

Note that the converse does not hold: P (X|X − Y − )
may be much more than κ-separable, and there are
many ways to get approximately separable models
other than through persistence.

4

Experiments

Figure 1 (a) shows the performance of various models
on the prediction task with no observations. The xaxis shows the degree of non-separability of the model:
the left endpoint is a fully separable model, while the
right endpoint is a completely non-separable model.
The experiments are performed on a small model with
two factors X and Y and a variable Z which is a noisy
observation of Y . The figure shows the KL-distance
between the true marginal over X and the approximate marginal obtained using the BK approach, averaged over 10000 runs with random parameters. As
the theory predicts, fully separable models have zero
error. What is interesting, though, is the shape of the
curve. We see that approximately separable models
also have very low error; even 50%-separability leads
to good performance.
Figure 1 (b) shows the performance of the different
models on the monitoring task. Again the results are
averaged over 10000 runs. Here the news is very good.
Although the theory does not predict that separable
models will perform well, in fact they have very small
error. The curve has a similar shape; again approximately separable models have small error.

One natural hypothesis as to why separable models
perform so well in these examples is that the factors are
less dependent on each other. This turns out not to be
the case. Figure 1 (c) shows the degree of dependence
between the factors, measured by the KL distance between the true joint distribution and the product of
its marginals. This is computed over the same kind
of models as for Figure 1 (a) and Figure 1 (b). The
lowest degree of dependence is actually attained for
models with an intermediate degree of separability.
U−

V−

W−

X−

Y−

Z−

U

V

W

X

Y

Z

Figure 2: A six node DBN
Example 4.1 : This example shows how using separability can lead to much better factorization of
a dynamic system. Figure 2 shows the structure
of a six-node DBN. The variables U ,V and W are
all dependent on U − ,V − and W − ; similarly X,Y
and Z are all dependent on X − ,Y − and Z − . The
only other interactions are between W − and X and
between X − and W . This structure suggests a
natural factorization into two factors: U V W and
XY Z. However, suppose the CPD of X is as follows:
X
X−
F
F
F
F
F
F
F
F
T
T
T
T
T
T
T
T

Y−
F
F
F
F
T
T
T
T
F
F
F
F
T
T
T
T

Z−
F
F
T
T
F
F
T
T
F
F
T
T
F
F
T
T

W−
F
T
F
T
F
T
F
T
F
T
F
T
F
T
F
T

F
0.9
0.5
0.7
0.3
0.7
0.3
0.5
0.1
0.5
0.9
0.3
0.7
0.3
0.7
0.1
0.5

T
0.1
0.5
0.3
0.7
0.3
0.7
0.5
0.9
0.5
0.1
0.7
0.3
0.7
0.3
0.9
0.5

This CPD is highly separable in terms of X − W −
and Y − Z − , but highly non-separable in terms of
X − Y − Z − and W − . (The reason will be explained
in Section 5. Note that this example is not highly
persistent.) If the other CPDs are similarly designed,
it turns out that {U V, W X, Y Z} are a self-sufficient
set of factors, while {U V W, XY Z} are not. This suggests that {U V, W X, Y Z} may be a better factorization. Note that it is not the case that Y − and Z −
together have smaller influence on X than W − . There-

0.12
0.1

0.01

0.016

0.009

0.014
Degree of dependence

0.008
0.007

0.08
Error

Error

0.006
0.06

0.005
0.004

0.04

0.003

0.012
0.01
0.008
0.006
0.004

0.002

0.02

0.002

0.001
0

0
0

0.2

0.4

0.6

0.8

1

Degree of non-separability

0
0

0.2

0.4

0.6

Degree of non-separability

(a)

0.8

1

0

0.2

0.4

0.6

0.8

1

Degree of non-separability

(b)

(c)

Figure 1: Error as a function of degree of non-separability: (a) without observations; (b) with observations; (c)
degree of dependence between the factors.
fore there is no reason to prefer this factorization based
on considerations of degree of influence.
The monitoring results, with Z being observed, are
as follows: the average absolute error in P (U = T )
for the {U V W, XY Z} factorization is 0.038, while for
the {U V, W X, Y Z} factorization it is 0.018. Similarly the average KL distance in the marginal of U
is 0.007 for the {U V W, XY Z} factorization and 0.002
for the {U V, W X, Y Z} factorization. Thus a factorization based on separability can lead to much better
performance than the “obvious” one.

5

Stucture of approximately separable
decompositions

Two natural questions arise immediately: what do approximately separable models look like, and why do
they perform well, at least on the examples studied? In
this section we analyze the structure of approximately
separable decompositions, by presenting a method to
determine the degree of separability of a model. The
notation zi refers to the i-th possible value of Z. The
goal is, given a CPD P (Z|XY ), to find an optimal approximately separable decomposition. In general, this
can be formulated as a linear programming problem:
max
s.t.

α 1 + α2
α1 pij + α2 qik + α3 rijk = P (zi |xj yk )
α1 + α 2 + α 3 = 1
α3 ≥ 0
0 ≤ pij ≤ 1
0 ≤ qik ≤ 1
0P
≤ rijk ≤ 1
Pi pij = 1
P i qik = 1
i rijk = 1

∀i, j, k

∀i, j
∀i, k
∀i, j, k
∀j
∀k
∀j, k

From the solution, we can identify α = α1 + α2 ,
γ = αα1 , PX (zi |xj ) = pij , PY (zi |yk ) = qik , and
PXY (zi |xj yk ) = rijk . Note that there is no constraint
that α1 or α2 be greater than 0. This is because γ is

not constrained to be between 0 and 1. However we do
have a constraint that α3 ≥ 0. Since α is the degree
of separability, it does not make sense to say that a
model is more than completely separable. In special
cases, the program can be solved directly. Examining
these special cases yields insights into the structure of
approximately separable decompositions.
Case 1: X,Y and Z are binary.
We can write down the equations defining αseparability as
P (z1 |x1 , y1 ) = αγPX (z1 |x1 ) + α(1 − γ)PY (z1 |y1 )
+ (1 − α)PXY (z1 |x1 y1 )

(1)

P (z1 |x1 , y2 ) = αγPX (z1 |x1 ) + α(1 − γ)PY (z1 |y2 )
+ (1 − α)PXY (z1 |x1 y2 )

(2)

P (z1 |x2 , y1 ) = αγPX (z1 |x2 ) + α(1 − γ)PY (z1 |y1 )
+ (1 − α)PXY (z1 |x2 y1 )

(3)

P (z1 |x2 , y2 ) = αγPX (z1 |x2 ) + α(1 − γ)PY (z1 |y2 )
+ (1 − α)PXY (z1 |x2 y2 )

(4)

Since P (z2 |xj yk ) = 1 − P (z1 |xj yk ), the corresponding
equations for z2 automatically hold when these hold.
Subtracting (1) from (2) and (3) from (4), we get
P (z1 |x1 y2 ) − P (z1 |x1 y1 )
= α(1 − γ)(PY (z1 |y2 ) − PY (z1 |y1 ))
+ (1 − α)(PXY (z1 |x1 y2 ) − PXY (z1 |x1 y1 ))

(5)

P (z1 |x2 y2 ) − P (z1 |x2 y1 )
= α(1 − γ)(PY (z1 |y2 ) − PY (z1 |y1 ))
+ (1 − α)(PXY (z1 |x2 y2 ) − PXY (z1 |x2 y1 ))

(6)

We see immediately that for a fully separable model,
it must hold that
P (z1 |x1 y2 ) − P (z1 |x1 y1 ) = P (z1 |x2 y2 ) − P (z1 |x2 y1 ) (7)
In other words, Y must have exactly the same effect
on Z, no matter what the state of X. If we examine
the CPD in Figure 4.1, we see that this holds for the
decomposition into {X − W − } and {Y − Z − }, but not
for the decomposition into {W − } and {X − Y − Z − }. If

(7) does not hold, we must have α < 1. In this case,
subtracting (5) from (6), we get
P (z1 |x2 y2 ) − P (z1 |x2 y1 )
−P (z1 |x1 y2 ) + P (z1 |x1 y1 )
= (1 − α)(PXY (z1 |x2 y2 ) − PXY (z1 |x2 y1 )
− PXY (z1 |x1 y2 ) + PXY (z1 |x1 y1 ))

(8)

If the left hand side of (8) is positive, α is maximized
when we set
PXY (z1 |x2 y2 ) = PXY (z1 |x1 y1 ) = 1
PXY (z1 |x2 y1 ) = PXY (z2 |x1 y2 ) = 0
α = 1 − 21 (P (z1 |x2 y2 ) − P (z1 |x2 y1 )
− P (z1 |x1 y2 ) + P (z1 |x1 y1 ))
We get a symmetric result when the left hand side of
(8) is negative. We see that PXY must either be an
equality test or a XOR of X and Y . Thus the pattern
in Example 3.3 is no coincidence. Once we have the
value of α, we can recover γ, PY and PX . It can be
shown that there always exists a feasible solution for
these variables, though γ might have to be negative.
Case 2: X and Y are binary, while Z has n values.
We can write down equations (1i ) to (4i ) similar to (1)
to (4) above for all values zi of Z. We then subtract
(1i ) from (2i ) and (3i ) from (4i ) to get (5i ) and (6i ).
We then subtract (5i ) from (6i ) to get (8i ), as above:
P (zi |x2 y2 ) − P (zi |x2 y1 )
−P (zi |x1 y2 ) + P (zi |x1 y1 )
= (1 − α)(PXY (zi |x2 y2 ) − PXY (zi |x2 y1 )
− PXY (zi |x1 y2 ) + PXY (zi |x1 y1 ))
Let Ai be the left hand side of (8i ). Now, the expression
PXY (zi |x2 y2 ) − PXY (zi |x2 y1 )−
PXY (zi |x1 y2 ) + PXY (zi |x1 y1 )
must be proportional to Ai . Furthermore, if we consider only positive Ai , in order to maximize α we must
maximize this expression. To do this, we first set
PXY (zi |x2 y1 ) = PXY (zi |x1 y2 ) = 0
We must then maximize PXY (zi |x2 y2 ) and
PXY (zi |x1 y1 ) subject to the constraints
that their
P
sum P
is proportional to Ai , and i PXY (zi |x2 y2 ) ≤ 1
and i PXY (zi |x1 y1 ) ≤ 1. This is achieved by setting
P
G = i:Ai >0 Ai
PXY (zi |x2 y2 ) = PXY (zi |x1 y1 ) =
α=1− G
2

Ai
G

P
Now let us consider negative Ai . Since i P (zi |xj yk )
must be 1, it follows that the sum of the positive Ai

equals the absolute sum of the negative Ai . Therefore
we can use the same G, and set, symmetrically
PXY (zi |x2 y2 ) = PXY (zi |x1 y1 ) = 0
Ai
PXY (zi |x1 y2 ) = PXY (zi |x2 y1 ) = −
G
Intuitively, Ai is the degree of deviation from the situation in which the influence of X does not depend
on the state of Y (and vice versa). α is determined
by G, which is the sum of all the positive deviations.
Therefore the deviations for different states of Z sum
together to limit the degree of separability. It would
seem, at first sight, that if Z has more states, there
will be more deviations to sum together, and it will
be more difficult to attain a high degree of separability. However, when Z has many states, the probability mass for P (Z|XY ) will be divided up among more
states, so we would expect the individual deviations to
be smaller. Thus in balance, it should be equally difficult to achieve a high degree of separability whether
Z has few or many states.
Case 3: X and Z are binary, while Y has n values.
We write down the following equations for each yk :
P (z1 |x1 yk ) = α(γPX (z1 |x1 ) + (1 − γ)PY (z1 |yk ))
+ (1 − α)PXY (z1 |x1 yk )
P (z1 |x2 yk ) = α(γPX (z1 |x2 ) + (1 − γ)PY (z1 |yk ))
+ (1 − α)PXY (z1 |x2 yk )
By subtracting, we get for k = 1, ..., n − 1
P (z1 |x1 yk+1 ) − P (z1 |x1 yk )
= α(1 − γ)(PY (z1 |yk+1 ) − PY (z1 |yk ))
+ (1 − α)(PXY (z1 |x1 yk+1 ) − PXY (z1 |x1 yk ))
P (z1 |x2 yk+1 ) − P (z1 |x2 yk )
= α(1 − γ)(PY (z1 |yk+1 ) − PY (z1 |yk ))
+ (1 − α)(PXY (z1 |x2 yk+1 ) − PXY (z1 |x2 yk ))
Subtracting again, we get
P (z1 |x2 yk+1 ) − P (z1 |x2 yk )
−P (z1 |x1 yk+1 ) + P (z1 |x1 yk )
= (1 − α)(PXY (z1 |x2 yk+1 ) − PXY (z1 |x2 yk )
− PXY (z1 |x1 yk+1 ) + PXY (z1 |x1 yk ))
For k = 1, ..., n − 1, let
Ak = P (z1 |x2 yk+1 ) − P (z1 |x2 yk )
− P (z1 |x1 yk+1 ) + P (z1 |x1 yk )

(9)

Intuitively, Ak is the deviation from the situation in
which the influence of X does not depend on whether
Y is equal to yk or yk+1 . We get to choose, for k =
1, ..., n
Bk = PXY (z1 |x2 yk ) − PXY (z1 |x1 yk )

Now, if we choose B1 , that determines all the other
Bk , because
Ak

= (1 − α)(Bk+1 − Bk )

from which
Bk+1 = Bk −

Ak
.
1−α

(10)

We can now derive constraints on the Bk , which will
lead in particular to constraints on B1 . Using the fact
that Bk must be between −1 and 1, we first get for Bn
−1 ≤ Bn ≤ 1
Then, for Bn−1 , we combine the constraint on Bn using (10), and add a new direct constraint:
−1
−1

≤
≤

Bn−1
n−1
Bn−1 − A1−α

≤ 1
≤ 1

Then, working our way down to B1 , we add one new
constraint at each step, finally getting
−1
−1

≤
≤

B1
A1
B1 − 1−α
...
P

≤
≤

1
1

influence would accumulate and α would necessarily
be reduced. If that were the case, the degree of separability would be inversely proportional to the number
of values of Y . Instead, α depends on the maximum
and minimum deviations. We would still expect α to
decrease with the number of values of Y , but not to
the same degree.

6
6.1

Why separable models perform well
Error bound

To attempt to answer the question of why separable
models appear to perform so well, we first prove a
bound on the expected error for a very special case
of separable models. These are models with two hidden binary variables X and Y , that both depend on
X − and Y − , and a binary observation Z that depends
only on Y . P (X | X − Y − ) and P (Y | X − Y − ) are
separable, with parameterization:
Y
X
(X | Y − )
(X | X − )+(1−γX )PX
P (X | X − Y − ) = γX PX

P (Y | X − Y − ) = γY PYX (Y | X − )+(1−γY )PYY (Y | Y − )
(11)

P (Z | Y ) = PZ (Z | Y )

n−1

−1

≤

B1 −

Ak
k=1
1−α

≤

1

It is clear that if all the constraints on B1 are satisfied,
then so are the constraints on higher Bk , so we only
have to satisfy those. Now, let Ck = A1 + . . . + Ak ,
i.e. Ck is a partial sum of deviations. Each Ck , from
k = 0 to n − 1 appears in one of the constraints (11).
The maximum positive Ck and the minimum negative
Ck place the strongest constraints on B1 . Let C ∗ =
n−1
n−1
Ck ).
Ck ), and C∗ = − min(0, mink=1
max(0, maxk=1
If we ensure that
B1 −
B1 +

C∗
1−α
C∗
1−α

≥ −1
≤1

(12)

all the constraints will be satisfied. α is maximized
when ∗constraints (12) are
tight. This occurs when α =
C ∗ −C∗
∗
1 − C +C
and
B
=
∗
1
2
C +C∗ .
To see why this result makes sense, note that from (9),
Ck = P (z1 |x2 yk+1 ) − P (z1 |x2 y1 )
− P (z1 |x1 yk+1 ) + P (z1 |x1 y1 )
This is the degree to which the influence of X changes
when Y is equal to yk+1 compared to y1 . It is the maximum and minimum of these changes that determines
α. The interesting, and encouraging, thing is that it is
not the sum of the positive, and absolute sum of the
negative, deviations that determine α. We might have
thought that if there were several positive Ak , their

The error bound depends on a number of quantities
characterizing the system. We define the following
quantities that characterize the degree to which the
value of the parent X or Y at the previous time step
influences the child at the current time step:
λX
X
λYX
λX
Y
λYY

X
X
= |PX
(x2 | x2 ) − PX
(x2 | x1 )|
Y
Y
= |PX (x2 | y2 ) − PX (x2 | y1 )|
= |PYX (y2 | x2 ) − PYX (y2 | x1 )|
= |PYY (y2 | y2 ) − PYY (y2 | y1 )|

We also define the following quantity that characterizes the degree to which the evidence is informative:
λZ

=

|PZ (z2 | y2 ) − P (z2 | y1 )|

The following quantities also turn out to be useful:
λX
XY

=

λYXY

=

ζX
ζY

=
=

X
PX
(x2 | x2 )PYX (y2 | x2 )
X
−PX
(x2 | x1 )PYX (y2 | x1 )
Y
PX (x2 | x2 )PYY (y2 | x2 )
Y
−PX
(x2 | x1 )PYY (y2 | x1 )
X
X
X
max(λX , λZ (λX
X − 2λXY + 2λY ))
Y
Y
Y
max(λX , λZ (λX − 2λXY + 2λYY ))

Let µ denote P (X = T, Y = T ) under the true
joint distribution at a particular time point. Let
µX = P (X = T ) and µY = P (Y = T ) under the true
marginals at the time point, and let µ̂X = P̂ (X = T )
and µ̂Y = P̂ (Y = T ) under the approximate marginals

X−

X

X−

Y−

Y

Y−

(a)

X
Y
(b)

Figure 3: Types of move for simple separable model
maintained by the BK algorithm. We define two error measures. The first, called the joint error, denoted
δ, is the `∞ distance (i.e. max norm) between the
true joint distribution at the current time point and
the product of its marginals. The second, called the
marginal error, denoted δX (respectively δY ), is the
`∞ distance between the true marginal over X (resp.
Y ) at the time point and the approximate marginal
produced by the BK method.
In the following theorem, the bounding expressions are
complicated and not all that interesting. However they
do provide quite good bounds on the marginal error.
Also, the intuition behind the proof is informative, and
the key lemmas describe how the error develops.
Theorem 6.1: For a system as defined above, at all
time points, taking the expectation over all observation
sequences up to the time point,
E[δ]
≤ H
E[δX ] ≤ J
E[δY ] ≤ K
where
H

=

J

=

K
L
M
N

=
=
=
=

O
P

=
=

(1−λ2Z )L
4(1−(1−λ2Z )M )
2H(1−(1−γY )λY
Y )λZ M
N
X
2HγY λY λZ M
N
Y Y
X
γ X γY λX
X λY + (1 − γX )(1 − γY )λX λY
X Y
Y X
γX (1 − γY )λX λY + (1 − γX )γY λX λY
(1 − (1 − γY )λYY )(1 − γX O)
−(1 − γX )γY λX
Y P
γY ζ X + (1 − γY )λX
X
γY λYX + (1 − γY )ζ Y

provided the errors at the initial time point are at most
these quantities.
Based on this theorem, we can produce an average
bound on the error in the marginal over X, taken over
10000 random parameterizations, of 0.000662. This
compares with a true average error of 0.000240. Thus
the bound is less than three times the actual error.
The proof of this result is based on the idea that in a
separable model, the system makes two kinds of moves,
illustrated in Figure 3. In the first kind of move, both
variables at the current time point depend on the same
variable at the previous time point. In the second kind

of move, they both depend on different variables at the
previous time point. In addition to the moves shown
in the figure, there are two more symmetric moves.
While any of the four moves may be chosen at any
point in time, only one of the moves will actually be
chosen, and the process switches between the different kinds of moves. The key point is that in the first
kind of move, all previous joint error is forgotten, because only one variable at the previous time point affects the current state. In the second type of move,
new marginal error may be introduced by ignoring dependencies between the variables when conditioning
on the observation. However, no new joint error will
be introduced. These two effects combine to keep the
marginal error small. In particular, in each kind of
move the error is characterized as follows:
Lemma 6.2: For the first kind of move, taking the
X
2
expectation over observations, E[δ] ≤ 41 λX
X λY (1−λZ ).
Thus the joint error after the first kind of move does
not depend on the previous error, and the new error
depends on the degree to which the old parent X influences both X and Y . This makes sense. The more
both X and Y depend on the previous X, the more dependence will be introduced between them. The new
error is also less the more informative the observation.
This is somewhat surprising, but makes sense when we
consider that perfectly informative evidence serves to
completely decouple X from Y .
Lemma 6.3 : For the first kind of move, E[δX ] ≤
−
ζ X δX
.
Lemma 6.4 : For the first kind of move, E[δY ] ≤
−
λX
Y δX .
Thus the new marginal error of both X and Y depends
on the old marginal error of X. The weaker the influence of the old X, the more the old error is forgotten.
As it turns out, the marginal error of X depends on a
more complicated expression, but λX
X is still one of its
main components.
Lemma 6.5 : For the second kind of move, E[δ] ≤
−
2
Y
λX
X λY (1 − λZ )δ .
Thus the new joint error depends on the old joint error. The less the influence of the old parents on their
respective children, and the more informative the observation, the quicker the old error is forgotten.
Lemma 6.6: For the second kind of move, E[δX ] ≤
Y
−
X −
2λX
X λY λZ δ + λ X δX .
The new marginal error of X is made up of two components. One component results from the previous

joint error, i.e. it results from ignoring the previous
dependencies between the factors. This component decreases with the lack of influence of the old parents on
their respective children, and the uninformativeness of
the observation. Note that the effect of the observation
is different from before. Now uninformative observations lead to lower error. The second component of
the marginal error is a result of the previous marginal
error, which is forgotten in proportion to the lack of
influence of the old X on the new X.
Lemma 6.7: For the second kind of move, E[δY ] ≤
λYY δY− .
6.2

Sources of Error

A second analysis examines the different sources of error that arise for the BK approach. We identify two
sources of error. In the following, we present the two
sources of error for a system with two factors, but the
concepts generalize to more than the two factors. Our
experiments only consider the two-factor case.
The first source of error, called Type A, results from
not taking into account previous dependencies between
the factors when propagating through the dynamics to
obtain the prior distribution at the current time point.
Let µ̂− be the approximate posterior distribution over
−
the previous state, and µ̂−
X and µ̂Y be its marginals.
Type A error is the error that results from instead of
computing the prior
ϕ0 (XY ) =

X

P (XY |x− y − )µ̂− (x− y − ),

x− y −

approximating it with
ϕ̃(XY ) =

X

− − −
P (XY |x− y − )µ̂−
X (x )µ̂Y (y )

x− y −

The Type A error is accumulated by repeatedly applying this approximation at every iteration, but assuming the correct dependencies are used when conditioning on observations.
The second source of error, called Type B, results from
not taking into account previous dependencies between
the factors when conditioning on the observations to
obtain the posterior distribution. It must be pointed
out that some dependencies between the factors are
taken into account when conditioning. These are dependencies introduced by a single step of the dynamics, due to the fact that different variables depend on
the same variable at the previous time step. It is only
old dependencies that held between the factors at the
previous time point that are not taken into account.
Type B error is produced when the prior is computed

using the correct degree of dependence, but when conditioning on the observations only the dependence introduced in ϕ̃ is used.
Note that separable models only have Type B error.
They have no Type A error because the degree of dependence at the previous time point does not affect
the prior marginals at the current time point.
To analyze the sources of error, we define a process for
each source that only has that source of error. The
following procedure isolates Type A error:
1. Determine degree of dependence d:
(a) Propagate µ− , the true posterior at the previous time point, through the dynamics, to
obtain the true prior ϕ.
(b) Let ϕX and ϕY be the marginals of ϕ.
(c) Let d = ϕ − ϕX ϕY . Thus d measures the
dependence between the factors in the true
prior at the current time point.
2. Propagation process:
−
(a) Propagate µ̂−
X µ̂Y to obtain ϕ̃, as described
above.
(b) Let ϕ̃X and ϕ̃Y be the marginals of ϕ̃. Thus
these are the prior marginals obtained by beginning with the approximate posterior at the
previous time point, and assuming the factors
were independent. These marginals incorporate Type A error.
(c) Let ϕ∗ = ϕ̃X ϕ̃Y + d. Thus ϕ∗ uses the incorrect prior marginals, but has the correct
degree of dependence between them.
(d) Condition ϕ∗ on the observation to obtain the
approximate posterior µ̂. Thus the correct
dependence between the factors is used when
conditioning on the observation.
(e) Project µ̂ onto µ̂X and µ̂Y and measure
the KL distance between the true posterior
marginal µX and µ̂X .

3. Repeat for the next iteration.
The following is the process for Type B error:
˜
1. Compute degree of dependences d− and d:
−
−
(a) Let d− = µ− − µ−
measures
X µY . Thus d
old dependencies between the factors according to the true posterior at the previous time
point.
(b) Let ϕ̃ be the prior distribution resulting from
−
propagating µ̂−
X µ̂Y through the dynamics, as
before.

(c) Let d˜ = ϕ̃ − ϕ̃X ϕ̃Y . Thus d˜ measures the
dependence introduced by a single step of the
dynamics.
2. Propagation process:
−
−
(a) Let µ∗− = µ̂−
X µ̂Y + d . This uses the approximate posterior marginals at the previous time point, but incorporates the correct
degree of dependence. This degree of dependence is taken into account when propagating
through the dynamics.
(b) Propagate µ̂− through the dynamics to obtain an approximate prior ϕ0 . (This is slightly
different from the ϕ0 defined earlier, because
it incorporates the true degree of dependence
contained in µ− , rather than the approximate
degree of dependence contained in µ̂− .)
(c) Project ϕ0 to obtain approximate prior
marginals ϕ0X and ϕ0Y .
˜ Only single-step depen(d) Let ϕ̂ = ϕ0X ϕ0Y + d.
dencies are taken into account for conditioning, thus producing Type B error.
(e) Condition ϕ̂ on the observation to obtain µ̂.
(f) Project µ̂ to obtain µ̂X and µ̂Y , and measure
the KL distance between µX and µ̂X .

3. Repeat for the next iteration.

0.003

This paper has presented approximate separability
as a characterization of weak interaction in dynamic
systems. Approximate separability leads to accurate
propagation of marginals using the factored approach
to monitoring. We have analyzed the structure of approximately separable models and provided some explanation as to why they perform well.
More experimentation is needed on a wider array of
examples to strengthen the conclusions in this paper.
In addition, it would be interesting to see if other forms
of weak interaction, such as noisy-or, also allow BK to
work well. An important open issue is defining a notion of approximate separability for continuous variables. Another issue to explore is fully exploiting approximate separability to improve efficiency, perhaps
extending the ideas of [Poole and Zhang, 2003]. It
would also be useful to find a way to decompose a
system automatically into weakly interacting factors.

References
[Asavathiratham, 2000] C. Asavathiratham. The Influence Model: A tractable representation for the
dynamics of networked Markov chains. PhD thesis,
MIT, Electrical Engineering and Computer Science
Department, 2000.

[Boyen and Koller, 1998] X. Boyen and D. Koller.
Tractable inference for complex stochastic processes. In UAI, 1998.

0.002
Error

Conclusion

[Boutilier et al., 1996] C. Boutilier, N. Friedman,
M. Goldszmidt, and D. Koller. Context-specific independence in Bayesian networks. In UAI, 1996.

Total error
Type A error
Type B error

0.0025

7

[Boyen and Koller, 1999] X. Boyen and D. Koller. Exploiting the architecture of dynamic systems. In
UAI, 1999.

0.0015
0.001
0.0005
0
0

0.2

0.4
0.6
0.8
Degree of non-separability

1

Figure 4: Contributions of sources of error.
The two types of error are measured using these processes, and the results are shown in Figure 4. We see
that Type A error is almost equal to the total error,
while Type B error is a small fraction of the total error, even for the most non-separable models. It seems
that taking into account old dependencies when conditioning on observations is not that important. Since
separable models only have this source of error, this
goes some way towards explaining why they do well.

[Dean and Kanazawa, 1989] T.
Dean
and
K. Kanazawa.
A model for reasoning about
persistence and causation. Computational Intelligence, 5:142–150, 1989.
[Murphy, 2002] K. Murphy. Dynamic Bayesian Networks: Representation, Inference and Learning.
PhD thesis, UC Berkeley, Computer Science Division, 2002.
[Pfeffer, 2001] A. Pfeffer. Sufficiency, separability and
temporal probabilistic models. In UAI, 2001.
[Poole and Zhang, 2003] D. Poole and N.L. Zhang.
Exploiting contextual independence in probabilistic inference. Journal of Artificial Intelligence Research, 18:263–313, 2003.

