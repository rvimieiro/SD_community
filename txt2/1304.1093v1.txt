I

Introduction
Algorithms for belief networks (Bayesian networks )
are the cornerstone of many applications for proba­
bilistic reasoning. Effective algorithms exist for cal­
culating posterior probabilities of nodes given the
evidence, even in the case of undirected cycles in the
network. Some of these are based on Pearl's mes­
sage passing algorithms (see [ Pearl, 1988]), where
some preprocessing is needed, such as clustering or
conditioning.
While much has been written about finding pos­
terior probabilities of nodes, not much has been
done about finding maximum probability assign­
ments (MAPs ) for belief networks1• One algorithm
to compute MAPs is given by Pearl in [ Pearl,

1988].

That algorithm, however is rather complicated, and
finding the next best assignments with that algo­
rithm is not as simple as with the algorithm we
present.

Cooper, in his PhD thesis (see [ Cooper, 1984], or
[ Neapolitan, 1990]), performs a best-first search for
a most probable set of "diseases", or causes, given
the evidence. That, however, is not equivalent to

calculating a complete MAP, as he assumes mutual
independence of all causes (i.e. they all have to be
root nodes ) . Peng and Reggia, in [Peng and Reg­
gia, 1987], have defined a diagnostic problem that
uses a 2-level belief network, and designed a best­
first algorithm that finds hypotheses in decreasing
order of probability. It is not clear, however, how
their methods would extend to a general belief net­
work, given that one of their assumptions is that
all symptoms have causes (thus root nodes cannot
be evidence) .
We propose an algorithm that transforms the
belief net into a weighted boolean-function DAG,
and then performs a best-first search to find the
least cost assignment, which induces the MAP as­
signment on the belief network (it can find any
next-best assignments as a natural extension ) . In
the next section we define our transformation, and
show that a minimum cost assignment for the cost­
based DAG induces a MAP assignment on the be­
lief network. In sections following that, we de­
scribe the algorithm and discuss complexity issues.
We then present some experimental results of using
two variants of the algorithm for limited belief net­
works, and conclude with a summary of our results
and a discussion of future research.

Belief Nets as Weighted DAGS
In this section, we define weighted boolean func­
tion DAGs (W BFDAGs) , and show how to repre­
sent any given Bayesian net as a W B FDAG. We as­
sume that the Bayesian network uses only discrete
random variables. We also assume, without loss of
generality, that all nodes take on the same values2,
i.e. values from the domain 'D = {L11 L2 ,
, Lm } ·
•••

A W BFDAG is a DAG where nodes can be as­
signed values from some domain 'D' . Nodes have
labels, which are functions in :F , the set of all func­

*This work has been supported in part by the Na­
tional Science Foundation under grants IRI-8911122
and Office of Naval Research under grant N00014-88K-0589. We wish to thank Robert Goldman for many
helpful comments.

tions with domain 'D'Tr. for some k, and range 'D' .
Formally, we define such DAGs as follows:

1MAP stands for "Maximum A-posteriori Probabil­
ity", and we use it to refer to a complete assignment,
unless specified otherwise.

2If this is not the case, we simply take 'D to be the
union of all node domains. This need not be done in
practice, but we use it for simplicity of presentation.

99
Definition 1 A WBFDAG il a -1-tuple (G, c,r, e),
where:
1. G is a connected directed acyclic graph,
G = (V, E).
2. r is a function from V to :F , called the label. If
a node v has lc immediate predecessors, then the
domain of r(v) is 1J'Ir.. We use the notation r, to
denote the function r( v)
3. c is a function from (V, 'D' ) to the non-negative
reals, called the co1t function.
4. e is a pair (1, d), the evidence. I is a sink node
in G and d is the value in 7)' 461igned to 1.
Definition 2 An aiJIJignment for a WBFDAG il
a function / from V to 1)' An 461ignment il a
(po111ibly partial) model iff the following condition/J
hold:
1. If v is a root node then /(v) E 'D' .
2. If v is a non-root node, with immediate predeces­
sors {v1,... ,v�r.}, then /(v) = r,(/(v1),... , f(v�c )).

I
I
I
I

Figure

1:

W BFDAG Segment for a Root Node

I

•

I
I

Intuitively, an assignment is a model if the node
functional constraints are obeyed everywhere in the
W BFDAG. That is, each node can only assume a
value dictated by the values of its parents and its
label.

Definition
ing

3 A model for a WBFDAG
iff /(1) = d.

il satisfy­

Definition 4 The co1t of an auignment A for a
WBFDAG i6 the 1um
C =

I

L c(v,f(v))

Figure

2:

-log(P(f(u) = Li)), c(u�, F)= 0,

all other values4•
The label ru' of

vEV

The Best Selection Problem (BSP) is the problem
of finding a minimal cost (not necessarily unique)
satisfying model for a given WBFDAG. We exam­
ine the BSP in [Charniak and Shimony, 1990]. In
that paper, we proved that BSP is NP-hard. We
noted there, however, that using standard best first
search, we have found minimal cost satisfying (par­
tial) models relatively efficiently.
We now show how to construct a W BFDAG from
a Bayesian network, where we make the assumption
that only one sink node is an evidence node, and the
evidence is of the form "node assumes single value".
We then show that the solution to the BSP on the
W BFDAG provides us with a MAP assignment for
the Bayesian network, and vice versa. Later, the
above limitation on evidence nodes is relaxed.
We construct the WBFDAG from the Bayesian
network via a local operator on nodes and their
immediate predecessors3• The domain we use for
the WBFDAG is V'=VU{T, F, U}. For each root
node u, construct a node u' (the image of u) with
lVI parents u� (see figure 1), and costs c(u�,T) =

'Henceforth, we will use the term "parents" to de­
note "immediate predecessors".

I

Example Belief Network Segment

u'

and oo cost for

is defined as follows:

3!i. /(uD =T

I

1\

'r/j.j-:j=i- f(uj)=F

otherwise
Intuitively,

u'

is an exclusive-or node. For all

1)', set c(u',d)=0.

I

dE

I

For non-root nodes, the construction is more

complicated (consider the belief network segment
of figure 2, and the corresponding WBFDAG seg­
ment of figure 3 as we describe the construction).
For each non-root node v with in-degree k and
parents U ={u 1, ... , u�c} in the Bayesian network,
do the following:

1.

For each assignment (do, d1, ... , d�c} of values in
'D to U and v (where do = f(v) and�= /(Ui}),
do the following:

(a) Construct a root node

u

such that

c(u, T)

is

-log(P( /('u) =dol/(ul)=d1, ... f(u�c)=d�c))
and c(u, F ) = 0. The cost of all other values
for the node is oo.
'We use the assignment function,/,for nodes in the
belie£ net as well as for the WBFDAG. Its meaning in
this case should be obvious.

I
I
I
I
I
I
I

100

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

(b) Construct a node w with parents U' (i.e. the
images of the nodes in U) and u, and with label
function r..,, as follows:
T d� = T 1\ Vi. di = tit
I
I
r.., ( dl, d'l• d.. ) =
F otherwise

To see this, consider the following property
of Bayesian networks (see Pearl's book, [Pearl,
1988]). The probability distribution of Bayesian
networks can be written as:

{

P(vt, ... , vn) =

···

2. Construct a node v' with n.C+l parents, the nodes
constructed above. Define rv•, the label of v' , as
follows:
3!w. w E parents( v' ) 1\
f(w) = T
otherwise
where d(w) is the value of v for which w was
constructed in step 1 b. For all d E 7J', set
c (u', d) = 0. Intuitively, rv• gets a non-U value
just in case exactly one of the parents, w is T.
We call the node v' constructed in this step the
image of v (thus, we call v the inverse image of
v' ).
In our example, the belief network segment, with
nodes u1, u:h v, all 2-state nodes, as shown in fig­
ure 2, is transformed into the network of figure 3,
where the probabilities used to determine the costs
of the new root nodes are shown (the actual costs
are negative logarithm of the probabilities shown) 5•
The evidence node in the belief network is treated
as follows: set s to be the node which is the image
of the evidence node, and d to the value of the
evidence node.
Theorem 1 All minimal co5t 5ati1Jfying modeliJ for
tke WBFDAG induce MAP assignmentiJ given tke
evidence on the Bayesian network.
Proof outline: we show that any satisfying model
for the WBFDAG induces a unique assignment
to the nodes of the Bayesian network. We then
show that a minimal cost satisfying model for the
WBFDAG induces a maximum probability assign­
ment for the Bayesian network.
• The node s can only get a value equal to d if
ezactly one of its parents, w, has value T, and
all others have value F. This can happen only
if all the parents of each w are assigned values
different from U. These parents of w are exactly
the images of the parents of the inverse image of
s (with one new "cost" node constructed in step
1a) . Proceeding in this manner to the roots, all
image nodes are assigned values in 1J in any sat­
isfying model for the WBFDAG. Using exactly
these values for the reverse image nodes, we get
a unique assignment for the belief network.
• The cost 0 of a satisfying model is exactly the
negative logarithm of the probability of the as­
signment it induces on the Bayesian network.
5 As v is a 2-state node, we do not really need all the
nodes in figure 3, but we show them anyway, so that
the generalization to the m-state node is self evident.

IT P(viparents(v))

ttEV

•

But in each layer of image nodes, we select ex­
actly one "cost" node to be T. The cost of this
node is the negative logarithm of the conditional
probability of the node state of node v given the
state of its parents in tke model. Now since sum­
ming costs is equivalent to multiplying probabil­
ities, the overall cost of the model is the nega­
tive logarithm of the overall probability of the
induced assignment.
Finding the MAP is finding the satisfying model
A that maximizes P(Aievidence). By the defini­
tion of conditional probabilities, the latter is:
P(A ' evidence) =

P(A) P(evidenceiA)
.
P(ev�dence)

where P(evidence) is a constant (we are consid­
ering a particular evidence instance). Thus, it is
sufficient to maximize the numerator.
But P(evidenceiA) is exactly e-c, where c is
the cost of the node selected in the level of the
"grandparents" of the evidence node ( in figure 3,
if v' were the evidence node, we refer to the level
of root nodes labeled with P(FIFT). .. etc. ) . The
latter is true because P(evidenceiA) is equal to
P(evidenceiA'), where A' is a partial assignment
of A , which only assigns values to the parents
of the original evidence node (the same values
assigned to them by A ) .
Likewise P(A) is the exponent of the ( nega­
tive) cumulative cost selected in the rest of the
WBFDAG. Since e= is monotonically increasing
in z, minimizing the cost of the assignment for
the WBFDAG is equivalent to maximizing prob­
ability of the assignment to the Bayesian net­
work, Q.E.D.
We now relax the constraint on the evidence, so
that the evidence can consist of any partial assign­
ment to the nodes of the Bayesian network. Given
such a presentation of evidence, we construct an ex­
tra node s ( in the WBFDAG) , with parents exactly
the nodes assigned values in the evidence, and as­
sign it the following label function: the node s gets
value T just in case its parents are assigned values
exactly as in the evidence, and value F otherwise6•
We now require that s get value T for a satisfying
model (the original constraints on the values of the
original evidence nodes can be removed) . If the ev­
idence is more than just an assignment of one value
11Essentially, tJ is now an
AND'ing all the evidence.

AND

node,

used for

101

I
I

u· nodes

I
Costs of nodes

(-1"1 amia.din fi&mo)
"u" nodes (step l.a)

I
I

v'

Figure

3:

node (step 2)

WBFDAG Constructed from Belief Network

to each evidence node, we use the method suggested
by Pearl before constructing the WBFDAG (see
[Pearl, 1988]).
Computing MAPs with W BFDAGs
In the previous section we showed how to construct
a WBFDAG from a Bayesian network and evidence
such that a minimal cost satisfying model for the
WBFDAG induces a maximum probability given
the evidence model on the Bayesian network. We
now discuss an algorithm for computing MAPs us­
ing this construction, and determine its complexity
relative to the complexity of the Bayesian network.
Algorithm: compute MAP given evidence e .
1. Construct WBFDAG as in the previous section,
where an extra node e is constructed with parents
all the nodes in e if the evidence involves more
than just one sink node.
2. Run the best-first search algorithm on the
WBFDAG, where the termination condition is
a satisfying model7•
7 At this point we will apply standard best-first­
search on AND-OR trees to our WBFDAG to find the
minimum cost. Since our WBFDAG is, however, an
AND-XOR DAG, not an AND-OR tree, it is, perhaps
worth describing the best-first search technique to show
why it still applies. Best-first search on AND-OR trees
works by starting at the sink and constructing alter­
native partial solutions. Whenever an OR node with
lc parents is encountered, we split our partial solution
into lc, each one of which will contain the previous par­
tial solution but now extended to include on of the OR
possibilities. W henever an AND node is encountered,
all of its predecessors are added as things we must now
handle. If we have a DAG then we must simply check

Determine the MAP assignment from the model
from the roots down.
By letting the best-first search continue after
finding the MAP, we can enumerate the assign­
ments to the belief network in order of decreasing
probability. We can see that the best-first search
algorithm has to run on a graph larger than the
Bayesian network, but the size of the WBFDAG is
still linear in the size of the Bayesian network. It is,
however, exponential in the in-degree of the nodes
of the Bayesian network.
If the given Bayesian network has mostly boolean
valued nodes, or most of the conditional probabil­
ities are 0 or 1, we can omit most of the construc­
tion described above, and save on the size of the
WBFDAG. The savings occur because whenever we
have a conditional probability of 0, the relevant u
and w nodes can be omitted (as the u node has
cost oo) . Whenever we have a conditional proba­
bility of 1, we can essentially omit the u node ( and
modify the w node label accordingly). In the ex­
treme case, where all non-root nodes in the belief
network have only boolean conditional probabilities

3.

whenever a new node is added to the partial solution
that it has not been added before. If it has, it is sim­
ply not added the second time. As for the XOR nodes,
in fact, best-first-search is commonly used in exclusive­
or situations (e.g., graph coloring, where the choice of
color for a region is exclusive.) Using the technique in
the XOR case is simply a matter of making sure that
a variable (region, or random variable) gets only one
value (color, or value of the random variable). In our
case this is complicated by the seeming possibility that
we assign random variables 111 = T and v2 = F, whereas
in our distribution we have it that v1 => v2. In fact, this
cannot occur, but we omit the proof.

I
I
I
I
I
I
I
I
I
I
I
I
I
I

102

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

(i.e. only values of 0 or 1 in their conditional distri­
bution arrays), no special construction is necessary,
as shown in [Charniak and Shimony, 1990].
The best-first search will run in linear time on
poly trees, assuming that the correct bookkeeping
operations are made (i.e. the best assignment cost
for the ancestors of a node is kept at every node,
for every possible value assigned to the node). This
is true because once we have these least-cost val­
ues for a node, there is no need to expand its an­
cestors again. In fact, Pearl's algorithm for com­
puting MAPs relies on this property (see [Pearl,
1988]). Thus, if the poly tree belief network has
only boolean distribution for all nodes, then, be­
cause the WBFDAG constructed is also a poly tree,
we have an algorithm that runs in time linear in
the size of the network8• When finding next-best
MAPs, however, we can no longer rely on the above
property, and thus can no longer guarantee linear
time.
Unfortunately, for general poly tree belief net­
works, once we construct the WBFDAG, we no
longer have a poly tree! We can show, however,
that we still have an algorithm with running time
linear in the size of the network. N ote that the
WBFDAG is still separable into components, where
the separating nodes are the images of the nodes of
the original poly tree. Also, from the "cost" nodes
constructed for a certain node v, only one is se­
lected to be assigned T. Using these constraints,
the algorithm still runs in time linear in the size of
the belief network.
Finally, our algorithm can be easily modified to
compute certain partial MAPs. If we are only inter­
ested in assignments to some subset of root nodes
in the belief network (the root nodes could repre­
sent diseases in medical diagnosis, for instance), all
we need to do is set to 0 the costs of all root nodes
in the WBFDAG that are not parents of images of
root nodes.
Implementation
The algorithm has been implemented for the be­
lief networks generated by WIMP (see [Charniak
and Goldman, 1988]), where most nodes have only
two states and many conditional probabilities are
either 0 or 1. The results are rather optimistic, as
partial MAPs were computed faster than evaluat­
ing posterior probabilities for the nodes of the same
network given the same evidence. For that experi­
ment, a very trivial admissible heuristic was used9,
'Pearl's algorithm for finding MAP is also efficient
(time linear in the size of the network), for poly trees.
In some cases (i.e. if local best assignments also happen
to be global best assignments) our algorithm will avoid
many operations that Pearl's algorithm has to perform,
but in general the running times will be equ al.
8Whereby the cost of the complete assignment is
evaluated at the cost collected until now.

and it is certainly reasonable to hope that a bet­
ter admissible heuristic will improve performance
even further. N o conclusive timing tests have been
conducted, however.
In WIMP, only one set of evidence is used per
belief network, as networks are constructed on the
fly as new evidence comes in. If we need to use
the same belief net with different evidence, how­
ever, the WBFDAG can be used again (with minor
changes to cater for the different evidence). It is
possible that many of the best-first search compu­
tations are also re-usable, but we did not try to do
that, because it was not useful for our domain.
We have an improved implementation of the al­
gorithm, where the assumption that 0 and 1 con­
ditional probabilities abound is dropped. The im­
plementation avoids the actual construction of the
extra nodes, even though conceptually the nodes
are still there. This version of the algorithm ex­
ploits cases where many adjacent entries in the con­
ditional distribution array are equal, but not neces­
sarily 0 or 1. Using this property, many of the (vir­
tual) w nodes are collapsed together, and likewise
the u nodes. Advantages of this method over the
method described earlier in this section is that it
facilitates treatment of noisy ORs and ANDs (and
many other types of nodes), as well as pure 0 Rs
and ANDs. Detailed discussion of the modified al­
�orithm is outside the scope of this paper, but see
[Shimony, 1990].
Conclusions and Future Research
We demonstrated a new algorithm for finding
MAPs for belief networks, using a best-first search
on a WBFDAG constructed from the given belief
network and the evidence. We also made it evi­
dent that any algorithm that solves the BSP on the
WBFDAG is also good for finding MAPs, and thus
any possible improvement there will also make find­
ing the MAP more efficient. We showed an effective
way of finding successive next-best assignments for
Bayesian networks.
Further research is needed for improving the
best-first search for finding the minimal cost model
for the WBFDAG, which may be possible for lim­
ited networks generated for special cases, such as
in the case of networks generated by WIMP. Also,
more empirical data comparing this algorithm to
existing ones, is necessary. For that purpose, we
intend to test the algorithm on randomly generated
belief networks.
References
[Charniak and Goldman, 1988] Eugene Charniak
and Robert Goldman. A logic for semantic in­
terpretation. In Proceeding' of the AAAI Con­
ference, 1988.

103

[Charniak and

Shimony, 1990] Eugene
Charniak and Solomon E. Shimony. Probabilistic se­
mantics for cost-based abduction. Technical Re­
port CS-90-02, Computer Science Department,
Brown University, February 1990.

[Cooper, 1984] Gregory Floyd Cooper. NESTOR:
A compu.ter-Brued Medical Diagnosis Aid that
Integrates Causal and Probabilistic Knowledge.
PhD thesis, Stanford University, 1984.

[Neapolitan, 1990} Richard E. Neapolitan. Proba­
bilistic Reasoning in Ezpert Systems, chapter 8.
John Wiley and Sons, 1990.

[Pearl, 1988] J. Pearl. Probabilistic Reruoning in
Intelligent Systems: Networks of Plausible Infer­
ence. Morgan Kaufmann, San Mateo, CA, 1988.
[Peng and Reggia, 1987] Y. Peng and J. A. Reg­
gia. A probabilistic causal model for diagnostic
problem solving ( parts i and ii ) . In IEEE 71-ans­
actions on Systems, Man and Cybernetics, pages
146-162 and 395-406, 1987.

[Shimony,

1990] Solomon E. Shimony. On irrele­
vance and partial assignments to belief networks.
Technical Report CS-90-14, Computer Science
Department, Brown University, 1990.

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

I
I
I
I

Session

I
I
I

4:

First Poster Session
Reducing Uncertainty in Navigation and Exploration

K. Bayse, M. Lejter, and K. Kanazawa
Ergo: A Graphical Environment for Constructing Bayesian

I

Belief

Networks

I

Decision Making with Interval Influence Diagrams

I. Beinlich and E. Herskovits

J.S. Breese and K.W. Fertig

I

A Randomized Approximation Algorithm of Logic Sampling

I

Occupancy Grids: A Stochastic Spatial Representation

R.M. Chavez and G. F. Cooper

for Active Robot Perception

I
I
I
I
I
I
I
I

A. Elfes
Time, Chance, and Action

P. Haddawy
A Dynamic Approach to Probabilistic Inference
Using Bayesian Networks

M. C. Horsch and D. Poole
Approximations in Bayesian Belief Universe
for Knowledge Based Systems

F. Jensen and S.K. Andersen
Robust Inference Policies: Preliminary Report

P.E. Lehner

I
I
I
Minimum Error Tree Decomposition

L. Liu, Y. Ma, D. Wilkins, Z. Bian and X. Ying
A Polynomial Time Algorithm for Finding Bayesian
Probabilities from

Marginal Constraints

J. W. Miller and R.M. Goodman
Computation of Variances in Causal Networks

R.E. Neapolitan and J.R. Kenevan
A Sensitivity Analysis of Pathfinder

K.C. Ng and B. Abramson
IDEAL: A Software Package for Analysis of Influence Diagrams
S. Srinivas and J Breese
On the Equivalence of Causal Models

T.S. Verma and J. Pearl
Applications of Confidence Intervals to the Autonomous
Acquisition of High-level Spatial Knowledge

I
I
I
I
I
I
I
I

L. Wixson

I
I
I
I
I
I
I
I

