certainty in the underlying scientific knowledge base. This
paper presents such a dialectical formalism for an intelli­
gent system, which we termed a Risk Agora in our earlier
work. We begin by examining the nature of scientific dis­
course.

2

2.1

SCIENTIFIC DISCOURSE

A MODEL OF SCIENTIFIC ENQUIRY

Our chosen application domain is a scientific one. To repre­
sent this domain, therefore, we seek to ground our formal­
ism in a philosophical model of scientific enquiry. Firstly,
we require a theory of the nature of modem science. Fol­
lowing Pera (1994), we view the enterprise of science as
a three-person dialogue, involving a scientific investiga­
tor, Nature and a skeptical scientific community. In Pera's
model, the investigator proposes theoretical explanations of
scientific phenomena and undertakes scientific experiments
to test these. The experiments lead to "replies" from Na­
ture in the form of experimental evidence. However, Na­
ture's responses are not given directly or in a pure form,
but are mediated through the third participant, the scien­
tific community, which interprets the evidence, undertakes
a debate as to its meaning and implications, and eventually
decides in favor or against proposed theoretical explana­
tions. The consequence of this model for our formalism is
that we provide Nature with a formal role, but manifest it
through those of the other participants.
But Pera's model of modem science as a dialogue game
could apply to many other human dialogues, most of which
do not share science's success in explaining and predict­
ing natural phenomena. Our model of science therefore
requires an explanation of its success. Some philosophers
of science believe this is due to the application of univer­
sal principles of assessment of proposed scientific theo­
ries, such as the falsificationism of Popper or the confir­
mationism of Camap. However, we do not share these
views, instead believing, with Feyerabend (1993), that the
standards of assessment used by any scientific community
are domain-, context- and time-dependent. This view, that
there are neither universal nor objective standards by which
scientific theories can be judged, was called "epistemolog­
ical anarchism" by Lakatos (Lakatos & Feyerabend 1999).
Instead of universal principles of assessment of theories,
we believe science's success arises in part from applying
two normative principles of conduct: firstly, that every the-

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

372

oretical explanation proposed by a scientific investigator is
contestable by anyone; and secondly, that every theoretical
explanation adopted by a scientific community is defeasi­
ble. In other words, all scientific theories, no matter how
compelling, are always tentative, being held only until bet­
ter explanations are found, and anyone may propose these.1
To build an intelligent system based on these principles,
we therefore require a (normative) model of scientific
discourse which enables contestation and defeasibility of
claims. Our model has several components. At the high­
est level, we are attempting to model a discourse between
reasonable, consenting scientists, who accept or reject ar­
guments only on the basis of their relative force. An in­
fluential model for debates of this type is the philosophy
of Discourse Ethics developed by Habermas ( 1991) for de­
bates in ethical and moral domains. Our formalism there­
fore draws on Habermas, in particular his rules of discourse
first fully articulated by Alexy (1990), and these form the
basis of the desired properties of the Agora formalism pre­
sented later in this section.2
Next, within this structure, we wish to be able to model
dialogues in which different participants variously posit,
assert, contest, justify, qualify and retract claims. To rep­
resent such activity requires a model of an argument, and
we use Toulmin's (1958) model, within a dialectical frame­
work. To embody our belief in epistemological anarchism,
we permit participants to contest any component of a sci­
entific argument: its premises; its rules of inference (Toul­
min 's "warrants"); its degrees of support (his "modalities");
and its consequences. We believe this is exactly what real
scientists do when confronted with new theoretical expla­
nations of natural phenomena (Feyerabend 1993). When
a scientific claim is thus contested, its proponent may re­
spond, not only by retracting it, but by qualifying it in some
way, perhaps reducing its scope of applicability. Naess
( 1966) called this process "precizating", and we seek to
enable such responses in the system. We thus ground our
formalism for the Agora in a model of scientific discourse
as dialectical argumentation.3
2.2 DESIRED AGORA PROPERTIES

As mentioned, we desire our Agora formalism to satisfy the
rules for a reasoned discourse proposed by Alexy (1990),
which are listed here. In restating these, we have modi­
fied and re-ordered them slightly, and have ignored rules
which deal specifically with discussion of ethical matters.
Also, because our formalism is intended for debate regard­
ing only one chemical at a time, we have ignored Alexy's
1

These two principles are each necessary to explain science's

success, but not sufficient.
2 Alexy's

rules have some similarity with Grice's (1975) Max­

ims for Conversation.
3
Further details of our philosophy of science are contained in

(McBurney & Parsons 2000b).

rules regarding the relevance of utterances. We have also
added a property concerning precization.
Pl Anyone may participate in the Agora, and they may
execute dialogue moves at any time, subject only to
move-specific conditions (defined below).
P2 Participation entails acceptance of the semantics for the
logical language used, and of the associated modality
(degrees of support) dictionaries.
P3 Any participant may assert any claim or consequence
of a claim, but may do so only when they have a
grounded argument for the claim (respectively, a con­
sequential argument from the claim).
P4 Any participant may question or challenge any claim
or any consequence of a claim.
PS Any participant who asserts a claim (respectively,
a consequence of a claim) must provide a valued
grounded argument for that claim (respectively, a val­
ued consequential argument from the claim) if queried
or challenged by another participant.
P6 Any participant may question or challenge the grounds,
the rules of inference or the modalities for any claim.
P7 Whenever a participant asserts a valued grounded argu­
ment for a claim (or a valued consequential argument
from a claim), any other participant may assert a val­
ued grounded argument (respectively, a valued conse­
quential argument) for the same claim with different
dictionary values.
PS A participant who has provided a grounded argument
for a claim which has been challenged should be able
to respond by qualifying (precizating) the original
claim or argument.
P9 Any participant who provides a grounded argument
for, or a consequential argument from, a claim is
not required to provide further defence if no counter­
arguments are provided by other participants.
PlO No participant may contradict him or herself.
3

3.1

THE RISK AGORA FORMALISM

PRELIMINARY DEFINITIONS

We begin by assuming the system is intended to represent
debate regarding the carcinogenicity of a specific chemi­
cal, and that statements concerning this can be expressed
in a propositional language £, whose well-formed formu­
lae (wffs) we denote by lower-case Greek letters. Subsets
of £ (i.e. sets of wffs) are denoted by upper-case Greek
letters, and £ is assumed closed under the usual connec­
tives. We assume multiple modes of inference (warrants)

373

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

are possible, these being denoted by 1- i. These may include
non-deductive modes of reasoning, and we make no pre­
sumptions regarding their validity in any truth model. We
assume a finite set of debate participants, denoted by Pi.
who are permitted to introduce new wffs and new modes of
inference at any time. We denote Nature, in its role in the
debate, by PN.
Definition 1: A grounded argument for a claim 0, de­
noted A(-+ 0), is a 3-tuple (G,R,O), where G =
(0o,01,81,02,. . . ,0n-2,On-1,0n-1) is an ordered se­
quence of wffs()i and possibly -empty sets of wffs 0i, with
n � 1 and withR = (h, h, . . . ,1-n) an ordered sequence
ofinference rules such that:

In other words, each ()k (k = 1, ... , n 1) is derived from
the preceding wff ()k_1 and set of wffs 0k-1 as a result of
the application of the k-th rule of inference, 1- k. The rules
of inference in any argument may be non-distinct. We call
the set {Ok-1} u 0k_1 the grounds(or premises) for Ok.
-

Definition 2: A consequential argument from a claim (),
denoted A(O --+ ), is a 3-tuple (0,R,C), where C =
(0o,01,81,02,. . . ,0n-2,On-1,0n-1,On) is an ordered
sequence of wffs()i and possibly -empty sets of wffs0i, with
n � 1, and with R = (l-1,h, . . . ,1-n) an ordered se­
quence ofinference rules such that:

On-1, 0n-1 1-n On.
In other words, the wffs ()k in C are derivations from ()
arising from the successive application of the rules of in­
ference in R, and we call each ()k in C a consequence of
0.
In order that participants may effectively state and con­
test degrees of commitment to claims, we require a com­
mon dictionary of degrees of commitment or support (what
Toulrnin called "modalities"). Our formalism will support
any agreed dictionary, whether quantitative (such as a set of
probability values or belief measures) or qualitative (such
as non-numeric symbols or linguistic qualifiers), provided
there is a partial order on its elements. We define dictionar­
ies for modalities for claims, grounds, consequences and
rules of inference.

Definition 3: Four modality dictionaries are defined as fol­
lows, each being a (possibly infinite) set of elements having
a partial order. Theclaims dictionary is denoted by Vc, the
grounds dictionary by Va, the consequences dictionary by
VQ, and the inference dictionary by VI.
Because claims, grounds and consequences are all elements
of the same language .C, two or more of the dictionaries
Vc, Va and VQ may be the same. However, a distinct
dictionary will generally be required for VI. 4 Because of
our belief in epistemological anarchism, we do not specify
rules of assignment of dictionary labels by participants in
the Agora. In particular, the labels assigned to the conclu­
sions and consequences of arguments are not constrained
by those assigned to premises or rules of inference.
Example I: The generic argumentation dictionary defined
for assessment of risk by (Krause et al. 1998) is an exam­
ple of a linguistic dictionary for statements about claims,
grounds or consequences, comprising the set: {Certain,
Confirmed, Probable, Plausible, Supported, Open}. The
elements of this dictionary are listed in descending order,
with each successive labelindicating a weaker beliefin the
claim.
Example 2: Two examples of Inference Dictionaries are
VI= {Valid, Invalid} andVI= { Acceptable, Sometimes
Acceptable, Open, Not Acceptable}.
Definition 4: A valued grounded argument for a claim
(), denoted A(-+ O,D), is a 4-tuple (G,R,O,D), where
(G,R,0) is a grounded argument for () and D
(do,d1,. . . ,dn-1,do,r1,r2,... ,rn) is an ordered se­
quence of labels and vectors of labels, with eachdi a vector
of dictionary labelsfrom Vc (fori = 0, . . , n 1), with
do E Vc and with ri E VI (fori = 1, . . . , n). Each
vector di comprises those values of theClaimsDictionary
assigned to grounds{()i} u 0i, the elementdo is that value
of the Claims Dictionary assigned to() and each element
ri is that value of the InferenceDictionary assigned to 1- i.
A valued consequential argument from a claim 0, denoted
A(() --+,D), is defined similarly .
.

3.2

-

DISCOURSE RULES

We next define the rules for discourse participants, building
on the definitions above. Moves are denoted by 2-ary or
3-ary functions of the form name(Pi: . ), where the first
argument denotes the participant executing the move. If the
move responds to an earlier move by another participant,
that earlier move is the second argument. Arguments are
separated by colons. In Section 4, we will show that these
rules give operational effect to the Desired Properties.

4

In (McBurney & Parsons 2000c), we model degrees of ac­

ceptability of inference rules.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

374

Rule 1: Query and Assertion Moves
1.1 Pose Claim: Any participant Pi at any time may
move:
pose(Pi :---+ (}?)

which asks the Agora if there is a grounded argument
for e.

1.8 Assert Consequence: Similarly to Assert Claim, a
participant may move:

asserLcons(Pi : (0,¢, d¢))
where ¢ is a consequence of 0.
1.9 Query Consequence: Similarly to Query Claim, a
participant may move:

1.2 Propose Claim: Any participant Pi at any time may
propose a claim with move:

propose(Pi : ((}, de))
where(} E £ and de E Vc, which informs the Agora
that Pihas a valued grounded argument for(}, and has
assigned it a modality of de.
1.3 Assert Claim: Any participant Pi at any time may
assert a claim with move:

assert(Pi : ((},de))
where (} is a wff and de E Vc, which informs the
Agora that Pi has a valued grounded argument for (},
which she believes is compelling.
1.4 Query Claim: W henever a propose or assert move
relating to ((},de) has been made by participant Pi.
any other participant Pi may move:

query(Pj : propose(Pi : (0,de)))
or
query(Pj : assert(Pi: (O,de))).
These ask participant Pi to provide her valued
grounded argument for 0, which she must provide im­
mediately with move:
show_arg(Pi : A(---+ 0,D)).
1.5 Show Grounded Argument: Any participant Pi may
at any time provide a valued grounded argument for 0
with the move:

query_cons(Pj : propose(Pi: (0,¢, d¢))).
1.10 Show Consequential Argument: Any participant Pi
may at any time provide a valued consequential argu­
ment from 0 with the move:

show_cons(Pi : A(O ---+,D)).
1.11 Propose Mode of Inference: Any participant Pi at
any time may move:

where I-t is a mode of inference and rt E V1. This
move informs the community that participant Pi be­
lieves that I-t is a mode of inference of strength at least
Tt.
Note that the query and assertions rules are not symmet­
ric between grounded and consequential arguments; partic­
ipants may only propose or assert claims for which they
have grounded arguments, but they need not necessarily
have considered the consequences of these claims. Next,
we explicitly define the ContestClaimrule, with other con­
testation rules being defined similarly. For brevity in the
following, we sometimes write A for A(---+ (},D).
Rule 2: Contestation Moves
2.1 Contest Claim: W henever proposeor assert relating
to (0,de) has been moved by participant Pi. any other
participant Pi may contest this by moving:

contest(Pj : propose(Pi : (B, de)))

show_arg(Pi: A(-+ O,D)).
1.6 Pose Consequence: A participant Pk may at any time
move:
pose_cons(Pk : () ---+?)

which asks the Agora if there is a consequential argu­
ment from 0.
1.7 Propose Consequence: Similarly to ProposeClaim, a
participant may move:

propose_cons(Pi : (0,¢, d¢))
where ¢ is a consequence of 0.

or
contest(Pj :assert(Pi : (0, de))).
If any participant Pk subsequently queries this contes­
tation with:
query(Pk : contest(Pj : propose(Pi : (0,de))))
(or likewise for assert), participant Pi must respond
immediately, either with an assignment of an alterna­
tive modality d0 for claim 0, thus:
propose(Pi : (0,d0))

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

or
assert(Pj : (B,d�))
(where d� =/: do), or with a stronger assertion of the
negation of B, thus:
propose(Pj : (•B,d�))

3.4 Accept Mode of Inference: Similarly to accept.prop:

accepLinf(Pj : propose_inf(Pi: (f-t,rt))).
3.5 Accept Consequence: Similarly to accept.prop:

accepU:ons(Pj : show_cons(Pi : A(B --+,D))).

or
assert(Pj : (•B,d�))
(where d� >do).
2.2 Contest Ground:

375

3.6 Precizate Claim: Any participant Pi who proposes or
asserts a claim for (}, and follows this with a demon­
stration of a valued grounded argument for(} by:

show..arg(Pi : A(--+ B,D))

contesLground(Pj :show...arg(Pi : A : (Bt,do,)).

may subsequently qualify her argument with:

2.3 Contest Inference:

prec(Pi : show...arg(Pi : A(--+ B,D)): A' (--+ B,D'))

contesLinf(Pj : show...arg(Pi : A : f-t)).

where A' (--+ (}, D') is an argument for(} identical with
A(--+ B,D) except that: (a) it begins from ground <I> U
80 instead of 00, where <I> is not equal to { B} nor to
any ground of(}, and (b) D' may be different to D.

2.4 Contest Modality:

contest.mod(Pj : show...arg(Pi : A(--+ B,D))).
2.5 Contest Consequence:

contesLcons(Pj : show_cons(Pi : A: (Bt,do,))).

3.7 Retract Claim: Any participant Pi who asserts:

assert(Pi: (B,do))
Rule 3: Participant Resolution Moves
3.1 Accept Proposed Claim: Whenever a claim has been
proposed by Piand its grounds demonstrated by moving:
show...arg(Pi : A(--+ B,D)),

any other participant P1 may declare that they accept
the proposed claim, with move:
accept_prop(Pj : show...arg(Pi : A(--+ B,D))).

may at any time subsequently withdraw the claim by:
retract(Pi: assert(Pi: (B,do))).
Likewise, for those claims by others accepted by Pi.
3.8 No contradiction: Any participant Pi who asserts (or
accepts an assertion for) (} may not at any time subse­
quently assert (or accept an assertion for) -.(}, unless
they have in the interim moved:

This move is identical with the sequence:

retract(Pi : assert(Pi : ((}, do)))

propose(Pj : ((}, do))

(or, respectively, its equivalent for accepted claims).

show..arg(Pj : A(--+ B,D)).
3.2 Accept Asserted Claim: Similarly to accept.prop:

accept..assert(Pj : show...arg(Pi : A(--+ B,D))).
3.3 Change Modalities: Any participant Piwho proposes
or asserts a claim for(}, and follows this with a demon­
stration of a valued grounded argument for(} by moving:
show...arg(Pi : A(--+ B,D)),

may subsequently revise her assignment of modalities
with a later move of:
show..arg(Pi : A(--+ B,D')),
where D' =/: D. Likewise, declarations of modal be­
liefs expressed in other moves (e.g. in accept..assert)
may also be revised by subsequently executing the
same move with a different set of dictionary values.

3.3

DIALOGUE RULES

Definition 5: A Dialogue is a finite sequence of discourse
moves by participantsin theAgora, in accordance with the
rules above.
As in (Hamblin 1971; Walton & Krabbe 1995; Amgoud,
Maudet, & Parsons 2000), we define sets called Commit­
ment Stores which contain the proposals and assertions
made by participants, both individually and for the Agora
as a community, and track these as they change.
Definition 6: The commitment store of player Pi,
i = 1, 2, .., denoted CS(Pi), is a possibly empty set
{(B,do) I (} E £, do E Vc}. Each do is the claim dic­
tionary value assigned by Pi toB.
.

The values in participants' stores are updated by the fol­
lowing rule:

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

376

Rule 4: Participant Commitment Store Update: When­
ever participantPi executes the moves

Rule 5: Nature's Modalities: The modality do,N of Na­
ture for the claim () is assigned as follows:

propose(Pi:((),do)),

•

If () is a wff for which no grounded argument has yet
been provided by a participant, then do,N is assigned
the value Open.

•

If() is a wff for which at least one grounded argument
has been provided by a participant, then do,N is as­
signed the value Supported.

•

If() is a wff for which a grounded and consistent argu­
ment has been provided by a participant, then do,Nis
assigned the value Plausible.

•

If () is a wff for which a grounded and consistent ar­
gument has been provided by a participant, and for
which no rebutting arguments have been provided,
then do,Nis assigned the value Probable.

•

If () is a wff for which a grounded and consistent ar­
gument has been provided by a participant, and for
which neither rebutting nor undercutting arguments
have been provided by participants, then do,N is as­
signed the value Confirmed.

•

If () is a logical tautology, then do,N is assigned the
value Certain.

accept_prop(Pi: propose(Pj:((),do))),
assert(Pi: (e,do)),
accept..assert(Pi: assert(Pj :((),do)))
or their equivalents, then the tuple ((),do) is inserted into
CS (Pi). Whenever participant Pi executes a retraction
move for ((), do), the tuple ((), do)is removed fromCS (Pi).
Similarly, whenever Piexecutes a Change Modality move
for ((), do), the value of ((), do)in C S (P i)is revised.
We next define an analogous concept for Nature, with
claims inserted into Nature's Commitment Store on the ba­
sis of the debate at that point in the Agora. This could be
achieved in a number of ways. For example, a skeptical
community could define Nature's modality for a claim() to
be the minimum claim modality assigned by any of those
Participants claiming or supporting (). A credulous com­
munity could instead assign to Nature the maximum claim
modality assigned by any of the participants to e. Varia­
tions on these approaches could utilize majority opinion or
weighted voting schemes.
Because we wish to model dialectical discourse, we have
instead chosen to assign Nature's modalities on the basis
of the existence of arguments for and against the claim.
To do this, we draw on the generic argumentation dictio­
nary for debates about carcinogenicity of chemicals pre­
sented in (Krause et al. 1998), which is based on Toulmin 's
(1958) schema. We begin by defining certain relationships
between arguments and then the Claims Dictionary for Na­
ture.
Definition 7: An argument A(-t () ) = (G, R,() ) is con­
sistent if G = (eo,()1, e1,()2, ..., en-2, ()n-1, en-d is
consistent, thatis fi there do not exist a, (3 E eo U {()1 } U
e1 u {()2} u
u en-1 such that·f3 is a consequence of
0 0 0

a.

Definition 8: Let A(-t ())
(G,R,()) and B(-t
¢>)
(H, S, ¢>) be two arguments, where G
(eo,e1, e1, ()2, . . . ,()n-1, en-d· We say thatB(-t ¢>)re­
buts A(-t ()) if¢>= () We say that B(-t ¢>) undercuts
A(-t () )if,for somea E eou{el}ue1u{e2}u...uen-1>
a= •¢>.
=

=

...,

_

Definition 9: The claims dictionary for Nature is the set
Vc,N = {Certain, Confirmed, Probable, Plausible, Sup­
ported, Open}.
Definition 10: The commitment store of Nature, denoted
CS(PN), is a non-empty set{(e,do,N) I () E £, do,N E
Vc,N}. Eachdo,N is the claim modality assigned by the
Agora community on Nature's behalf to(), in accordance
with the next two rules.

Rule 6: Nature Commitment Store Update: The entries
in CS(PN) are updated after each legal move by Agora
participants.
3.4

ARCHITECTURE AND USER INTERFACE

We anticipate the Risk Agora system being used to rep­
resent a completed or on-going scientific debate, but not in
real-time. Once instantiated with a specific knowledge base
in this way, the Agora could be used for a number of differ­
ent purposes, which led us (McBurney & Parsons 1999), to
propose a layered architecture for the Agora, corresponding
to these different functions. The main purposes to be ful­
filled are: (a) automated reasoning to find arguments for,
and the consequences of, particular claims; (b) compari­
son of the various arguments for and against a claim; and
(c) development of an overall case for a claim, coherently
combining all the arguments for and against it.
4

AGORA PROPERTIES

The rules defined in the previous section were intended to
operationalize the desired Agora properties of Section 2.2.
We now verify that this is indeed the case.
Theorem I: The Agora sy stem defined in Section 3 has
PropertiesP1 throughP10.
Proof. This is straightforward, from the definitions of the

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

permitted moves. Thus, Properties P l and P2 are fulfilled
through the overall system design; Property P3 by Rules
1.1-1.3 and 1.6-1.8; Property P4 by Rules 1.4, 1.9, 2.1 and
2.5; Property P5 by Rules 1.2, 1.3, 1.5, 1.7, 1.8 and 1.10;
Property P6 by Rules 2.2-2.4; Property P7 by Rules 3.13.5; Property P8 by Rule 3.6; Property P9 by Rules 1 and
0
2; and Property P lO by Rule 3.8.
Moreover, we can use the definition of the claim modalities
for Nature provided by Rule 5 to construct a valuation func­
tion on wffs and to define a notion of "proof" of claims, as
follows.
Definition 11: Natural valuation is afunction v N defined
from the set of wffs ofC to the set{0, 1 }, such thatv N (B)
1 precisely whendo,N = Confirmed; otherwise, vN(B) =
0.
=

Definition 12: A provisional proof for a claimB is a
grounded and consistent argument forB for which neither
rebuttal nor undercutting arguments exist.
Our belief in the defeasibility of all scientific claims leads
us to use the term "provisional proof" rather than "proof."
Likewise, we can think of a natural valuation equal to 1 as
signifying "Currently Accepted as True" (or "Defeasibly
True") and 0 as "Not Currently Accepted as True." Our
definition of natural valuation thus says that a claim is de­
feasibly true iff there are no arguments attacking it. We
could readily define additional valuation functions which
capture degrees of conviction regarding the truth of claims,
mapping, for instance, to Probable or to Plausible. With
the definitions above, we can now prove soundness of pro­
visional proofs in the Agora, with respect to the natural
valuation function.
Theorem 2: With the notion of provisional proof, theAgora
is consistent and complete with respect to theNatural Val­
uationFunctionvN, provided that all grounded arguments
for claims are eventually asserted by someParticipant.
Proof. Consistency here says that all claims B for which
there exists a provisional proof are also assigned a valu­
ation of 1 by the function VN. Completeness says, con­
versely, that all claims B which are assigned a valuation of
1 by v N also have a provisional proof. Both of these follow
from our definitions of v N and of provisional proof, unless
a consistent grounded argument for a claim B exists but is
0
not asserted by any Participant.
The model of science we have adopted asserts that scien­
tific claims are regarded as "defeasibly true" only when
the relevant scientific community agrees to so regard them.
(After all, even if a transcendent truth exists, science has
no privileged means of accessing it.) Our definition of nat­
ural valuation is in effect a proxy for the scientific commu­
nity's opinion on the truth of a claim. Accordingly, The­
orem 2 says that the provisional proof procedure neither
under-generates nor over-generates defeasibly true claims,

377

provided all grounded arguments for claims are eventually
asserted.
5

EXAMPLE

To illustrate these ideas we present a simple and hypothet­
ical example of an Agora debate. In a real debate, partic­
ipants would be free to introduce supporting evidence and
modes of inference at any time. For reasons of space, in this
example we first list the statements and modes of inference
to be asserted, labeled K l through K4, and R l through R3,
respectively, about a chemical X:
Kl: X is produced by the human body naturally (i.e. it is
endogenous).
K2: X is endogenous in rats.
K3: An endogenous chemical is not carcinogenic.
K4: Bioassay experiments applying X to rats result in sig­
nificant carcinogenic effects.
Rl (And Introduction): Given a wff ¢ and a wff B, we
may infer the wff ( ¢ 1\ B).

R2 (Modus Ponens): Given a wff¢ and the wff (¢-+B),
we may infer the wff B.
R3: If a chemical is found to be carcinogenic in an ani­
mal species, then we may infer it to be carcinogenic
in humans.

We now give an example of an Agora dialogue concern­
ing the statement: X is carcinogenic to humans, which
we denote by ¢. The moves are numbered M l , M2, .. .,
in sequence, and for simplicity we assume the participants
are using the claims dictionary of Example 1, abbreviated
to {Cert, Conf, Prob, Plaus, Supp, Open}, and the in­
ference dictionary V1 = {Val, Inval} . Before any dis­
course move is made, Nature's modality for this claim is
dq, N = Open, as is its modality for •¢. Ignoring claims
about any other chemicals, we thus have at commencement
thatCS(PN) = {(¢,0pen),(• ¢, 0pen)}. Through the
dialogue, we show the contents of Nature's commitment
store as it changes, in steps numbered NCSO, NCS l , .. .
,

NCSO: CS(PN) ={(¢, Open), (• ¢, Open)}.
Ml: assert(P1 : (¢,Con!)).
M2: query(P2 : assert(P1 : (¢,Con!))).
M3: show....arg(P1 : (K4, R3, ¢,(Conf, Val,Con!))).
NCSl: CS(PN) ={(¢, Con!), (• ¢,Open)}.
M4: contest(P2 :assert(P1 : (¢, Con!))).

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

378

M5: query(P3 :contest(P2 :assert(P1 :(¢;,Con!))))
M6: propose(P2 : (•¢;,Plaus)).
M7: query(P1 :propose(P2: ( •¢>, Plaus ))).
M8: show...nrg(P2 : ((Kl, K3), R2, •¢>,
(Conf,Prob, Val,Plaus))).
NCS2: CS( PN) = {(¢;,Plaus),(•¢;,Plaus)}.
M9: contest_ground(P4 :
show...nrg(P2:((Kl, K3), R2, •¢;,
(Conf,Prob, Val,Plaus)) : (K3,Prob))).
MlO: show...nrg(P4:((K2, K4), RI, -.K3,
(Conf,Con/, Val, Con!)) )
NCS3: CS( PN) = {( ¢>, Plaus),(•¢>,Plaus)}.

Observe that Participant P4 in Move MlO, by providing an
argument for -.K3, undercuts the argument presented for
¢> by Participant P2 in Move MS. We can also observe the
changes in the Natural Valuation of ¢; through the course
of this debate. At the start, we have VN(¢>)
0, which
changes to vN(¢) = 1 after Move M3, since then de,N =
Con/. However, after Move M8, de,N = Plaus, so once
again VN(¢>) = 0.
=

6

DISCUSSION

Characterization of scientific discourse as dialectical argu­
mentation is not new. Rescher ( 1977) claims to have been
the first to propose a dialectical framework for modeling
the progress of scientific inquiry, and Pera's (1994) work is
also a dialectical approach to science. Among argumen­
tation theorists, Freeman (1991) also discusses scientific
discourse in his study of argument structure. Both Carlson
(1983) and Walton and Krabbe (1995) aim to model generic
dialogues, but their focus is (respectively) on question-and­
answer and persuasion dialogues. In addition, neither for­
malism explicitly permits degrees of support for commit­
ments to be expressed, which our formalism does.
None of these works appears intended for encoding in in­
telligent systems. Within AI, intelligent systems for scien­
tific domains have used argumentation for some time (e.g.
Fox, Krause, & Ambler 1992). However, these applications
have typically involved monolectical rather than dialectical
argumentation. More recently, Haggith (1996) developed
a dialectical argumentation formalism and applied the re­
sulting system to a carcinogenicity debate. However, the
primary focus of her work was on knowledge representa­
tion in generic domains of conflict, and so her formalism
is not grounded in an explicit philosophy of science. The
work of Amgoud, Maudet, & Parsons (2000) is closest in
approach to that presented here (and we have drawn upon
their formalism), but it is focused on negotiation dialogues,

2000

agam m a generic context. Their formalism only permits
two participants, although this would be relatively easy to
amend. As with Haggith's system, their formalism does not
permit debate over the rules of inference used. Recent legal
argumentation systems, such as those of Verheij ( 1999), do
permit this.
Our formal definition of the Risk Agora enables contes­
tation and defeasibility of scientific claims. Our system
therefore operationalizes the two normative principles of
conduct for scientific discourses presented in Section 2.1.
We are currently exploring a number of refinements to the
Agora. Firstly, Rehg ( 1997) has demonstrated the ratio­
nality of incorporation of rhetorical devices (such as epi­
deictic speech and appeals to emotions) in dialectical argu­
ment and decision-making, and we seek a means to incor­
porate such devices in the Agora. This would not be novel:
the argumentation system of Reed ( 1998), for example, al­
lows for the modeling of rhetorical devices, although in a
monolectical context. Secondly, using the Agora in a de­
liberative context would require incorporation of values for
the projected consequences and the development of an ap­
propriate qualitative decision-theory, as in (Fox & Parsons
1998; Parsons & Green 1999).
We believe the Risk Agora has a number of potential bene­
fits. Firstly, by articulating precisely the arguments used to
assert carcinogenicity, gaps in knowledge and weaknesses
in arguments can be identified more readily. Such iden­
tification could be used to prioritize bio-medical research
efforts for the particular chemical. Secondly, by explor­
ing the logical consequences of claims, the Risk Agora can
serve a social maieutic function, making explicit knowl­
edge which may only be latent. Thirdly, once instantiated
with the details of a particular debate, the system could
be used for self-education by others outside the scientific
community concerned. Indeed, it could potentially form
the basis for the making of regulatory or societal deci­
sions on the issues in question (e.g. Should the chemical
be banned?) , and thereby give practical effect to notions
of deliberative democracy (McBurney & Parsons 2000a;
2000 In press). Finally, with argumentation increasingly
being used in the design of multi-agent systems (Parsons,
Sierra, & Jennings 1998), the formalism presented here
could readily be adapted for deliberative dialogues between
independent software agents.
Acknowledgments

This work was partly funded by the British EPSRC under
grant GRIL84117 and a PhD studentship. We also thank
the anonymous reviewers for their comments.
References

Alexy, R. 1990. A theory of practical discourse. In
Benhabib, S., and Dallmayr, F., eds., The Communica-

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

tive Ethics Controversy .Cambridge, MA, USA: MIT Press.
151-190. (PublishedinGerman1978).
Amgoud, L.; Maudet, N.; and Parsons, S. 2000. Mod­
elling dialogues using argumentation. In Durfee, E., ed.,
Proceedings of the 4th International Conference on Multi­
AgentSy stems (ICMAS- 2000). Boston, MA, USA: IEEE.
Carlson, L. 1983. Dialogue Games: AnApproach toDis­
courseAnaly sis. Dordrecht, The Netherlands: D. Reidel.
E.P.A. U.S.A. 1986. Guidelines for carcinogen risk assess­
ment. U.S.Federal Register 51:33991-34003.
Feyerabend, P. 1993. Against Method.London, UK: Verso,
third edition. First edition published 1971.
Fox, J., and Parsons, S. 1998. Arguing about beliefs and
actions. In Hunter, A., and Parsons, S., eds., Applications
of Uncertainty Formalisms, LNAI 1455. Berlin, Germany:
Springer Verlag. 266-302.
Fox, J.; Krause, P.; and Ambler, S. 1992. Arguments, con­
tradictions and practical reasoning. Proceedings of ECAI
1992, Vienna, Austria.
Freeman, J. B. 1991. Dialectics and the Macrostructure
of Arguments: A Theory of Argument Structure. Berlin,
Germany: Foris.
Graham, J. D.; Green, L. C.; and Roberts, M. J. 1988. In
Search ofSafety: Chemicals and Cancer Risk. Cambridge,
MA, USA: Harvard University Press.
Grice, H. P. 1975. Logic and conversation. In Cole, P., and
Morgan, J. L., eds., Sy ntax andSemanticsIll: SpeechActs.
New York City, NY, USA: Academic Press. 41-58.
Habermas, J. 1991. Moral Consciousness and Communica­
tiveAction. Cambridge, MA, USA: MIT Press. (Published
inGerman1983).
Haggith, M. 1996. A Meta-levelArgumentation Frame­
work for Representing and Reasoning aboutDisagreement.
Ph.D. Dissertation, University of Edinburgh, UK.
Hamblin, C. L. 1971. Mathematical models of dialogue.
Theoria37:130-155.
Krause, P.; Fox, J.; Judson, P.; and Patel, M. 1998. Qual­
itative risk assessment fulfils a need. In Hunter, A., and
Parsons, S., eds., Applications of UncertaintyFormalisms,
LNAI 1455. Berlin, Germany: Springer Verlag. 138-156.
Lakatos, 1., and Feyerabend, P. 1999. For andAgainst
Method. Chicago, IL, USA: University of Chicago Press.
McBurney, P., and Parsons, S. 1999. Truth or con­
sequences: using argumentation to reason about risk.
In British Psy chological Society Sy mposium on Practical
Reasoning. London, UK: BPS.
McBurney, P., and Parsons, S. 2000a. Intelligent systems
to support deliberative democracy in environmental regula­
tion. In Peterson, D., ed., AJSBSy mposium onAI andLegal
Reasoning. Birmingham, UK: AISB.

379

McBurney, P., and Parsons, S. 2000b. Modeling scientific
discourse. In Pearce, D., ed., Proceedings of the Work­
shop onScientific ReasoninginAI andPhilosophy ofSci­
ence, 14th European Conference onArtificial Intelligence
(ECAI- 2000).Berlin, Germany: ECAI.
McBurney, P., and Parsons, S. 2000c. Tenacious tortoises:
a formalism for argument over rules of inference. Techni­
cal report, Department of Computer Science, University of
Liverpool, UK.
McBurney, P., and Parsons, S. 2000 (In press). Risk Ago­
ras: Using dialectical argumentation to debate risk. Risk
ManagementJournal.
Naess, A. 1966. Communication andArgument: Elements
of Applied Semantics. London, UK: Allen and Unwin.
(PublishedinNorwegian1947).
Parsons, S., and Green, S. 1999. Argumentation and qual­
itative decision making. In Hunter, A., and Parsons, S.,
eds., The5th European Conference onSy mbolic and Quan­
titative Approaches to Reasoning and Uncertainty (EC­
SQARU99), LNAI 1638. Berlin, Germany: Springer Ver­
lag. 328-339.
Parsons, S.; Sierra, C.; and Jennings, N. R. 1998. Agents
that reason and negotiate by arguing. Journal ofLogic and
Computation8(3):261-292.
Pera, M. 1994. TheDiscourses of Science. Chicago, IL,
USA: University of Chicago Press.
Reed, C. 1998. GeneratingArguments in Natural Lan­
guage. Ph.D. Dissertation, University College, University
of London, London, UK.
Rehg, W. 1997. Reason and rhetoric in Habermas's The­
ory of Argumentation. In Jost, W., and Hyde, M. J., eds.,
Rhetoric andHermeneutics in Our Time: A Reader.New
Haven, CN, USA: Yale University Press. 358-377.
Rescher, N. 1977. Dialectics: A Controversy -Oriented
Approach to the Theory ofKnowledge. Albany, NY, USA:
State University of New York Press.
Toulmin, S. E. 1958. The Uses ofArgument. Cambridge,
UK: Cambridge University Press.
Verheij, B. 1999. Automated argument assistance for
lawyers. In Proceedings of theSeventh International Con­
ference onArtificial Intelligence andLaw, Oslo, Norway ,
43-52. New York City, NY, USA: ACM.
Walton, D. N., and Krabbe, E. C. W. 1995. Commitment
in Dialogue: Basic Concepts of Interpersonal Reasoning.
Albany, NY, USA: State University of New York Press.

