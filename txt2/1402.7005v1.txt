the framework of sequential design.
In this context, by allowing xt âˆˆ X to depend on previous
points and corresponding function evaluations Dtâˆ’1 =
{(x1 , f (x1 )), . . . , (xtâˆ’1 , f (xtâˆ’1 ))}, the algorithm constructs a sequence x1:n = (x1 , x2 , . . . , xn ) and returns the
element x(n) of highest possible value. That is, it returns
the value x(n) that minimizes the loss:
rn = sup f (x) âˆ’ f (x(n)).
xâˆˆX

This loss is not the same as the cumulative regret used often
in
Pnthe online learning literature: Rn = n supxâˆˆX f (x) âˆ’
t=1 f (x(t)).
Bayesian optimization (BO) is a popular sequential design
strategy for global optimization; see Brochu et al. (2009)
for an introductory treatment. Since the objective function
f is unknown, the Bayesian strategy is to treat it as a random function and place a prior over it. The prior captures
our beliefs about the behaviour of the function. After gathering the function evaluations Dtâˆ’1 , the prior is updated to
form the posterior distribution over f . The posterior distribution, in turn, is used to construct an acquisition function
that determines what the next query point xt should be.
Examples of acquisition functions include probability of
improvement, expected improvement, Bayesian expected
losses, upper confidence bounds (UCB), and dynamic portfolios of these (MocÌŒkus, 1982; Jones, 2001; Garnett et al.,
2010; Srinivas et al., 2010; Chen et al., 2012; Hoffman
et al., 2011). If we were to implement Thompson sampling strategies (May et al., 2011; Kaufmann et al., 2012;
Agrawal & Goyal, 2013) for Gaussian processes (GPs), we
would also encounter the difficult problem of having to find
the maximizer of a sample from the GP at each iteration,
unless we were considering only a finite set of query points
(Hoffman et al., 2014).
The maximum of the acquisition function is typically found
by resorting to discretisation or by means of an auxiliary optimizer. For example, Snoek et al. (2012) use discretisation, Bardenet & KeÌgl (2010) use adaptive grids,
Brochu et al. (2007); Martinez-Cantin et al. (2007) and
Mahendran et al. (2012) use the DIRECT algorithm of
Jones et al. (1993), Lizotte et al. (2011) use a combination
of random discretisation and quasi-Newton hill-climbing,

Bayesian Multi-Scale Optimistic Optimization

Bergstra et al. (2011) and Wang et al. (2013) use the CMAES method of Hansen & Ostermeier (2001), Hutter et al.
(2011) apply multi-start local search. (Approaches within
the framework of Bayesian nonlinear experimental design,
such as (Hennig & Schuler, 2012) for finding maxima
and (Kueck et al., 2006, 2009; Hoffman et al., 2009) for
learning functions and Markov decision processes, have to
rely on expensive approximate inference for computing intractable integrals. An analysis of these approaches is beyond the scope of this paper.)

inates the need for auxiliary optimization of the acquisition function in BO. We derive theoretical guarantees for
the method that do not depend on the assumption that
the acquisition function needs to be optimized exactly.
The method uses SOO to optimize the objective function directly, but eliminates the need for SOO to sample
points that are deemed unfit by Gaussian process posterior
bounds. That is, BaMSOO uses the posterior distribution
to reduce the number of function evaluations in SOO, thus
increasing the efficiency of SOO substantially.

The auxiliary optimization methodology is problematic for
several reasons. First, it is difficult to assess whether the
auxiliary optimizer has found the maximum of the acquisition function in practice. This creates important theoretical
concerns about the behaviour of BO algorithms because the
typical theoretical convergence guarantees are only valid
on the assumption that the optimum of the acquisition function can be found exactly; see for example Srinivas et al.
(2010); Vazquez & Bect (2010) and Bull (2011). Second,
running an auxiliary optimizer at each iteration of the BO
algorithm can be unnecessarily costly. For any two consecutive iterations, the acquisition function may not change
drastically. This questions the necessity of re-starting the
auxiliary optimization at each iteration.

The experiments with benchmarks from the global optimization literature demonstrate that BaMSOO outperforms
both GP-UCB and SOO. The paper also introduces a novel
application in the domain of knowledge discovery and information extraction. Finally, our theoretical results show
that BaMSOO can attain, up to log factors, a polynomial
finite sample convergence rate.

Recent optimistic optimization methods provide a viable alternative to BO (Kocsis & SzepesvaÌri, 2006; Bubeck et al.,
2011; Munos, 2011). Instead of estimating a posterior distribution over the unknown objective function, these methods build space partitioning trees by expanding leaves with
high function values or upper-bounds. The term optimistic,
in this context, is used to refer to the fact that the algorithms expand at each round leaves that may contain the
optimum. Remarkably, a variant of these methods, Simultaneous Optimistic Optimization (SOO) by Munos (2011),
is able to optimize an objective function globally without
knowledge of the functionâ€™s smoothness. SOO is optimistic
at all scales in the sense that it expands several leaves simultaneously, with at most one leaf per level. For this reason, instead of adopting the term â€œSimultaneous OOâ€ we
opt for the descriptive term â€œMulti-Scale OOâ€.
We will describe SOO in more detail in Section 3. We also
note that a stochastic variant of SOO has been recently proposed by Valko et al. (2013), but we restrict the focus of this
paper to the deterministic case.

2

BO with GP confidence bounds

Classical BO approaches have two ingredients that need to
be specified: The prior and the acquisition function. In this
work, as in most other works, we adopt Gaussian process
(GP) priors. We review GPs very briefly and refer the interested reader to the book of Rasmussen & Williams (2006)
for an in-depth treatment. A GP is a distribution over functions specified by its mean function m(Â·) and covariance
Îº(Â·, Â·). More specifically, given a set of points x1:t , with
xi âˆˆ X âŠ† RD , we have
f (x1:t ) âˆ¼ N (m(x1:t ), K),
where K, with entries Ki,j = Îº(xi , xj ), is the covariance
matrix. A common choice of Îº in the BO literature is the
anisotropic kernel with a vector of known hyper-parameters

Îº(xi , xj ) = Îº
e âˆ’(xi âˆ’ xj )T D(xi âˆ’ xj ) , (1)
where Îº
e is an isotropic kernel and D is a diagonal matrix
with positive hyper-parameters along the diagonal and zeros elsewhere. Our results apply to squared exponential
kernels and MateÌrn kernels with parameter Î½ â‰¥ 2. In this
paper, we assume that the hyper-parameters are fixed and
known in advance. We refer the reader to Martinez-Cantin
et al. (2007); Brochu et al. (2010); Wang et al. (2013);
Snoek et al. (2012) for different practical approaches to estimate the hyper-parameters.

These optimistic optimization methods do not require the
auxiliary optimization of acquisition functions. However,
due to the lack of a posterior that interpolates between the
sampled points, it is conceivable that these methods may
not be as competitive as BO in practical domains where
prior knowledge is available. This claim does not seem to
have been backed up by empirical evidence in the past.

An advantage of using GPs lies in their analytical tractability. In particular, given observations Dt = {x1:t , f1:t },
where fi = f (xi ), and a new point xt+1 , the joint distribution is given by:





K
k
f1:t
âˆ¼ N m(x1:t+1 ), T
ft+1
k
Îº(xt+1 , xt+1 )

This paper introduces a new algorithm, BaMSOO, which
combines elements of BO and SOO. Importantly, it elim-

where kT = [Îº(xt+1 , x1 ) Â· Â· Â· Îº(xt+1 , xt )]. For simplicity,
we assume that m(Â·) = 0. Using the Sherman-Morrison-

Ziyu Wang, Babak Shakibi, Lin Jin, Nando de Freitas

Woodbury formula, one can easily arrive at the posterior
predictive distribution:
f+

ft+1 |Dt , xt+1 âˆ¼ N (Âµ(xt+1 |Dt ), Ïƒ 2 (xt+1 |Dt )),
with mean Âµ(xt+1 |Dt ) = kT Kâˆ’1 f1:t and variance
Ïƒ 2 (xt+1 |Dt ) = Îº(xt+1 , xt+1 ) âˆ’ kT Kâˆ’1 k. We can compute the posterior predictive mean Âµ(Â·) and variance Ïƒ 2 (Â·)
exactly for any point xt+1 .
At each iteration of BO, one has to re-compute the predictive mean and variance. These two quantities are used
to construct the second ingredient of BO: The acquisition function (or utility function). In this work, we report
results for the
âˆš GP-UCB acquisition function U(x|Dt ) =
Âµ(x|Dt ) + Bt Ïƒ(x|Dt ), which is the upper confidence
bound (UCB) on the objective function (Srinivas et al.,
2010; de Freitas et al., 2012). We also make use of
the lower confidence bound
âˆš (LCB) which is defined as
L(x|Dt ) = Âµ(x|Dt ) âˆ’ Bt Ïƒ(x|Dt ). In these definitions, Bt is such that f (x) is bounded above and below
by U(x|Dt ) and L(x|Dt ) with high probability (de Freitas
et al., 2012).
BO selects the next query point by optimizing the acquisition function U(x|Dt ). Note that our choice of utility
favours the selection of points with high variance (points
in regions not well explored) and points with high mean
value (points worth exploiting). As mentioned in the introduction, the optimization of the closed-form acquisition
function is often carried out by off-the-shelf global optimization procedures, such as DIRECT and CMA-ES.
Many other acquisition functions have been proposed, but
they often yield similar results; see for example the works
of MocÌŒkus (1982) and Jones (2001). The idea of learning portfolios of acquisition functions online was explored
by Hoffman et al. (2011). We do not consider these acquisition functions for brevity. The BO procedure is summarized in Algorithm 1.
Algorithm 1 GP-UCB
for t = 1, 2, . . . do
xt+1 = arg maxxâˆˆX U(x|Dt ).
Augment the data Dt+1 = {Dt , (xt+1 , f (xt+1 ))}
end for

Finite sample bounds for GP-UCB were derived by Srinivas et al. (2010). However, the bounds depend on the algorithm being able to optimize the UCB acquisition function,
at each iteration, exactly. Unless the action set is discrete, it
is unlikely that we will be able to find the global optimum
of the UCB with a fixed budget optimization method. That
is, we may not be able to guarantee that we can find the exact optimum of the UCB, and hence the theoretical bounds
seem to make a very strong assumption in this regard.

True Objective.
Discarded Region.
Confidence Region.
Sampled Points.

Figure 1: The global shrinking method of de Freitas et al.
(2012). If the unknown objective function lies within the
(green) confidence bounds with high probability, we can
discard regions of the space where the upper bound is lower
than the best lower bound encountered thus far.
2.1

Shrinking feasible regions

de Freitas et al. (2012) introduced a different GP-based
scheme to trade off exploration and exploitation. Instead
of optimizing the acquisition function, they proposed to
sample the objective function using a finite lattice within
a feasible region R. The feasible region at the tth iteration
is defined as
Rt = {x : Âµt (x) + Bt Ïƒt (x) > sup Âµt (x) âˆ’ Bt Ïƒt (x)}.
xâˆˆRtâˆ’1

That is, one should only search in the region where the upper bound is greater than the best lower bound encountered
thus far, as illustrated in Figure 1. With high probability,
the optimizer lies within Rt .
de Freitas et al. (2012) proved that if we double the density
of points in the lattice at each iteration, the feasible region
shrinks very quickly. More precisely, they showed that the
simple regret vanishes at an exponential rate and that the
cumulative regret is bounded by a constant.
With this approach, they did not have to resort to optimizing an acquisition function. However, even in moderate dimensions, their algorithm is impractical since the lattice often becomes too large to be sampled in a reasonable amount
of time.
In this paper, we will argue that to overcome this problem,
an optimistic strategy may have to be employed. Such a
strategy enables us to sample the most promising regions
first, so as to avoid the computational cost associated with
covering the whole space. In the next section, we begin our
discussion of optimistic strategies.

3

Simultaneous optimistic optimization

Deterministic optimistic optimization (DOO) and simultaneous optimistic optimization (SOO) are tree-based space

Bayesian Multi-Scale Optimistic Optimization
Tree Built by SOO

Algorithm 2 SOO
Evaluate f (x0,0 )
Initialize the tree T1 = {0, 0}
Set n = 1
while true do
Set Î½max = âˆ’âˆ
for h = 0 : min{depth(Tn ), hmax (n)} do
Select (h, j) = arg maxjâˆˆ{j|(h,j)âˆˆLn } f (xh,j )
if f (xh,j ) > Î½max then
Evaluate the children of (h, j)
Add the children of (h, j) to Tn
Set Î½max = f (xh,j )
Set n = n + 1
end if
end for
end while

partitioning methods for black-box function optimization
(Munos, 2011, 2014). They were inspired by the UCT algorithm, which enjoyed great success in planning (Kocsis &
SzepesvaÌri, 2006). UCT was shown to have no finite-time
guarantees by Coquelin & Munos (2007). This prompted
the development of a range of optimistic, in the face of uncertainty, approaches. The term optimism, here, refers to
the fact that the strategies expand at each round tree cells
that may contain the optimum.
DOO and SOO partition the space X hierarchically by
building a tree. Let us assume that each node of the tree
has k children. A node (h, j) at level h of the tree has children {(h + 1, kj + i)}0â‰¤i<kâˆ’1 . The children partition the
parent cell Xh,j into cells {Xh+1,kj+i , 0 â‰¤ i < k âˆ’ 1}.
The root cell is the entire space X . A node is always evaluated at the center of the cell, which we denote as xh,j .
Instead of assuming that the target function is a sample
from a GP, DOO and SOO assume the existence of a symmetric semi-metric ` such that f (xâˆ— ) âˆ’ f (x) â‰¤ `(x, xâˆ— )
where xâˆ— is the maximizer of f . Although, SOO assumes
that ` exists, it does not require explicit knowledge of it.
DOO on the other hand does require knowledge of `. DOO
builds a tree Tn incrementally, where n denotes the index over node expansions. DOO expands a leaf (h, j)
from the set of leaves Ln (nodes whose children are not
in Tn ) if it has the the highest upper bound: f (xh,j ) +
supxâˆˆXh,j `(xh,j , x). This value for any cell containing xâˆ—
upper bounds the best function value f âˆ— . The performance
of DOO depends crucially on our knowledge of the true local smoothness of f . SOO aims to overcome the difficulty
of having to know the true local smoothness.
SOO, as summarized in Algorithm 2, expands several
leaves simultaneously. When a node is expanded, its children are evaluated. At each round, SOO expands at most
one leaf per level, and a leaf is expanded only if it has
the largest value among all leaves of the same or lower
depths. The SOO algorithm takes as input a function

Sampled Points.

Tree Built by BaMSOO

Sampled Points.

Figure 2: [TOP]: The tree built by SOO when optimizing
the function f (x) = 12 sin(15x) sin(27x) in [0, 1]. [BOTTOM]: The tree built by BaMSOO. The 20 blue dots represent nodes where the objective was evaluated. BaMSOO, in comparison, does not evaluate the objective function for points known to be sub-optimal with high probability. Hence, BaMSOO can achieve a better coverage of the
search space with the same number of function evaluations
as SOO.

n â†’ hmax (n), which limits the maximum height of the
tree after n node expansions. hmax (n) defines a tradeoff
between deep versus broad exploration. At the end of the
finite horizon, SOO returns the x with the highest objective
function value. Figure 2 illustrates the application of SOO
to a simple 1-dimensional optimization problem.

4

BaMSOO

SOO offers a different way of trading off exploration and
exploitation that does not require the optimization of an
acquisition function. However, it does not utilize all the
information brought in by the previously evaluated points
effectively. To improve upon SOO in practice, we consider
the additional assumption that the objective function is a
sample from a GP prior.
We define the LCB and UCB to be LN (x|Dt ) = Âµ(x|Dt )âˆ’
BN Ïƒ(x|Dt ) and
p UN (x|Dt ) = Âµ(x|Dt ) + BN Ïƒ(x|Dt )
where BN = 2 log(Ï€ 2 N 2 /6Î·) and Î· âˆˆ (0, 1).

Ziyu Wang, Babak Shakibi, Lin Jin, Nando de Freitas

The BaMSOO algorithm is very similar to SOO. As with
SOO, we only evaluate the cell at the center point. However, when a nodeâ€™s UCB is less than the function value
of the best point already sampled, denoted f + , we do not
evaluate the objective function at this node because with
high probability the center point is sub-optimal. Instead,
we simply assign to this node its LCB value. Note that if
the center-point of a cell is sub-optimal, the cell may still
contain the optimizer. Hence this cell must also be further
expanded in subsequent iterations. To manage these two
types of node in the pseudo-code (see Algorithm 3), we introduce a place-holder function g which is set to f when
the UCB of the cell of interest is bigger than f + , and it is
set to the LCB of the node otherwise. For clarity, we remind the reader that the indices N, k, t and n are over node
evaluations, branches (children), function evaluations and
node expansions respectively.
In the pseudocode, we have highlighted in blue the additional lines of code brought in by BaMSOO. Effectively,
BamSOO only involves a slight modification of SOO (Algorithm 2) provided we have GP routines to evaluate the
LCB and UCB.
We found the assignment of the LCB values to nodes that
do worse than f + to work well in practice. For this reason our presentation, experiments and theory focus on this
choice.
BaMSOO improves upon SOO by making use of the available information more efficiently. Moreover, by using an
optimistic proposal, it avoids the need to sample exhaustively before shrinking the feasible region as in (de Freitas
et al., 2012). Figure 2 illustrates how BaMSOO can cover
the search space more effectively, even though it incurs the
same number of expensive function evaluations as SOO.

5

Analysis

In this section, we provide an overview of the theoretical
analysis of BaMSOO, which appears in the Appendix. Our
discussion here will focus on our assumptions. At the end
of this section, we will present the main result and sketch
the proof coarsely.
We denote the global maximum by f âˆ— = supxâˆˆX f (x) and
the maximizer by xâˆ— = arg maxxâˆˆX f (x).
We make similar assumptions to those made by de Freitas
et al. (2012). As in their case, we make the global assumption that the objective function is a sample from a GP and
a local assumption about the behavior of the objective near
the optimum.
D

Assumption 1 (Conditions on the GP kernel). X âŠ† R is
a compact set, and Îº is a kernel on RD that is twice differentiable along the diagonal such that âˆ‚x âˆ‚x0 Îº(x, x0 )|x=x0
exists.

Algorithm 3 BaMSOO
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:

Set g0,0 = f (x0,0 )
Set f + = g0,0
Initialize the tree T1 = {0, 0}
Set t = 1, n = 1, N = 1, and Dt = {(x0,0 , g(x0,0 ))}
while true do
Set Î½max = âˆ’âˆ.
for h = 0 to min{depth(Tn ), hmax (n)} do
Select (h, j) = arg maxjâˆˆ{j|(h,j)âˆˆLn } g(xh,j )
if g(xh,j ) > Î½max then
for i = 0 to k âˆ’ 1 do
Set N = N + 1
if UN (xh+1,kj+i |Dt ) â‰¥ f + then
Set g(xh+1,kj+i ) = f (xh+1,kj+i )
Set t = t + 1
Dt = {Dtâˆ’1 , (xh+1,kj+i , g(xh+1,kj+i ))}
else
Set g(xh+1,kj+i ) = LN (xh+1,kj+i |Dt )
end if
if g(xh+1,kj+i ) > f + then
Set f + = g(xh+1,kj+i )
end if
end for
Add the children of (h, j) to Tn
Set Î½max = g(xh,j )
Set n = n + 1
end if
end for
end while

Assumption 2 (Local smoothness of f ). f âˆ¼ GP(0, Îº) is
a continuous sample on X that has a unique global maximum xâˆ— , such that f âˆ— âˆ’ c1 kx âˆ’ xâˆ— kÎ±
2 â‰¤ f (x) âˆ€x âˆˆ X
and f (x) â‰¤ f âˆ— âˆ’ c2 kx âˆ’ xâˆ— k22 âˆ€x âˆˆ B(xâˆ— , Ï) for
some constants c1 , c2 , Ï > 0 and Î± âˆˆ {1, 2}. Also
f âˆ— âˆ’ maxxâˆˆX \B(xâˆ— ,Ï) f (x) > 0 for some 0 > 0.
As argued by de Freitas et al. (2012), in many practical
cases the local conditions follow almost surely from the
global condition. For example, if we were to employ the
Matern kernel with Î½ > 2 or a kernel that is 6 times differentiable along the diagonal, we would have that the samples
of the GPs are twice differentiable with probability one.
The first case was shown by (Adler & Taylor, 2007, Theorem 1.4.2) and (Stein, 1999, Â§2.6), while the second result
was shown by (Ghosal & Roy, 2006, Theorem 5). If the xâˆ—
lies in the interior of X , then the Hessian of f at xâˆ— would
be almost surely non-singular as at least one of the eigenvalues of the Hessian is a co-dimension 1 condition in the
space of all functions that are smooth at a given point (de
Freitas et al., 2012). In this case, we would have that
âˆ—
âˆ— 2
f âˆ— âˆ’ c1 kx âˆ’ xâˆ— kÎ±
2 â‰¤ f (x) â‰¤ f âˆ’ c2 kx âˆ’ x k2

with Î± = 2.
If xâˆ— lies on the boundary of X which we assume to be
smooth, then âˆ‡f (xâˆ— ) 6= 0 since the additional event of
the vanishing of âˆ‡f (xâˆ— ) is a co-dimension d phenomenon
in the space of functions with global maximum at xâˆ— (de

Bayesian Multi-Scale Optimistic Optimization

Freitas et al., 2012). In this case, we would have that
âˆ—
âˆ— 2
f âˆ— âˆ’ c1 kx âˆ’ xâˆ— kÎ±
2 â‰¤ f (x) â‰¤ f âˆ’ c2 kx âˆ’ x k2

with Î± = 1.
Finally, a sample from a GP on a compact domain has a
unique maximum with probability one. This is because the
space of continuous functions on a compact domain that
attain their global maximum at more than one point have
co-dimension 1 in the space of all continuous functions on
that domain (de Freitas et al., 2012).
The subsequent assumptions are about the hierarchical partitioning of the search space. They are the same as Assumptions 3 and 4 in Munos (2011).
Assumption 3 (Bounded diameters). There exists a decreasing sequence Î´(h) > 0, such that for any depth
h â‰¥ 0, for any cell Xh,i of depth h, we have
supxâˆˆXh,i `(xh,i , x) â‰¤ Î´(h). Here `(x, y) := c1 kx âˆ’ ykÎ±
2
where Î± âˆˆ {1, 2} and Î´(h) = cÎ³ h for some constant c > 0
and Î³ âˆˆ (0, 1).
Assumption 4 (Well-shaped cells). There exists Î½ > 0
such that for any depth h â‰¥ 0, any cell Xh,i contains an
`-ball of radius Î½Î´(h) centered in Xh,i .
Note that depending on the value of Î±, Î³ would have to take
on a different value for Assumptions 3 and 4 to be satisfied.
Regardless of the choice of Î± and as illustrated in Example
1 of Bubeck et al. (2011), Assumptions 3 and 4 are easy to
satisfy in practice; for example when x âˆˆ [0, 1]D and the
split is done along the largest dimension of a cell. This is
the case in all our experiments.
Assumption 2 together with Assumptions 3 and 4 impose a
â€œnear optimalityâ€ condition as defined by Munos (2011).
We can now present our main result, which is in the form
of a corollary to Theorem 1 in the Appendix.
Corollary 1. Let d = âˆ’(D/4 âˆ’ D/Î±) and hmax (n) =
n . Given Assumptions 1 âˆ’ 4, we have that with
probability
 Î·, the loss of BaMSOO is
 1âˆ’ at Î±least 1 âˆ’
O nâˆ’ d log 4âˆ’Î± (n2 /Î·) .
It is worth pointing out that the result presented in Corollary 1 is based on the number of node expansions n instead of the number of function evaluations. The theory
can therefore be strengthened.
If Î± = 2 and  = 1/2, then the above result trans2
lates to O nâˆ’ D log(n2 /Î·) . If Î± = 1 with  being
thesame as before, then
 the rate of convergence becomes
1
2
âˆ’ 3D
2
3
O n
log (n /Î·) .
The structure of the proof follows that in Munos (2011).
Let xâˆ—h denote the optimal node at level h (that is, the node
at height h in the branch that contains the optimum xâˆ— ).

Our proof shows that once xâˆ—h is expanded, it does not take
long for xâˆ—h+1 to be expanded. Once an optimal node xâˆ—h is
expanded, by Assumptions 2 and 3, we have that the loss
of BaMSOO is no worse than Î´(h) .
The main difficulty of the proof lies in the fact that we
sometimes do not sample nodes when their UCB values
are less than the best observed value. In this case, we can
no longer make the claim that an optimal node is expanded
soon after its parent. This is because when a node is not
expanded, its LCB can be very low due to a high standard
deviation. Fortunately, we can show that this is not the case
for optimal nodes in the optimal region. This is accomplished by showing that the standard deviation at a point is
no more than its distance to the nearest sampled point up
to a constant factor (shown in Lemma 3). This enables us
to show that every optimal node in the optimal region must
have a low standard deviation. Given this result, we can
adopt the proof structure outlined in Munos (2011).

6

Experiments with global optimization
benchmarks

In this section, we validate the proposed algorithm with
a series of experiments that compare the three algorithms
(GP-UCB, SOO, BaMSOO) on global optimization benchmarks. We have omitted the feasible region shrinking
algorithm (described in Section 2.1) as it is not practical for problems of even moderate dimensions. We have
also omitted comparisons to PI and EI as these appear in
Hoffman et al. (2011) for the optimization benchmarks described in this paper.
In our experiments, we used the same hyper-parameters in
GP-UCB and BaMSOO for each test function. We also
randomized the initial sample point for BaMSOO and GPUCB so that they are not deterministic. To optimize the acquisition function for GP-UCB, we used DIRECT followed
by a local optimization method using gradients.
We use 5 test functions: Branin, Rosenbrock, Hartmann3,
Hartmann6, and Shekel. All of these test functions are
common in the global optimization literature and with the
exception of the Rosenbrock, they are all multi-modal. 1
We rescaled the domain of each function to the [0, 1]D
hypercube, and we used the log distance to the true optimum as our evaluation metric. This metric is defined as
log10 (f âˆ— âˆ’ f + ) where f + is the best objective value sampled so far and f âˆ— is the true maximum value of the objective. For each test function, we repeat our experiments
50 times for GP-UCB and BaMSOO and run SOO once as
SOO is a deterministic strategy. We plot the mean and a
confidence bound of one standard deviation of our metric
1

Detailed information about the test functions is available
at the following website: http://www-optima.amp.i.kyoto-u.ac.jp/
member/student/hedar/Hedar_files/TestGO_files/Page364.htm.

Ziyu Wang, Babak Shakibi, Lin Jin, Nando de Freitas
GP-UCB
SOO-UCB
SOO

6

0

âˆ’2

âˆ’4

âˆ’6

âˆ’8

4

2

0

âˆ’2

âˆ’4

âˆ’6

âˆ’10

50

100

150

200

âˆ’8

50

No. of Iterations (t)

100

150

No. of Iterations (t)

200

GP-UCB
SOO-UCB
SOO

2

Log Distance to optimal

GP-UCB
SOO-UCB
SOO

2

Log Distance to optimal

Log Distance to optimal

4

0

âˆ’2

âˆ’4

âˆ’6

âˆ’8

âˆ’10

50

100

150

200

No. of Iterations (t)

Figure 3: Comparison of GP-UCB, SOO, and BaMSOO on multi-modal test functions of low dimensionality (Branin,
Rosenbrock and Hartmann3D). GP-UCB and BaMSOO perform similarly whereas SOO does poorly. The poor performance of SOO is caused by having weaker assumptions on the smoothness of the objective function. The good performance
of GP-UCB indicates that when the dimensionality is low optimizing the acquisition function is reasonable.

across all the runs for all the tests.
For simplicity, we only consider binary-trees for space partitioning in SOO and BaMSOO. Specifically, the largest dimension in the parentâ€™s cell is split to create two children.
First, we test the global optimization schemes on 3 test
functions with low dimensionality: Branin, Rosenbrock
and Hartmann3. The Branin function (Jones, 2001) is a
common benchmark for Bayesian optimization and has 2
dimensions. The Rosenbrock function is a commonly used

Log Distance to optimal

4

GP-UCB
SOO-UCB
SOO

2

0

âˆ’2

âˆ’4

âˆ’6

âˆ’8

100

200

300

400

500

No. of Iterations (t)
GP-UCB
SOO-UCB
SOO

Log Distance to optimal

2

0

âˆ’2

âˆ’4

âˆ’6

100

200

300

400

500

No. of Iterations (t)

Figure 4: Comparison of GP-UCB, SOO, and BaMSOO on
multi-modal test functions of moderate dimensionality: 4D
Shekel function (top) and 6D Hartmann function (bottom).
Here, GP-UCB performs poorly. This is due in part to the
hardness of optimizing the acquisition function.

non-convex test function for local optimization algorithms,
and although it is unimodal, its optimum lies in a long
narrow valley, which makes the function hard to optimize.
Finally, the Hartmann3 function is 3-dimensional and has
four local optima.
As we can see from Figure 3, BaMSOO performs competitively against GP-UCB on these low dimensional test
functions. Both BaMSOO and GP-UCB achieve very high
accuracies of up to 10âˆ’8 in terms of the distance to the optimal objective value. In comparison, SOO, due to the lack
of a strong prior assumption, cannot take advantage of the
points sampled and thus is lagging behind.
In the experiments shown in Figure 4, we compare the approaches in consideration on the Shekel function and the
Hartmann6 function. The Shekel function is 4-dimensional
and has 10 local optima. The Hartmann6 function is 6dimensional, as the name suggests, and has 6 local optima.
On these higher dimensional problems, the performance of
GP-UCB begins to dwindle. Despite the increase in dimensionality, BaMSOO is still able to optimize the test functions to a relatively high precision. SOO does not perform
as well as BaMSOO again because of its weak assumptions. The poor performance of GP-UCB on these two test
functions may be due in part to the inability of a global optimizer to optimize the acquisition function exactly in each
iteration. As the dimensionality increases, so is the difficulty of optimizing a non-convex function globally as the
cost of covering the space grows exponentially. The optimization of the acquisition function through algorithms
like DIRECT demands the repartitioning of the space in
each iteration. To reach a finer granularity, we either have
to sacrifice speed by building very fine partitions in each
iteration or accuracy by using coarser partitions.
The proposed approach is not only competitive with GPUCB in terms of effectiveness, it is also more computationally efficient. As we can see in Table 1, BaMSOO is
about 10-40 times faster than GP-UCB on the test func-

Bayesian Multi-Scale Optimistic Optimization

Table 1: Time required for the test functions measured in seconds. SOO is very fast as it does not maintain a GP. BaMSOO
maintains a GP to produce more accurate posterior estimates and is hence slower. The rejection of proposals also results
in bigger trees, further slowing down the algorithm. GP-UCB is slow compared to the other two algorithms as it not only
maintains a GP but also optimizes its acquisition function at each iteration.
Branin
29.9438
3.0680
0.1810

Rosenbrock
29.5716
3.4693
0.1835

tions that we have experimented with. This is because instead of optimizing the acquisition function in each iteration the SOO algorithm, that sits inside, only optimizes
once. BaMSOO, however, is much slower than SOO. This
is because BaMSOO also employs a GP to reject points
proposed by SOO. To sample one point, SOO may have to
propose many points before one is accepted. For this reason, BaMSOO would build much bigger trees compared to
SOO and it is therefore slower.

Hartmann3
34.0311
3.9722
0.1871

Hartmann6
115.2402
2.0918
0.4313

Shekel
100.7770
3.8951
0.4350

0.70

0.65

F-score

Algorithm
GP-UCB
BaMSOO
SOO

0.60

0.55

0.50

7

Application to term extraction

In this section, we evaluate the performance of the BaMSOO algorithm on optimizing the parameters in a term extraction algorithm. Term extraction is the process of analyzing a text corpus to find terms, where terms correspond
to cohesive sequences of words describing entities of interest. Term extraction tools are widely used in industrial
text mining and play a fundamental role in the construction
of knowledge graphs and semantic search products. Recently Parameswaran et al. (2010) proposed a term extraction method, and showed that it outperforms state-of-theart competitors, but their method has many free parameters that require manual adjustment. Here, we compare the
performance of BaMSOO, GP-UCB and SOO in automatically tuning the 4 primary free parameters of the algorithm
(support-thresholds). We define our deterministic objective
function to be the F-score of the extracted terms, which is a
weighted average of precision and recall. Precision is calculated using a predefined set of correct terms and recall is
estimated by simply normalizing the number of extracted
correct terms to be in the range [0,1]. We run the experiment on the GENIA corpus (Kim et al., 2003), which is a
collection of 2000 abstracts from biomedical articles. The
results of the experiment are shown in Figure 5. It is evident from this figure that BaMSOO outperforms GP-UCB
and SOO in this application.

8

Discussion

This paper introduced a new global optimization algorithm
BaMSOO, which does not require the auxiliary optimization of either acquisition functions or samples from the GP.
In trials with benchmark functions from the global opti-

0.45
0

GP-UCB
SOO
BaMSOO
50

100

150

No. of function evaluations (t)

Figure 5: Comparison of GP-UCB, SOO, and BaMSOO on optimizing 4 parameters in term extraction from
the GENIA corpus using a term extraction algorithm by
(Parameswaran et al., 2010). In this plot, higher is better.

mization literature, the new algorithm outperforms standard BO with GPs and SOO, while being computationally
efficient. The paper also provided a theoretical analysis
proving that the loss of BaMSOO decreases polynomially.
The careful reader may have noticed that, despite the effectiveness of BaMSOO in the experiments, the convergence rate of BaMSOO is not as good as that of SOO for
Î± = 2. This is because we were only able to prove that the
standard deviation at a point decreases linearly, instead of
quadratically, when a nearby point is sampled (Lemma 3
in the Appendix). Since by assumption the objective function behaves quadratically in the optimal region, the linear decrease of the standard deviation gives rise to a suboptimal convergence rate. It is also interesting to note that
the same type of bound on the standard deviation was used
by Bull (2011), who achieved similar convergence rates to
the ones in this paper. de Freitas et al. (2012) showed that
if the samples form a Î´-cover on a subset of D âŠ† X , then
the standard deviation of all points on D is bounded by
2
a quadratic term Q
4 Î´ . Via this observation, the authors
achieved a geometric convergence rate. The requirement
of the Î´-cover, however, renders their algorithm impractical. Finding a practical GP-based algorithm that achieves
geometric convergence rates remains an open problem.

Ziyu Wang, Babak Shakibi, Lin Jin, Nando de Freitas

Acknowledgements
We would like to thank Remi Munos for many valuable
discussions. We also thank NSERC and the University of
Oxford for financial support.

References
Adler, R. J. and Taylor, J. E. Random Fields and Geometry.
Springer, 2007.
Agrawal, S. and Goyal, N. Thompson sampling for contextual bandits with linear payoffs. In International Conference on Machine Learning, 2013.
Bardenet, R. and KeÌgl, B. Surrogating the surrogate: accelerating Gaussian-process-based global optimization with
a mixture cross-entropy algorithm. In International Conference on Machine Learning, pp. 55â€“62, 2010.
Bergstra, J., Bardenet, R., Bengio, Y., and KeÌgl, B. Algorithms for hyper-parameter optimization. In Advances in
Neural Information Processing Systems, pp. 2546â€“2554,
2011.
Brochu, E., de Freitas, N., and Ghosh, A. Active preference
learning with discrete choice data. In Advances in Neural
Information Processing Systems, pp. 409â€“416, 2007.
Brochu, E., Cora, V. M., and de Freitas, N. A tutorial on
Bayesian optimization of expensive cost functions, with
application to active user modeling and hierarchical reinforcement learning. Technical Report UBC TR-200923 and arXiv:1012.2599v1, Dept. of Computer Science,
University of British Columbia, 2009.
Brochu, E., Brochu, T., and de Freitas, N. A Bayesian
interactive optimization approach to procedural animation design. In Proceedings of the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, pp. 103â€“112, 2010.
Bubeck, S., Munos, R., Stoltz, G., and Szepesvari, C. Xarmed bandits. Journal of Machine Learning Research,
12:1655â€“1695, 2011.
Bull, A. D. Convergence rates of efficient global optimization algorithms. Journal of Machine Learning Research,
12:2879â€“2904, 2011.
Chen, B., Castro, R.M., and Krause, A. Joint optimization and variable selection of high-dimensional Gaussian processes. In International Conference on Machine
Learning, 2012.
Coquelin, P.A. and Munos, R. Bandit algorithms for tree
search. In Uncertainty in Artificial Intelligence, pp. 67â€“
74, 2007.

de Freitas, N., Smola, A., and Zoghi, M. Exponential regret
bounds for Gaussian process bandits with deterministic
observations. In International Conference on Machine
Learning, 2012.
Garnett, R., Osborne, M. A., and Roberts, S. J. Bayesian
optimization for sensor set selection. In ACM/IEEE
International Conference on Information Processing in
Sensor Networks, pp. 209â€“219. ACM, 2010.
Ghosal, S. and Roy, A. Posterior consistency of Gaussian
process prior for nonparametric binary regression. The
Annals of Statistics, 34:2413â€“2429, 2006.
Hansen, N. and Ostermeier, A. Completely derandomized
self-adaptation in evolution strategies. Evol. Comput., 9
(2):159â€“195, 2001.
Hennig, P. and Schuler, C.J.
Entropy search for
information-efficient global optimization. The Journal of
Machine Learning Research, 98888:1809â€“1837, 2012.
Hoffman, M., Kueck, H., de Freitas, N., and Doucet, A.
New inference strategies for solving Markov decision
processes using reversible jump MCMC. In Uncertainty
in Artificial Intelligence, pp. 223â€“231, 2009.
Hoffman, M., Brochu, E., and de Freitas, N. Portfolio allocation for Bayesian optimization. In Uncertainty in
Artificial Intelligence, pp. 327â€“336, 2011.
Hoffman, M.W., Shahriari, B., and de Freitas, N. On correlation and budget constraints in model-based multiarmed-bandit optimization with application to automatic
machine learning. In AI and Statistics, 2014.
Hutter, F., Hoos, H. H., and Leyton-Brown, K. Sequential
model-based optimization for general algorithm configuration. In Proceedings of LION-5, pp. 507523, 2011.
Jones, D. R., Perttunen, C. D., and Stuckman, B. E. Lipschitzian optimization without the Lipschitz constant.
Journal of Optimization Theory and Applications, 79(1):
157â€“181, 1993.
Jones, D.R. A taxonomy of global optimization methods
based on response surfaces. J. of Global Optimization,
21(4):345â€“383, 2001.
Kaufmann, E., Korda, N., and Munos, R. Thompson sampling: An asymptotically optimal finite-time analysis.
In Algorithmic Learning Theory, volume 7568 of Lecture Notes in Computer Science, pp. 199â€“213. Springer
Berlin Heidelberg, 2012.
Kim, J., Ohta, T., Tateisi, Y., and ichi Tsujii, J. GENIA corpus - a semantically annotated corpus for bio-textmining.
In ISMB (Supplement of Bioinformatics), pp. 180â€“182,
2003.

Bayesian Multi-Scale Optimistic Optimization

Kocsis, L. and SzepesvaÌri, C. Bandit based Monte-Carlo
planning. In European Conference on Machine Learning, pp. 282â€“293. 2006.
Kueck, H., de Freitas, N., and Doucet, A. SMC samplers
for Bayesian optimal nonlinear design. In IEEE Nonlinear Statistical Signal Processing Workshop, pp. 99â€“102,
2006.
Kueck, H., Hoffman, M., Doucet, A., and de Freitas, N.
Inference and learning for active sensing, experimental
design and control. In Araujo, H., Mendonca, A., Pinho,
A., and Torres, M. (eds.), Pattern Recognition and Image Analysis, volume 5524, pp. 1â€“10. Springer Berlin
Heidelberg, 2009.
Lizotte, D., Greiner, R., and Schuurmans, D. An experimental methodology for response surface optimization
methods. J. of Global Optimization, pp. 1â€“38, 2011.
Mahendran, N., Wang, Z., Hamze, F., and de Freitas, N.
Adaptive MCMC with Bayesian optimization. Journal
of Machine Learning Research - Proceedings Track, 22:
751â€“760, 2012.
Martinez-Cantin, R., de Freitas, N., Doucet, A., and Castellanos, J. A. Active policy learning for robot planning
and exploration under uncertainty. Robotics Science and
Systems, 2007.
May, B. C., Korda, N., Lee, A., and Leslie, D. S. Optimistic
Bayesian sampling in contextual bandit problems. Technical Report 11:01, Statistics Group, School of Mathematics, University of Bristol, 2011.
MocÌŒkus, J. The Bayesian approach to global optimization.
In Systems Modeling and Optimization, volume 38, pp.
473â€“481. Springer, 1982.
Munos, R. Optimistic optimization of a deterministic function without the knowledge of its smoothness. In Advances in Neural Information Processing Systems, pp.
783â€“791, 2011.
Munos, R. From Bandits to Monte-Carlo Tree Search:
The Optimistic Principle Applied to Optimization and
Planning. Technical Report hal-00747575, INRIA Lille,
2014.
Parameswaran, A., Garcia-Molina, H., and Rajaraman, A.
Towards the web of concepts: Extracting concepts from
large datasets. Proceedings of the VLDB Endowment, 3
(1-2):566â€“577, 2010.
Rasmussen, C. E. and Williams, C. K. I. Gaussian Processes for Machine Learning. The MIT Press, 2006.
Snoek, J., Larochelle, H., and Adams, R. P. Practical
Bayesian optimization of machine learning algorithms.
In Advances in Neural Information Processing Systems,
2012.

Srinivas, N., Krause, A., Kakade, S. M., and Seeger, M.
Gaussian process optimization in the bandit setting: No
regret and experimental design. In International Conference on Machine Learning, pp. 1015â€“1022, 2010.
Stein, M. L. Interpolation of Spatial Data: Some Theory
for Kriging. Springer, 1999.
Valko, M., Carpentier, A., and Munos, R. Stochastic simultaneous optimistic optimization. In International Conference on Machine Learning, 2013.
Vazquez, E. and Bect, J. Convergence properties of the
expected improvement algorithm with fixed mean and
covariance functions. J. of Statistical Planning and Inference, 140:3088â€“3095, 2010.
Wang, Z., Zoghi, M., Matheson, D., Hutter, F., and de Freitas, N. Bayesian optimization in high dimensions via
random embeddings. In International Joint Conference
on Artificial Intelligence, 2013.

Ziyu Wang, Babak Shakibi, Lin Jin, Nando de Freitas

A

Proofs

We begin by introducing some notation. Let xâˆ—h denote the optimal node at level h. That is the cell of xâˆ—h contains the
optimizer xâˆ— . Also let f + and x+ represent the best function value observed thus far and the associated node respectively.
A.1

Technical Lemmas

Lemma 1 (Lemma 5 of de Freitas et al. (2012)). Given a set of points x1:T := {x1 , . . . , xT } âˆˆ D and a Reproducing
Kernel Hilbert Space (RKHS) H with kernel Îº the following bounds hold:
1. Any f âˆˆ H is Lipschitz continuous with constant kf kH L, where k Â· kH is the Hilbert space norm and L satisfies the
following:
L2 â‰¤ sup âˆ‚x âˆ‚x0 Îº(x, x0 )|x=x0
xâˆˆD

and for Îº(x, x0 ) = Îº
e(x âˆ’ x0 ) we have

L2 â‰¤ âˆ‚x2 Îº
e(x)|x=0 .

2. The projection operator P1:T on the subspace span {Îº(xt , Â·)} âŠ† H is given by
t=1:T

P1:T f := k> (Â·)Kâˆ’1 hk(Â·), f i
>

where k(Â·) = k1:T (Â·) := [Îº(x1 , Â·) Â· Â· Â· Îº(xT , Â·)] and K := [Îº(xi , xj )]i,j=1:T ; moreover, we have that
ï£¹
ï£¹ ï£®
f (x1 )
hÎº(x1 , Â·), f i
ï£¯
ï£º ï£¯ .. ï£º
..
hk(Â·), f i := ï£°
ï£» = ï£° . ï£».
.
ï£®

hÎº(xT , Â·), f i

f (xT )

Here P1:T P1:T = P1:T and kP1:T k â‰¤ 1 and k 1 âˆ’P1:T k â‰¤ 1.
3. Given tuples (xi , fi ) with fi = f (xi ), the minimum norm interpolation fÂ¯ with fÂ¯(xi ) = f (xi ) is given by fÂ¯ = P1:T f .
Consequently its residual g := (1 âˆ’ P1:T )f satisfies g(xi ) = 0 for all xi âˆˆ x1:T .
Lemma 2 (Lemma 6 of de Freitas et al. (2012)). Under the assumptions of Lemma 1 it follows that
|f (x) âˆ’ P1:T f (x)| â‰¤ kf kH ÏƒT (x),
âˆ’1
k1:T (x) and this bound is tight. Moreover, ÏƒT2 (x) is the posterior predictive
where ÏƒT2 (x) = Îº(x, x) âˆ’ k>
1:T (x)K
variance of a Gaussian process with the same kernel.

Lemma 3 (Adapted from Proposition 1 of de Freitas et al. (2012)). Let Îº : RD Ã— RD â†’ R be a kernel that is twice
differentiable along the diagonal {(x, x) | x âˆˆ RD }, with L defined as in Lemma 1.1, and f be an element of the RKHS
with kernel Îº. If f is evaluated at point x, then for any other point y we have ÏƒT (y) â‰¤ Lkx âˆ’ yk.
Proof. Let H be the RKHS corresponding to Îº and f âˆˆ H an arbitrary element with g := (1 âˆ’P1:T )f ; the residual defined
in lemma 1.3. Since g âˆˆ H, we have by Lemma 1.1, g is Lipschitz. Thus we have that for any point y:
|g(y)| â‰¤ LkgkH ky âˆ’ xk â‰¤ Lkf kH ky âˆ’ xk,

(2)

where the second inequality is guaranteed by Lemma 1.2. On the other hand, by Lemma 2, we know that for all y we have
the following tight bound:
|g(y)| â‰¤ kf kH ÏƒT (y)
(3)
Now, given the fact that both inequalities (2) and (3) are bounding the same quantity and that the latter is a tight estimate,
we necessarily have that:
kf kH ÏƒT (y) â‰¤ Lkf kH ky âˆ’ xk.
Canceling kf kH gives us the result.

Bayesian Multi-Scale Optimistic Optimization

Lemma 4 (Adapted from Lemma
Pâˆ 5.1 of Srinivas et al. (2010)). Let f be a sample from a GP. Consider Î· âˆˆ (0, 1) and set
BT = 2 log(Ï€T /Î·) where i=1 Ï€Tâˆ’1 = 1, Ï€T > 0. Then,
1

|f (xT ) âˆ’ ÂµT (xT )| â‰¤ BT2 ÏƒT (xT ) âˆ€T â‰¥ 1
holds with probability at least 1 âˆ’ Î·.
Proof. For xT we have that f (x) âˆ¼ N (ÂµT (xT ), ÏƒT (xT )) since f is a sample from the GP. Now, if r âˆ¼ N (0, 1), then
Z
2
2
P(r > c) = eâˆ’c /2 (2Ï€)âˆ’1/2 eâˆ’(râˆ’c) /2âˆ’c(râˆ’c) dr
<

eâˆ’c

2

/2

P(r > 0) =

1 âˆ’c2 /2
e
.
2

Thus we have that


1
1/2
1/2
P f (x) âˆ’ ÂµT (x) > BT ÏƒT (x) = P(r > BT ) < eâˆ’BT /2 .
2


1/2
By symmetry and the union bound, we have that P |f (x) âˆ’ ÂµT (x)| > BT ÏƒT (x) < eâˆ’BT /2 . By applying the union
bound again, we derive
âˆ

 X
1/2
P |f (x) âˆ’ ÂµT (x)| > BT ÏƒT (x) âˆ€T â‰¥ 1 <
eâˆ’BT /2 .
T =1

By substituting BT = 2 log(Ï€T /Î·), we obtain the result. As in Srinivas et al. (2010), we can set Ï€T = Ï€ 2 T 2 /6.
Since each nodeâ€™s UCB and LCB are only evaluated at most once, we give the following shorthands in notation. Let N (x)
be the number of evaluations of confidence bounds by the time the UCB of x is evaluated (line 12 of Algorithm 3) and let
T (x) = |Dt | be the time the UCB of x is evaluated. Define U(x) = UN (x) (x|DT (x) ) = Âµ(x|DT (x) ) + BN (x) Ïƒ(x|DT (x) )
and L(x) = LN (x) (x|DT (x) ) = Âµ(x|DT (x) ) âˆ’ BN (x) Ïƒ(x|DT (x) ).
Lemma 5. Consider B(xâˆ— , Ï) and Î³in(0, 1) as in Assumptions 2 and 3. Suppose L(xâˆ—h ) â‰¤ f (xâˆ—h ) â‰¤ U(xâˆ—h ). If xâˆ—h âˆˆ
h
B(xâˆ— , Ï) and Î´(h) < 0 then there exists a constant cÌ„ such that L(xâˆ—h ) â‰¥ f âˆ— âˆ’ cÌ„BN (xâˆ—h ) Î³ 2 .
âˆ—
Proof. If xâˆ—h is not evaluated then f (x+ ) â‰¥ UT (xâˆ—h ) â‰¥ f âˆ— âˆ’ Î´(h) â‰¥ f âˆ— âˆ’ 0 which implies that x+ âˆˆ B(x
q , Ï). Therefore,
Î´(h)
f âˆ— âˆ’ c2 kx+ âˆ’ xâˆ— k2 â‰¥ f (x+ ) â‰¥ UT (xâˆ—h ) â‰¥ f âˆ— âˆ’ Î´(h) which in turn implies that kx+ âˆ’ xâˆ— k â‰¤
c2 . Similarly
q
f âˆ— âˆ’ c2 kxâˆ—h âˆ’ xâˆ— k2 â‰¥ f (xâˆ—h ) â‰¥ f âˆ— âˆ’ Î´(h). Therefore kxâˆ—h âˆ’ xâˆ— k â‰¤ Î´(h)
c2 . By the triangle inequality, we have

s
kx+ âˆ’ xâˆ—h k â‰¤ kx+ âˆ’ xâˆ— k + kxâˆ—h âˆ’ xâˆ— k â‰¤ 2

Î´(h)
.
c2

q
By Lemma 3, we have that ÏƒT (xâˆ—h ) (xâˆ—h ) â‰¤ 2L Î´(h)
c2 . By the definition of LT , we can argue that
s

Î´(h)
c2
s
Î´(h)
â‰¥ f âˆ— âˆ’ Î´(h) âˆ’ 4BN (xâˆ—h ) L
c2
s
cÎ³ h
= f âˆ— âˆ’ cÎ³ h âˆ’ 4BN (xâˆ—h ) L
.
c2
q
Note that since Î³ âˆˆ (0, 1), Î³ < Î³ 1/2 . Assume that B1 = b. Let cÌ„ = c/b + 4L cc2 . Since BN > B1 âˆ€N > 1, we have the
statement.
L(xâˆ—h ) â‰¥

U(xâˆ—h ) âˆ’ 4BN (xâˆ—h ) L

If xâˆ—h is evaluated then the statement is trivially true.

Ziyu Wang, Babak Shakibi, Lin Jin, Nando de Freitas
1

Definition 1. Let Î³Ì„ := Î³ 2 , Î´Ì„h := cÌ„BN (xâˆ—h ) Î³Ì„ h , and Ih = {(h, i) : f (xh,i ) +  â‰¥ f âˆ— }.
âˆš
Lemma 6. Assume that hmax = n . For a node xh,i at level h, BN (xh,i ) = O( h).
Proof. Assume that there are ni nodes expanded at the end of iteration i of the outer loop (the while loop). In the i + 1th
iteration of the outer loop, there can be at most hmax (ni ) additional expansions added. Thus the total number of expansions
1
at the end of iteration i is at most niâˆ’1 + hmax (niâˆ’1 ). We can prove by induction that ni â‰¤ i 1âˆ’ . Since any node
at level h would be expanded after at most 2h iterations, at the time of expansion of any node at level h, we have that
1
h
h
n < (2h ) 1âˆ’ = 2 1âˆ’ where n is the total number of expansions. Thus, there would be at most 2 Ã— 2 1âˆ’ evaluations.
Hence,
q
q
âˆš
2h
2h
BN (xh,i ) â‰¤ 2 log(Ï€ 2 2 1âˆ’ +2 /6Î·) â‰¤ 2 log(2 1âˆ’ +2 ) + 2 log(Ï€ 2 /6Î·) = O( h).
Lemma 7. After a finite number of node expansions, an optimal node xâˆ—h0 âˆˆ B(xâˆ— , Ï) is expanded such that cÌ„BN (xâˆ—h ) Î³Ì„0h â‰¤
0

0 . Also âˆ€h > h0 , we have that cÌ„BN (xâˆ—h ) Î³Ì„ h â‰¤ 0 and xâˆ—h âˆˆ B(xâˆ— , Ï).

Proof. Since it is clear that BaMSOO would expand every node after a finite number of node expansions,
âˆš we only have to
show that there exists an h0 that satisfies the conditions. By Lemma 6, we have that âˆ€h BN (xâˆ—h ) = O( h). Since Î³Ì„ < 1,
there exists an h0 such that cÌ„BN (xâˆ—h ) Î³Ì„ h â‰¤ 0 âˆ€h > h0 . Since f (xâˆ—h ) > f âˆ— âˆ’ Î´(h) > f âˆ— âˆ’ cÌ„BN (xâˆ—h ) Î³Ì„ h â‰¥ f âˆ— âˆ’ 0 , we have
by Assumption 2 that, xâˆ—h âˆˆ B(xâˆ— , Ï).

D/2
PH
Î´Ì„(H)
Lemma 8.
|I
|
â‰¤
C
B
Î³ (D/4âˆ’D/Î±)H for some constant C for all H > h0 .
âˆ—
h=0 h
N (x )
H

Proof. By Lemma 7, we know that Î´Ì„(H) = cÌ„BN (xâˆ—H ) Î³Ì„ H < 0 if H > h0 . Therefore, by Assumption 2, we have that
Ï‡Î´Ì„(H) = {x âˆˆ Ï‡ : f (x) â‰¥ f âˆ— âˆ’ Î´Ì„(H)} âŠ† B(xâˆ— , Ï). Again by Assumption 2, we have that
f âˆ— âˆ’ Î´Ì„(H) â‰¤ f (x) â‰¤ f âˆ— âˆ’ c2 kx âˆ’ xâˆ— k22 âˆ€x âˆˆ Ï‡Î´Ì„(H) .


âˆ—

Thus Ï‡Î´Ì„(H) âŠ† B x ,

q

Î´Ì„(H)
c2

r



âˆ—

=B x ,

cÌ„B

(

N xâˆ—
H

)

Î³ H/2

c2

!
.

Since each cell (h, i) contains a `-ball of radius Î½Î´(h) centered at xh,i we have that each cell contains a
Î´Ì„(H)
1/Î± h/Î±
ball B(xh,i , (Î½Î´(h))1/Î± ) = B(xh,i , ( Î½c
Î³
).
By the argument of volume, we have that |Ih | â‰¤
c1 )

D/2
C1 BN (xâˆ— )
Î³ HD/4âˆ’hD/Î± for some constant C1 . Finally,
H

H
X

Î´Ì„(H)

|Ih

|

â‰¤

h=0

C1

H 
X

BN (xâˆ— )

D/2

Î³ HD/4âˆ’hD/Î±

H

h=0

=



D/2



D/2



D/2

C1 BN (xâˆ— )
H

Î³ HD/4

H
X

Î³ âˆ’hD/Î±

h=0

=
â‰¤
=
=
=

C1 BN (xâˆ— )
H
C1 BN (xâˆ— )
H

D/2

Î³ HD/4
Î³ HD/4

H 
X
h=0
âˆ 
X

Î³ D/Î±

hâˆ’H

Î³ D/Î±

hâˆ’H

h=0
âˆ’DH/Î±

Î³
C1 BN (xâˆ— )
Î³ HD/4
H
1 âˆ’ Î³ D/Î±

D/2
C1
B
Î³ HD/4âˆ’DH/Î±
âˆ—
N (xH )
1 âˆ’ Î³ D/Î±

D/2
C1
B
Î³ (D/4âˆ’D/Î±)H .
âˆ—
N (xH )
1 âˆ’ Î³ D/Î±


Bayesian Multi-Scale Optimistic Optimization

Setting C =

C1
1âˆ’Î³ D/Î±

gives us the desired result.

Lemma 9. Suppose L(xâˆ—h ) â‰¤ f (xâˆ—h ) â‰¤ U(xâˆ—h ). If xâˆ—h is not evaluated (that is U(xâˆ—h ) < f + ) then f + is Î´(h)-optimal.
Proof. f + > U(xâˆ—h ) â‰¥ f (xâˆ—h ) > f âˆ— âˆ’ Î´(h).
A.2
A.2.1

Main Results
Simple Regret

Let hâˆ—n be the deepest level of an expanded optimal node with n node expansions. This following lemma is adapted from
Lemma 2 of Munos (2011).
Lemma 10. Suppose L(x) â‰¤ f (x) â‰¤ U(x) for all x whose confidence region are evaluated. Whenever h â‰¤ hmax (n) and
D/2

Ph
n â‰¥ Chmax (n) i=h0 BN (xâˆ— )
Î³ (D/4âˆ’D/Î±)i + n0 for some constant C, we have hâˆ—n â‰¥ h.
i
Proof. We prove the statement by induction. By Lemma 7, we have that after n0 node expansions, a node xâˆ—h0 âˆˆ B(xâˆ— , Ï) is
expanded. Also âˆ€h > h0 , we have that cÌ„BN (xâˆ—h ) Î³Ì„ h â‰¤ 0 and xâˆ—h âˆˆ B(xâˆ— , Ï). For h = h0 , the statement is trivially satisfied.
D/2
Ph+1 
Thus assume that the statement is true for h. Let n be such that n â‰¥ Chmax (n) i=h0 BN (xâˆ— )
Î³ (D/4âˆ’D/Î±)i + n0 .
i
âˆ—
âˆ—
By the inductive hypothesis we have that hn â‰¥ h. Assume hn = h since otherwise the proof is finished. As long as the
optimal node at level h+1 is not expanded, all nodes expanded at the level are Î´Ì„(h+1)-optimal by Lemma 5. By Lemma 8,

D/2
we know that after Chmax (n) BN (xâˆ— )
Î³ (D/4âˆ’D/Î±)(h+1) node expansions, the optimal node at level h + 1 will be
h+1
Ph+1 Î´Ì„(h+1)
Î´Ì„(h + 1)-optimal nodes at or beneath level h + 1. Thus hâˆ—n â‰¥ h + 1.
expanded since there are at most i=0 Ii
Theorem 1. Suppose L(x) â‰¤ f (x) â‰¤ U(x) for all x whose confidence region is evaluated. Let us write h(n) to be the
smallest integer h â‰¥ h0 such that
Chmax (n)

h 
X

BN (xâˆ— )

D/2

i

Î³ (D/4âˆ’D/Î±)i + n0 â‰¥ n.

i=h0

Then the loss is bounded as
rn â‰¤ Î´(min{h(n), hmax (n) + 1})
and hâˆ—n â‰¥ min{h(n) âˆ’ 1, hmax (n)}.
Proof. From Lemma 8, and the definition of h(n) we have that

Chmax (n)

h(n)âˆ’1 

X

BN (xâˆ— )
i

D/2

Î³ (D/4âˆ’D/Î±)i + n0 < n.

i=h0

By Lemma 10, we have that hâˆ—n â‰¥ h(n) âˆ’ 1 if h(n) âˆ’ 1 â‰¤ hmax (n) and hâˆ—n â‰¥ hmax (n) otherwise. Therefore hâˆ—n â‰¥
min{h(n) âˆ’ 1, hmax (n)}.


By Lemma 9, we know that if xâˆ—hâˆ—n +1 is not evaluated then f + is Î´(hâˆ—n + 1)-optimal. If xâˆ—hâˆ—n +1 is evaluated, then f xâˆ—hâˆ—n +1
is Î´(hâˆ—n + 1)-optimal. Thus rn â‰¤ Î´(min{h(n), hmax (n) + 1}).
Proof of Corollary 1. Suppose L(x) â‰¤ f (x) â‰¤ U(x) for all x whose confidence region is evaluated. By Lemma 4, we
know that this holds with probability at least 1 âˆ’ Î·.

Ziyu Wang, Babak Shakibi, Lin Jin, Nando de Freitas

By the definition of h(n) we have that
h(n)

n

â‰¤ Chmax (n)

X

BN (xâˆ— )

D/2

i

Î³ (D/4âˆ’D/Î±)i + n0

i=h0


â‰¤ Chmax (n) BN xâˆ—

D/2 h(n)
X


h(n)


â‰¤ Chmax (n) BN xâˆ—

Î³ âˆ’di + n0

i=h0

D/2


h(n)

Î³ âˆ’dh0

Î³ âˆ’dh(n) âˆ’ 1
+ n0
Î³ âˆ’d âˆ’ 1

(4)

If h(n) â‰¤ hmax (n) + 1, then by Theorem 1, we have that hâˆ—n â‰¥ h(n) âˆ’ 1. After n expansions, the optimal node xâˆ—h(n)âˆ’1


has been expanded which suggests that its childrenâ€™s confidence bounds have been evaluated. Hence, N xâˆ—h(n) < 2n
since there have only been n expansions. Therefore,
D/2

(4) â‰¤ Kn (B2n )

Î³ âˆ’dh(n)

for some constant K which implies that
2Î±

4âˆ’Î±
Î³ h(n) â‰¤ K 1/d B2n
nâˆ’

1âˆ’
d


 Î±
1âˆ’
= K 1/d 2 log(4Ï€ 2 n2 /6Î·) 4âˆ’Î± nâˆ’ d .

By Theorem 1, we have that
n
o
 1âˆ’


 Î±
1âˆ’
Î±

rn â‰¤ c min K 1/d 2 log(4Ï€ 2 n2 /6Î·) 4âˆ’Î± nâˆ’ d , Î³ (n+1) = O nâˆ’ d log 4âˆ’Î± (n2 /Î·) .

