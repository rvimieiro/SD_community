Shenoy (1988), and Jensen et al. (1990), an effi­
cient algorithm called clique tree propagation has come
into being. When the underlying graphs are triangu­
lated, the algorithm arranges the cliques of the un­
derlying graphs into join trees and obtains the wanted
marginals and posteriors by passing messages around
in those trees. The computation in each clique is

simply direct marginalization. When the underlying
graphs are not triangulated, one needs to triangulate
them first (Lauritzen and Spiegehalter 1988), or ap­
peal to the technique of conditioning on loop cutsets
(Pearl 1988). A similar preprocessing step is also re­
quired in the goal directed approach of shacter et a/.
(1990). In this paper, we shall refer to the problem
of finding an optimal or a "good" triangulation for an
untriangulated graph as the triangulation problem.
It is NP-hard to find either optimal loop cutsets
(Stillman 1990) or optimal triangulations (Yannakakis
1981). Although heuristics are available, Stillman
(1990) has "demonstrated that no heuristic algorithm
.... can be guaranteed to produce loop cutsets within a
constant difference from optimal". We conjecture that
the same thing is true for triangulation. Thus, it is in­
teresting to investigate the possibility of approaches
that sidestep the triangulation problem.
Poole and Neufeld (1991) describes an interesting im­
plementation of Bayesian nets in Prolog. The imple­
mentation encodes conditional probabilities as Prolog
goal reduction rules and it organizes reasoning by us­
ing Bayes' Theorem and causality relationships among
the variables. The need for triangulation is avoided.
In this paper, we describe a new approach for comput­
ing posterior probabilities in Bayesian nets, which also
sidesteps the triangulation problem. What differenti­
ates our approach from Poole and Neufeld's approach
is that the former makes full use of decompositions,
while the latter does not at all. The cornerstone of
our approach is Tarja.n's algorithm for decomposing
undirected graphs.
An undirected graph may contain many complete sep­
arators even if it not triangulated. Tarjan (1985)
presents a O(nm) time algorithm for decomposing an
undirected graph at all its minimal complete separa­
tors, where n and m are the numbers of vertices and
edges of in the graph respectively. It is easy to con­
ceive a procedure that decomposes a Bayesian net into
as many components as possible by using Tarjan's al­
gorithm on the moral graph.
Our approach can be viewed as an advancement of the

Sidestepping the Triangulation Problem in Bayesian Net Computations

clique tree propagation approach. Like the clique tree
approach, this approach first arranges the components
of a Bayesian net into a tree, and then computes poste­
rior probabilities by appropriately passing information
around in that tree. Unlike the clique tree approach,
the computation in each component is not carried out
by direct marginalization. Rather it is carried by re­
peating the whole procedure from the beginning.
The organization of the paper is as follows. Section
2 starts the paper by reviewing some graph theory
terminologies and some basic concepts pertaining to
Bayesian nets. Section 3 introduces the concept of de­
composition in terms of semi-Bayesian nets. Section 4
shows how decomposition leads to the parallel reduc­
tion technique. Section 5 introduces serial reduction the basic means for passing messages from one com­
ponent to another, and section 6 constructs the com­
ponent tree - the basic means for organizing message
passing. The algorithm is given in section 7, together
with an example illustrating how it works. The paper
concludes at section 8.
2

PREREQUISITE

no parents are roots. The set of all the parents of a
vertex v will be notated by 1r(v). A directed cycle is a
sequence of vertices in which every vertex is a parent
of the vertex after it and the last vertex is a parent
of the first vertex. An acyclic directed graph is one in
which there are no directed cycles.
To marry all the parents of a vertex in a directed graph
is to add an (undirected) edge between each pair of its
parents. The moral graph m(G) of a directed graph G
is an undirected graph obtained from G by marrying
all the parents of each vertex respectively and ignoring
all the directions on the arcs. A directed graph is
connected if its moral graph is.
Let us now review a few concepts pertaining to
Bayesian nets. A Bayesian net}/ is a triplet (V, A, P),
where

1. V is a set of variables.
2. A is a set of arcs, which together with V consti­
tutes a directed acyclic graph G = (V,A).
3. P = {P(vJ7r(v)) : v E V}, i.e P is the set the
conditional probabilities of the all variables given
their respective parents 1.

Let us begin by reviewing some graph theory termi­
nologies. An (undirected) graph G = (V, E) consists
of a set V of vertices and a set E of edges, which are
(unordered) pairs of vertices. A path is a sequence
of vertices in which every pair of consecutive vertices
is an edge. A loop is a path where no vertex appear
twice except the first one and the last one, which are
the same. A chordless loop is a loop in which no pair
of non-consecutive vertices is an edge. A genuine loop
is a chordless loop of length greater than three. A
triangulated graph is one without genuine loops.

Variables in a Bayesian net will also be referred to
as vertices and nodes when the emphasis is on the
underlying graph.

A graph is connected if there is a path between any
two vertices. If a graph G is disconnected, a connected
component of G consists of a subset of vertices in which
vertices are connected to each other but not to vertices
outside the subset, and of all the edges that are com­
posed of vertices in the subset.

Y � V be the set of variables observed and Yo be
the corresponding set of values. Let X � V be the
set of variables of interest. The posterior probability
Px(XIY; Yo) of X in a Bayesian net N' given obser­
vations Y = Yo is obtained by first conditioning Px on
Y = Yo and marginalizing the resulting joint probabil­
ity onto X 2• The problem of concern to this paper is
how to compute P(XIY =Yo)?

A separator of a graph is a subset of its vertices whose
deletion from the graph will leave it disconnected. Two
vertices is separated by a separator if every path con­
necting them contains at least one vertex in the separa­
tor. A separator is minimal if none of it proper subsets
are separators. A subset of vertices is complete if ev­
ery pair of its elements is an edge. Separators can be
complete.
Cliques are maximal complete subsets of vertices.
When the set of all the vertices is complete, we say
that the graph is complete.

A directed graph G = (V, A) consists of a set V of
vertices and a set A of arcs, which are ordered pairs
of vertices. If there is an arc from vertex v1 to vertex
v2, then v1 is a parent of v2 and v2 is a child of v1.
Vertices with no children are leaves and vertices with

The prior joint probability Px of a Bayesian net }/ is
defined by
Px(V) =

IT P(vl1r(v)).

(1)

vEV

Observations may be made about variables.

Let

Developed by Lauritzen and Spiegehalter (1988),
Shafer and Shenoy (1988), Jensen et al (1990), the
clique tree propagation approach consists of three
steps: (1) Triangulate the moral graph of G; (2) ar­
range the cliques into a join tree, and (3) properly pass
messages around in the tree.
Consider the Bayesian net nett in Figure 1 (1). The
following prior probability and conditional probabil­
ities are given as part of the specification: P(c),
P(aic), P(eia), P(fle), P(gJJ), P(bia,g), P(hlb), and
1Note that when vis a root 1r(v) is empty. In such a
case, the expression P(vJlr(v)) simply stands for the prior
probability of v.
2That is to sum out all the variables not in X.

361

362

Zhang and

Poole

are not given. Thus, Bayesian nets are semi-Bayesian
nets whose set of unspecified roots is empty.
In a semi-Bayesian net N = (V,A, R, 1'), the R i s the
set of roots of the directed graph ( V, A) whose prior
probabilities are not given in P. So, we call R the set
(I)

of unspecified roots of (V, A, 1').

1111U

The prior joint potential P.N a semi-Bayesian net .N =
( V, G, R, P) is defined by

Px(V) =

IT

P(vj1r(v)).

(2)

11EY-R

-

And for any subset X of V , the {marginal) potential
Px(X) of X in .N is obtained by marginalizing P#(V)
onto X, i.e by summing out all the variables in V- X.

�.

-�

---Q

(3) l'rop;>plioa

If N is a. Bayesian net, then its prior joint potential and
marginal potentials are exactly the same as its prior
joint probability and marginal probabilities defined in
the previous section.

,.,

Figure 1: Bayesian net, triangulation and message
passing.

P(dJc, h). The moral graph of its underlying graph
is shown 1 (2). The dashed edges are added for tri­
angulation. The corresponding join tree is shown in
Figure 1 (3). One can obtain Pnttl (dJe =eo) by pass­
ing messages in the way as indicated by the arrows.
See the cited papers for details about the contents of
the messages and how are they actually passed in the
join tree.
Let X be the set of variables of interest. Let Y be
the set of observed variables, and Yo be the corre­
sponding set of observed values. We are concerned
with queries of the form P#(XJY = Yo). Noticing
that P#(XIY =Yo) can be obtained normalizing the
marginal probability P.N(X, Y =Yo), we shall concern
ourselves with queries of the form P.N(X, Y = Yo) in
this paper.

3

DECOMPOSITION

In this section, we shall introduce the concept of de­
composition in terms semi-Bayesian nets.
A semi-Bayesian net .N is a quadruplet .N

(V, A, R, 1'), where

=

1. V is a set of variables.
2. A is the set of arcs, which together with V con­
stitutes a directed acyclic graph G =(V, A).
3. R is a set of roots of G.
4. 1' = {P(vj1r(v)): v E (V- R)} .
In words, semi-Bayesian net are Bayesian nets with un­
specified roots, i.e with roots whose prior probabilities

Let us now turn to decomposition of undirected
graphs. An undirected graph G = (V, E) decomposes
into Gt = (Vi,Et) and G2 = ( V2,E2), or simply into
Vt and V2 if:

1. Vt and V2 are proper subsets of V and Vt U V2 = V,
2. Ei (i = 1, 2) consists of all the edges in E that lie
completely within v; and Et U E2 = E, and
3. Vt n V2 is a complete separator of G.
In such a case, we say that G is de c o m p osable.
There is a one-to-one correspondence between com­
plete separators and decompositions. Given any com­
plete separator S of an undirected graph G, G can
be decomposed at s into to Vi and v2 such that
VI n v2 = s. And if G decomposes into vl and v2,
then Vt n V2 is a complete sep arator. Tarjan (1985)
presents an O(nm) time algorithm for decomposing an
undirected graph at all its minimal complete separa­
tors, where n and m stands for the numbers of ver­
tices and edges of G respectively. Tarjan's algorithm
is very important to our approach for computing pos­
terior probabilities in Bayesian nets.
A semi-Bayesian net .N =(V, A, R, 1') is decomposable
if the moral graph m ( G) of the underlying directed
graph G = (V, A) is decomposable. A decomposi­
tion {Vt, V2} of m( G) induces the decomposition of the
semi-Bayesian net N into .Nt

J/2 = (V2, A2, R2, 1'2), where
•

•

= (Vt, At, R1, PI) and

P1 is obtained from 1' by removing all the items
that do not involve any variables in V1 - V2 and
1'2 =1' -1'1.

And fori

E

{1, 2},

- A, is obtained A by removing any arcs whose

two vertices do not simultaneously appear in
any item of 'Pi, and

Sidestepping the Triangulation Problem in Bayesian Net Computations

Px(X,z, Y ) =

E Px, (X,S)Px,(S,Y).

(3)

S-Z

Figure

- R;

is

2:

The theorem follows from the definition decomposi­
tions of semi-Bayesian nets and the distributivity of
multiplication w .r .t summation. Instead of giving the
detailed proof, we shall provide an illustrating exam­
ple.

A decomposition of net 1.

the

("', A;' 'P;).

set

of

unspecified

roots

of

Consider the Bayesian net net1 in Figure 1 (a). The
The
set {a, b} is a minimal complete separator.
moral graph of the directed graph decomposes into
{a,b,c,d,h} and {a,b,e,f,g}. This induces the de­
composition ofnet1 into semi-Bayesian nets net2 and
net3 as shown in Figure 2. In net2, P(c), P(alc),
P(hlb), and P(dlb,c) are inherited from net1. There is
no arc from a to b because they do not simultaneously
appear in any of those (conditional) probabilities of
net2. The root b of net2 is unspecified because there
is no P( b) in net2. The conditional probabilities in
net3 are P(ela), P(fle), P(glf), and P(bla,g). The
root a is unspecified because there is no P(a).
A semi-Bayesian net is simple if it contains onl y one
leaf node and all other nodes are parents of this leaf
node. In any semi-Bayesian net that is not simple,
there is always a leaf node, say l. The set 1r(l) of the
parents of { is a complete separator, which separates I
from all the nodes not in 1r(l) U {1}. So, there always
a minimal complete separator S that separates l from
all the nodes not in S U {1}. Thus we have

Proposition 1 If a semi-Bayesian net is not simple,
then it is decomposable.
Remark: We do not want to decompose a Bayesian
net at a separator that is not complete for two rea­
sons. First, it increases the complexity of the problem
itself, and second the number of separators that are
not complete may be too large.

4

PARALLEL REDUCTION

In this section, we shall show how the concept of de­
composition can be used in computing marginal po­
tentials in semi-Bayesian nets (marginal probabilities
in Bayesian nets). First of all, we have the following
theorem.

Theorem 2 Suppose a semi-Bayesian net }/ decom­
poses into }/1 and }/2 at a minimal complete separator
S {of the moral graph of its underlying directed graph).
Let X be a subset of variables of J./1 which do not ap­
pear in S, let Y bet a subset of variables of J/2 which
do not appear in S, and let Z be a subset of S. Then

Suppose we want to compute Pnan(d,a,e) in the
Bayesian net net1 shown Figure 1 (1). By definition,
we have

Prun(d ,a,e)
=

L P(c)P(alc)P(eia)P(fle)P(glf)

c,b,j,g,h

P(bla, g)P(h]b)P(dic, b)

= L {L P(c)P(alc)P(hlb)P(dlc, b)}
b

c,h

{L P(ela)P(f le)P(ulf)P(bla, g)} (4)
J,g

=L;Pnatz(d,a,b)Pnat3(a,b,e).
b

(5)

Equation (4) is true because of the distributivity of
multiplication w.r.t summation, and equation (5) im­
mediately follows from Lhe definition of marginal po­
tentials in semi-Bayesian nets.
Suppose we want to compute the marginal potential
in a semi-Bayesian net N. And sup­
pose )1/ decomposes into J./1 and J./2 at a minimal com­
plete separator S of the moral graph of its underlying
directed graph. Let X1 be the set of variables in X
and in J./1 but not in S, X:� be the set of variables in
X and in J./2 but not in S, and Xs =X n S. The sets
Yt, Y2 and Ys are defined from Y in the same way.
Let Y-1 = YsUYz and Y-2 = YsUYt. Let (Y-t)a and
(Y-2)o be the corresponding sets of values of Y_1 and

Px(X,Y =Yo)

Y-2·

The query induced in }/1 by Px(X,Y = Yo) is
Px,(Xt , S- Ys,Y-2 = (Y-2)o), and the query induced
in J/2 is Px,(Xz,S- Ys,Y-1 = (Y-do). According to
Theorem 2, we have

PN(X,Y =Yo)
L

Px,(Xt,S - Ys,Y-2 = (Y-z)o)

5-Xs-Ys

PN,(Xz,S- Ys, Y-1 = (Y-t)o)

(6)

Px(X,Y = Yo), we can first com­
Ys,Y-2 = (Y-2)o) and Px,(X2,S­
Ys, Y-t = (Y-do) (possibly in parallel), and then

Thus to compute
pute Px1 (X t,S-

363

364

Zhang and Poole
combine the results by equation (6). This leads to
the technique of parallel reduction.

gin by introducing the concept of parametric semi­
Bayesian nets.

Given a semi-Bayesian net /11, the following procedure
computes PJI(X, Y =Yo):

A parametric semi-Bayesian net /II = (V, A, R, 'P) is a
semi-Bayesian net, except that some of its conditional
probabilities contain parameters, i.e variables that are
not members of V. Thus, semi-Bayesian nets are para­
metric semi-Bayesian nets that do not contain any pa­
rameters.

Parallel-Reduction
H /II is simple, then compute PJ,/(X, Y =Yo)

directly by marginalization,
else find a minimal complete separator S of
the moral graph of the underlying directed
graph of /If, decompose /If at S into /111 and
/112 .
1. Repeat the procedure to compute the
induced query Px. (XI. S- Ys, Y -2 =
(Y-2)o),
2. Repeat the procedure to compute the
induce query Px2(X2,S- Ys,Y-1 =
(Y-do),
3. Combine the answers to the two induced
queries by using equation (6).
The procedure is termed parallel reduction because line
1 and line 2 can be executed strictly in parallel.
Because of Proposition 1, the procedure Parallel Re­
duction is able to compute marginal potentials in semi­
Bayesian nets without triangulating the underlying
graphs. But the algorithm can be very inefficient.
The main purpose of this paper is to describe another
algorithm called component tree propagation, which
we hope is as efficient as the clique tree propagation
approach based on an optimal triangulation. The algo­
rithm computes posterior probabilities in a Bayesian
net as follows: first the Bayesian net is decomposed
into components, the components are arranged into a
tree, and then posterior probabilities are obtained by
appropriately passing messages around in that tree. In
the next section, we shall introduce the basic means
for passing messages from one component to another
- the serial reduction technique. In the section af­
ter, we shall present the basic means for controlling
message passing - component trees.

5

SERIAL REDUCTION

Suppose a semi-Bayesian net /II decomposes into two
components. A query in /II reduces into two sub­
queries, one in each of the components. The paral­
lel reduction procedure first computes both of the two
subqueries (possibly in parallel), and then use an ad­
ditional formula to combine the answers to get the
answer to the original query. In contrast, the serial
reduction procedure will first compute only one of two
subqueries. The answer is then send to the other sub­
query, which is thereby updated. The answer to the
updated subquery is the same as the answer to the
original query.
This section is devoted serial reduction. Let us be-

Suppose /II is a semi-Bayesian net with parameters
W . And suppose we want to compute the potential
of (X, Y =Yo). The answer will be a function of the
parameters W as well as of X. So, we shall write
the query as PJ,/(X,Y = Yo : W), where the column
mark separates parameters from variables in the semi­
Bayesian net.
Given a query PJ,/(X, Y = Yo : W) in a parametric
semi-Bayesian net /11 , a node is laden if it is a leaf of
/II and it is in the set Y (i.e it is observed ).
Suppose /II decompose into /111 and N2 at a minimal
complete separator S. Let the sets X1, Xs, X2, Y1,
Ys, Y2, Y_1, and Y_2 are defined as before. Let W1
and w2 be the parameters of /Ill and /112 respectively.
As in the case of parallel reduction, the query Q:
PJ,/(X, Y =Yo : W) induces a subquery Ql in /111 and
a subquery Q2 in /112. The induced subquery Q1 is
P.N1 (XI. S- Ys , Y -2 = (Y-2)o : WI), and the induced
subquery Q2 is PJ1l(X2, S- Ys , Y- 1 = (Y-do : W2).
The answer to the subquery Q2 is a function fo(X2, S­
Ys , W2). We extend it to be a function f(X2, S, W2)
by setting
2!(X2' S ' W:)
_

To
to

{ fo(Xz,S- Ys,W2)
0

append the answer

ifYs = (Ys)o
otherwise

f(X- 2, S, W2) of Q2

to

/111 is

1. Introduce an auxiliary variable v into /111, which
has only two possible values 0 and 1,
2. Draw an arc to v from each variable inS,
3. Set P(v =OI S) = !(X2,S, W2),
Let /II{ denote the resulting semi-Bayesian net. To use
the query Q is to replace
it by Q': PJI:(X�,Xs,Y-2 = (Y-2)o,v = 0: X2, W).
Note that the auxiliary variable v is a laden variable
in the updated subquery. Also note that the the semi­
Bayesian net /II{ contains the parameters W U X2 .
the answer of Q2 to update

Theorem 3 Suppose /II is semi-Bayesiant net with

parameters W, which decomposes at a minimal com­
p l ete separator S into /111 and /112. Let all the symbols
be as defined above. Then

PJI(X,Y =Yo: W) =
PJit(Xt,Xs, Y-2 = (L2)o,v

=

0: X2, W).(7)

Sidesteppin g the Triangulation Problem in Bayesian Net Computations

(1 �.>
(]) l
��
-

"""

Q

0-0>

-

�: ···:·
y

�

,

G)�

...

-

(

�

aotiO

Figure 3: A serial reduction of net1.

Px(X, Y ==
Yo : W), we can first compute the induced subquery
Px,(X2,S- Ys, Y_t = (Y-do : W), use the answer

�..e---Q
-

lidII

The theorem says, to compute the query

to construct Jl{, and then compute the updated query
P.w;(X1,Xs,Y_2 = (Y-2)0 : X2, W). This procedure

Figure 4: A (the) component tree for nett.

Again, instead of give the detailed proof of the theo­

all the non-trivial minimal complete (NMC) separators
of J1 and decompose Jl at them into smaller compo­
nents. The component tree of J1 is a tree whose nodes
consists of all those smaller components. The tree is

is called serial reduction.

rem, we shall provide an illustrating example. Con­
sider computing Pnetl (d, e ) in the Bayesian net net 1
shown in Figure 1 (1). The net decomposes into net4
and netS (in Figure 3) at the minimal separator {a, g }.
"\Ve first compute Pnets(a,g,e ) , and append the result
to net4. This gives us net6. Drawn in dotted cycle,
v is an auxiliary laden variable introduced. The con­
ditional probability of v = 0 give a and g is set by
P(v = Oja,g) == Pnets(a,g, e ) . Thus net6 contains the
parameter e. The updated query is Pnet6(d,v == 0: e).
To see that Pnd6(d,v = 0: e) = Pnen(d,e), we notice
that net6 decomposes into net4 and net7, and that

Pnea(a,g,v = 0)
Pnet6(d,v
==

=

=

P(v

=

0: e )

L Pnet4 (d,a, g)Pnet1 (a,g,v

==

0)

2: Pnet4(d,a, g)Pnecs (a,g,e)
a, g

==

constructed as follows:
Start with an arbitrary component. While
there are still components left out of the tree,
choose one such component, say C, which
contains an NMC separator that is also con­
tained in one or more components in the tre€.
Add C to the tree by connect it to one of
those components that also contain the NMC
separator. 3.

Ola,g) = Pnecs(a,g,e). So

a,g
==

algorithm, one can easily conceive a procedure to find

Pnen(d,e).

For the Bayesian net net1 in Figure 1, there are three
NMC separators are: {c ,h}, {a,b}, and {a,g}. As
shown in Figure 4 the resulting components are netB
with prior probability P(c) and conditional proba­
bilities P(djc, h); net9 with conditional probabilities

P(alc) and P(hlb); netlO with conditional probabil­
P(bla,g); and net11 with conditional probabilities
P(e/a), P{f/e), and P(g/f). In this example, there is

ity

only one component tree, which is denoted by treel

and is shown in Figure 4.

6

COMPONENT TREES

Given a query Px(X,Y = Yo : W) in a parametric
semi-Bayesian net J./, a minimal complete separator
(of the moral graph of the underlying directed graph
of J./) is trivial if it is a subset of r(l) for some laden
node l and its deletion from the moral graph only re­
sult in no more than two connected components.
Tarja.n's algorithm decomposes an undirected graph
at all its minimal complete separators. Based on this

The union of two semi-Bayesian nets (Vt,A,,R,,Pt)
and (V2, A2,R2, 1'2) is the semi-Bayesian net (Vt U
V2,A1 U A2,R,1't U 1'2), where R is the set of un­
specified roots of (Vt U V2, A, U A2, 1', U 1'2).
SupposeT is a component tree of a semi-Bayesian net
N, and )/1 is a leaf of T. Then )./ decomposes into
)/1 and )/2 - the union of all other nodes ofT. Let
Q be a query in J1 and let Q1 be the induced query in
3It can be proved that this construction does result in
a tree

365

366

Zhang and Poole

N1• Then we can use the answer to Q1 to update the

0�---.

query Q.
7

�:.�-=

COMPONENT TREE
PROPAGATION

1111t10•

We are now ready to give the component tree propaga­
tion algorithm. Let N be a parametric semi-Bayesian
net with parameters W. Here is our algorithm for com­
puting the answer to the query PJI(X, Y = Yo : W).

.....

Main{N, (X, Y =Yo)):
1. If N has NMC separators, call the pro­
cedure Serial�Reduction(N, (X, Y =
Yo)).
2. If N does not have any NMC sep­
arators, call the procedure Parallel·
Reductionl(N,

Serial-Reduction(N,

(X, Y =Yo)).

(X, Y =Yo)):

1. Decompose N at all its NMC separators,
and construct a component tree T.
2. While there is at least two nodes in T
do
•

•

•

Pick a leaf N1 of T by calling the pro­
cedure Pick-leaf(T, (X, Y =Yo)),
Call Main to compute the induced
subquery Q1 in N1,
Append the answer of Q1 to the com­
ponent that is the neighbor of N1 in
T.

•

Remove N1 from T and use the an­
swer to Q1 to update the current
query.

3. When there is only one node left in T,
call Main to compute the current query.

In each pass of the while loop, the current query is
updated. According to Theorem 3, the answer to the
current query is the same as the answer to the updated
query. Thus, the answer to the current query when
there is only one node left in T is the same as the
answer to the original query P.N(X, Y =Yo : W).
The input of to the procedure Parallel-Reductionl
is parametric semi-Bayesiant net N and a query
P.N(X, Y = Y0 : W) such that N does not have any
NMC separators. The output is the answer to the
query.
Parallel-Reductionl(N,

(X, Y = Yo)):

If N is simple, then compute the answer to
the query P.N(X, Y = Yo : W) directly by
marginalization,
else

Figure 5: Semi-Bayesian nets created in computing
Pnen(d, e ) .

1. Pick a laden node

1 by calling the pro­
cedure Pick-laden-node(N, (X, Y =

Yo)),
2. Decompose N at the set 1r(l) of parents
of l into N1 and N2,
3. Call Main to compute the induced sub­
squeries in N1 and in N2,

4. Combine the answers to the subqueries
using equation (6).

Our primary investigations indicate that the proce­
dures Pick-leaf and Pick-laden-node are important
to the the performance of the algorithm. How can one
define those procedures so that the al gorithm achieves
its optimal performance and how does this optimal
performance compare to the performance of the clique
tree propagation approach are topics for future re­
search.
To end this section, let us look at an example. Con­
sider computing Pnetl ( d, e ) in the Bayesian net net 1
shown in Figure 1. Since net1 has NMC separators,
the procedure Serial-Reduction will be called. The
procedure will first decompose net 1 into components
(semi-Bayesian nets) netS, net9, net10, and net 1 1
(Figure 4). It will then arrange those components into
a component tree tree1, which has two leaves netS
and net11. If the procedure Pick-leaf first returns
net11, then the induced query Pnem(e,a,g) will first
be computed. The answer will be appended to the
neighbor net10 of netll in tree1, resulting in the
parametric semi-Bayesian net net10 1 (Figure 5) with
auxiliary variable v1 and parameter e. The answer will
also be used to update the query Pnetl ( d, e ) to the new
query Pnetu(d, v1 = 0 : e ) , where net 12 stands for the
union of netS, net9, and net 101•
After net11 is removed from tree1, there are
again two leaves netS and net tO'. The current
query Pnetl2( d, VJ = 0 : e) induces the query
Pnetl o•(a,b,vl =0: e) in net101• I!net10' is chosen
by Pidc-leafthis time, the induced query will be com­
puted. Its answer will be appended to net9, resulting

Sidestepping the Triangulation Problem in Bayesian Net Computations

in net9 1 (Figure 5). The answer will also be used to
update the current query PMn2(d, v1 = 0: e) to the
new query Pnen3 (d,v2 = 0: e), where net13 stands
for the union of netS and net9 1 •
If Pick. leaf now picks net9 1 , then the query
Pnet9' ( c, h, v2 = 0 : e) induced in net9' by the cur­
rent query Pnen3(d, v2 = 0,: e) will be computed. Its
answer will be appended to netS, resulting in netS'.
And the answer will also to used to update the cur­
rent query Pne113(d, v2 = 0 : e) to the new query
Pnets•(d, V3 = 0: e).
On the next level, PnetlO' (a, b, v1 = 0 : e) and
Pnets•(d, V3 = 0 : e) will be computed by the pra.
cedure Parallel·Reductionl since the parametric
semi-Bayesian nets net 10' and netS' do not have any
NMC separators. On the other hand, net11 andnet9'
do have NMC separators. So, the procedure Serial·
Reduction will again be called in computing both
Pnetu(a,g,e) and Pnet9•(c,h,v2 = 0: e).
We notice that there was no triangulation in the above
process of computing Pnett(d,e), while triangulation
is a must in the clique tree propagation approach (see
section 2).
8

CONCLUSIONS

In this paper, we have described a approach for com­
puting posterior probabilities in a Bayesian net, which
sidesteps the triangulation problem. Our approach
begin by decomposing the Bayesian net into smaller
components by making use of Tarjan's algorithm for
decomposing undirected graphs at all their minimal
complete separators. Like the clique tree approach,
our approach arranges those components into a tree,
and then computes posterior probabilities by appropri·
ately passing information around in that tree. Unlike
the clique tree approach, the computation in each com­
ponent is not carried out by direct marginalization.
Rather it is carried by repeating the whole procedure
from the beginning. Thus, the need for triangulation
is avoided.
How does the performance of our approach compare to
that of the clique tree propagation approach based on
an optimal triangulation? This question is yet to be
answered. Our hope is that proper choices of the pro.
cedure Pick-leaf' and Pick-laden-node c ou l d ensure
the performance of our approach lay close to optimal.

Acknowledgement
The first author gained his understanding of Bayesian
nets when he was with Glenn Shafer and Prakash
Shenoy at Business School, University of Kansas from
October 1987 to October 1988 and from September
1989 to August 1990. The paper has benefited from
comments by D'Ambrosio and the reviewers for UAI
92. Research is supported by NSERC Grant OG-

P0044121.

References
Baker and T. E. Boult (1990), Pruning Bayesian net­
works for efficient computation, in Proceedings of the

Si�th Conference on Uncertainty in Artificial Inte/li.
gence, July, Cambridge, Mass. , pp. 257- 264.

A. P. Dawid and S. L. Lauritzen (19�9), Markov distri­
butions, Hyper Markov laws and meta Markov models
on decomposable gr aphs with applications to Bayesian
learning in expert systems, R 89-31, Institute for Elec­
tronic Systems, Department of Mathematics and Com­
puter Science, The University of Aalborg, Aalborg.
Denmark.

F. V. Jensen, K. G. Olesen, and K. Anderson (1990),
An algebra of Bayesian belief universes for knowledge­
based systems, Networks, 20, pp. 637- 659.
S. L. Lauritzen and D. J. Spiegehalter (1988), Local
computations with probabilities on graphical struc­
tures and their applications to expert systems, Journal
of Royal Statistical Society B, 50: 2, pp. 157- 224.
J. Pearl (1988),
Systems:

Probabilistic Reasoning in Intelligence
Networks of Plausible Inference, Morgan

Kaufmann Publishers, Los Altos, CA.
M.A. Peot and R.D. Shacter (1991), Fusion and prop­
in belief networks,
Artific ial Intelligence, 48, pp. 299-318.

agation with multiple observations

D. Poole and E. Neufeld (1991), Sound probabilis­
tic inference in Prolog: An executable specification
of Bayesian networks, Department of Computer Sci­
ence, University of British Columbia, Vancouver, B.
C., V6T 1Z2, Canada.
R. D. Shacter, B. D'Ambrosio, and B. A. Del Favero
(1990), Symbolic Probabilistic Inference in Belief Net­
works, in AAAI.90, pp. 126-131.
G. Shafer and P. Shenoy (1988), Local computation in
hypertrees, Working Paper No. 201, Business School,
University of Kansas.
J. Stillman ( 1990), On heuristics for finding loop cut­
sets in multiply connected belief networks, in Proceed­
ings of the Sixth Conference on Uncertainty in Arti­
ficial Intelligence, July, Cambridge, Mass., pp. 265
•

272.
R. E. Tarjan ( 1985) , Decomposition by Clique Sepa·
rators, Discrete Mathematics, 55, pp. 221·232.

Yannakakis (1981), Computing minimal fill-in is
NP-complete, SIAM J. Algebraic Discrete Methods, 2,
pp. 77- 79.

M.

367

