failure occurs on line i, the output of the device is
f (i.e., false) irrespective of what the input is, i.e.,
•

Also with Rockwell International Science Center,

Palo Alto Laboratory, Palo Alto, CA 94301.

=

u; f. When a line failure does not occur on line
i the device just transmits its input to its output,
i.e., U;'
U;. This non-failure event occurs with
=

probability 1 q;.
This overall structure induces a probability
distribution P(XIU1, U2 ..., Un) which is easily
computable[Pearl1988] .
When each U; is interpreted as a "cause" of the
"effect" X, the Boolean Noisy-Or models disjunctive
interaction of the causes. Each cause is "inhibited"
with probability q;, i.e., there is a probability q; that
even when the cause U; is active, it will not affect
-

X.

In a Bayesian network interpretation, each of
the variables U; can be considered as a predeces­
sor node of the variable X. The conditional proba­
bility distribution P(XIU1, U2 ..., Un) is computed
from the probabilities q;. In domains where such dis­
junctive interactions occur, instead of fully specify­
ing opaque conditional probability distributions, the
Noisy-Or model can be used instead. The inhibitor
probabilities are few in number (one associat.ed with
each predecessor U; of X) and would be intuitively
easier to specify because of their direct relation to
the underlying mechanism of causation.
This paper generalizes the Noisy-Or model to
the case where both the 'cause' variables U; and 'ef­
fect' variable X need not be Boolean. Instead, they
can be discrete variables with any number of states.
Furthermore the underlying deterministic function
is not restrict ed to be the Boolean OR function, it
can be any discrete function. In other yvords, in Fig
1, F can be any discrete function.
Seen as a modeling tool, this generalization pro­
vides a framework to move from an underlying ap­
proximate deterministic model (the function F) to a
more realistic probabilistic model (the distribution
P(XIU1, U2 ..., Un)) with the specification of only a
few probabilistic parameters (the inhibitor probabil­
ities).

A Generalization of the Noisy-Or Model

u1

Nt

u�

209

similarly defined quantities u�(j), u;, I; associated
with the variable u;
The line failure function M associates a prob­
ability value P/ nh(j) with every index 0 � j < m;.
This quantity can be read as the inhibitor probabil­
ity for the jth state of input ui
The line failure function can be conceptualized
as a non-deterministic device that takes the value of
U; as the input and outputs a value for u;. This de­
vice fails with probability prh(j) in state j. When
a failure in state j occurs, the output of the device
is u;(j) regardless of the input. When no failure oc­
curs, if the input is u;(j) the output is u; (j) - this
can be viewed as "passing the input through to the
output" (note that the index j of the output state
and the input state are same in this case). The probability of no failure occuring is denoted by Ptofail.
We see that:
0

u2
U·

N2
r- 1

�M

u�
u:
t

X

F

0

L_...J

Un

Nn

u'n

Figure 1: The generalized Noisy-Or model.
In domains where the generalized Noisy-Or is
applicable, it makes the modeling task much easier
when compared to the alternative of direct specifi­
cation of the probabilistic model P(XjU1,U2 . ..Un)·
In such domains, the task of creating a Bayesian network would proceed as follows:

Ptojail

·

•

•

•

•

2

Variables and deterministic functions that re­
late
them
approximate
and
the non-deterministic behaviour of the domain
are identified.
A network is created with this information with
a node for each variable, and a link from each
of U1, U2, ..., Un to X for each relation of
form X = F(Ut,U2,...,Un)· (The network is
assumed to be acyclic).
Inhibitor probabilities for each link in the net­
work are elicited.
The generalized Noisy-Or model is used to au­
tomatically 'lift' the network from the previ­
ous step into a fully specified Bayesian network
which has the same topology as the network.

THE GENERALIZED MODEL

The generalized Noisy-Or model is illustrated in Fig
1.

Each U; is a discrete random variable. Each u;
is a discrete random variable with the same number
of states as U;.
We will refer to the number of states of U; and
u; as m;. We will refer .to the jth state of U; as
u;(j) where 0 � j < m;. We call j the index of state
u;(j). We will use u; to denote "any state of U;". As
an example of the use of u;, consider the statement,
"Every state u; of U; has a unique index associated
with it" .
We define I; to be the function that returns the
index of a state u; of U;, i.e., I;(u;) = j where j is
the index of state u; of variable U;. We also have

=

1

_ L P/nh(j)
O�j<m;

The output X is a discrete random variable with
states. We will refer to the jth state of X as x(j)
and use x to refer to "any state of X".
F (see Fig 1) can be conceptualized as a de­
terministic device that outputs some value x of X
for each possible joint state u�, u�, ..., u� of the
inputs U�,U�, ...,U�. In other words F is a dis­
crete function that maps the space of joint states of
U� X U� X
X U� into the set of states of X.
We note that the model described above induces
an uncertain relationship between the output X and
the variables Ui. This relationship is captured by the
conditional distribution P(XIU1,U2,..., Un)·
In the next section we proceed to show how
this conditional distribution is computed from the
function F and the inhibitor probabilities. We will
use the notation U to denote the vector of vari­
ables [U1, U2,...,Un]. Similarly, we will use u to
denote any joint state [u1, U21 , Un] of U. U' and
'
u are defined similarly with respect to the variables
u;. Note that P(XIUt,U2,...,Un) abbreviates to
m.,

•

.

•

• • •

P(XIU).

In the special case where every inhibitor proba­
bility is zero each variable u; always has the "same"
value as U; (i.e., the state of u; has the same index
as the state of U;). In this special case the variables
I
U; become superfluous, we could JUSt as well remove
the line failure functions and connect the each input
U; directly through to F.
In this special case, the overall model degener­
ates to a deterministic function where the value of
output X is determined from the values of the input
variables U; by the function F. Thus the general•

210

Srinivas

ized Noisy-Or model can be viewed as starting with
a deterministic model (the function F) and then in­
troducing failures in the inputs, viz, the inhibitor
probabilities, resulting finally in a non-deterministic
model.
3

CHARACTERIZING P(XIU)

We note that we have already defined P(u; j u; )
in terms of the inhibitor probabilities.
The above equation is easily converted to an al­
gorithm (described later) to generate a conditional
probability table given the inhibitor probabilities
and the function F.

BOOLEAN NOISY-OR AS A
3.1
Each line failure function M defines a probability
SPECIAL CASE
distribution P;(U;jU;) relating u; and U;. From the
The generalized Noisy-Or collapses to be the
model for .M we see that the distribution P; is calBoolean Noisy-Or [Pearl1988] when all the variables
culated as:
are Boolean1, the function F is the Boolean OR,
�,
ofail
(O) = q; and Pjnh(1) = 0. In other words, N;
+
i
I;(
)�inh
;))
;)
Pt
� nh(I;(u
if u = I;(ui
Pi (U; u; ) - pfnh(l(
can
fail
with probability q; with the output being
otherwise
u1.))
a
a
a
"false"
but
it cannot fail with output being "true" .
(1)
Let
/;
�nd t; den�te. the "true" and "false"
The equation above summarizes the following
states
f
vanable
U;. Simi!arly we have fx and .tx
?
facts: if the the output u; of .M is the "same" as
for variable. X. The followmg can be shown easily
·
· d'Ices of both are the same),
t u; ('1.e., the m
the Inpu
from equatiOn 2 above:
then either the device M is working normally or it
has failed in the state u�. If the output u; is not
the "same" as input u;, then the device has failed in
P(fx l u) =
I
IT q;
state u;.
{ilu;=ti}
We now characterize the distribution P(XjU )
1- IT
in terms of the inhibitor probabilities for each U;
{ilu;=t;}
and the function F.
We note that:
_

{

1

We note that once we know the state U of U 1 ,
we know the value x of X, since x = F(u'). In other
words, X is independent of U once U1 is known.
The above equation therefore simplifies to:

P(xju) = L P(xju')P(u1ju)
ul
We note that P(xju1) = 1 when x = F(u1) and
P(xju1) = 0 when x -f:. F(u1). This simplifies the

defining equation to:

I

P(u ju)

P(xju) =
{U1Ix=F(U1)}

1

Now we note that the dependence of U =
[u1, u2, ... , un ] on u = (u1, u2, . . . , Un] can be split
into n pairwise dependences of u; on u;. This is be­
cause the value of a variable U/ depends solely on
U; and not on any other variable Uj where i-f:. j.
Thus we can simplify the equation to:

P(xju) =

P(ulju)

L

{U1Ix=F(U1)}

IT P;(u;ju;)
u'

CHOICE OF A FUNCTION F
The generalized model described above allows the
use of any discrete function F relating U to X. We
now suggest a particular form of F that is 'compat­
ible' with the Boolean Noisy-Or, i.e., F degenerates
to the Boolean OR function when the inputs and
outputs are Boolean23:

4.1

In essence, this function is a weighted average
- we are finding the fraction of each input's state's
index over the maximum possible index of that in­
put, averaging these fractions, scaling this quantity
to the maximum index of the output, and mapping
back to an actual state of the output after converting
the scaled result to an integer.
1 For Boolean variables we define the index of the
"false" state to be 0 and the index of the "true" state to
be 1.

2We use the syntax rl for the Ceiling function. For a
real number x, r X l is the smallest integer i that satisfies

{U1Ix=F(U1)}
=

INTERESTING SPE CIAL
CASES

4

P(xju) = L P(xju1, u)P(u1ju)
ul

(2)

i >
-

x.

3In the following equation, note again that

notes the jth state of

X.

x(j)

de­

A Generalization of the Noisy-Or Model

This additive function will have the characteris­
tic that as any input goes 'higher' it will tend to drive
the output 'higher'. Further, the inputs are 'equally
weighted' regardless of their arity. So, for example,
a change from state 0 to state 1 in a Boolean input
will have just the same effect as a change from 0 to
5 in an input with 6 states. Finally, the output is 0
if and only if all the inputs are 0.
We note that this function reduces to the
Boolean OR function in the case where all inputs
are Boolean and the output is Boolean.
CASE OF BOOLEAN OUTPUT AND
nARY INPUTS
Consider the case where X is a Boolean variable
and the inputs U; are nary. The function F is de­
fined as in the previous section. Further, we define
Pinh(O) = q; and Plnh(j) = 0 for j f. 0. We see that
we have a restricted generalization of the Boolean
Noisy-Or.
This special case of nary inputs and Boolean
output is interesting since it has better computa­
tional properties than the general case while be­
ing more general than the Boolean Noisy-Or (see
Sec 5.2).
4.2

OBTAINING STRICTLY POSITIVE
DISTRIBUTIONS
In some situations it is desirable for the condi­
tional distribution of a Bayesian network node X
with predecessors U to be strictly positive, i.e.,
VxVuP(xju) > 0.
For the generalized Noisy-Or model, the defini­
tion of P(xju) is in Equation 2. From this definition
we note that the following condition is necessary to
ensure a strictly positive distribution:
4.3

For all states x of X, the set {u'jx =
F(u')} is not empty. In other words, F
should be a function that maps onto X.
This condition is a natural restriction - if F
does not satisfy this condition, the variable X, in ef­
fect, has superfluous states. For example, the func­
tion defined in Section 4.1 satisfies this restriction.
Assuming that the above condition is satisfied,
the following condition is sufficient (though not nec­
essary) to ensure a strictly positive distribution:
For any u' and u, P(u'ju) > 0, i.e.,
Tiu P;(u;ju;) > 0.

'

This second condition is a stronger restriction.
From Equation 1 we note that this restriction is
equivalent to requiring that all inhibitor probabil­
ities be strictly positive, i.e., that prh(j) > 0 for
allOs;j<m;.

211

Finally, we note that the Boolean Noisy-Or for­
mulation of [Pearl 1988] and its generalization to
nary inputs described in Section 4.2 always result
in a distribution which is not strictly positive since
P(txif ) = 0.
5

COMPUTING P(XIU)

We consider the complexity of generating the prob­
abilities in the table P(XIU).
Let S = IJ; m; be the size of the joint state
space of all the inputs U;. We first note that
P;(u�lui) can be computed in e(1) time from the
inhibitor probabilities. This leads to:

P(u'ju) =

II P;(u�iu;) = e(n)

Therefore:

P(u'ju) = e(Sn)

P(xju) =
{xlx=F(U')}

This is because, for a given x and u we have to
traverse the entire state space of u' to check which
u' satisfy x = F(u').
To compute the entire table we can naively
compute each entry independently in which case we
have:
P(XIU) = mxSe(Sn) = e(mxnS2)
However the following algorithm computes the
table in e(nS2):
Begin Algorithm
For each state u of U:
• For all states x of X set P(xju) to 0.
'
• For each state u' of u :
- Set x = F(u').
- Increment P(xju) by P(u'ju).
End Algorithm
BOOLEAN NOISY-OR
In the case of the Boolean Noisy-Or, all U; and X
are Boolean variables. We see from Sec 3.1 that:
P(fxiu)
II q; = e(n)
5.1

{ilu;=t;}

For computing the table, we see that since
P(txlu) = 1- P(fxlu), we can compute both prob­
abilities for a particular u in e(n) time. So the
time required to calculate the entire table P(XIU)
is e(Sn).
We see that in the case of the Boolean Noisy-Or
there is a substantial saving over the general case in
computing probabilities. This saving is achieved by
taking into account the special characteristics of the
Boolean OR function and the inhibitor probabilities
when computing the distribution.

212
5.2

Srinivas

BOOL EAN OUTPUT AND nARY
INPUTS

A

From an analysis similar to the previous section we
note that computation of P(XIU) takes 0 ( Sn) time
in this case too.
5.3

B

STORAG E COMPLEXITY

c

For the general case we need to store mi inhibitor
probabilities per predecessor. Therefore in this
case 0(nmmax) storage is required where mmax =
mruq (mi). This contrasts with O(mxm�ax) for stor­
ing the whole probability table.
For the Boolean Noisy-Or we need to store
one inhibitor probability per predecessor and this is
e(n). Using tables instead would cost 0(2 X 2n) =
0(2n).
In the case of nary inputs and Boolean output
( as described above) one inhibitor probability per
predecessor is stored. Thus storage requirement is
0(n). Using a table would cost O(m�ax) ·
5.4

Each line has lhe probability of failure marked on il.

Figure 2: A digital circuit

For every link the failure function N hazJ the following inhibitor
probabilities (where X is the predecessor variable of the link):

REDUCING COMPUTATION
COMPL EXITY

In general, one could reduce the complexity of com­
puting P(:z:lu) if one could take advantage of special
propertie.s of the function F to efficiently generate
those u' that satisfy x = F(u') for a particular x.
Given a function F, we thus need an efficient
algorithm Invert such that lnvert ( x) = {ul x =
F(u)}. By choosing F carefully one can devise ef­
ficient Invert algorithms. However, to be useful as
a modeling device, the choice of F has also to be
guided by the more important criterion of whether
F does indeed model a frequently occurring class of
phenomena.
This Noisy-Or generalization has high complex­
ity for computing probability tables from the in­
hibitor probabilities4. If the generalization is seen
mostly as a useful modeling paradigm, then this
complexity is not a problem, since the inhibitor
probabilities can be pre-compiled into probability
tables before inference takes place. Inference can
be then performed with standard Bayesian network
propagation algorithms.
If this generalization, however, is seen as a
method of saving storage by restricting the models
to a specific kind of interaction, the cost of com­
puting the probabilities on the fly may outweigh the
gains of saving space.
4 However,

the Boolean Noisy Or does not suffer from

this p roblem since the special structure of the F function
and the fact that the inputs and outputs are Boolean
reduce the complexity dramatically by a factor of S.

F

p;rh(f)

=

O.Ql and

p;rh(t)

=

0

Figure 3: A generalized Noisy or model of the circuit

6

EXAMPLES

DIGITAL CIRCUIT DIAGNOSIS
The generalized Noisy-Or provides a straight­
forward method for doing digital circuit diagnosis.
Consider the circuit in Fig 2. Let us assume that
each line ( i.e., wire) in the circuit has a probability
of failure of 0.01 and that when a line fails, the input
to the devices downstream of the line is false.
Each of the inputs to the devices in the circuit
is now modeled with a state variable in a Noisy-Or
model ( see Fig 3). The function F for the general­
ized Noisy-Or which is associated with each node is
the truth table of the digital device whose output
the node represents. We have an inhibitor probabil­
ity of 0.01 associated with the false state along each
link and an inhibitor probability of 0 associated with
the true state ( since the lines cannot fail in the true
state in our fault model) .
A Bayesian network is now constructed from
the Noisy-Or model ( see Fig 4) using the algorithm
described in Section 5. Note that to complete the
Bayesian network one needs the marginal distribu­
tions on the inputs to the circuit. Here we have
made a choice of uniform distributions for these
6.1

A Generalization of the Noisy-Or Model

Gf

"'g}
U2--

Un

PF=tD,E
D -g Prob
I
I

f
f

I
f

I
f

0.0198
0.9900
0.9900
0.0000

PD tA,B
A B
Prob
I
I
f
f

I
f
I
f

The node� A, B and C are as::ngned

0.9801
0.0000
0.0000
0.0000

umform

P(E- t B,C
B c
Prob
I

I

f

I

I

f

f

f

ma.rg1na.l

P(A"' t)"' P(B"' t)"' P(C"' t)

"'0.5.

G

X

213

----a:-•
I

Ul
U2

X

Un
L -----·

Figure 5: Modeling device failure with an 'extended'
device.

0.9999
0.9900
0.9900
0.0000

dtstnbution�.

Figure 4: Bayesian network for digital circuit exam­
ple.
marginals.5
As an example of the use of the resulting
Bayesian network, consider the diagnostic question
"What is the distribution of D given F is false and
B is true ?". The evidence B = t and F = f
is declared in the Bayesian network and any stan­
dard update algorithm like the Jensen-Spiegelhalter
[Jensen 1989, Lauritzen 1988] algorithm is used to
yield the distribution P(D = t!F = j, B = t) =
0.984 and P(D =!IF= j, B = t) = 0.016.
Note that this example does not include a model
for device failure - only line failures are considered.
However the method can be extended easily to han­
dle device failure by replacing every device G in the
circuit with the 'extended' device c' as shown in
Fig 5. In this figure, the input (variable) G1 has a
marginal distribution which reflects the probability
of failure of the device. All the inhibitor probabilities
on the line G1 are set to 0. Note that the particu­
lar fault model illustrated here is a 'failed at false'
model, i.e., when the device is broken, its output
is false. One nice feature of the method described
above is that it is incremental. If a device is added
or removed from the underlying circuit a correspond­
ing node can be added or removed from the Bayesian
5These marginals can be seen as the distribution over
the inputs provided by the enVironment outside the cir­
cuit. Such a distribution is not usually available. But

when the distribution is not available, all diagnosis is
out with the assumption that all inputs
are known. Furthermore, when all the inputs are known,
it is to be noted that the answer to any diagnostic ques­
tion is not affected by the actual choice of marginal as
long as the marginal is any strictly positive distribution.

perforce carried

Ea.ch link ha.s the probability

of

failure marked on it.

Figure 6: A network with unreliable links.
network - there is no need to construct a complete
diagnostic model from scratch.
This method relates very well to the model
based reasoning approach in this particular do­
main [deKleer 1987, deKleer 1989, Geffner 1987].
We describe a probabilistic approach to model­
based diagnosis using Bayesian networks in detail
in [Srinivas 1993b, Srinivas 1993a].
NETWORK CONNECTIVITY
The following example uses the Boolean Noisy-Or
and the following example generalizes it to use the
generalized Noisy-Or.
Consider the network shown in Fig 6. Say each
link is unreliable- when the link is 'down' the link
is not traversable. The reliability of each link L is
quantified by a probability of failure I (marked on
the link in the network). Now consider the question
"What is the probability that a path exists from A
toG?".
Consider the subset of the network consisting
of A and its descendants (in our example, for sim­
plicity, this is the whole network). We first asso­
ciate each node with the Boolean OR as the F func­
tion. Each of the link failure probabilities translates
directly into the inhibitor probability for the false
state along each link. The inhibitor probability for
the true state is 0.
This network is now used to create a Bayesian
network using the algorithm of Sec 5. The Bayesian
6.2

214

Srinivas

network has the same topology as the network
in Fig 6. To complete the distribution of the
Bayesian network the root node A has to be as­
signed a marginal distribution. We assign an arbi­
trary strictly positive distribution to the root node
A (since evidence is going to be declared for the root
node, the actual distribution is irrelevant).
The answer to the question asked originally is
now obtained as follows: Declare the evidence A = t
(and no other evidence), do evidence propagation
and look at the updated belief of G. In this example,
we get Bel(G = t) = 0.7874 and Bel(G = /) =
0.2126.6 These beliefs are precisely the probabilities
that a path exists or does not exist respectively from
A to G.
To see why, consider the case where link failures
cannot happen (i.e., link failure probability is zero) .
Then if any variable in the network is declared to
be true then every downstream variable to which it
has some path will also be true due to the nature of
the Boolean OR function. Once the failure proba­
bilities are introduced, belief propagation gives us,
in essence, the probability that a connected set of
links existed between A and G forcing the OR gate
at G to have the output true.
Furthermore, it is to be noted that because be­
lief propagation updates beliefs at every node, the
probability of a path existing from A to any node X
downstream of it is available as Bel(X = t).
This method can be extended with some minor
variations to answer more general questions of the
form "What is the probability that there exists a
path from any node in a set of nodes Sto a target
node T ?".
NETWORK CONNECTIVITY
EXTENDED
Consider the exact same network as in the previ­
ous example. The question now asked is "What is
the probability distribution over the number of paths
existing from A to G ?".
Consider the subset of the network consisting of
A and its descendants. For every node U we make
the number of states be nu + 1 where nu is the
number of paths from root node A to the node U.
The states of U are numbered from 0 through nu.
We will refer to the ith state of node U as u(i) .
The number nu can be obtained for each node
in the network through the following simple graph
traversal algorithm:
6.3

Begin Algorithm
6The updated belief Bel(X = x) of a variable X is
the conditional probability P(X = xiE) where E is all
the available evidence.

For root node A, set nA = 1.7
For every non root node U in the graph
considered in graph order ( with ances­
tors before descendants):
nu = LpeParents(U) np
End Algorithm
•

•

To build the Noisy-Or model, we now associate
integer addition as the function F associated with
each node. For example, if R and S are parents of
T and the state of R is known to be r2 and the state
of Sis known to be s3, then the function maps this
state of the parents to state t(2+3) = t5 of the child
T.

We now set the inhibitor probabilities as fol­
lows: Say the predecessor node of some link L in the
graph is a node U. We set the inhibitor probabil­
ity for state u(O) to be the link failure probability l
and all other inhibitor probabilities to be 0. That is
P{ph(O) = l, where l is the link failure probability
and P{ph(i) = 0 for i = 1, 2 ... , nu.
We now construct the Bayesian network from
the network described above. The marginal proba­
bility for the root node is again set arbitrarily to any
strictly positive distribution since it has no effect on
the
. result.
The answer to the question posed above is ob­
tained by declaring the evidence A = 1 and then
doing belief propagation to get the updated beliefs
for G. The updated belief distribution obtained for
G is precisely the distribution over the number of
paths from A to G.
To see why, consider the case where there are no
link failures. Then when A is declared to have the
value 1, the addition function at each downstream
nodes counts exactly the number of paths from A
to itself. Once the failures are introduced the ex­
act count becomes a distribution over the number of
active paths.
In this example, we get the distribution:
Bel(G = 0) = 0.2126, Bel(G == 1) = 0.3466,
Bel(G = 2) = 0.2576, Bel(G = 3) = 0.1326 and
Bel(G = 4) = 0.0506. We see that Bel(G = 0) is
the same probability as Bel(G = f) in the previ­
ous example, viz, the probability that no path exists
from A to G.
Note that after belief updating, the distribution
of number of paths from A to any node X down­
stream of it is available as the distribution Bel(X)
after belief propagation.
This method can be extended with to answer
more general questions of the form "What is the
distribution over the number of paths that originate
7We define the root node to have
itself.

a

single path to

A Generalization of the Noisy-Or Model

in any node in a set of nodes S and terminate in a
target node T ?".
Another interesting example which can be
solved using the generalized Noisy-Or is the prob­
abilistic minimum cost path problem: Given a set
of possible (positive) costs on each link of the net­
work and a probability distribution over the costs,
the problem is to determine the probability distri­
bution over minimum cost paths between a specified
pair of nodes.
The generalized Noisy-Or, in fact, can be
used to solve an entire class of network problems
[Srinivas 1993c]. The general approach is as in the
examples above - the problem is modeled using the
generalized Noisy-Or and then Bayesian propagation
is used in the resulting Bayesian network to find the
answer.
All the examples described above use the Noisy­
Or model at every node in the network. However,
this is not necessary. Some sections of a Bayesian
network can be constructed 'conventionally', i.e., by
direct elicitation of topology and input of probabil­
ity tables while other sections where the Noisy-Or
model is applicable, can use the Noisy-Or formal­
Ism.
7

IMPLEMENTATION

This generalized Noisy-Or model has been imple­
mented in the IDEAL [Srinivas 1990] system. When
creating a Noisy-Or node, the user provides the in­
hibitor probabilities and the deterministic function
F.

IDEAL ensures that all implemented inference
algorithms work with Bayesian networks that con­
tain Noisy-Or nodes. This is achieved by 'compiling'
the Noisy-Or information of each node into a con­
ditional probability distribution for the node. The
distribution is available for all inference algorithms
to use.
Acknowledgements
I thank Richard Fikes, Eric Horvitz, Jack Breese
and Ken Fertig for invaluable discussions and sug­
gestions.

References
[deKleer 1987] de Kleer, J. and Williams, B. C.
(1987) Diagnosing multiple faults.
Artificial Intelligence, Volume 32,
Number 1, 97-130.
[deKleer 1989] de Kleer, J. and Williams, B. C.
(1989) Diagnosis with behavioral
modes. Proc. of Eleventh Interna-

215

tional Joint Conference on AI, De­
troit, MI. 1324-1330.
[Geffner 1987] Geffner, H. and Pearl, J. (1987)
Distributed Diagnosis of Systems
with Multiple Faults. In Proceed­
ings of the 3rd IEEE Conference on
AI Applications, Kissimmee, FL,
February 1987. Also in Readings
in Model based Diagnosis, Morgan
Kauffman.
[Jensen 1989] Jensen, F. V., Lauritzen S. L. and
Olesen K. G. {1989) Bayesian up­
dating in recursive graphical mod­
els by local computations. Report R
89-15, Institute for Electronic Sys­
tems, Department of Mathematics
and Computer Science, University
of Aalborg, Denmark.
[Lauritzen 1988] Lauritzen, S. L. and Spiegelhal­
ter, D. J. (1988) Local computa­
tions with probabilities on graph­
ical structures and their applica­
tions to expert systems
J. R.
Statist. Soc. B, 50, No. 2, 157-224.
Pearl, J. (1988) Probabilistic Rea­
[Pearl 1988]
soning in Intelligent Systems: Net­
works of Plausible Inference. Mor­
gan Kaufmann Publishers, Inc.,
San Mateo, Calif.
[Srinivas 1990] Srinivas, S. and Breese, J. (1990)
IDEAL: A software package for
analysis of influence diagrams.
Proc. of 6th Conf. on Uncertainty
in AI, Cambridge, MA.
[Srinivas 1993a] Srinivas, S. (1993) A probabilistic
ATMS. Technical Report, Rockwell
International· Science Center, Palo
Alto Laboratory, Palo Alto, CA.
[Srinivas 1993b] Srinivas, S. (1993)
Diagnosis with behavioural modes us­
ing Bayesian networks. Techni­
cal Report, Knowledge Systems
Laboratory, Computer Science De­
partment, Stanford University. (in
preparation).
[Srinivas 1993c] Srinivas, S. (1993) Using the gen­
eralized Noisy-Or to solve proba­
bilistic network problems. Tech­
nical Report, Knowledge Systems
Laboratory, Computer Science De­
partment, Stanford University. (in
preparation).

