Section on Medical Informatics
Division of General Internal Medicine
Stanford University Medical Center
Stanford, California 94305

system computes the value of information of that test.
The system recommends that the test be performed if
and only if the value of information exceeds the cost
of the test.1
In most decision contexts, a decision maker has the
option to perform several tests, and can decide which
test to perform after seeing the results of all previous tests. Thus, an expert system should consider the
value of all possible sequences of tests. Such an analysis is intractable, because the number of sequences
grows exponentially with the number of tests. Builders
of expert systems have avoided the intractability of
complete value-of-information analyses by implementing myopic or greedy value-of-information analyses. In
such analyses, a system determines the next best test
by computing value of information based on the assumption that the decision maker will act immediately after seeing the results of the single test (Gorry
et al., 1973; Heckerman et al., 1990). In this paper,
we present an approximate nonmyopic analysis. The
analysis avoids the traditional myopic assumption by
making use of the statistical properties of large samples.

2

VALUE-OF-INFORMATION
COMPUTATIONS FOR
DIAGNOSIS

We
discuss
myopic
and
nonmyopic
value-of-information computations in terms of the simple model for diagnosis under uncertainty represented
by the influence diagram in Figure 1. In this model,
the chance node H represents a mutually exclusive and
exhaustive set of possible hypotheses, and the decision
node D represents a mutually exclusive and exhaustive
set of possible alternatives. The value node U represents the utility of the decision maker, which depends
on the outcome of H and the decision D. The chance
nodes E1 , E2 , . . . , En are observable pieces of evi1
This prescription for action assumes that the delta
property holds. See Section 3.

D
U
H

E2

En
•••

E1

such that we should take action D if and only if the
probability of H exceeds p∗ . This threshold is the
probability of H at which the decision maker is indifferent between acting and not acting. That is, p∗ is the
point where acting and not acting have equal utility,
or
p∗ U (H, D) + (1 − p∗ )U (¬H, D) =
(2)
p∗ U (H, ¬D) + (1 − p∗ )U (¬H, ¬D)
In Equation 2, U (H, D) is the decision maker’s utility
for the situation where H occurs and action D is taken,
U (H, ¬D) is the utility when H occurs and action D
is not taken, and so on. Solving Equation 2 for p∗ , we
obtain
C
p∗ =
(3)
C +B
where C is the cost of the decision

E3

C = U (¬H, ¬D) − U (¬H, D)

(4)

and B is the benefit of the decision
Figure 1: An influence-diagram representation of the
problem of diagnosis under uncertainty. The decisionmaker’s utility (rounded rectangular node, U ) depends
on a hypothesis (oval node, H) and a decision (square
node, D). The variables Ei are pieces of evidence or
tests about the true state of H.

B = U (H, D) − U (H, ¬D)

(5)

If the decision maker has observed pieces of evidence
E1 , E2 , . . . , Em , then the decision maker should choose
action D if and only if p(H|E1 . . . , Em ) > p∗ . In terms
of the odds formulation, he should act if and only if
O(H|E1 , . . . , Em ) ≥

dence or tests about the true state of H. This model
is identical to that for Pathfinder (Heckerman, 1990).

p∗
1 − p∗

(6)

We make several simplifying assumptions. First, we
assume that H is a binary chance variable and D is a
binary decision variable. We use H and ¬H to denote
the two outcomes of H, and D and ¬D to denote the
two outcomes of D. For definiteness, we assume that
the decision maker chooses D (as opposed to ¬D),
when H occurs. Second, we assume that each piece
of evidence, E1 , E2 , . . . , En , is binary. Finally, we
assume that each piece of evidence is conditionally independent of all other evidence, given H and ¬H. In
Section 6, we relax these assumptions.

The weight of evidence, wi , is defined as the log of the
likelihood ratio, ln λi . Mapping likelihood ratios into
weights of evidence allows us to update the probability of H through the addition of the weights of evidence. Referring to Equations 1 and 6, we can rewrite
the threshold-probability condition in terms of the loglikelihood ratio where wi = ln λi . The decision maker
should choose action D if and only if

Using the assumption of conditional independence of
evidence, we can calculate the posterior probability
of the hypothesis by multiplying together all of the
p(Ei |H)
p(H)
likelihood ratios, p(E
, with the prior odds, p(¬H)
.
i |¬H)

In this expression, W ∗ is the decision threshold in
terms of weights of evidence.

p(H|Ei , . . . , Em )
p(E1 |H)
p(Em |H) p(H)
=
...
P (¬H|Ei , . . . , Em )
p(E1 |¬H)
p(Em |¬H) p(¬H)

We can write this equation more compactly in odds
form as
m
Y
O(H|Ei , . . . , Em ) = O(H)
λi
(1)
i=1

where λi is the likelihood ratio

p(Ei |H)
p(Ei |¬H) .

Because D and H are binary, it follows from the MEU
principle that there exists a threshold probability p∗ ,

W =

m
X
i=1

3

wi ≥ ln

p∗
− ln O(H) = W ∗
1 − p∗

(7)

MYOPIC ANALYSIS

Let us assume that the user of a diagnostic system has
instantiated zero or more pieces of evidence in the influence diagram shown in Figure 1. We can propagate
the eﬀects of these instantiations to the uninstantiated
nodes, and remove the instantiated nodes from the influence diagram. This removal leaves an influence diagram of the same form as that shown in Figure 1. To
simplify our notation, we continue to refer to the remaining pieces of evidence as E1 , E2 , . . . , En ; also, we
use p(H) to refer to the probability of the hypothesis
H, given the instantiated evidence.

The decision maker now considers whether he should
observe another piece of evidence before acting. A
myopic procedure for identifying such evidence computes, for each piece of evidence, the expected utility
of the decision maker under the assumption that the
decision maker will act after observing only that piece
of evidence. In addition, the procedure computes his
expected utility if he does not observe any evidence
before making his decision. If, for each piece of evidence, the expected utility given that evidence is less
than the expected utility given no evidence, then the
decision maker acts immediately in accordance with
Equation 6. Otherwise, the decision maker observes
the piece of evidence with the highest expected utility;
then, the myopic procedure repeats this computation to
identify additional evidence for observation. Because
the myopic procedure allows for the gathering of additional evidence, the procedure is inconsistent with its
own assumptions. We return to this observation in the
next section.
In the remainder of this section, we examine the computation of expected utilities and introduce notation.
Let EU (E, CE ) denote the expected utility of the decision maker who will observe E at cost CE , and then
act. Let CE(E, CE ) be the certain equivalent of this
situation. That is,
U (CE(E, CE )) = EU (E, CE )

(8)

or

CE(E, CE ) = U −1 (EU (E, CE ))
(9)
where U (·) is the decision maker’s utility function: a
monotonic increasing function that maps the value of
an outcome (e.g., in dollars) to the decision maker’s
utility for that outcome. Similarly, let EU (φ, 0) denote the expected utility of the decision maker if he
acts immediately, and let CE(φ, 0) denote the certain
equivalent of this situation. Thus, in the myopic procedure, a decision maker should observe the piece of
evidence E for which the quantity
CE(E, CE ) − CE(φ, 0)

(10)

is maximum, provided it is greater than 0.
In this paper, to simplify the discussion, we assume
that the delta property holds.2 The delta property
states that an increase in value of all outcomes in a
lottery by an amount 4 increases the certain equivalent of that lottery by 4 (Howard, 1967). Under this
assumption, we obtain
CE(E, CE ) = CE(E, 0) − CE

(11)

where CE(E, 0) is the certain equivalent of observing
E at no cost. Therefore, we have
CE(E, CE ) − CE(φ, 0) = V I(E) − CE

(12)

2
The primary result of this research—that we can
use the central-limit theorem to make tractable an
approximate nonmyopic analysis—is unaﬀected by this
assumption.

where
V I(E) = CE(E, 0) − CE(φ, 0)
(13)
3
is the value of information of observing E. The quantity V I(E) represents the largest amount that the decision maker would be willing to pay to observe E.
When we compare Expression 10 with Equation 12,
we see that, in the myopic procedure, a decision maker
should observe the piece of evidence E (if any) for
which the quantity
V I(E) − CE ≡ N V I(E)
(14)
is maximum and positive. We call N V I(E) the net
value of information of observing E.
The decision maker usually specifies directly the cost
of observing evidence. In contrast, we can compute
V I(E) from the decision maker’s utilities and probabilities. Specifically, from Equations 9 and 13, we have
V I(E) = U −1 (EU (E, 0)) − U −1 (EU (φ, 0))
To simplify notation, we use the abbreviations
EU (E, 0) ≡ EU (E) and EU (φ, 0) ≡ EU (φ)
Thus, we obtain
V I(E) = U −1 (EU (E)) − U −1 (EU (φ))
(15)
The computation of EU (φ) is straightforward. We
have

p(H)U (H, ¬D) + p(¬H)U (¬H, ¬D),




p(H) ≤ p∗

EU (φ) =


p(H)U (H, D) + p(¬H)U (¬H, D),



p(H) > p∗
(16)
by definition of p∗ .
To compute EU (E), let us assume that E is defined
such that the observation of E increases the probability of H. If p(H|E) > p∗ and p(H|¬E) > p∗ , then
V I(E) = 0, because the decision maker will not change
his decision if he observes E. Similarly, if p(H|E) < p∗
and p(H|¬E) < p∗ , then V I(E) = 0. Thus, we need
only to consider the case where p(H|E) > p∗ and
p(H|¬E) < p∗ . Let us consider separately the cases H
and ¬H. We have
EU (E|H) =
(17)
p(E|H)U (H, D) + p(¬E|H)U (H, ¬D)
and
EU (E|¬H) =
(18)
p(E|¬H)U (¬H, D) + p(¬E|¬H)U (¬H, ¬D)
where EU (E|H) and EU (E|¬H) are the expected utilities of observing E, given H and ¬H, respectively. To
obtain the expected utility of observing E, we average
these two quantities
EU (E) = p(H)EU (E|H) + p(¬H)EU (E|¬H) (19)
To compute V I(E), we combine Equations 15, 16, and
19.
3
Other names for V I(E) include the value of perfect
information of E and the value of clairvoyance on E.

4

NONMYOPIC ANALYSIS

As we mentioned in the previous section, the myopic procedure for identifying cost-eﬀective observations includes the incorrect assumption that the decision maker will act after observing only one piece of
evidence. This myopic assumption can aﬀect the diagnostic accuracy of an expert system because information gathering might be halted even though there
exists some set of features whose value of information
is greater that the cost of its observation. For example,
a myopic analysis may indicate that no feature is cost
eﬀective for observation, yet the value of information
for one or more feature pairs (were they computed)
could exceed the cost of their observation.
There has been little investigation of the accuracy of
myopic analyses. In one analysis, Kalagnanam and
Henrion, 1990, showed that a myopic policy is optimal, when the decision maker’s utility function U (·)
is linear, and the relationship between hypotheses and
evidence is deterministic. In an empirical study, Gorry,
1968, demonstrated that the use of a myopic analysis
does not diminish significantly the diagnostic accuracy
of an expert system for congenital heart disease.
In a correct identification of cost-eﬀective evidence,
we should take into account the fact that the decision maker may observe more than one piece of evidence before acting. This computation must consider
all possible ordered sequences of evidence observation,
and is, therefore, intractable.
Let us consider, however, the following nonmyopic approximation for identifying features that are cost effective to observe. Again, we assume that the delta
property holds. First, under the myopic assumption,
we compute the net value of information for each piece
of evidence. If there is at least one piece of evidence that has a positive net value of information,
then we identify for observation the piece of evidence
with the highest net value of information. Otherwise, we arrange the pieces of evidence in descending order of their net values of information. Let us
label the pieces of evidence E1 , E2 , . . . , En , such that
N V I(Ei ) > N V I(Ej ), if and only if i > j.
Next, we compute the net value of information of
each subsequence of E1 , E2 , . . . , En . That is, for m =
1, 2, . . . n, we compute the diﬀerence between the value
of information for observing E1 , E2 , . . . , Em , and the
cost of observing this sequence of evidence. If any
such net value of information is greater than 0, then
we identify E1 as a piece of evidence that is cost eﬀective to observe. Once the decision maker has observed
E1 , we repeat the entire computation described in this
section.
This approach does not consider all possible test sequences, but it does overcome one limitation of the
myopic analysis. In particular, the method can iden-

tify sets of features that are cost eﬀective for observation, even when the observation of each feature alone
is not cost eﬀective.

5

VALUE OF INFORMATION FOR
A SUBSET OF EVIDENCE

As in the myopic analysis, we assume that the decision
maker can specify the cost of observing a set of evidence. In this section, we show how we can compute
the value of information for a set of evidence from the
decision maker’s utilities and probabilities.
As in the previous section, let us suppose that the
decision maker has the option to observe a particular subset of evidence {E1 , E2 , . . . , Em } before acting.
There are 2m possible instantiations of the evidence in
this set, corresponding to the observation of Ei or ¬Ei
for every i. Let E denote an arbitrary instantiation;
and let ED and E¬D denote the set of instantiations E
such that p(H|E) > p∗ and p(H|E) ≤ p∗ , respectively.
The computation of the value of information for the
observation of the set {E1 , E2 , . . . , Em } parallels the
myopic computation. In particular, we have
EU (E1 . . . Em ) =
p(H)EU (E1 . . . Em |H)+
p(¬H)EU (E1 . . . Em |¬H)

(20)

where

and

EU (E1 . . . Em |H) =
£P
§
E∈ED p(E|H) U (H, D)+
£P
§
E∈E¬D p(E|H) U (H, ¬D)

EU (E1 . . . Em |¬H) =
£P
§
E∈ED p(E|¬H) U (¬H, D)+
£P
§
E∈E¬D p(E|¬H) U (¬H, ¬D)

(21)

(22)

To obtain V I(E), we combine Equations 15, 16, and
20.
When m is small, we can compute directly the sums in
Equations 21 and 22. When m is large, we can compute these sums using an approximation that involves
the central limit theorem as follows. First we express
the sums in terms of weights of evidence. We have
X
p(E|H) = p(W > W ∗ |H)
(23)
E∈ED

X

p(E|¬H) = p(W > W ∗ |¬H)

(24)

p(E|H)) = 1 − p(W > W ∗ |H)

(25)

p(E|¬H)) = 1 − p(W > W ∗ |¬H)

(26)

E∈ED

X

E∈E¬D

X

E∈E¬D

where W and W ∗ are defined in Equation 7. The term
p(W > W ∗ |H), for example, is the probability that the
sum of the weight of evidence from the observation of
E1 , E2 , . . . , Em exceeds W ∗ . That is, p(W > W ∗ |H) is
the probability that the decision maker will take action
D after observing the evidence, given that H is true.

p(W|H)

Next, let us consider the weight of evidence for one
piece of evidence. We have
wi

p(wi |H)

p(wi |¬H)

p(Ei |H)
ln p(E
i |¬H)

p(Ei |H)

p(Ei |¬H)

p(¬Ei |H)
ln p(¬E
i |¬H)

p(¬Ei |H)

p(¬Ei |¬H)

W*

To simplify notation, we let p(Ei |H) = α and
p(Ei |¬H) = β. The expectation and variance of w,
given H and ¬H, are then
α
(1 − α)
EV (w|H) = α ln + (1 − α) ln
β
(1 − β)

(27)

α(1 − β)
β(1 − α)

(28)

V ar(w|H) = α(1 − α)ln2
EV (w|¬H) = β ln

α
(1 − α)
+ (1 − β) ln
β
(1 − β)

(29)

α(1 − β)
β(1 − α)

(30)

V ar(w|¬H) = β(1 − β)ln2

Now, we take advantage of the additive property of
weights of evidence. The central-limit theorem states
that the sum of independent random variables approaches a normal distribution when the number of
variables becomes large. Furthermore, the expectation and variance of the sum is just the sum of the
expectations and variances of the individual random
variables, respectively. Because we have assumed that
evidence variables are independent, given H or ¬H,
the expected value of the sum of the weights of evidence for E1 , E2 , . . . , Em is
EV (W |H) =

m
X
i=1

EV (wi |H)

(31)

m
X
i=1

V ar(wi |H)

(32)

Thus, p(W |H), the probability distribution over W , is
m
X

p(W |H) ∼ N (

i=1

EV (wi |H),

m
X
i=1

The expression for ¬H is similar.

V ar(wi |H))

Figure 2: The probability that the total weight of evidence will exceed the threshold weight is the area under the normal curve above the threshold weight W ∗
(shaded region).
Finally, given the distributions for H and ¬H, we evaluate Equations 23 through 26 using an estimate or
table of the cumulative normal distribution. We have
Z ∞
−(t−µ)2
1
p(W > W ∗ |H) = √
e 2σ dt
(34)
σ 2π W ∗
where µ = EV (W |H) and σ = V ar(W |H). The probability that the weight will exceed W ∗ corresponds to
the shaded area in Figure 2. Again, the expression
for ¬H is similar. In this analysis, we assume that no
probability (p(Ei |H) or p(Ei |¬H)) is equal to 0 or 1.
Thus, all expected values and variances are finite. We
relax this assumption in the next section.

6

RELAXATION OF THE
ASSUMPTIONS

We can relax the assumption that evidence is twovalued with little eﬀort. In particular, we can extend
easily the odds-likelihood inference rule, Equation 1,
and its logarithmic transform, to include multiplevalued evidential variables. In addition, the computation of means and variances for multiple-valued evidential variables (see Equations 27 through 30) is straightforward.
In addition, we can relax the assumption that no probability is equal to 0 or 1. For example, let us suppose
that

The variance of the sum of the weights is
V ar(W |H) =

W

(33)

0 < p(Ej |H) = α < 1
p(Ej |¬H) = β = 1
0 < p(Ei |H) < 1,

0 < p(Ei |¬H) < 1,

i = 1, 2, . . . , n (i 6= j)
i = 1, 2, . . . , n (i 6= j)

Using Equations 27 through 30, we obtain
EV (wj |H) = +∞

V ar(wj |H) = +∞
EV (wj |¬H) < 0
V ar(wj |¬H) = 0

(a)

Therefore, although the computation of p(W >
W ∗ |¬H) is straightforward, we cannot compute
p(W > W ∗ |H) as described in the previous section.
Instead, we compute p(W > W ∗ |H), by considering
separately the cases Ej and ¬Ej . We have

H

G

1

G2

• • •

G

j

p(W > W ∗ |H) = p(Ej |H) p(W > W ∗ |HEj ) +
p(¬Ej |H) p(W > W ∗ |H¬Ej )
(35)

If ¬Ej is observed, W = +∞, and p(W >
W ∗ |H¬Ej ) = 1. Consequently, Equation 35 becomes

H

(b)

p(W > W ∗ |H) = p(Ej |H) p(W > W ∗ |HEj ) +
p(¬Ej |H)

We compute p(W > W ∗ |HEj ) as described in Equations 31 through 34, replacing EV (wj |H) with wj in
the summation of Equation 31, and V ar(wj |H) with 0
in the summation of Equation 32. The other terms in
the summations remain the same, because we have assumed that evidence variables are independent, given
H or ¬H. This approach generalizes easily to multiplevalued evidence variables and to cases where more
than one probability is equal to 0 or 1.
We can extend our analysis to special cases of conditional dependence among evidence variables. For example, Figure 3 shows a schematic of the belief network for Pathfinder. In this model, there are groups
of dependent evidence, where each group is conditionally independent of all other groups. We can apply
our analysis to this model by using a clustering technique described by Pearl (Pearl, 1988) (pp. 197-204).
As in the previous section, suppose we want to compute the value of information for the set of evidence
S = {E1 , E2 , . . . , Em }. For each group of dependent
features Gk , we cluster those variables in the intersection of S and Gk into a single variable. Then, we
average out all variables in the belief network that are
not in S. What remains is a set of clustered variables that are conditionally independent, given H and
¬H. We can now apply our analysis—generalized to
multiple-valued variables—to this model.
There are special classes of dependent distributions for
which the central-limit theorem is valid. We can use
this fact to extend our analysis to other cases of dependent evidence. For example, the central-limit theorem
applies to distributions that form a Markov chain, provided the transition probabilities in the chain are not
correlated (Billingsley, 1968). Thus, we can extend
our analysis to belief networks of the form shown in
Figure 4. We can generalize the value-of-information
analysis even further, if we use the Markov extension
in combination with the clustering approach described
in the previous paragraph.

Gk
Ei

E i+1

E i+2

Figure 3: A schematic belief network for Pathfinder.
(a) The features in Pathfinder can be arranged into
groups of evidence variables G1 , G2 , . . . Gj . The variables within each group are dependent, but the groups
are conditionally independent, given the disease variable H. (b) A detailed view of the evidence variables
Ei , Ei+1 , and Ei+2 within group Gk .

H

E1

E2

E3

•••

En

Figure 4: A conditional Markov chain. The evidence
variables form a Markov chain conditioned on the variable H. We can extend our analysis involving the
central-limit theorem to this case.

It is diﬃcult for us to extend the analysis to include
multiple-valued hypotheses and decisions. The algebra
becomes more complex, because the simple p∗ model
for action no longer applies. There is, however, the
opportunity for applying our technique to more complex problems. In particular, we can abstract a given
decision problem into one involving a binary hypothesis and decision variable. For example, we can abstract the problem of determining which of n diseases
is present in a patient into one of determining whether
the disease is benign or malignant. In doing so, we
ignore details of the decision maker’s preferences, and
we introduce dependencies among evidence variables.
Nonetheless, the benefits of a nonmyopic analysis may
outweigh these drawbacks in some domains.

7

SUMMARY AND CONCLUSIONS

We presented work on the use of the central-limit theorem to compute the value of information for sets
of tests. Our technique provides a nonmyopic, yet
tractable alternative to traditional myopic analyses for
determining the next best piece of evidence to observe.
Our approach is limited to information-acquisition decisions for problems involving (1) specific classes of dependencies among evidence variables, and (2) binary
hypothesis and action variables. Additional research,
however, may help to relax these restrictions. For now,
we pose the nonmyopic methodology as a new specialcase tool for identifying cost-eﬀective observations. We
hope to see empirical comparisons of the relative accuracy of the nonmyopic analysis with that of traditional
myopic analyses. We expect that the results of such
evaluations will be sensitive to the details of the application areas.

Acknowledgments
This work was supported by the National Cancer Institute under Grant RO1CA51729-01A1, and by the
Agency for Health Care Policy and Research under
Grant T2HS00028.

References
Billingsley, P. (1968). Dependent variables. In Convergence of Probability Measures, chapter 4. Wiley
and Sons, New York.
Gorry, G. and Barnett, G. (1968). Experience with
a model of sequential diagnosis. Computers and
Biomedical Research, 1:490–507.
Gorry, G., Kassirer, J., Essig, A., and Schwartz,
W. (1973). Decision analysis as the basis for
computer-aided management of acute renal failure. American Journal of Medicine, 55:473–484.

Heckerman, D. (1990). Probabilistic Similarity Networks. PhD thesis, Program in Medical Information Sciences, Stanford University, Stanford, CA.
Report STAN-CS-90-1316.
Heckerman, D., Horvitz, E., and Nathwani, B. (1989).
Update on the Pathfinder project. In Proceedings
of the Thirteenth Symposium on Computer Applications in Medical Care, Washington, DC, pages
203–207. IEEE Computer Society Press, Silver
Spring, MD.
Heckerman, D., Horvitz, E., and Nathwani, B.
(1990). Toward normative expert systems: The
Pathfinder project. Technical Report KSL-9008, Medical Computer Science Group, Section on
Medical Informatics, Stanford University, Stanford, CA.
Howard, R. (1967). Value of information lotteries.
IEEE Transactions of Systems Science and Cybernetics, SSC-3(1):54–60.
Kalagnanam, J. and Henrion, M. (1990). A comparison of decision analysis and expert rules for sequential diagnosis. In Shachter, R., Kanal, L.,
Levitt, T., and Lemmer, J., editors, Uncertainty
in Artificial Intelligence 4, pages 271–281. NorthHolland, New York.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo, CA.

