Introduction

Over the past few years, there has been a growing in­
terest in the problem of planning under uncertainty.
Markov decision processes (MDPs) have received much
attention as a basic semantics for this problem. An
MDP represents the domain via a set of states, with
actions inducing stochastic transitions from one state
to another. The key problem with this type of rep­
resentation is that, in virtually any real-life domain,
the state space is quite large. However, many large

Ronald Parr
Computer Science Department
Stanford University
Stanford, CA 94305- 9010
parr@cs. stanford. edu

MDPs have significant internal structure, and can be
modeled very compactly if that structure is exploited
by the representation. In factored MDPs, a state is de­
scribed implicitly as an assignment of values to some
set of state variables. A dy namic Bay esian network
(DEN) [7] can then allow a compact representation of
the transition model, by exploiting the fact that the
transition of a variable often depends only on a small
number of other variables. The momentary rewards
can often also be decomposed as a sum of rewards re­
lated to individual variables or small clusters of vari­
ables.
W hile these representations allow very large, com­
plex MDPs to be represented compactly, they do not
help address the planning problem. Standard algo­
rithms for solving MDPs require the representation
and manipulation of value functions- functions from
the exponentially many states to values. Unfortu­
nately, structure in a factored MDP rarely induces any
type of structure in the value function.
An obvious solution is to restrict attention to ap­
proximate value functions that can be represented
compactly [3]. One very useful approach is to use lin­
ear value functions- functions that are weighted lin­
ear combinations of some small number of basis func­
tions. In recent work, there has been some success in
using this approach to address the policy evaluation
problem- determining the value function for a fixed
policy. Generally, sampling is used to avoid explicit
manipulation of the entire state space [5, 10]. In [9]
( KP hereafter) , we presented an approach based on
approximate dynamic programming. The key to our
approach was the use of factored linear value functions,
where each basis function is restricted to some small
subset of the domain variables. We showed that, for
a factored MDP and factored value functions, the ap­
proximate dynamic programming steps can be imple­
mented in closed form without enumerating the entire
state space.
All of these methods compute a linear value function
that minimizes error in a weighted least squares sense,

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

where the (squared) approximation error is weighted
by the stationary distribution of the Markov chain
induced by the current policy. This means that fre­
quently visited states will have high priority, while in­
frequently visited states will have only a slight influ­
ence on the value function. In a pure prediction con­
text, this is a very natural approximation. However, it
is very poorly suited to the task of policy improvement,
as the weights can result in very misleading estimates
of value in states that are outside the range of the
current policy, leading to poor choices in the policy
improvement phase.
Our first key result in this paper is a new approach
for computing linear value functions that removes the
dependence of the error metric on the stationary dis­
tribution. In Section 4, we present a closed form set
of linear equations whose solution minimizes the Bell­
man error relative to any set of weights1 . By divorcing
the value determination algorithm from the stationary
distribution of the current policy, we can pick an error
metric that is more conducive to policy search. Thus,
we finally have the capability of using linear value func­
tions for policy iteration.
For the case of factored value functions and a fac­
tored MDP, the techniques of KP can be used to gener­
ate the equations efficiently, thereby providing an effi­
cient implementation of the value determination step.
To construct a full policy iteration algorithm, we must
also deal with the issue of representing and manipulat­
ing policies over very large state spaces. In Section 7
we show that, for factored value functions and factored
MDPs, we can represent the one-step greedy policy
compactly as a decision list, and compute its value
effectively. The computational cost depends on natu­
ral structural parameters of the MDP and the value
functions.
When approximately solving an MDP, it is impor­
tant to evaluate how far our proposed solution is from
the optimal. There are known results that allow us
to bound this error, but they depend on a max- norm
bound on the Bellman residual. In Section 8, we
present an algorithm that exploits the problem struc­
ture to compute max-norm bounds on the Bellman
error of a factored value function. This algorithm can
be used to bound the overall max-norm error of our
approximate value function, and thereby provide guid­
ance on how to adjust our approximation to provide
1

We note that there are two interpretations of the least
squares solution to the Bellman equations. The first is
as the direct minimization of the mean-squared Bellman
residual error as in [1], while the second is as the fixed point
of a Bellman iteration with a least-squares projection of the
value function, i.e., the standard linear temporal difference
approximation method. We adopt the latter approach in
this paper, although our methods can be used for direct
minimization of the Bellman residual error as well.

327

better results.
2

Markov Decision Processes

A Markov Decision Process (MDP) is defined as a 4tuple (S, A, R, P) where: S is a set of N states; A is a
set of actions; R is a reward function R : S t--t IR, such
that R(s) represents the reward obtained by the agent
in state s; and P is a transition model where Pa(s' Is)
represents the probability of going from state s to state
s' with action a.
A policy 1r for an MDP is a mapping from S to A. It
is associated with a value function v1r : S t--t IR, where
v1r( s) is the total cumulative value that the agent gets
if it starts at state s. We will be assuming that the
MDP has an infinite horizon and that future rewards
are discounted exponentially with a discount factor f.
Thus, v1r is defined using the fixed point equation:
V1r(s)

=

R( s) + 1 LP1r(s' I s)V1r(s1 ).
s'

It is useful to view this computation from the perspec­
tive of matrices and vectors. If we view v1r and R as
N-vectors, and P7r as an N x N matrix, we have the
equation
(I)
This is a system of linear equations with one equation
for each state, and can be solved easily for small N.
There are several ways to find the optimal policy
*
1r . A commonly used method is policy iteration which
repeats the following steps until convergence:
compute v1r.

•

For our current policy

•

For each action a, compute the function Qa:
Qa

•

Redefine 1r(s)

:=

=

1r,

R + 1Pav1r

(2)

argmaxa Qa(s).

The new policy is called "greedy" with respect to the
previous policy and value function because it looks a
single step into the future through the Qa functions.
In practice, this process often converges in a very small
number of iterations, making it the preferred method
for solving MDPs if v1r can be computed efficiently.
3

Linear Value Functions

In many domains, our state space is very large, and
we want to approximate our value functions with more
compact ones that can be maintained more easily. A
very popular choice is to approximate a value function
using linear regression. Here, we define our space of
allowable value functions V � JR8 via a set of basis
functions H = {h1 , . . . , hk}. A linear value function

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

328

over H is a function V that can be written as V =
L:;=l Wjhj for some coefficients w = (w1 , . .. , wk)· It
is often useful to define an N x k matrix A whose
columns are the k basis functions, viewed as vectors.
Our approximate value function is then Aw.
For a given value function V, we are often interested
in finding the value function V =Aw that most closely
approximates V. The notion of distance that is compu­
tationally most convenient is weighted L2 norm, where
we try to minimize L:s p(s)(V(s)- V( s))2 for some set
of non-negative weights p that sum to 1 (weighted least
squares). We can find the optimal V using a simple
projection process. Roughly speaking, we define each
weight Wi by taking the weighted dot product with
the corresponding basis function, and then correcting
for the fact that our basis is not orthonormal. More
precisely, the projection operation consists of comput­
ing w =(AT AA)-1 AT AV, where A is a weight matrix
with diagonal entries equal to our projection weights
p. This operation computes the least-squares projec­
tion of V onto the linear space defined by H. This
computation can be implemented via the weighted dot
product operation (f • g)p , defined as L:s p(s)f(s)g(s):
The entries of our correction matrix AT AA are simply
(hi • hj)p , and the entries of the vector AT AV are
(hi • V)p · Thus, an efficient implementation of the
weighted dot product is the key to the feasibility of
this computation.
Now, consider the task of evaluating some policy 1r.
In this case, our goal is to approximate the true value
function vn. unfortunately, we typically do not have
vn; hence, we usually try to find v that minimizes the
Bellman error: V - (/'Pn V + R). In KP, we provided
an iterative algorithm for approximate value determi­
nation; this approach was aimed at factored MDPs,
but applies to the general setting. Let P n be the tran­
sition model defined by the policy 1r. The iterative
value determination equation is
v(t+l) ='YP1r v<tl

+

R

(3)

A used a weighted least-squares approximation
to Eq. (3) is:

Under certain assumptions, this process converges to
a fixed point which a bounded distance from the
weighted projection of the true value function. One of
the key assumptions is that the projection weights p
must be very close to the stationary distribution of P n
(in relative error). This assumption was necessary to
ensure a contraction of the iterative algorithm. A sim­
ilar assumption is also crucial to the other (sampling­
based) approaches to the problem [5, 9, 10]. In gen­
eral, the stationary-weights least-squares approxima-

Exact VaJue •
WeighLed tea�t Squares E.�timate •••••·

: �=��s
-I
-2

··
· ··••..

··..
··•·

•..
.
··...

-3 '--�----'---�--'
2
State

Figure 1: Least squares estimates of the value of policy
RRRR.

tion has been the only type of approximation error for
which theoretical convergence results could be shown.
This approximation is natural in a purely predictive
context since it emphasizes the most frequently visited
states. However, value determination is often primar­
ily a stepping stone to our ultimate goal, which is the
construction of a good policy. Unfortunately, although
weighted least-squares is a suitable approximation for
predicting the performance of a given policy, it can be
extremely unreliable when used as the value determi­
nation phase of a policy iteration algorithm.
To understand this issue, consider an MDP with the
four states {so, . . . , s3} and the two actions L and
R. The R action moves "right" - from si to Si+I
(if available) - with probability 0. 9; with probability
0. 1 it fails and moves left. The L action has the op­
posite effect. The two middle states s1 and s2 have
reward + 1. We specify policies as a string of let­
ters 7r(s0)1r(s1 )1r(s2)7r(s3). The optimal policy for this
problem is RRLL.
Suppose that we try to perform policy iteration using
weighted least squares approximate value functions,
with the basis functions: h1(sx) =1, h2(sx) =x, and
h3(sx) =x2• If we view value functions as continuous
functions over the reals (with Bx representing x) , our
approximate value functions span the space of parabo­
las.
Assume we start with the policy RRRR, and com­
pute the approximate value function that minimizes
the Bellman error relative to the stationary distribu­
tion of this policy. The value function is shown in
Figure 1. At first, it looks like a reasonable approxima­
tion, but it has some critical flaws: The approximation
is much better for states 2 and 3 than for states 0 and
1. The reason is that states 0 and 1 are visited very in­
frequently: for policy RRRR, the stationary distribu­
tion is p =[0. 00113, 0. 01096, 0. 09913, 0.88879], giving
states 0 and 1 very little significance in the weighted
least squares fit. The more serious problem is quali-

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

tative. The shape of the value function is lost, giving
state 0 a higher value than state 1. The greedy pol­
icy for this value function is LLLL. Symmetrically, the
greedy policy for LLLL is RRRR and, thus, policy iter­
ation oscillates between these two suboptimal policies.
This phenomenon is not specific to this problem; it
has been observed by several researchers on a variety
of problems (see, e.g., [2]).
In general, we would prefer to have a V that mini­
mizes maximum norm error. Since it is difficult to con­
struct such approximations, a uniform-weighted (un­
weighted) projection often serves as a tractable substi­
tute. Indeed, F igure 1 also shows the the results of a
uniform-weighted value function projection for the pol­
icy RRRR. The value function correctly assigns state 0
a lower value than state 1. This leads to a greedy pol­
icy of RLLL and the optimal policy of RRLL is found
in the following iteration.
4

Value determination

As discussed above, all of the approximate value de­
termination procedures proposed so far have relied on
the use of stationary-weights least-squares projection
to guarantee convergence to a fixed point. In particu­
lar, the iterative approximate DP process we used in
KP relies on this assumption. In this section, we pro­
vide a new approach for computing an approximate
linear value function for a given policy 1r. The key in­
sight is that an iterative process is not required; we can
find the fixed point directly by writing an approximate
version of Eq. (1) and solving it:

Aw
AT AAw
w

>:::;j
>:::;j
>:::;j

f'P11"Aw + R
AT A(!'P11"Aw + R)
(AT AA)-1 AT A(!'P11"Aw + R) (4)

Letting B =!'(AT AA)-1 AT AP11"A, Eq. (4) is equiva­
lent to (I- B)w =(AT AA)- 1 AT AR. As B is a k x k
matrix, we can solve this equation easily if I B is
invertible. Surprisingly, this is almost always the case:
Theorem 4.1: For B = !'(AT AA)-1 AT AP11"A and
1' < 1, I- B is invertible for all but finitely many I'·
-

Proof: The determinant of I B is a polynomial
function in /'i therefore, it is either uniformly 0 for all
1' or has only finitely many roots. I
B is invertible
for 1' = 0. Hence, the determinant of I B is not
uniformly 0. Therefore, B can fail to be invertible for
at most finitely many 1'- I
-

-

-

Corollary 4.2: Eq. (4) has a unique solution, which

can be computed in closed form.

Thus, systems without solutions are extremely rare
and that if the solution does not exist, a perturbation

329

in 1' will make the system solvable. Of course, some
care must be taken to avoid numerical instability and
large errors when inverting matrices that are slightly
perturbed from singular matrices.
It is important to point out that the existence con­
ditions for our direct solution are much weaker than
the convergence conditions for iterative approximate
dynamic programming methods; an iterative solution
to Eq. ( 4) may diverge for almost all starting points
even though a unique fixed point solution exists.
The major advantage of our closed form solution is
that it can be used to find a weighted least-squares
approximation for any weighting p. Thus, we are free
to choose our projection weights to minimize the prob­
lem described in Section 3, where using weights corre­
sponding to the stationary distribution of the current
policy always misleads us about the value of rarely­
visited states. In other words, by allowing different
projection weights, we can more evenly distribute our
function approximation error and largely overcome the
main obstacle to using factored value functions for pol­
icy iteration.
This closed form solution defines a computation each
of whose operations- the dot product steps- seem
to grow linearly with the number of states in the sys­
tem. Hence, it might appear that this procedure is
not particularly helpful: If our state space is small
enough to make this computation feasible, it is also
small enough to allow an exact solution to the MDP.
In the next section, we show that for factored MDPs
and factored linear value functions, the relevant dot
product operations can be executed exactly in closed
form, without an exhaustive enumeration of the (ex­
ponentially large) state space.
5

Factored MDPs

In a factored MDP, the set of states is described via a
set of random variables X= {X1, .. ., Xn}, where each
X; takes on values in some finite domain Dom(X;). A
state x defines a value Xi E Dom(Xi) for each variable
Xi. Thus, the set of states S = Dom(X) is expo­
nentially large, making it impractical to represent the
transition model explicitly as matrices. Fortunately,
the framework of dy namic Bay esian networks (DENs)
gives us the tools to describe the transition model and
reward function concisely.
A Markovian transition model T defines a probabil­
ity distribution over the next state given the current
state. Let Xi denote the variable Xi at the current
time and XI the variable at the next step. The tran­
sition graph of a DBN is a two-layer directed acyclic
graph Gr whose nodes are {XI. ..., Xn, X{, ..., X�}.
For simplicity of exposition in the rest of the paper, we
assume that Parentsr(XI ) � X; in graphical terms, all

330

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

F igure 2: A simple DBN with 5 state variables.
arcs in the DBN are between variables in consecutive
time slices. This assumption can be relaxed, but our
algorithm becomes somewhat more complex. We de­
note the parents of XI in the graph by Parentsr(XI).
Each node XI is associated with a conditional proba­
bility distribution (CPD) Pr (XI I Parentsr(XI )). The
transition probability Pr(x' I x) is then defined to be
rt Pr(X� I Ui ) , where Ui is the value in X of the vari­
ables in Parentsr(XI ). F igure 2 shows a DBN with
5 binary state variables. We will use this extremely
simple DBN throughout the paper to illustrate some
of the concepts we introduce.
We can define the transition dynamics of an MDP
by defining a separate DBN model Ta =(Ga, Pa) for
each action a. However, in many cases, different ac­
tions have very similar transition dynamics, only dif­
fering in their effect on some small set of variables.
In particular, in many cases a variable has a default
evolution model, which only changes if an action af­
fects it directly [4]. We therefore use the notion of
a default transition model Td = (Gd, Pd)· For each
action a, we define Effects[a ] � X' to be the vari­
ables in the next state whose local probability model
is different from Td, i.e., those variables XI such that
Pa(XI I Parentsa(XI )) f:. Pd(XI I Parentsd(XI)).
(We note that d can be an action in our model, in
which case Effects[d] = 0.) In our example DBN,
we will define 5 actions, a1 . . . a5 and a default ac­
tion, d. Action ai changes the CPD of variable XI, so
Effects[ai] ={XI}.
Finally, we need to provide a compact representation
of the reward function. We assume that the reward
function is factored additively into a set of localized re­
ward functions, each of which only depends on a small
set of variables. For this, and for other reasons, the
following definition turns out to be crucial:
Definition 5.1: We say that a function f is restricted
to a domain C � X iff : Dom(C) H JR. If f is
restricted toY and Y C Z, we will use f (z) as short­
hand for f(y) where y is the part of the instantiation
z that corresponds to variables in Y. I

Let R1 , . . . , Rr be a set of functions, where each Ri is
restricted to a cluster of variables W; C {X1 , . . . , Xn} ·
The reward function associated with the state x is then
defined to be L:�=l Ri(x) E JR. For simplicity of nota­
tion, we assume that there is a single reward function
R that has bounded domain W. (Since our methods
are linear, this assumption is totally innocuous.)
One might be led to believe that factored transi­
tion dynamics and rewards would result in a structured
value function. Unfortunately, this usually is not the
case, as shown by KP in Example 2.1. In general, the
value function will eventually depend, in an unstruc­
tured manner, on all of the variables that have any
influence whatsoever, direct or indirect, on a reward.
6

Factored value functions

In KP, we observed that, although value functions are
not structured, there are many domains where they
are "close" to structured. Hence, we might be able to
approximate value functions well as a linear combina­
tion of functions each of which refers only to a small
number of variables. More precisely, we define a value
function to be a factored (linear) value function if it is
a linear value function over the basis h1 , . . . , hk, where
each hi is restricted to some subset of variables Ci (as
in Definition 5. 1). In our simple example DBN, we
might have 5 basis functions, h1 . . . h5, restricted to
X1 , . . . , X5 respectively. The function hi would evalu­
ate to 1 if X ; is true and 0 otherwise.
Factored value functions provide the key to doing
efficient computations over the exponential-size state
sets that we have in factored MDPs. The key insight is
that restricted domain functions (including our basis
functions) allow certain basic operations to be imple­
mented very efficiently. We now describe these compu­
tational building blocks, that are central to our later
development.
The first operation is the dot product. Assume that
f is restricted to Y and g is restricted to Z, and let
W =Y U Z. It can easily be shown that
IDom(X)I "'
(! • g) =IDom(W) I L.., f (w ). g(w )
w

This computation can be done in time which is lin­
ear in IDom(W)I. Assuming that W is substantially
smaller than X, this cost is exponentially lower than
the straightforward exhaustive enumeration. In our
example, W ={X1 , X 2} for (h1 · h2), so there are 4
terms in the summation.
Next, consider a factored transition model Pr de­
fined via a DBN (Gr.Pr)· A key operation is to back­
project a function f through Pr. In linear algebra
notation, we want to compute Prf, where we view Pr
as an N x N matrix (for N =l SI) and f as an N-

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

vector. The result is a function over S. Assume that
f is restricted to Y. We define the back-projection of
Y throu gh r as the set of parents of Y' in the transition
graph Gr; more precisely:

Now, we can compute

(Pr f )(x)

L Pr(x' I
x'

x) f (x' )

x'

L Pr(Y1 I x)f(y') L
y'

u' Ex' -y

Pr(u' I x)

'

y'

where z is the value of rr (Y) in x. Thus, we see
that (Prf) is a function whose domain is restricted
to r r(Y ). Note that the cost of the computation de­
pends linearly on IDom(r r(Y)) I, which in turns de­
pends both on Y, the domain of f, and on the com­
plexity of the process dynamics. In our example DBN,
the domain of Prh2 is Parentsr(X D = {X1,X2}.
We can extend this idea to compute the weighted
version of these operations, assuming our weights are
represented in a factored way. Assume that X is par­
titioned into a set of disjoint clusters E1, ..., Eq, such
that the weights p can be represented as a product
of factors (marginals) p1 , . .. , pk , where each Pi is a
factor (distribution) over Ei. It is easily verified that
(f • p) = Lyf(y)p(y). We can easily compute p(y) as
follows: let Yi denote the part of y that overlaps with
variables in the cluster Ei. We compute Pi(Yi) by a
simple marginalization process, and then multiply the
results for all i = 1, ... , q. We can now use this sub­
routine for doing the weighted projection, simply by
noting that (f • g)p = ((! • g) • p).
7

Policy Iteration for Factored MDPs

In this section, we show how to implement policy it­
eration using factored value functions. To make this
process tractable, we need to address two fundamen­
tal issues. We need to represent policies over an ex­
ponentially large space, and we need to show how to
perform the computation required for our closed form
value determination efficiently. As we now show, we
can address both of these issues within the context of
our framework.
7.1

Policy representation

Assume that we have computed a factored value func­
tion V over our basis H. Initially, this computation

331

will not be a problem: we can start from some default
policy that takes a fixed action a0 in every state, and
solve Eq. (4) for Pa0,as described in the previous sec­
tion. If w is the result, the V = Aw is our factored
value function.
Based on V, we can easily compute Qa as in Eq. (2):
Q a = 'YPaAw + R. As discussed in Section 6,FaA can
be computed efficiently; it consists of a set of k func­
tions with restricted domains fa(Ci)· Thus, PaAw is
a weighted combination of restricted domain functions.
We can compute this Qa function for every action, in­
cluding the default transition model d.
We now have a set of linear Q-functions which im­
plicitly describes a policy 1r. It is not immediately
obvious that these Q functions will result in a com­
pactly expressible policy. The key insight is that most
of the components in the weighted combination will be
identical in FaA and in Fd A· Intuitively, a component
corresponding to basis function hi will only be differ­
ent if the action a influences one of the variables in Ci.
More formally, recall that
c�

where z is the value of r a(Ci) in X. Now, assume that
Effects[a] n Ci = 0. In this case, all of the variables in
Ci have the Same transition model in Ta and Td, SO that
Pa(c� I z ) = Pd(c� I z ) and [Pahi](x) = [Pd hi](x). Let
Ia be the set of indices i such that Effects[a ] n Ci -:1 0.
These are the indices of those basis functions whose
back-projection differs in Pa and Pd. In our example
DBN, actions and basis functions involve single vari­
ables, so Ia; = i.
We can now define 8a(s) = Q a(s) - Qd(s). Our
analysis shows that 8a( s) is a function whose domain
is restricted to Ta = UiEiar a(Ci)· In our example
DBN, Ta2 = {X 1,X 2}.
Intuitively, we now have a situation where we have a
"baseline" value function Qd(s) which defines a value
for each state s. Each action a changes that baseline
by adding or subtracting an amount from each state.
The key point is that this amount depends only on
Ta, so that it is the same for all states in which the
variables in Ta take the same values.
We can now define the optimal policy relative to
our Q functions. For each action a, define a set of
conditionals (t, a,8),where each t is some assignment
of values to the variables Ta,and 8 is 8a(t). Now, sort
all of the conditionals for all of the actions by order of
decreasing 8:

(5)
Consider our optimal action in a state x. We would
like to get the largest possible "bonus" over the default

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

332

value. If x is consistent with t1 , we should clearly
take action a1, as it will give us bonus 61 . If not, then
we should try to get 62; i.e., we should check if x is
consistent with t2, and if so, take a2 . In general, we
can view Eq. (5) as a decision list representation of a
policy, where the optimal action to take in state x is
the action aj corresponding to the first event tj in the
list with which x is consistent.
Theorem 7.1 : The optimal one-step lookahead pol­

icy for a factored value function V has the form of a
decision list as in Eq. (5).

Note that the number of conditionals in the list is
La J Dom (Ta )l ; Ta, in turn, depends on the set of basis
function clusters that intersect with the effects of a.
Thus, the size of the policy depends in a natural way
on the interaction between the structure of our process
description and the structure of our basis functions. In
our example DBN, the number of conditionals is 18: 2
from a1 and 4 each from a 2
a5.
•

7.2

•

X

.

Value determination for decision-list
policies

The second issue that we must resolve in order to
"close the loop" is the computation of the value func­
tion for a given policy 1r.
The computational ideas described in the previous
section allow us to provide an efficient implementa­
tion for the value determination algorithm described
in Section 4, assuming both the process dynamics and
the value function basis are factored. To understand
why, consider a factored process Pr and a matrix A
representing our basis. Recall that the main operation
in the value determination process is computing the
k x k matrices AT AA and AT APr A· We can compute
the former by performing k2 weighted dot product op­
erations (h; • hj)A; we can compute the latter by per­
forming the k2 dot product operations ( h; • Pr hi)A.
The cost of these dot product operations depends on
the overlap of the domains of the two functions. (For
simplicity of notation, we ignore the weights in our
analysis from here on.) More precisely, we define the
structural cost cost(A,Pr) to be
max[�axJDom(Ci U Cj)J, ma_.xjDom(rr(C;) U Cj)IJ.
t,J

We want to apply this idea to the optimal one-step
lookahead policy 1r defined in Eq. (5). In other words,
we want to solve our fixed point Eq. (4) for a tran­
sition model Prr. In order to apply our efficient dot
product operations, the transition model Prr must be
factored as a DBN. Unfortunately, even with a decision
list policy, Prr does not have the appropriate structure.
Specifically, Prrhi may not have a restricted domain.
The solution to this problem is based on the observa­
tion that PrrA is a combination of Pa,A for the different
conditionals in the list (tt, a1, 61), with the proportions
of the different conditionals being the number of states
in which we apply this conditional. As we now show,
there is enough structure in Prrhi that we can directly
compute entries of AT PrrA efficiently.
We do this by computing (h;)T Prr(hj) for each pair
of basis functions h;, hj:

t,J

This expression measures the worst-case cost of com­
puting the dot-product of one of our basis functions
with another, or with the back-projection of another.
It depends on the extent to which the domains of ba­
sis functions overlap, and the extent to which back­
projection causes them to entangle. (Note that if X;
is always a parent of XI, the second term in the max
is no smaller than the first.) In our example DBN,
the structural cost is 8 since Dom(rr(C;) U Cj) can
contain at most 3 binary variables.

where [Prrhj](x) is the back-projection of hi through
Prr, evaluated at x. We can partition the states accord­
ing to the conditionals that are taken in the decision
list. For l = 1, . . . , L, let St be the set of states in
which the conditional (t1, a1, 61) is taken. Thus,

(h;fPrrhj =

L

LL

l=l xESz

h;(x)[Pa,hj](x).

Consider one of the terms in L xESz h;(x)[Pa,hj](x).
Recall that Pa,hj is a restricted domain function whose
domain is r a,(C i). The basis function h; is also re­
stricted domain, with domain C;. We can now define
Za, ,i , i = r a,(C i) U C; and rewrite the summation:

(h;fPrrhj =

L

L L

l=l zEZa1,;,;

h;(z)[Pa,hj](z)

1

zESz: Za1 ,i,j (x)=z

Let f a,,i ,j(z) be the function h;(z)[Pa,hj](z). This is
the product of two restricted domain functions and
can be computed easily using our techniques in time at
most cost(A, Pa1). The innermost summation simply
counts the number of states in the current partition
that are consistent with z. Define this as Na,i, ,j =
i{x E S1 : Za,,i ,j(x) = z}J. Putting it together, we
get:

(h;)T Prrhj =

L

L L

l=l zEZa1,i,j

N a,,t,J·fa,,l,J·(z)
·

·

·

(6)

It remains only to compute Na,,i,j. To understand this
task, consider the simpler one of counting the number
of states in S1. The states in S1 are the ones where
we used the l'th conditional for selecting our action.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

These are states that are consistent with t1, and which
are not consistent with t1, ... , tt-l· Each tm is an as­
signment of values to some set of variables; x can be in­
consistent with tm by being inconsistent with any one
of these values. Thus, inconsistency with the previous
conditionals can be expressed logically as the conjunc­
tion of a set of disjunctions, i.e., a CNF formula. It
is fairly straightforward to show, based on this obser­
vation, that computing Na1,i,j is #P-complete (and
therefore NP-hard).
Fortunately, this problem will inherit some of its
structure from the DBN describing the transition
model, and we can use the tools of Bayes net infer­
ence to make this computation more tractable. In ef­
fect, we can view each statement - that x does not
agree with tm for i = 1, . . . ,l - 1, that x does agree
with t1, and that x agrees with z
as a constraint
on x. Our goal is to count the number of satisfying
instances to this constraint satisfaction problem. Each
of these constraints is local, and depends only on a few
variables. The constraint on tm depends only on the
variables in Ta�, whereas the constraint on z depends
only on the variables in Za1,i,j· We can compute Na1,i,j
in time which is exponential in the induced width of
this constraint graph. In our DBN example, we will
have one cluster of size 3,arising from the constraint
for Za1 ,i,j which involves a pair of variables for raz (Xj)
and one variable for Xi, and a chain structure for the
constraints on each of the previous action tests. The
maximal clique size in a clique tree for this graph is
also 3,so that the induced width of resulting constraint
graph is at most 2. In general, if each action affects a
single variable and the domain of each basis function
is also a single variable, then the constraint graph has
exactly the same structure as the original DBN (which
has the same structure for all actions in this case).
This analysis shows that we can compute the matrix
AT P1rA· We have already shown how to compute the
other parts of Eq. (4), which do not vary with 1r. Our
result in Theorem 4.1 now applies, so we can compute
the value function that is the least-squares approxi­
mation to Eq. (4). This value function is factored,
allowing us to compute its one-step lookahead policy,
thereby closing the loop. Thus, we have shown that
we can do policy iteration over these factored value
functions and the policies they induce.
-

8

Error Bounds

So far, we have shown how to compute value functions
that minimize the (weighted) mean squared Bellman
error and then how to use these value functions to
select policies in policy iteration. To compute error
bounds using standard methods, we need to compute
the max-norm error in our value function, which we

333

then can use to bound the distance from our policy
to the optimal policy. When we have reached policy
7r1, which is the greedy policy for v1r, we compute the
maximum Bellman residual error as:
BellmanErr(V7r)

=

max
- V7r(x)]
a max[Qa(x)
x

where Qa is the one-step greedy Q-function for v1r as
in Section 7. v1r and each Qa are sums of restricted
domain functions. Hence each inner maximization is
over a linear combination of functions, each of which
is restricted to some small subset of variables. This
type of optimization problem is a cost network [8],
and can be solved using standard variable elimination
algorithms. The computational cost, as for other re­
lated structures, is exponential in the induced width
of the graph induced by the hyper-edges consisting of
the function domains.
For BellmanErr(V7r) :S t, we get [11]:
'
·
I I V1r - v7r lloo :::;

1

2t

_,

-

Thus, the true value of following 1r1 is bounded by a
function of the maximum Bellman error of v1r.
The above computation gives us a method of com­
puting the worst-case policy loss for any policy we
produce through policy iteration. In general, policy
iteration with approximate value functions can pro­
duce a sequence of policies of increasing quality. How­
ever, approximate policy iteration differs from the ex­
act case in that it can get trapped, repeatedly oscillat­
ing through a family of policies without ever finding
the globally optimal policy. The loss of the worst pol­
icy in this family can be bounded as a function of the
worst-case error in the corresonding value functions.
[2]. Thus, it is also useful to compute the maximum
Bellman error in our policy evaluation phase:
BellmanErr1r(V)

=

max[V(x) - ('y(P1r V)(x) + R(x))].
X

This can be computed using a combination of the cost­
network method and the policy evaluation method de­
scribed in Section 7. We omit the details for lack of
space.
The maximum Bellman error durin g the policy eval­
uation phase can be used to catch potentially mislead­
ing value functions and help correct them. In addi­
tion to yielding the maximum Bellman error, the cost
network computation tells us the state at which the
Bellman error is maximized. If the Bellman error is
large, we may wish to change our basis functions, e.g.,
by adding a basis function that can capture some im­
portant correlation. As an alternative, if we are using
factored projection weights, we might simply adjust
the weights to give the offending state greater impor­
tance in the least-squares approximation. Thus, we

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

334

could gradually adjust our weights with the aim of
minimizing the max-norm error in our policy evalua­
tion phase. These methods provide a means of moni­
toring the quality of the value function approximation
during policy iteration, a guide for adjusting the ap­
proximation, if necessary, and a means of evaluating
any final policy that is selected in comparison to the
optimal policy.
9

Discussion and future work

In this paper, we have provided a new policy iteration
algorithm for factored MDPs, using a factored linear
approximation to the value function. A key compo­
nent of our algorithm is a closed-form value determi­
nation method using weighted least squares with ar­
bitrary weights, rather than the stationary distribu­
tion weights This method is justified by a theorem
showing that the fixed point solution to the approxi­
mate dynamic programming equation exists for almost
any discount factor. The second key component of
our algorithm is the observation that the basic opera­
tions can be done effectively in closed form for factored
value functions, despite the fact that they are functions
over an exponentially large space. This observation
also permits the efficient computation of error bounds
which, if desired, can be used to adjust the projec­
tion weights and evaluate the quality of the resulting
policy.
An important theme that recurs throughout our
work is the systematic way in which the algorithm
exploits the structure of the model. The structure
is utilized in many ways: in the operations used for
basic value determination, in the compact representa­
tion of our decision-list policies, in the counting argu­
ment that allowed us to perform value determination
for these decision-list policies, and in the computation
of the Bellman error. In all of these cases, we saw
the same structural features playing the key role: the
clusters defined by the domains of the basis functions,
their back-projections (for the dynamic programming
step) and their forward projections (for the effects of
actions). The complexity of our algorithm is deter­
mined by the size of these clusters, and by the extent
to which they interact with each other: the joint size
of overlapping clusters, and the induced width of the
graph defined by these clusters. This is a very natu­
ral structural property that incorporates properties of
the transition dynamics as well as of our chosen basis
functions.
This paper opens up many interesting avenues for
future work. In one direction, it is clear that we can
extend our idea of doing closed-form computations to
other MDP solution algorithms, such as linear pro­
gramming. In a very different direction, we believe

that we can extend this approach to exploit various
other types of structure in the model, including struc­
tured action spaces, where at each stage several actions
are taken in parallel, and the context-sensitivity uti­
lized by [3, 6]. As a more ambitious goal, we would
also like to extend it to deal with the much harder
problem of planning in Partially Observable MDPs.
Acknowledgments

We thank Xavier Boyen for an insightful discussion
that led to Theorem 4.1 and Carlos Guestrin, Uri
Lerner and Simon Tong for useful discussions. This
work was supported by ARO grant DAAH04-96-l0341 under the MURI program "Integrated Approach
to Intelligent Systems" , by the Terman Foundation
and by the Sloan Foundation.
References

(1] Leemon Baird. Residual algorithms: Reinforcement
learning with function approximation. In Proceedings
of the Twelfth International Conference on Machine
Learning, pages 30-37, Tahoe City, CA, Proceedings

of the Twelfth International Conference on Machine
Learning 1995. Morgan Kaufmann.
(2] D. Bertsekas and J. Tsitsiklis.
Neuro-Dynamic
Programming.
Athena Scientific, Belmont, Massachusetts, 1996.
(3] C. Boutilier, R. Dearden, and M. Goldszmidt. Ex­
ploiting structure in policy construction. In Proc.
IJCAI-95. Morgan Kaufmann, 1995.
(4] C. Boutilier and M. Goldszmidt. The frame problem
and bayesian network action representations. In Proc.
11th Biennial Canadian Confer ence on Artificial In­
telligence,

1996.

(5] S. Bradtke and A. Barto. Linear least-squares al­
gorithms for temporal difference learning. Machine
Learning, 2(1):33-58, January 1996.
(6] T. Dean and R. Givan. Model minimization in Markov
decision processes. In Proc. AAAI-97. MIT Press,
1997.
(7] T. Dean and K. Kanazawa. A model for reasoning
about persistence and causation. Computational In­
telligence, 5(3):142-150, 1989.
(8] R. Dechter. Bucket elimination: A unifying framework
for reasoning. Artificial Intelligence, 113(1-2):41-85,
1999.
(9] D. Koller and R. Parr. Computing factored value func­
tions for policies in structured MDPs. In Proc. IJCAI99. Morgan Kaufmann, 1999.
(10] B. Van Roy. Learning and Value Function Approxi­
mation in Complex Decision Problems. PhD thesis,
Massachusetts Institute of Technology, 1998.
[11] R. Williams and L. Baird. Tight performance bounds
on greedy policies based on imperfect value functions.
Technical Report NU CCS-93-14 Northeastern Uni­
versity, 1993.
-

,

