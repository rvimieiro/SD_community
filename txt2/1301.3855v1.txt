ticularly useful for learning complex networks
with many hidden variables. In such cases, re­
peated likelihood computations are required for
E M or other parameter optimization techniques.
Since these computations are repeated with re­
spect to the same evidence set, our methods can
provide signifc
i ant speedup to the learning pro­
cedure. We demonstrate the algorithm on genetic
linkage problems where the use of value abstrac­
tion sometimes differentiates between a feasible
and non-feasible solution.

1

Introduction

Inference in probabilistic models plays a signifc
i ant role in
many applications. In this paper we focus on an applica­
tion in genetics: linkage analysis. Linkage analysis is a
crucial tool for locating the genes responsible for complex
traits (e.g., genetically transmitted diseases or susceptibil­
ity to diseases) . This analysis uses statistical tools to locate
genes and identify the biological function of proteins they
encode.
Linkage analysis is based on a clear probabilistic model
of genetic events. Mapping of disease genes is done by
performing parameter optimization to fn
i d the genetic map
location that maximizes the likelihood of the evidence (i.e.,
maximum likelihood estimation). This probabilistic infer­
ence is closely related to Bayesian network inference.
Our starting point is the V ITE S SE algorithm [11],a fairly
recent algorithm for linkage analysis that implements some

interesting heuristics for speeding up computations. These
heuristics achieve impressive speedups that allow to an­
alyze linkage problems that could not be dealt with us­
ing the prior state of the art procedures. In the language
of Bayesian networks these heuristics can be understood
as fn
i ding Value abstractions (reminiscent of the abstrac­
tions studied by [1 5]). These abstractions are found in an
evidence-specific manner to save computations for a spe­
cific training example.
In the remainder of this paper we review genetic linkage
analysis problems. Then we develop a method to fn
i d value
abstractions that generalizes the ideas of [11] in a manner
that is independent of the inference procedure used. We
then extend these ideas in combination with clique-tree in­
ference procedures. Finally, we describe experimental re­
sults that examine the effectiveness of these ideas.

2

Genetic Linkage Analysis

We now briefly introduce the relevant genetic notions that
are needed for the discussion below. We refer the reader to
[12] for a comprehensive introduction to linkage analysis.
The human genetic material consists of 22 pairs of auto­
somal chromosomes, and a pair of the sex chromosomes.
The situation with the later pair is slightly different,and we
will restrict the discussion here to the autosomal case, al­
though all the techniques we discuss apply to this case with
minor modifications. In each pair of chromosomes, one
chromosome is the paternal chromosome, inherited from
the father, and the other is the maternalchromosome, in­
herited from the mother. We distinguish particular loci in
each chromosome pair. Loci that are biologically expressed
are called genes. At each locus, a chromosome encodes a
particular sequence of D NA nucleotides. The variations
in these sequences are the source of the variations we see
among species members. The possible variants that might
appear at a particular locus are called alleles. In general,
the maternal copy and paternal copy of the same locus can
be different.
The aim of linkage analysis to construct genetic mapsof
known loci, and to position newly discovered loci with re­
spect to such maps. Genetic maps describe the relative
positions of loci of interest (which can be genes, or ge-

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

A

193

B

No crossover between A and B

Crossover between A and B

: g� * g;;
eta

:

-------

!SSi

Figure 1: Illustration of recombination during meiosis.

netic markers) in terms of their genetic distance. This dis­
tance measures the probability of crossoversbetween pairs
of loci during meiosis, the process of cell division leading
to creation of gemetes (either sperms or egg cells). During
the formation of gemetes, the genetic material undergoes
recombination, asshown in a schematic form in Figure 1.
The genetic distance between two loci is measured in
terms of the recombination fraction between two loci,
which is just the probability of recombination of the two
loci. The smaller this fraction is, the closer the two loci
are. A recombination fraction close to 0.5 indicates that the
two loci are sufficiently far so that their inheritance appears
independent.
Estimation of these fractions is complicated by the fact
that we do not observe alleles on chromosomes. Instead
we can observe phenotype, which might be traits, such as
blood type, eye color, or onset of a disease, or they might
oe the result of genetic typing. Genetic typing provides
the alleles present in each locus, but does not provide an
alignment with the maternal/paternal chromosome. Thus,
when genetic typing shows an individual has alleles aand
A in one locus, and alleles b and B in another locus, we
do not know if a and b where in inherited from the same
parent. In such a situation there are 4 four possible con­
figurations (ab/AB, aB/Ab, AbfaB, ABfab). W hen we
consider multiple loci, the number of possible configura­
tions grows exponentially.

2.1

Probabilistic Networks Models of Pedigrees

We start by showing how the underlying model of linkage
analysis problems can be represented by probabilistic net­
works, and then discuss standard approached for comput­
ing likelihoods in pedigrees. We note that the representa­
tion of pedigrees in terms of graphical models, have been
discussed in [6,8].
A pedigree defines a joint distribution over the genotype
and phenotypeof the individuals. We denote the genotype
and phenotype of individual i as G[i]and P[i],respectively.
The semantics of a pedigree are: given the genotype of i's
parents, G[i] is independent from G[j] for any ancestor j
of i; and given the genotype G[i], the phenotype P[i] is
independent of all other variables in the pedigree. We can
represent these assumptions on the distribution of G[i]and
P[i] by a network where the parents of G[i] are the G[j]
and G[k] where j and k are i's parents, and the parent of

P[i] is G[i]. Not surprisingly,this network has essentially
the same topology as the original pedigree.
The local probability models in the network have one of
the following forms:
•

•

General population genotype probabilities: Pr ( G (i] ),
when i is a founder.

Transmission models: Pr(G(i] I G(j], G[k]) where j
and kare i's parents in the pedigree_!

•

Penetrance models: Pr(P[i] I G[i]).

This discussion shows that there is a simple transforma­
tion from pedigrees to probabilistic networks. This simple
transformation obscures many of the details of the pedigree
model within the transition and penetrance models. Both of
these local probability models are quite complex. We gain
more insight into the "structure" of the joint distribution if
we model the pedigree at a more detailed level. This can
be done in various ways; e.g., [6, 8]. We find it most con­
venient to use a representation that is motivated by Lander
and Green's [ 9] representation of pedigrees. For this repre­
sentation we introduce several types of random variables:
Genetic Loci. We denote by A, B, C,... the lociof in­
terest in the genetic analysis. For example, these can be
marker loci and disease loci. For each individual i and lo­
cus A, we defn
i e random variables A[i,p], A[i, m] whose
values are the specific value of the locus Ain individual i's
parentaland maternalhaplotypes (chromosomes),respec­
tively. That is, A[i,p] was inherited from i's father, and
A[i, m]was inherited from i's mother.
Phenotypes. We denote by F, G,... the phenotypes that
are involved in the analysis. These might include disease
manifestations, genetic typing, or other observed pheno­
type such as blood types. For each individual iand pheno­
type F , we define a random variable F [i] that denote the
value of the phenotype for the individual i.
Selector variables. Similar to Lander and Green [ 9],we
use auxiliary variables that denote the inheritance pattern
in the pedigree. We denote by SA[i,p] and SA[i, m] the
selection made by the meiosis that resulted in i's genetic
makeup. Formally,if j and kdenote i's father and mother,
1 We

make the standard assumption that if individual i is not a

founder, then both of her parents are in the pedigree.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

194

Figure 2: A fragment of a probabilistic network represen­
tation of the transmission model,and the penetrance model
in a 3-loci analysis.
respectively,then

A[i ' p]
and similarly

A[k,m].

=

{ A[A[J,m]
�,p]

if
if

SA[i,p]
SA[i,p]

=

0

=

1

Figure 3: Schematic of a network corresponding to three­
loci pedigree. The dark nodes are loci variables in the
model (e.g., A[i,p]), the dark gray nodes are phenotype
variables, and the light gray nodes are selector variables
(e.g., SA[i,p]). Each tree-like "slice" corresponds to one
locus,and represents the inheritance model for that locus.

A[i,m] depends on SA[i,m], A[k,p], and

Using this finer grain representation of the genotype and
phenotype we can capture more of the independencies
among the variables. For example, A[i,p] and A[i,m] are
independent given the genotype of i's parents. Note that,
they are dependent given evidence on i's children, or on
phenotype that depends on both. Another example,occurs
when we know that loci Aand Bare unlinked (say they are
on different chromosomes), then A[i,p] is independent of
B[i,p]given the genotype of i's father.
Figure 2 shows a fragment of the network that describes
parents-child interaction in a simple 3-loci analysis. The
dashed boxes contain all of the variables that describe a
single individual's genotypes or phenotype. In this model
we assume that loci are mapped in the order A, B, and C.
This assumption is refe
l cted in the lack of an edge from
SA[i,p] to Sc[i,p], which implies that the two are inde­
pendent given the value of Sn[i,p]. Figure 2 also shows
the penetrance model for this simple 3-loci analysis. In this
model we assume that each phenotype variable depends on
the genotype at single locus. Again,this is reflected by the
fact that each phenotype has edges only from the two hap­
lotypes of a single loci.

2.1.1 Likelihood Computation in Pedigrees
There are two main approaches to likelihood computation
on pedigrees: Elston- Stewart [3, 5] and Lander- Green [ 9].
The representation of pedigrees as probabilistic networks
allows us to give a unified perspective of both. Broadly
speaking,both are variants of variable elimination methods
that depend on different strategies for finding elimination
ordering,or equivalently,cluster-tree separators.
Figure 3 shows an example of a pedigree. Elston­
Stewart's algorithm and later extensions essentially tra­
verse this network along the structure of the family tree.

At each cluster they aggregate variables that correspond to
an individual across all slices. On the other-hand, Lander­
Green's algorithm traverses this network from one slice to
another. At each step they aggregate all the separator vari­
ables at one slice. In this sense, Lander- Green's algorithms
treats a pedigree as a factorial HMM.
This discussion makes the (known) properties and restric­
tions of each procedure visible. W hen the pedigree has
loops, the genotypes of individuals are no longer neces­
sarily separators. Thus, one has to resort to approaches for
breaking loops. On the other hand, the Lander- Green pro­
cedure is not sensitive to loops in the pedigree. However,
their procedure cannot applied for pedigree's with many
selector variables in each slice and thus, their algorithm is
limited to small pedigrees.

2.2

Genetic Mapping

The main task for linkage analysis is identifying the map
location of disease genes from pedigree data. The standard
approach for performing this analysis is to use non-linear
optimization procedures that attempt to maximize the like­
lihood function. Such procedures evaluate the likelihood
in several points that are close to each other and estimate
the derivative by examining the differences in likelihood
between these points. This approach requires several eval­
uations of the likelihood.
It is important to note that during this optimization,there
are many repeated likelihood computations with respect
to the same evidence. Moreover, the only parameters
that change are the recombination fractions. That is, the
only variables whose conditional probability distribution
changes are the selector variables.
These repeated computation have been optimized by vari­
ous approaches. In particular,current linkage analysis soft-

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

ware perform some amount of genotype exclusion [13].
These exclusions use several rules of deduction to deter­
mine which genotypes are possible for individuals given
their phenotype, or the possible genotypes of their direct
relatives.
In addition,several researchers made the observation that
maintaining the distinction between some of a these values
often does not change the probability of the observations.
This fact has been exploited in F A S T L I N K to combine all
marker alleles that do not appear in any typed individual
in the pedigree [13]. A more powerful use of this idea has
been proposed by O'Connell and Weeks [11] and imple­
mented in the V I T E S S E program,where there is localized
allele grouping for each individual in the pedigree. The cur­
rent V I T E S S E algorithm applies only to Ioopless pedigree
within the framework of bottom-up Elston- Stewart style
variable elimination.
3

Safe Value Abstractions

In the next sections we develop theory and algorithms that
exploit value abstraction in contexts similar to the genetics
linkage analysis problems we describe above.
Let X be a variable with a finite domain Val(X) and
a probability distribution function P(X = x). An ab­
straction of the domain of X is a collection A of subsets
of Val(X) that form a non-trivial partition of Val(X):
no set in A is empty, every two sets in A are disjoint,
and the union of all sets in A equals Val(X). For every
v E Val(X), let va stand for the set in A that contains
v. We call va the abstract valuecorresponding to v. Each
abstraction defines a partition function a : Val(X) -+ C
which maps a value vto its abstract value vavia va = a( v) .
An abstraction of X, denoted by xa, is a variable with a
domain Val(Xa) = A which is an abstraction of Val(X),
and a probability distribution function pagiven by

P(X

=

v) ,

{vE Val(X) lva=u(v)}
or in a shorter notation by,

The set of abstractions for Val(X) forms a natural par­
tial order (or more precisely, a lattice) as follows. An
abstraction A1 is finer than abstraction A2 if every set in
A1 is a subset of a set in A2, in which case we also say
A2 is coarser than A1. An abstraction A1 is strictly finer
(coarser) than abstraction A2 if A1 is finer (coarser) than
A2and A1 ,P A2• The maximal abstractionconsists of one
set and the minimal abstractionconsists of singletons.
A refinement of two abstractions A1 and A2 is an ab­
straction Asuch that Ais finer than A1 and finer than A2.
A tight refinementof two abstractions A1 and A2 is a re­
finement Asuch that every other refn
i ement A' of A1 and
A2 is finer than A. In other words, suppose that a1 and
a2 are the two partition functions defined by abstractions

195

A1 and A2, respectively. Then, the partition function of
2 i ed
1
their tight refinement A is given by a = a 1\ a defn
1
1
such that a(v) = a(v' ) if and only if a (v) = a (v' ) and
a2 (v) = a2 (v' ) . Intuitively,the refinement of two partition
functions defn
i es a partition function that preserves the dis­
tinctions made by both partition functions and introduces
no new distinctions. W hen two abstractions are not related
through refinement,they are said to be incomparable.
Evidencewith respect to a variable Xis an assertion ethat
the value observed for X is among a subset 0 C Val(X)
of the possible values. W hen 0 is a singleton, Xis said to
have been observed.
An abstraction a of X is safe with respect to e if P(e I
X x) = P(e I X = x') for all x, x' such that a(x) =
a(x'). That is, the distinctions blurred by the abstraction
a do not effect the probability of the evidence. We can
always find a safe abstraction, since the trivialabstraction
that consists of singletons is always safe. Moreover, it is
clear that there exist a maximally safeabstraction which is
the coarsest safe abstraction.
As a simple example,consider a game where a player can
bet on a dice outcome and wins if the outcome matches
his bets. To formalize, suppose we have three variables
Bet that can take the values odd and even, Dice that can
take the values 1, . . . , 6, and Win that can be either yesor
no. Suppose also that we observe that the player won,that
is Win = yes. Clearly, the likelihood P(Win = yesiDice)
does not depend on the distinction between all possible out­
comes of the dice. Since the player can only bet on even or
odd outcome, the abstraction of values {1, 3, 5}, {2, 4, 6}
is a safe one. This abstraction is clearly the maximally safe
abstraction of Dice. However,there are many other safe ab­
stractions. Note that in this example,if the dice is fair,then
P(Win,Dice = x) is the same for all values of x in the
same partition. However, the abstraction is still safe even
if the dice is not fair. The point is that we can compute
P(Dice E {1, 3, 5})without worrying about the rest of the
domain (e.g.,probability of various bets,etc.).
This simple example suggests that it suffices to consider
an abstraction of the dice when we compute the proba­
bility of winning in the betting game. This can lead to
saving in the number of operations we perform in our
calculations. Such computational savings can be much
more drastic when one is presented with many intercon­
nected variables as is the case with Bayesian networks. Let
X = {X1, ... , Xn} be a set of variables each associated
with a finite domain Val(Xi ) · Also, let B,with a directed
acyclic graph G, stand for a Bayesian network over Xand
let Pa(Xi )be the parents of each Xi in B. An abstraction
Baof Bis a Bayesian network with the same set of vertices
and edges as in B,and where each variable Xi is replaced
with an abstraction Xf.
We now want to determine the conditional probability
distributions in Ba. We start by defining the probability
of an abstraction given the "un-abstracted"parents:
=

P(Xf

=

xf I pa(Xi )

= u

)

=

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

196

ALGORITHM ValueAbstract(B,e)
Input: A Bayesian network B and evidence
Output: A safe abstraction

e

sawrt e

Discard:

For every X;in S,remove from
Val(X;)all values that are ir.compatible
withe (e.g.,using arc-consistency algorithm)
Comment: Nodes in eremain with one value
Abstract:

Set I:; := { Val(X;)},fori = 1, ... , n
Iterate over X; in reverse topological order:
Suppose that Pa(X;) = {X1,..., Xk }.
1. Set a i = 1\ L:;
Comment: This defines Xf.
2. Find partition functions ai , . . . , a1
of xl'... 'xk such that

P(Xia I x1, ... , xk ) = P(Xf I x� , ... , xU

whenever

a1(xi ) = a1(x� , ) , ... , a1(xk ) = a1( x U.

3. for each Xi E Pa(Xi )

�j

:=

�j

U

{ a}}

Construct Tables:

Iterate over Xi in reverse topological order:
Abstract the table P(Xf I pa(X;)a)
according P(X; I pa(X;)).

Figure 4: Computing Value Abstraction

where x 't is an abstract value of x;. We say that an abstrac­
tion of X;'s parents is cautiousif P(Xf = x't I pa(Xi) =
u) = P(Xf = x't I pa(X;) = u') for all values u of
Pa(X;)that are mapped to the same partition. In this case,
ie
we defn

P(Xf = x't I pa(X;)a = ua) =
P(Xf = va I pa(X;) = u)for u E ua.
Note that the notion of an abstract value of a variable is
naturally extended to a set of variables via the Cartesian
product of the domains of the individual variables.
An abstracted Bayesian network Ba is a a (safe and cau­
tious) abstractionof a Bayesian networkS over Xwrt ev­
idence e if P(eiS) = P(eiSa). It is also maximal if for

'

any other Bayesian network sa with this property either
'
Val(Xj )is fni er than Val(Xf)or the two sets are incom­
parable for all variables X;.

4

Finding Safe & Cautious Abstractions

We now describe a simple iterative algorithm, ValueAb­
stract,which finds a safe abstraction of a Bayesian network
S with respect to evidence e. The algorithm consists of
three phases:
Discard The algorithm starts by examining the variables
in the network, and for each Xi, discarding all values that
are incompatible with the evidence e. This is done via any
arc-consistency algorithm as described in the C S P litera­
ture.
AbstractIn this phase the algorithm traverses the net­
work from the leafs upwards and computes cautious ab­
stractions for the parents of each abstracted variable. Since
a variable Xi can be a parent of several variables,we need
to collect the abstractions that are cautious with respect to
each of these children. Thus, during this phase, the algo­
rithm maintains a set of abstractions, L:i, that contains the
abstractions required for Xi by Xi's children. When the
i ement of
procedure processes X;,it finds the minimal refn
all these abstractions of Xi. We denote by 1\ L:i the tightest
i ement of all the abstractions in I:;.
refn
Construct Tables In the last phase the algorithm con­
structs the conditional probabilities in the abstracted net­
work.
The full algorithm is given in Figure 4.
Theorem 4.1: The network sa returned by Value Abstract
is a safe, cautious abstraction ofS wrt e.

Ignoring for the moment the cost of the Discard phase,
we see that each iteration (either of Abstract, or Construct
Tables phase) examines a single family. The cost of such an
iteration can be exponential in the number of parents in the
family. Thus,the running time of ValueAbstractis linearin
the number of variables in the network, but exponential in
the maximal indegree of the network. We stress, however,
that for networks in which conditional probabilities are rep­
resented by tables, the running time is linear in the size of
the network description (since the description of the condi­
tional probability tables are also exponential in the number
of parents).
This implies that the running time of this algorithm is not
sensitive to the topology of the network, and the complex­
ity of inference with it. VI TE S SE [11] (see Section 2) is
a specialized version of ValueAbstractthat yields impres­
sive speedup in likelihood calculations in a genetic analy­
sis domain. Thus,this simple algorithm can often make the
difference between feasible and infeasible calculations.

5

Message-Specific Abstraction

The ValueAbstractalgorithm has a desirable property: it is
independent of the particulars of the inference procedure
we use for computing likelihoods. Thus, it can be applied
as a preprocessing step before likelihood computation. The
simplicity and low complexity of the algorithm make it at­
tractive.
Nonetheless, there are some regularities that are missed
by ValueAbstract. First, the main processing is strictly
bottom-up: the abstraction is constructed from the leaves

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

of the network upward. However, we note that the first
phase (Discard using edge-consistency) can propagate im­
plication of evidence to lower nodes.
Second,we considered each variable separately from the
others. This limits us to abstractions that are the Carte­
sian products of the abstractions of variables in the par­
ents set. Thus, rather than holding abstractions per vari­
able,we may wish to hold abstractions for select groups of
variables. Suppose, for example, that we have two binary
variables X and Y each with values {0, 1}. Suppose the
evidence e is such that X and Y must have had the same
values but e does not determine which one of their values.
A maximal abstraction of Va l (X) x Val(Y)wrt eis the set
{eq = {(0, 0), ( 1, 1)}, neq = {(0, 1), ( 1, 0)}}. There exist
no maximal abstractions wrt e for Val(X) or for Va l ( Y )
which are strictly coarser than the original sets of values.
Finally,another opportunity for improvement rests on the
observation that rather than holding one abstraction per
variable, we can hold several abstractions per variable, so
that the likelihood computations of different parts of e can
be treated more efficiently. Suppose, for example, that we
have a Markov chain XI -+ X2 · · -+ Xnand that XIand
Xn are observed. That is, the evidence e is composed of
two parts eiand en. Then, for any Xi, 1 < i < n, we can
think of two natural abstractions,one is a maximal abstrac­
tion wrt ei, and the other is a maximal abstraction wrt en.
To compute the posterior P(Xi I e)one would need the tight
refinement of both abstractions. However,to pass messages
to its neighbors ala Pearl propagation style, we only need
to use one of the abstractions, which in general are coarser
than their tight refinement,and thus more efficient.
To deal with these issues,we need to develop abstractions
that depend on the details of the of the inference procedure
we use. We now address these issues within the context
of cluster-tree (aka clique tree) algorithms [7, 10, 14]. We
start with a presentation of one variant of tree-based algo­
rithms. The other variants have slightly different details,
but our algorithm can be easily adopted to deal with these.
·

5.1

197

It is easy to see that by simple rearrangement of products
the probability P (xi, ...,Xn B)
I
can be rewritten as:

If we have evidence,say on a set of variables E,we can up­
date the functions to reflect that. For example, if XI = a.
We can multiple 9l(XI) by a function oi(XI) such that
oi (xi ) = 1 if XI = a and oi(xi) = 0 otherwise. If
we update the nodes in this manner for all variables in E
according to the specific evidence e then

To see this,note that if XI,... ,Xnis consistent withe,then
its value is not changed by the modification to the g1• On
the other hand, if it is not consistent,then one of the 91 is 0,
and thus, the probability of the joint assignment is 0.
Let l and m be adjacent nodes in the tree. We define the
separator St,m = CtnCm. A separator defn
i es a partitions
of the clusters { CI,..., Ck } into two sets: the clusters on
the l-side of the separator, and the clusters on the m-side
of the separator. We denote these sets A:·m and A�m. In
addition, we define the sets of variables in both groups of
clusters x:·m = uA:·m ,and x�m = UA�m
The key property of separators is that they allow us to
factor the computation of probabilities into two separate
cases. Using the properties of the tree, it is easy to show
that
Proposition 5.1:

Pe
( B)
I
XI

•

where

Clique Tree Propagation Algorithm

•

Each node l in the tree is annotated with a cluster
Ct � {XI,...,Xn}·

Each variable xi is assigned to one cluster cl(X;)
such that Xi E Ct(X;) and Pa(Xi) � Ct(X;)·

If Xi E Ct and Xi E Cm,then Xi E Cj for any node
jon the path from l tom.

Let lbe a node. By definition,if Xiis assigned to l,then
Xi and Pa(Xi) are subsets of Ct. Thus, we can define a
function on values Ct E Val(Ct)

9t(ct)

=

II P( xi
i,l(X;)=l

I Pa(Xi))

(If l is a node that is not assigned any variable, then we
define 9t(ct) = 1.)

l

L
f;;,m(sl,m,e)ff'm(sl,m,e)
s1,� EVal(S,,�)

Assume thatB is a fixed network. A cluster-treeforB is a
tree over k nodes such that:
•

Xn

2:::

II

2:::

II

The key property of this factorization is that it is recur­
sive.
Proposition 5.2:

Consider a node l whose adjacent nodes
are mi,. ..,mk. Then

l, 1
fl m ( St,m,,e)

=

9t(ct) II f;;,;'i(sl,mi,e).
L
j>I
yEVal(C,-5,,�1)
(1)

Thus,to compute the message ff'm1 (st,m,,e)we need to

combine the messages from the other clusters adjacent to
l with conditional probabilities that are assigned to l and
then sum out all of the variables except these on St,m,.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

198

Using this recursive rule we can compute the likelihood
We choose a separator S1,m· According to
Proposition 5.1,all we need do is to compute the messages

P(e I B).

ff'mand f�mand then sum over the values of variables in
Sl,m· To compute these two messages,we apply the recur­

sion rule of Proposition 5.2 until we get to the leafs of the
rt ee. It is easy to see that this procedure is closely related
to variable elimination algorithms [3, 4, 16], except that
we eliminated several variables at each step. Moreover,the
structure of the clique tree determines the order of elimina­
tion.
In addition to likelihood computations,we can also com­
pute the posterior for every cluster: P(e,c1 / B). We do so
by combining the messages from all of l's adjacent nodes:

P(e,cl I B)= g1(c1) IT!�',"i(sl,mi'e)
j

Cluster tree algorithm compute such a posterior for each
cluster. This can be done efficiently by dynamic program­
ming: for each separator we only need to compute two mes­
sages. By appropriate use of dynamic programming all of
these messages can be computed in two passes over the tree
[7, 14].

5.2

Clique Tree Abstractions

Suppose we are given a cluster tree and an evidence e. Can
we abstract the values of cliques and separators?The ideas
from the previous section can be applied here in a straight­
forward fashion. Let S1,m be a separator. An abstraction

af'mof V l(SL ,m)is safefor ff'm(st,m,e)if
fll,m( S[,m,e) = Jll,m(sl,m• e)
m
m
for all S!,mand sLmsuch that af' (sl,m) =af' (sLm).
m
To construct a safe abstraction for the message ff' ,
a

1

we examine the recursive definition given Proposition 5.2.
m
This defn
i ition implies that ff' is a function of gl and

!�':'' for

'

nodes m adjacent to l. Thus, if we have safe
abstractions to all these terms, we only need to preserves
values of

L

yEC1-S1,m1

gl(c't). II f�',"i(s't,mi'e)
j>l

We construct these abstraction using a dynamic program­
ming procedure that is analogous to the clique-tree propa­
gation algorithm. The difference is that instead of propa­
gating probabilistic messages we are propagating abstrac­
tions. We define two operations on abstractions that are the
analogs of message multiplication and of marginalization.
We start by combination of abstractions.
Definition 5.3: Suppose that ax,ay are abstractions that
are safe with respect to f(X)and g(Y),respectively. (The
sets XandY can overlap.) Let Z = XUY.The combined
abstraction a = ax ay over Val(Z) is such that a(z) =
a(z') whenax(x) =ax(x')anday(y) =ay(y' ),where
·

x and y are the values of
similarly for x' andy'). I

X and Y specifei d

by

z

(and

It is easy to check that if axand ayare safe for /(X) and
g(Y),then ax· ayis safe for f(X)g(Y).
The second operation we need to examine is marginaliza­
tion. Let f(X,
Y) be a factor. We want to fn
i d an abstrac­
tion that is safe with respect to g(y) = Ex f(x,
y). To do
so, we need to identify valuesy for which we are going to
add the same values in the same order.
Definition 5.4 :

Suppose that

a

is an abstraction of
f(X,Y). We de­
fine the abstraction a -!-Y over Val(Y) so that
aty (y) =aty (y')if

Val (X,Y) that is safe with respect to

a(x,y) =a(x,y')for all x
I
Given these two operations,we can define the abstraction
algorithm for clique-trees. We start by computing an ab­
straction a1 of g1(c1) for each clique l. This can be done
either by combining abstractions for the conditional dis­
tributions of variables that are assigned to l or by fri st con­
structing g1()and then finding the coarsest safe abstraction.
The fri st option can introduce unnecessarydistinctions,but
can be more efficient.
Next,we define the analog of the recursive rule of Propo­
sition 5.2. Consider a node l whose adjacent nodes are
m1, ... , mk. Then,

l,m2 ....al,mk)
aIl,mt = (aI .am2
mk

I

+SI,mt

To construct the abstraction we perform dynamic pro­
gramming that determines the abstraction for each mes­
sage in terms of the abstractions for neighboring separators.
This dynamic programming is analogous to the propaga­
tion of messages in the probabilistic inference algorithm
on clique-trees.
Once we compute the abstraction of the messages we can
perform inference. The key saving of the abstraction is
that in computation we perform multiplication and addition
once for every abstract value. Thus, if sa is the abstracted
version of S, the saving in computation in construction of
the message on Sis I ValSal/1 ValSI.
We can show that the resulting algorithm preserves cor­
rectness of inferences.
Theorem 5.5: Inference on the abstracted clique tree com­
putes exactly all queries of the form P( C1 I e) for the evi­
dence e specified at the construction of the abstraction.

We note that the cost of the construction of the clique-tree
abstractions depends on the cost of the basic operations. In
the most naive instantiation, we represent abstractions as
tables. In this case,the cost of the operations is exactly the
same as the cost of probabilistic computation on the clique­
tree.

6

Abstractions and 0 values

Our algorithm can be easily extended to exploit an addi­
tional "structural" feature in conditional probability distri-

199

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

10000

1e+OB

:::

Ui
-.!'

0

fl/
..

1000

�
"
1l

�
1h
...

Ui

....

z

..,"

:::

100

10

10

100

1000

100000
10000

�
1h
.0

...

oo

•
d. ' o

10000

1e+06

�

..,

�

.0

1e+07

�
C"
(3

..

�

/0

... 0
...
....
10 .
100
•
•

/
'
/
.

/1/. 0
•§.�.

/.
,t'

1000
100

18+07

·•

•

"
·.t·
•

0

1e+4

1e+6

.

.

·•
..

/

.

"'

:::
"
"
C"
(3
Ui

•••

·

100000
10000
1000

0

It
1e+8

1e+06

0

·�i:··

a:

·""'��'/>

10
1

1e+10

.

1

10

�

•

·.

100
*

1000

10000

genotype size

(c)

(b)

(a)

-:""'

# individuals

Original Clique Size

Original Network Size

fl

8.��
�· .

100

�

io

.
�
"'· ·

Figure 5: Display of the saving achieved by Value Abstract on 280 linkage analysis networks. Each point corresponds to a
network. Graph (a) shows reduction in network size (x-axis is original network size andy-axis is reduced network size);
Graph (b) shows reduction in clique tree size (x-axis is original clique tree size andy-axis is the size of the clique tree of
the abstracted network);and graph (c) relates the ratio of reduction in clique tree size y-axis)
(
the a complexity estimate of
the linkage analysis problem (x-axis).

m

butions. If at some stage in the algorithm ff' (st,m) = 0,
then multiplications by this value will always result in 0.
We can record this fact in our abstractions by introducing
a special abstract value 0 that corresponds to all the values
of the variables that are given a value 0. Then, we modify
the definition of combination and marginalization to take
the special properties of 0 into account:
•
•

(ax· ay )(x) = 0 if either ax(x) = 0 or ay(y)
a -l..x (y) = 0 if a(x, y) = 0for all x.

=

0.

These modifc
i ations allow us to deal with evidence more
easily. Suppose that a variable X E Ct is assigned the
value x in the evidence. Then we combine at() with an
abstraction axsuch that all values x' E Val(X) - {x} are
assigned to the abstract value 0, and x is assigned to the
singleton set {x}. W hen we combine this abstraction with
the clique abstraction we ensure that all assignments to Ct
in which X f. x are assigned to 0.
We note that this simple modification of our procedure
essentially implements partial constraint satisfc
i ation prop­
agation to discover unattainable joint assignments to clus­
ters/separators.
7

Evaluation

We tested our methods on a collection of standard bench­
mark pedigrees that are reported in [1, 13]. These pedi­
grees come from 10 different studies and contain 90 dif­
ferent pedigrees of sizes varying from 5 to 200 individu­
als. From these we generated 280 different linkage analy­
sis problems by including different numbers of loci in the
analysis. These were translated into a Bayesian network of
the form described in Section 2.1. For each network we
also constructed evidence assignment based on the original
fn
i dings in the studies and used these in the analysis below.
In the fri st phase of our experiments we tested the Value­
Abstract procedure. This procedure implements the ideas
of V ITE S SE combined with constraint propagation to re­
move impossible values. Figure 5 (a) shows the reduction

10000

:::

Ui
-.!'
0

fl./
..

1000

�
"

z

..,"
1l
�

100

�
.

�

....
10

10

100

.

.

.

1000

10000

Original Network Size

Figure 6: Display of the saving achieved by abstracting val­
ues inside the clique tree on the network returned by Val­
ue Abstract. The x-axis is the size of the clique tree before
abstraction and they-axis is the size of the clique tree after
abstraction.
in the size of the networkachieved by Value Abstract. This
reduction is due to eliminating and combining values of
variables. The reduction in the network size can be ap­
proximated as 2.6 n°·68 (the line in Figure 5 (a)).
However,since the computation time depends linearly on
the size of the clique tree constructed from the network,
we also want to measure the reduction in this size. This is
shown in Figure 5 (b). As we can see, the ratio of reduc­
tion can vary significantly. We believe that this is due to
structural features of the pedigree. Figure 5 (c) shows that
the ratio of improvement in the clique tree size is roughly
proportional to the product of the number of individuals in
the pedigree and the number of genotype values for each
individual. This later quantity is a rough estimate of the
complexity of the problem.
In the next stage we applied the clique tree abstract proce­
dure described in Section 5. Here we measured the reduc­
tion in effective size of the clique tree due to the abstraction
of values in cliques and separators. Figure 6 compares the
sizes before and after we applied this procedure to the net­
work returned by Value Abstract. As we can see,we get ad·

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

200

ditional saving, especially for networks with large cliques.
The reduction is estimated as 5.2·n°·64 (the line in Figure 6.
As we can see,the savings can be drastic.

8

[8]

Concluding Remarks

In this paper we introduced an approach to exploit regular
structure in Bayesian networks to reduce computation time.
This approach exploits symmetry to merge values of vari­
ables or groups of variables at different stages of the com­
putation. Our motivation is from linkage analysis, where
this type of heuristics have been very successful [11]. We
are currently extending our implementation to deal with
larger networks and plan to incorporate our methods within
a linkage analysis software.
It is clear that this approach can be beneficial to other
forms of structured Bayesian networks. In particular, net­
works with C S I [2]. Value abstraction suggests a general
framework within which we can evaluate the utility of al­
gorithms that work with tree C P Ts. A structured represen­
tation of message (i.e., [17]) is essentially an abstraction. If
an algorithm is exact, then the representation it uses must
be a refn
i ement of the abstraction our algorithm constructs.
We plan to exploit this to design "optimal"structure repre­
sentations for message passing with C S I.
Acknowledgements

We thank Tal El- Hay for his help in implementing the
Clique tree construction algorithm and Ann Becker and Gal
Elidan for help with the benchmark pedigrees. This work
was supported by Isreal Science Foundation grant number
224/ 9 9- 1 and by the generosity of the Michael Sacher fund.
Nir Friedman was also supported by Harry & Abe Sherman
Senior Lectureship in Computer Science. Experiments re­
ported here were run on equipment funded by an I SF Basic
Equipment Grant.
References
[1] A. Becker, D. Geiger,and A. A. Schaffer. Automatic
selection of loop breakers for genetic linkage analy­
sis. Human Heredity, 48:4 9-60, 1 9 98.
[2] C. Boutilier, N. Friedman, M. Goldszmidt, and
D. Koller. Context-specific independence in Bayesian
networks. In UAI1 9 96.
[3] C. Cannings, E. A. Thompson, and M. H. Skolnick.
Probability functions on complex pedigrees. Ad­
vances in Applied Probability, 10:26-61,1 978.
[4] R. Dechter. Bucket elimination: A unifying frame­
work for probabilistic inference. In UAI1 9 96.
[ 5] R. C. Elston and J. Stewart. A general model for the
analysis of pedigree data. Human Heredity, 21: 523542, 1 971.
[6] C. Harbron and A. Thomas. Alternative graphical rep­
resentations of genotypes in a pedigree. IMA Jour­

nal of Mathematics Applied in Medicine and Biology,
11:217-228, 1 9 94.
[7] F. V. Jensen, S. L. Lauritzen, and K. G. Olesen.
Bayesian updating in causal probabilistic networks by

[ 9]

[10]

[11]

[12]
[13]

[14]
[1 5]

[16]

[17]

local computations. Computational Statistics Quar­
terly, 5 (4):26 9-282, 1 9 90.
A. Kong. Efficient methods for computing linkage
likelihoods of recessive diseases in inbred pedigrees.
Genet Epidem, 8:81-103, 1 9 91.
E. S. Lander and P. Green. Construction of multilocus
genetic maps in humans. Proc. National Academy of
Science, 84:2363-2367, 1 987.
S. L. Lauritzen and D. J. Spiegelhalter. Local compu­
tations with probabilities on graphical structures and
their application to expert systems. Journal of the
Royal Statistical Society, B 50 (2):1 57-224,1 988.
J. R. O' Connell and D.E. Weeks. The V I TE S SE algo­
rithm for rapid exact multilocus linkage analysis via
genotype set-recording and fuzzy inheritance. Nat.
Genet. , 11:402-408,1 9 9 5.
J. Ott. Analysis of Human Genetic Linkage. 1 9 91.
A.A. Schaffer. Faster linkage analysis computations
for pedigrees with loops or unused alleles. Human
Heredity, 46:226-23 5,1 9 96.
G. Shafer and P. Shenoy. Probability propagation.
Ann. Math. and Art. Int. , 2:327-3 52,1 9 90.
M. P. Wellman and C.- L. Liu. State-space abstraction
for anytime evaluation of probabilistic networks. In
UAI1 9 94.
N.L. Zhang and D. Poole. Exploiting causal indepen­
dence in bayesian network inference. Journal of A./.
Research, 5:301-328,1 9 96.
N.L. Zhang and D. Poole. On the role of context­
specific independence in probabilistic inference. In
IJCAI. 1 9 9 9.

