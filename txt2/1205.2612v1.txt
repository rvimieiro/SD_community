Bayesian networks are being widely used for probabilistic inference and causal modeling [Pearl, 2000,
Spirtes et al., 2001]. One major challenge is to learn
the structures of Bayesian networks from data. In
the Bayesian approach, we provide a prior probability distribution over the space of possible Bayesian
networks and then computer the posterior distributions P (G|D) of the network structure G given data D.
We can then compute the posterior probability of any
hypothesis of interest by averaging over all possible
networks. In many applications we are interested in
structural features. For example, in causal discovery,
we are interested in the causal relations among vari-

Recently, a dynamic programming (DP) algorithm
was developed that can compute the exact marginal
posterior probabilities of any subnetwork (e.g., an
edge) in O(n2n ) time [Koivisto and Sood, 2004] and
the exact posterior probabilities for all n(n − 1) potential edges in O(n2n ) total time [Koivisto, 2006],
assuming that the indegree, i.e., the number of parents of each node, is bounded by a constant. One
main drawback of the DP algorithm and the order
MCMC algorithm is that they both require a special form of the structure prior P (G). The resulting prior P (G) is non uniform, and does not respect Markov equivalence [Friedman and Koller, 2003,
Koivisto and Sood, 2004]. Therefore the computed
posterior probabilities could be biased. MCMC algorithms have been developed that try to fix this
structure prior problem [Eaton and Murphy, 2007,
Ellis and Wong, 2008].
Inspired
by
the
DP
algorithm
in
[Koivisto and Sood, 2004, Koivisto, 2006], we have
developed an algorithm for computing the exact
posterior probabilities of structural features that does
not require a special prior P (G) other than the stan-

UAI 2009

TIAN & HE

dard structure modularity (see Eq. (4)). Assuming
a bounded indegree, our algorithm can compute the
exact marginal posterior probabilities of any subnetwork in O(3n ) time, and the posterior probabilities
for all n(n − 1) potential edges in O(n3n ) total time.
The memory requirement of our algorithm is about
the same O(n2n ) as the DP algorithm. We have
demonstrated our algorithm on data sets with up to
20 variables. The main advantage of our algorithm is
that it can use very general structure prior P (G) that
can simply be left as uniform and can satisfy Markov
equivalence requirement. We acknowledge here that
our algorithm was inspired by and used many techniques in [Koivisto and Sood, 2004, Koivisto, 2006].
Their algorithm is based on summing over all possible
total orders (leading to the bias on prior P (G) that
graphs consistent with more orders are favored).
Our algorithm directly sums over all possible DAG
structures by exploiting sinks, nodes that have no
outgoing edges, and roots, nodes that have no parents,
and as a result the computations involved are more
complicated. We note that the dynamic programming
techniques have also been used to learn the optimal Bayesian networks in [Singh and Moore, 2005,
Silander and Myllymaki, 2006].
The rest of the paper is organized as follows. In Section 2 we briefly review the Bayesian approach to learn
Bayesian networks from data. In Section 3 we present
our algorithm for computing the posterior probability
of a single edge and in Section 4 we present our algorithm for computing the posterior probabilities of
all potential edges simultaneously. We empirically
demonstrate the capability of our algorithm in Section 5 and discuss its potential applications in Section 6.

539
network G as
P (D|G)P (G)
.
P (D)

P (G|D) =

(1)

We can then compute the posterior probability of any
hypothesis of interest by averaging over all possible
networks. In this paper, we are interested in computing the posteriors of structural features. Let f be
a structural feature represented by an indicator function such that f (G) is 1 if the feature is present in G
and 0 otherwise. We have
X
f (G)P (G|D).
(2)
P (f |D) =
G

Assuming global and local parameter independence,
and parameter modularity, P (D|G) can be decomposed into a product of local marginal likelihood often
called local scores as [Cooper and Herskovits, 1992,
Heckerman et al., 1995]
P (D|G) =

n
Y

P (xi |xP ai : D) ≡

n
Y

scorei (P ai : D),

i=1

i=1

(3)
where, with appropriate parameter priors, scorei (P ai :
D) has a closed form solution. In this paper we will
assume that these local scores can be computed efficiently from data.
For the prior P (G) over all possible DAG
structures we will assume structure modularity:
[Friedman and Koller, 2003]
P (G) =

n
Y

Qi (P ai ).

(4)

i=1

In this paper we consider modular features:

2

Bayesian Learning of Bayesian
Networks

f (G) =

n
Y

fi (P ai ),

(5)

i=1

A Bayesian network is a DAG G that encodes a joint
probability distribution over a set X = {X1 , . . . , Xn }
of random variables with each node of the graph representing a variable in X. For convenience we will typically work on the index set V = {1, . . . , n} and represent a variable Xi by its index i. We use XP ai ⊆ X
to represent the set of parents of Xi in a DAG G and
use P ai ⊆ V to represent the corresponding index set.
Assume we are given a training data set D =
{x1 , x2 , . . . , xN }, where each xi is a particular instantiation over the set of variables X. We only consider
situations where the data are complete, that is, every variable in X is assigned a value. In the Bayesian
approach to learn Bayesian networks from the training data D, we compute the posterior probability of a

where fi (P ai ) is an indicator function with values either 0 or 1. For example, an edge j → i can be represented by setting fi (P ai ) = 1 if and only if j ∈ P ai
and setting fl (P al ) = 1 for all l 6= i. In this paper,
we are interested in computing the posterior P (f |D)
of the feature, which can be obtained by computing
the joint probability P (f, D) as
X
f (G)P (D|G)P (G)
(6)
P (f, D) =
G

=

n
XY

fi (P ai )Qi (P ai )scorei (P ai : D)

=

G i=1
n
XY

Bi (P ai ),

G i=1

(7)

540

TIAN & HE

where for all P ai ⊆ V − {i} we define
Bi (P ai ) ≡ fi (P ai )Qi (P ai )scorei (P ai : D).

(8)

It is clear from Eq. (6) that if we set all features fi (P ai )
to be constant 1 then we have P (f = 1, D) = P (D).
Therefore we can compute the posterior P (f |D) if we
know how to compute the joint P (f, D). In the next
section, we show how the summation in Eq. (7) can
be done by dynamic programming in time complexity
O(3n ).

3

G∈G + (S) i∈S

We have P (f, D) = RR(V ) since G + (V ) = G. Then
by the weighted inclusion-exclusion principle, Eq. (7)
becomes
P (f, D) = RR(V ) =

n
XY

P (f, D) = RR(V ),

(12)

where RR(S) can be computed recursively by
RR(S) =

|S|
X

(−1)k+1

X

RR(S − T )

Y

Aj (V − S)

j∈T

T ⊆S
|T |=k

k=1

(13)

X

(−1)k+1

X

X

n
Y

Bi (P ai )

k+1

(−1)

k=1
|V |
X

(−1)k+1

k=1

X Y

Bj (∅)

T ⊆V j∈T
|T |=k

X Y

G∈G + (S,T ) i∈S

(14)
where RF (S, ∅) = RR(S). Then by the weighted
inclusion-exclusion principle, RR(S) in Eq. (9) can be
computed as
|S|
X

RR(S) =

X

Y

=[
Bi (P ai )

G∈G + (V −T ) i∈V −T

Bj (∅)RR(V − T ),

RF (S, T ).

(15)

T ⊆S,|T |=k

X

i∈S−{j}

X

Bj (P aj )][

P aj ⊆V −S

Y

(10)

which says that P (f, D) can be computed in terms
of RR(S). Next we show that RR(S) for all S ⊆ V
can be computed recursively. We define the following
function for each i ∈ V and all S ⊆ V − {i}
X
Bi (P ai ),
(11)
Ai (S) ≡

Bi (P ai )]

G∈G + (S−{j}) i∈S−{j}

(see Figure 1(a))
= Aj (V − S)RR(S − {j}).

T ⊆V j∈T
|T |=k

P ai ⊆S

X

(−1)k+1

G∈G + (S,{j})

|V |

X

Proof: We will say a node j ∈ S is a source node
in G ∈ G + (S) if none of j’s parents are in S, that
is, P aj ∩ S = ∅. For T ⊆ S let G + (S, T ) denote
the set of DAGs in G + (S) such that all the variables
in T are source nodes (setting G + (S, ∅) = G + (S)).
It is clear that G + (S) = ∪j∈S G + (S, {j}), and that
∩j∈T G + (S, {j}) = G + (S, T ). The summation over the
DAGs in G + (S) in Eq. (9) can be computed by summing over the DAGs in G + (S, {j}) separately and correcting for the overlapped DAGs. Define
X
Y
Bi (P ai ), for any T ⊆ S,
RF (S, T ) ≡

RR(S) and RF (S, T ) can be computed recursively as
follows. For |T | = 1,
Y
X
Bi (P ai )
Bj (P aj )
RF (S, {j}) =

T ⊆V G∈G + (V −T ) i=1
|T |=k

k=1

with the base case RR(∅) = 1.

Bi (P ai )

|V |

=

Proposition 1

k=1

G∈G i=1

=

where in particular Ai (∅) = Bi (∅). We have the following results, which roughly correspond to the backward
computation in [Koivisto, 2006].

Computing Posteriors of Features

Every DAG must have a root node, that is, a node
with no parents. Let G denote the set of all DAGs
over V , and for any S ⊆ V let G + (S) be the set of
DAGs over V such that all variables in V − S are
root nodes (setting G + (V ) = G). Since every DAG
must have a root node we have G = ∪j∈V G + (V − {j}).
We can compute the summation over all the possible DAGs in Eq. (7) by summing over the DAGs in
G + (V − {j}) separately. However there are overlaps
between the set of graphs in G + (V − {j}), and in fact
∩j∈T G + (V − {j}) = G + (V − T ). We can correct for
those overlaps using the inclusion-exclusion principle.
Define the following function for all S ⊆ V
X Y
Bi (P ai ).
(9)
RR(S) ≡

=

UAI 2009

(16)

Similarly, for any T ⊆ S
Y
X
Y
Bi (P ai )
Bj (P aj )
RF (S, T ) =
i∈S−T

G∈G + (S,T ) j∈T

=

Y

[

X

j∈T P aj ⊆V −S

Bj (P aj )][

X

Bi (P ai )]

G∈G + (S−T ) i∈S−T

(see Figure 1(b))
Y
Aj (V − S) · RR(S − T ).
=
j∈T

Y

(17)

UAI 2009

TIAN & HE

V−S

Theorem 1 Given a fixed maximum indegree k,
P (f |D) can be computed in O(3n + kn2n ) time.

V−S

j

4

T

S−{j}

541

Computing Posterior Probabilities
for All Edges

S−T

(a)

(b)

Figure 1: Figures illustrating the proof of Proposition 1.
Combing Eqs. (15) and (17) we obtain Eq. (13).



Based on Proposition 1, provided that the functions Aj
have been computed, P (f, D) can be computed in the
manner of dynamic programming, starting from the
base case RR(∅) = 1, then RR({j}) = Aj (V − {j}),
and so on, until RR(V ).
Given the functions Bi , the functions Ai as defined
in Eq. (11) can be computed using the fast Möbius
transform algorithm in time O(n2n ) (for a fixed i)
[Kennes and Smets, 1990]. In the case of a fixed
indegree bound k, Bi (P ai ) is zero when P ai contains more than k elements. Then Ai (S) for all
S ⊆ V − {i} can be computed more efficiently using the truncated Möbius transform algorithm given in
[Koivisto and Sood, 2004] in time O(k2n ) (for a fixed
i).
The functions RR may be computed more efficiently
if we precompute the product of Aj . Define
Y
Aj (V − S) for T ⊆ S ⊆ V. (18)
AA(S, T ) ≡
j∈T

Then using Eq. (18) for AA(S, T − {j}) we have
AA(S, T ) = Aj (V − S)AA(S, T − {j})

for any j ∈ T.
(19)

If we want to compute the posterior probabilities of
all O(n2 ) potential edges, we can compute RR(V ) for
each edge separately and solve the problem in O(n2 3n )
total time. However, there is a large overlapping in
the computations for different edges. After computing
P (D) with constant features fi (P ai ) = 1 for all i ∈
V , the computation for an edge i → j only needs to
change the function fj and therefore Bj and Aj . We
can take advantage of this overlap and reduce the total
time for computing for all edges.
Inspired by the forward-backward algorithm in
[Koivisto, 2006], we developed an algorithm that can
compute all edge posterior probabilities in O(n3n ) total time. The computations of P (f, D) in Section 3 are
based exploiting root nodes and roughly correspond to
the backward computation in [Koivisto, 2006]. Next
we first describe a computation of P (f, D) by exploiting sink nodes (nodes that have no outgoing
edges) which roughly corresponds to the computation
in [Koivisto and Sood, 2004] called forward computation in [Koivisto, 2006]. Then we describe how to combine the two computations to reduce the total computation time.
4.1

Computing P (f, D) by exploiting sinks

For any S ⊆ V , let G(S) denote the set of all the
possible DAGs over S, and G(S, T ) denote the set of
all the possible DAGs over S such that all the variables
in T ⊆ S are sinks. We have G(V ) = G and G(S, ∅) =
G(S). For any S ⊆ V define
H(S) ≡

X Y

Bi (P ai ).

(21)

G∈G(S) i∈S

For a fixed S, AA(S, T ) for all T ⊆ S can be computed
in the manner of dynamic programming in O(2|S| )
time starting with AA(S, {j}) = Aj (V − S). We then
compute RR(S) using
RR(S) =

|S|
X

k+1

(−1)

k=1

X

RR(S − T )AA(S, T )

We have P (f, D) = H(V ) since G(V ) = G. As in
Section 3 we can show that H(S) can be computed
recursively. For any T ⊆ S ⊆ V , define
F (S, T ) ≡

in O(2|S| ) time. ThePfunctions
 RR(S) for all S ⊆ V
n
can be computed in k=0 nk 2k = 3n time.
In summary, we obtain the following results.

Y

Bi (P ai ),

(22)

G∈G(S,T ) i∈S

T ⊆S
|T |=k

(20)

X

where F (S, ∅) = H(S). We have the following results.
Proposition 2
P (f, D) = H(V ),

(23)

542

TIAN & HE

UAI 2009

and H(S) can be computed recursively by
H(S) =

|S|
X

S−{j}
X

k+1

(−1)

H(S − T )

(24)

j

with the base case H(∅) = 1.

|S|
X

X

(−1)k+1

k=1

j

(a)

Proof: Since every DAG has a sink we have G(S) =
∪j∈S G(S, {j}). It is clear that ∩j∈T G(S, {j}) =
G(S, T ). The summation over the DAGs in G(S) in
Eq. (21) can be computed by summing over the DAGs
in G(S, {j}) separately and correcting for the overlapped DAGs. By the weighted inclusion-exclusion
principle, H(S) in Eq. (21) can be computed as
H(S) =

S−T

Aj (S − T )

j∈T

T ⊆S
|T |=k

k=1

Y

F (S, T ).

(25)

T ⊆S,|T |=k

H(S) and F (S, T ) can be computed recursively as follows. For |T | = 1, we have
X
Y
Bi (P ai )
F (S, {j}) =

T−{j}
(b)

Figure 2: Figures illustrating the proof of Proposition 2.

Pn
for S ⊆ V can be computed in time k=1 nk k2k−1 =
n3n−1 . We could store all F (S, T ) and compute all
H(S) in time O(3n ). But the memory requirement
would become O(4n ) instead of O(n2n ).
We could compute the posterior of a feature using
P (f, D) = H(V ) but this is a factor of n slower than
computing RR(V ). Next we show how to reduce the
total time for computing the posterior probabilities of
all edges by combining the contributions of H(S) and
RR(S).

G∈G(S,{j}) i∈S

=[

X

X

Bj (P aj )][

Y

Bi (P ai )]

G∈G(S−{j}) i∈S−{j}

P aj ⊆S−{j}

(see Figure 2(a))
= Aj (S − {j})H(S − {j}).

(26)

Similarly, for any j ∈ T ⊆ S
X Y
Bi (P ai )
F (S, T ) =
G∈G(S,T ) i∈S

=[

X

Bj (P aj )][

P aj ⊆S−T

X

Y

Bi (P ai )]

G∈G(S−{j},T −{j}) i∈S−{j}

(see Figure 2(b))
= Aj (S − T )F (S − {j}, T − {j}).

(27)

Let T = {j1 , . . . , jk }. Repeatedly applying Eq. (27)
and using the fact (S − T ′ ) − (T − T ′ ) = S − T for any
T ′ ⊆ T , we obtain

4.2

Computing posteriors for all edges

Consider the summation over all the possible DAGs in
Eq. (7). Assume that we are interested in computing
the posterior probability of an edge i → v. We want
to extract the contribution of Bv from the rest of Bi .
The idea is as follows. For a fixed node v, we can
break a DAG into the set of ancestors U of v and the
set of nonancestors V − {v} − U .1 Roughly speaking,
conditioned on U , the summation over all DAGs can
be decomposed into the contributions from the summation over DAGs over U , which corresponds to the
computation of H(U ), and the contributions from the
summation over DAGs over V − {v} − U with the variables in U ∪ {v} as root nodes, which corresponds to
the computation of RR(V − {v} − U ).
Define, for any v ∈ V and U ⊆ V − {v} the following
function

F (S, T ) = Aj1 (S − T )Aj2 (S − T )F (S − {j1 , j2 }, T − {j1 , j2 }) Kv (U )
X
Y
= ...
Aj (U ).
≡
(−1)|T | RR(V − {v} − U − T )
= Aj1 (S − T ) · · · Ajk−1 (S − T )F (S − {j1 , . . . , jk−1 }, {jk })
j∈T
T ⊆V −{v}−U
Y
(29)
= H(S − T )
Aj (S − T ),
(28)
j∈T

where Eq. (26) is applied in the last step. Finally
combining Eqs. (28) and (25) leads to (24).

Based on Proposition 2, H(S) can be computed in
the manner of dynamic programming. Each H(S) is

P|S|
|S|−1
. All H(S)
computed in time k=1 |S|
k k = |S|2

We have the following results.
1

Or we can break a DAG into the set of nondescendants
U of v and the set of descendants V − {v} − U . It could
be shown that we can also use this way of breaking DAGs
to derive Proposition 3. But this is not exploited in the
paper.

UAI 2009

TIAN & HE
into Eq. (30)

Proposition 3
P (f, D) =

543

X

Av (U )H(U )Kv (U ).

(30)

P (f, D) =

U ⊆V −{v}

Note that in Eq. (30) the computations of H(U ) and
Kv (U ) do not rely on Bv and all the contribution from
Bv to P (f, D) is represented by Av . After we have
computed the functions Av , H and Kv , P (f, D) can
be computed using Eq. (30) in time O(2n ).

=

Y

Aj (U ).

(31)

j∈T

Then Kv (U ) can be computed as
X

X

(−1)|T | RR(V − {v} − U − T )η(U, T ),

T ⊆V −{v}−U

(32)

where η(U, T ) can be computed recursively as
η(U, T ) = Aj (U )η(U, T − {j})

=

Bv (P av )]H(U )Kv (U )

Based on Proposition 3, to compute the posterior
probabilities for all possible edges, we can use the following algorithm.

X

X

Bv (P av )

H(U )Kv (U )

P av ⊆U ⊆V −{v}

Bv (P av )Γv (P av ),

(34)

P av ⊆V −{v}

where for all P av ⊆ V − {v} we define
Γv (P av ) ≡

X

H(U )Kv (U ).

(35)

P av ⊆U ⊆V −{v}

For a fixed maximum indegree k, since we set Bv (P av )
to be zero for P av containing more than k variables we
need to compute the function Γv (P av ) only at sets P av
containing at most k elements, which can be computed
using the k-truncated downward Möbius transform algorithm described in [Koivisto, 2006] in O(k2n ) time
for all P av (for a fixed v). Then P (f, D) for an edge
u → v can be computed as
P (u → v, D) =

X

Bv (P av )Γv (P av ), (36)

u∈P av ⊆V −{v}
|P av |≤k

for any j ∈ T. (33)

For a fixed U , all η(U, T ) for T ⊆ V − {v} − U can
be computed in 2n−1−|U | time, and then Kv (U ) can be
computed in 2n−1−|U | time. For a fixed v allKv (U ) for
Pn−1
n−1−k
=
U ⊆ V −{v} can be computed in k=0 n−1
k 2
n−1
3
time.

X
P av ⊆V −{v}

To compute Kv (U ), we can precompute the product
of Aj . Define

Kv (U ) =

[

U ⊆V −{v} P av ⊆U

The proof of Proposition 3 is given in the Appendix.

η(U, T ) ≡

X

which takes O(nk ) time.
In summary, we propose the algorithm in Figure 3
to compute the posterior probabilities for all possible
edges. The main result of the paper is summarized in
the following theorem.
Theorem 2 For a fixed maximum indegree k, the posterior probabilities for all n(n − 1) possible edges can
be computed in O(n3n ) total time.

1. Precomputation. Set constant feature f (G) ≡ 1.
Compute functions Bi , Ai , RR, H, and Ki .

5

Experimental Results

2. For each edge u → v:
(a) For all S ⊆ V − {v}, recompute Av (S).
(b) Compute P (f, D) using Eq. (30). Then
P (f |D) = P (f, D)/RR(V ).
For a fixed maximum indegree k, Step 1 takes time
O(n3n ) as discussed before, and Step 2 takes time
O(n2 (k2n + 2n )). It takes O(n3n + kn2 2n ) total time
to compute the posterior probabilities for all possible
edges.
The computation time of Step 2 can be further reduced by a factor of n using the techniques described
in [Koivisto, 2006]. Plug in the definition of Av (U )

We have implemented the algorithm in Figure 3 in the
C++ language and run some experiments to demonstrate its capabilities. We compared our implementation with REBEL2 , a C++ language implementation
of the DP algorithm in [Koivisto, 2006].
We used BDe score for scorei (P ai : D) (with the hyperparameters αxi ;pai = 1/(|Dm(Xi )| · |Dm(P ai )|))
[Heckerman et al., 1995]. In the experiments our algorithm used a uniform structure prior P (G) =
1 and REBEL used a structure prior specified in
[Koivisto, 2006]. All the experiments were run under
Linux on an ordinary desktop PC with a 3.0GHz Intel
Pentium processor and 2.0GB of memory.

544

TIAN & HE

UAI 2009
5.1

Algorithm Computing posteriors of all edges given
maximum indegree k
1. Precomputation. Set trivial feature f (G) ≡ 1
(a) For all i ∈ V , P ai ⊆ V − {i} with
|P ai | ≤ k, compute Bi (P ai ). Time complexity O(nk+1 ).
(b) For all i ∈ V , S ⊆ V − {i}, compute Ai (S).
Time complexity O(kn2n ).
(c) For all S ⊆ V , compute RR(S). Time complexity O(3n ).
(d) For all S ⊆ V , compute H(S). Time complexity O(n3n ).
(e) For all i ∈ V , S ⊆ V − {i}, compute Ki (S).
Time complexity O(n3n ).
(f) For all i ∈ V , P ai ⊆ V − {i} with
|P ai | ≤ k, compute Γi (P ai ). Time complexity O(kn2n ).
2. For each edge u → v, compute P (u → v|D) using Eq. (36), and output P (u → v|D) = P (u →
v, D)/RR(V ). Time complexity O(nk+2 ).
Figure 3: Algorithm for computing the posterior probabilities for all possible edges in time complexity
O(n3n ) assuming a fixed maximum indegree k.

Table 1: The speed of our algorithm (in second).
Name
Iris
TicTacToe

n
5
10

m
150
958

Zoo

17

101

Synthetic

20

500

k
4
4
5
6
9
4
5
6
4

TB
2.2e-3
4.7e-1
9.1e-1
1.3
1.7
1.4
4.4
11.5
9.2

Ours
3.5e-3
6.2e-1
1.1
1.5
1.9
602.3
607.0
610.6
23083

REBEL
3.1e-3
5.1e-1
9.4e-1
1.4
1.7
13.4
19.2
28.7
128.3

Speed Test

We tested our algorithm on several data sets
from the UCI Machine Learning Repository
[Asuncion and Newman, 2007]:
Iris, Tic-Tac-Toe,
and Zoo. All the data sets contain discrete variables
(or are discretized) and have no missing values. We
also tested our algorithm on a synthetic data set
coming with REBEL. For each data set, we ran our
algorithm and REBEL to compute the posterior
probabilities for all potential edges. The time taken
under different maximum indegree k is reported in
Table 1, which also lists the number of variables n
and the number of instances m for each data set. We
also show the time TB for computing local scores in
Table 1 as this time also depends on the number of
instances m in a data set.
The results demonstrate that our algorithm is capable
of computing the posterior probabilities for all potential edges in networks over around n = 20 variables.
The memory requirement of the algorithm is O(n2n ),
the same as REBEL, which will limit the use of the
algorithm to about n = 25 variables. It may take our
current implementation a few months for n = 25.
5.2

Comparison of computations

For the Tic-Tac-Toe data set with n = 10, our algorithm is capable of computing the “true” exact edge
posterior probabilities by setting the maximum indegree k = 9,3 although an exhaustive enumeration of
DAGs with n = 10 would not be feasible. We then vary
the maximum indegree k and compare the edge posterior probabilities computed by our algorithm with
the true probabilities. The results are shown as scatter plots in Figure 4 (Note that in these graphs most
of the points are located at (0,0) or closely nearby).
Each point in a scatter plot corresponds to an edge
with its x and y coordinates denoting the posterior
computed by the two compared algorithms. We see
that with the increase of k the computed probabilities gradually approach the true probabilities. With
k = 3 the computed probabilities already converge to
the true probabilities. Studying the effects of the approximation due to the maximum indegree restriction
in general need more substantial experiments and is
beyond the scope of this paper.
We also compared the exact posterior probabilities
computed by REBEL (setting k = 9) with the true
probabilities. The results are shown in Figure 5. We
2
REBEL
is
available
at
http://www.cs.helsinki.fi/u/mkhkoivi/REBEL/.
3
We will call the exact posterior probabilities computed
using uniform structure prior P (G) = 1 the “true” probabilities.

UAI 2009

TIAN & HE

545

1.0

Comparison of Posteriors
Tic−Tac−Toe Data (n=10)

1.0

0.8

0.6
0.2

0.4

Posterior by REBEL (k=9)

0.8

0.6
0.4
0.2
0.0

Posterior by our method (k=1)

Comparison of Posteriors
Tic−Tac−Toe Data (n=10)

0.0

0.2

0.4

0.6

0.8

1.0

0.0

Posterior by our method (k=9)

0.0

Comparison of Posteriors
Tic−Tac−Toe Data (n=10)

0.2

0.4

0.6

0.8

1.0

0.2

0.4

0.6

0.8

Figure 5: A scatter plot that compares posterior probability of edges on the Tic-Tac-Toe data set as computed by REBEL against the “true” posterior.

Comparison of Posteriors
Synthetic Data (n=20, k=4)

0.0

Posterior by our method (k=2)

1.0

Posterior by our method (k=9)

0.2

0.4

0.6

0.8

1.0

1.0

0.0

0.4
0.2
0.0

0.8
0.6
0.4
0.2

0.0

0.2

0.4

0.6

0.8

1.0

Posterior by our method

0.0

Posterior by our method (k=3)

1.0

Posterior by REBEL

Comparison of Posteriors
Tic−Tac−Toe Data (n=10)

0.6

0.8

Posterior by our method (k=9)

0.0

0.2

0.4

0.6

0.8

1.0

Posterior by our method (k=9)

Figure 4: Scatter plots that compare posterior probability of edges on the Tic-Tac-Toe data set as computed by our algorithm with different k against the
true posterior.

Figure 6: A scatter plot that compares posterior probability of edges on the Synthetic data set as computed
by REBEL and our algorithm.

546

TIAN & HE

see that the exact probabilities computed by REBEL
without indegree bound sometimes still differ with the
true probabilities. This is due to the highly non uniform structure prior used by REBEL.
We compared our algorithm with REBEL over a larger
network, the synthetic data set with n = 20. The
results are shown in Figure 6. We see that with the
same maximum indegree, the computed probabilities
often differ. Again, this can be attributed to the non
uniform structure prior used by REBEL.

6

Conclusion

We have presented an algorithm that can compute the
exact marginal posterior probability of a single edge
in O(3n ) time and the posterior probabilities for all
n(n − 1) potential edges in O(n3n ) total time. We
demonstrated its capability on data sets containing up
to 20 variables.
The main advantage of our algorithm over the
current state-of-the-art algorithms, the DP algorithm in [Koivisto, 2006] and the order MCMC in
[Friedman and Koller, 2003], for computing the posterior probabilities of structural features is that those
algorithms require special structure prior P (G) that is
highly non uniform while we allow general prior P (G).
Our algorithm computes exact posterior probabilities
and works in moderate size networks (about 20 variables), which make it a useful tool for studying several
problems in learning Bayesian networks. One application is to assess the quality of the DP algorithm due
to the influence of non uniform prior P (G). Another
application is to study the effects of the approximation due to the maximum indegree restriction. We
have shown some initial experimental results in Section 5. Other potential applications include assessing
the quality of approximate algorithms (e.g., MCMC algorithms), studying the effects of data sample size on
the learning results, and studying the effects of model
parameters (such as parameter priors) on the learning
results.

UAI 2009
DAGs in Eq. (7) can be decomposed into
X
X
Y
P (f, D) =
[
Bi (P ai )]
S⊆V −{v} G∈G v (S) i∈S∪{v}

Y

Bi (P ai )]

G∈G + (V −{v}−S) i∈V −{v}−S

X

=

LLv (S)RR(V − {v} − S),

(37)

S⊆V −{v}

where for any S ⊆ V − {v} we define
X
Y
LLv (S) ≡
Bi (P ai ).

(38)

G∈G v (S) i∈S∪{v}

G v (S) consists of the set of DAGs over S ∪{v} in which
v is the unique sink. We have
G(S ∪ {v}, {v}) = G v (S) ∪ (∪j∈S G(S ∪ {v}, {v, j})),
(39)
from which, by the weighted inclusion-exclusion principle, we obtain
X

LLv (S) =

Y

Bi (P ai )

G∈G(S∪{v},{v}) i∈S∪{v}

−

|S|
X
X
(−1)k+1

X

= F (S ∪ {v}, {v}) −

=

Bi (P ai )

|S|
X
X
(−1)k+1
F (S ∪ {v}, T ∪ {v})
T ⊆S
|T |=k

k=1

X

Y

T ⊆S G∈G(S∪{v},T ∪{v}) i∈S∪{v}
|T |=k

k=1

(−1)|T | F (S ∪ {v}, T ∪ {v})

T ⊆S

=

X

(−1)|T | Av (S − T )F (S, T )

T ⊆S

=

X

(−1)|U |+|S| Av (U )F (S, S − U ).

(40)

U ⊆S

Plugging Eq. (40) into Eq. (37), we obtain
X X
P (f, D) =
(−1)|U |+|S| Av (U )
S⊆V −{v} U ⊆S

=

· F (S, S − U )RR(V − {v} − S)
X
(−1)|U |+|S| Av (U )

X

U ⊆V −{v} U ⊆S⊆V −{v}

Acknowledgments
This research was partly supported by NSF grant IIS0347846.

X

·[

=

· F (S, S − U )RR(V − {v} − S)
X
Av (U )H(U )
(−1)|U |+|S|

X
U ⊆V −{v}

Appendix : Proof of Proposition 3

U ⊆S⊆V −{v}

Y

·

Aj (U )RR(V − {v} − S)

j∈S−U

For a fixed node v, we can break a DAG uniquely into
the set of ancestors S of v and the set of nonancestors
V − {v} − S. For v ∈
/ S let G v (S) denote the set of
DAGs over S ∪ {v} such that every node in S is an
ancestor of v. Then the summation over all possible

=

X

Av (U )H(U )Kv (U ),

(41)

U ⊆V −{v}

where we have used the definition of function Kv (U )
in Eq. (29).

UAI 2009

TIAN & HE

References
[Asuncion and Newman, 2007] A. Asuncion and D.J.
Newman. UCI machine learning repository, 2007.
[Cooper and Herskovits, 1992] G. F. Cooper and
E. Herskovits. A Bayesian method for the induction of probabilistic networks from data. Machine
Learning, 9:309–347, 1992.

547
[Pearl, 2000] J. Pearl. Causality: Models, Reasoning,
and Inference. Cambridge University Press, NY,
2000.
[Silander and Myllymaki, 2006] T.
Silander
and
P. Myllymaki. A simple approach for finding the
globally optimal Bayesian network structure. In
Proceedings of the Conference on Uncertainty in
Artificial Intelligence (UAI), 2006.

[Eaton and Murphy, 2007] D. Eaton and K. Murphy.
Bayesian structure learning using dynamic programming and MCMC. In Proc. of Conference on Uncertainty in Artificial Intelligence, 2007.

[Singh and Moore, 2005] Ajit P. Singh and Andrew W. Moore. Finding optimal Bayesian networks by dynamic programming. Technical report,
Carnegie Mellon University, School of Computer
Science, 2005.

[Ellis and Wong, 2008] B. Ellis and W. H. Wong.
Learning causal Bayesian network structures from
experimental data. J. Am. Stat. Assoc., 103:778–
789, 2008.

[Spirtes et al., 2001] P. Spirtes, C. Glymour, and
R. Scheines. Causation, Prediction, and Search (2nd
Edition). MIT Press, Cambridge, MA, 2001.

[Friedman and Koller, 2003] Nir
Friedman
and
Daphne Koller. Being bayesian about network
structure: A bayesian approach to structure discovery in bayesian networks. Machine Learning,
50(1-2):95–125, 2003.
[Heckerman et al., 1995] D. Heckerman, D. Geiger,
and D.M. Chickering. Learning Bayesian networks:
The combination of knowledge and statistical data.
Machine Learning, 20:197–243, 1995.
[Heckerman et al., 1999] D. Heckerman, C. Meek, and
G. Cooper. A Bayesian approach to causal discovery. In Glymour C. and Cooper G.F., editors, Computation, Causation, and Discovery, Menlo Park,
CA, 1999. AAAI Press and MIT Press.
[Kennes and Smets, 1990] R. Kennes and P. Smets.
Computational aspects of the mobius transformation. In P. B. Bonissone, M. Henrion, L. N. Kanal,
and J. F. Lemmer, editors, Proceedings of the Conference on Uncertainty in Artificial Intelligence,
pages 401–416, 1990.
[Koivisto and Sood, 2004] M. Koivisto and K. Sood.
Exact Bayesian structure discovery in Bayesian networks. Journal of Machine Learning Research,
5:549–573, 2004.
[Koivisto, 2006] M. Koivisto.
Advances in exact
Bayesian structure discovery in Bayesian networks.
In Proceedings of the Conference on Uncertainty in
Artificial Intelligence (UAI), 2006.
[Madigan and York, 1995] D. Madigan and J. York.
Bayesian graphical models for discrete data. International Statistical Review, 63:215–232, 1995.

