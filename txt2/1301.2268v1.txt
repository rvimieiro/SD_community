
proximation. They propose to use a mixture of

paper, we consider variational approximations based

approximations in order to approximate multi-modal pos­

on two classes of models that are richer than standard

mean field

teriors. Both structured variational approximation and the

Bayesian networks, Markov networks or mixture mod­

mixture approximation methods allow for a more refined

els. As such, these classes allow to find better tradeoffs
in the spectrum of approximations. The first class of

trade-off between accuracy and computational complexity.

that are partially directed. The second class of mod­

by adding structure, while in the mixture approximation we

models are elwin graphs, which capture distributions

In the structured approximations more accuracy is gained

els are directed graphs (Bayesian networks) with addi­

can increase the number of mixture components.

tional latent variables. Both classes allow representa­

In this paper, we generalize and improve on these two

tion of multi-variable dependencies that cannot be eas­

methods in order to achieve greater accuracy given the

ily represented within a Bayesian network.

available computational resources. The resulting approx­
imation results enhance the range of approximating distri­

1

Introduction

butions and increase the ability to trade-off accuracy for

A central task in using probabilistic graphical models is

ference.

in­

Exact inference algorithms exploit the structure

of the model to decompose the task. In general, although
the problem is NP-hard, some structures (e.g., these with
bounded tree width) allow efficient inference. W hen the
model is intractable for exact inference, we can still hope
to perform approximate

inference (although that problem

is also known to be generally intractable).

One class of

approximations that received recent attention is the class

of variational approximation algorithms [6]. These algo­

rithms attempt to approximate the posterior
where

o is

P(T I o),
T are the
0) that has

an observation of some variable and

remaining variables, by a distribution

Q(T

:

tractable structure. Using this approximating distribution,
we can define a lower bound on the likelihood
parameters

P(o).

The

e that define Q are adapted by trying to maxi­

mize this lower bound.
The simplest variational approximation is the

mean-field

approximation [8, 9] that approximates the posterior distri­
bution with a network in which all the random variables are
independent. As such, it is unsuitable when there are strong
dependencies in the posterior. Saul and Jordan [ 1 0] sug­
gest to circumvent this problem by using

structured varia-

complexity.
We start by considering extensions of structured approx­
imations. Current structured approximation use Bayesian
networks

or

Markov networks as approximating distribu­

tions. These two classes of models have different expres­
sive power. We provide uniform treatment of both classes
by examining

chain graphs

-

a class of models that is

more expressive than Bayesian and Markov networks, and
includes each one of them as a special case.
We then consider how to add
approximating model.

extra hidden variables to the

This method generalizes both the

structured approximation and the mixture model approxi­
mation. It enables us to control the complexity of the ap­
proximating model both through the structure and through
the number

of values of the hidden variables. The extra hid­

den variables also enable us to maintain the dependency be­

tween different variables but control the level of complex­
ity, thus keeping the dependencies in a compressed man­
ner. As straightforward insertion of extra hidden variables
to the variational approximation framework results in an
intractable optimization problem, we need to combine ad­
ditional approximation steps. We present a natural gener­
alization of methods suggested by Jaakkola and Jordan [5]

UA12001

137

EL-HAY & FRIEDMAN

for mixtures of mean field models.
2

Variational Approximation with Directed
Networks

Structured approximations approximate a probability dis­
tribution using probability distribution with non-trivial de­
pendency structure. We re-derive standard structured ap­
proximation schemes with Bayesian networks (such as the
ones in [4, 10, 11]) using tools that will facilitate later de­
velopments.
Suppose we are given a distributionP over the set of ran­
dom variables X= {X1, ... ,Xn}· Let 0 C X be the
subset of observed variables. We denote by T = X \ 0
the set of hidden variables. Our task is to approximate the
distribution P(T I o) by another distribution Q(T : e),
where e is the set of parameters for the approximating dis­
tribution Q.
In this paper, we focus on approximating distributions
represented by discrete graphical models such as Bayesian
networks and Markov networks. That is, we assume that
{X1 , ... , Xn} are discrete random variables and thatP has
a factorized form
P(x)

1

=-IT
1/Ji{d;)
Zp .

logP(o) = F[Q] + D(QIIP) � F[Q ]
The inequality is true because the KL divergence is non­
negative. Hence, F[Q] is a lower bound on the log­
likelihood. The difference between F[Q] and the true
log-likelihood is the KL-divergence. Minimizing the KL­
divergence is equivalent to finding the tightest lower bound.
A simple approximation of this form uses a distribution
Q that is a Bayesian network
j

where Ui denotes the parents of Xi in the approximating
network, and ()x3 1u; are the parameters of the distribution.
The computational complexity of calculating the lower
bound depends on the complexity of inference in Q and
on the domain size of the factors of P. To see that let us
rewrite F(Q] in a factorized form.
Lemma2.1: /fQ(t) = Ti j ()x;lu;• then
F[Q I c]

=

(1 )

t

Where D1, Dk are subsets of X. This representation can
be a Bayesian network (in which case, each tPi is a con­
ditional distribution) or a Markov network (in which case,
each rp; is a potential over some subset of X).
The approximating distribution will be represented as an­
other graphical model Q(T : e). Once we specify the
form of this model, we wish to find the set of parameters
e that minimizes the distance between Q(T : e) and the
posterior distribution P(T I o). A common measure of
distance is the KL divergence [2] between Q(T : 8) and
the posterior distribution P(T I o). This is defined as

j

2: Eqclc) [logrp;(D;, o)]-log Zp

- L EQ(.Ic) [logBx;iuJ)+logQ(c)
j

..•

[

D(Q(T)IIP(T I o))= Eq(T) log

Q(T)
P(T I o)

]

(2)

Finding the parameters for Q will allow us to compute a
lower bound for log P( o) . To see this, we define a func­
tional F of the general form:

[

F[Q I cl = EQ(·Icl log

P(T, c, o)
Q(T I c)

]

where cis an evidence vector assigned to a subset C <:;:; T,
and QC I c) is a shorthand for Q(T I c). (The reasons
for using additional evidence in the definition will be clear
shortly.) In the special case where C = 0, F becomes
F[Q]

=

EQ

We can easily verify that

]
[1og P(T,o)
Q(T)

where cPi(Di, o) represents a random variable whose value
is tPi ( d;) if d; is consistent with o; otherwise it is 0.

Our goal is to find a set of parameters maximizing F
while conforming to the local normalization constraints.
The optimal parameters for Q are found by writing the La­
grangian for this problem and differentiating it with respect
to them. The Lagrangian is

To differentiate the Lagrangian we shall use the following
technical result.
Lernrna2.2: LetQ(t) = TijBx;lu;• then

8Eq (!(C)]
80x;lu;

=

Q(uj)·Eq(·lx;,u;) [/(C)]+Eq

[ 8f(C) ]
80 x;lu;

Corollary 2.3:

Note that logQ(xj, uj) = log Q(uj) +logBx;lur Equat­
ing the derivative of the Lagrangian to zero and dividing
both sides by Q(ui) and then rearranging, we get
(4)

138

EL-HAY & FRIEDMAN

UAI2001

where Zu; is a normalization constant, and
EsN(Xj , uJ)

=

F[Q I Xj,uJ]-logQ(ui)

EQ(·I•;,u;)

�

[�

)og¢; (D,, o) -

(5)

ft l g0
o

i

, 1 , u1,

]

-logZp

(b)

(c)

Figure 1: (a) A Bayesian network with an observed variable
(01). (b) A representation of the posterior distribution as a

To better understand this characterization, we examine
the term

(a )

F [Q I xi, Uj ].

chain graph. (c) an approximating chain graph network.

It is easy to verify that

D(Q(T I Xj, uj)IIP(T I Xj, uj, o) )
-F[Q I Xj, uj] +logP(xj, Uj, o)

We can then redefineEBN to be

=

Thus,

F[Q I Xj, Uj]

EBN(Xj,Uj)

L

=

iEFj

is a lower bound on logP(xj , u1,o).

This suggests that Eq. 4 can be thought of as approximat­

L

EQ(-!x;,u;) [log(Pi(Di,o)]­
EQ(lxi,u;) [IogQ(Xj' I Uj')]

P(xj I uj ,o)byBxilui· If we replaceEsN(x;,uj)
by logP(xj,Uj,o) in this equation, we would get that
Bx,Ju ; = P (xj I Uj, o).1

Depending on the decomposition of Q, this formula might

ative procedure that updates the parameters of one family

field approximation,

ing

In order to find optimal parameters, we can use an iter­

j'EFJ

involve much fewer terms then Eq. 5. For example, in mean

on each iteration. An asynchronous update of the parame­

clude X1, and

the lower bound F[Q] and converges to a local maximum.

work.

ters according to Eq. 4 guarantees a monotonic increase in

This is a consequence of the fact that, for every i and every

u1, F is a concave function of
{Bx;Ju; I Xj E dom(Xj)}. There­

assignment to the parents
the set of parameters

fore, the stationary point is a global maximum with respect
to those parameters. The concavity ofF follows from the
fact that the second order partial derivatives are negative

82F
--....,.2
8Bxilu;

=

1
--e-Q(uJ)

x;Ju;

<

o

and the mixed partial derivatives are all zero.
The complexity of calculatingEBN as defined in Eq. 5 is
determined by the number of variables, the size of the fam­
ilies in

P

and by the complexity of calculating marginal

probabilities in

Q.

It is important to realize that not all the

terms in this equation need to be computed. To see this
we need to consider conditional independence properties

X is independent of Y given Z in Q,
denoted Q F Ind(X; Y I Z), if Q(X I y, z ) = Q(X I z )
for all values y and z of Y and Z. If Q is a Bayesian
in

Q.

We say that

network, we can determine such independencies using d­

separation [7] Now, suppose that Q
.

Q(c I Xj, ui)

i.e.,

EQ(c!x;,u;)

=

Q(c I uj).

F Ind(Xj; C I Uj),
Terms of the form

[/(c)] can be ignored in the update equations

since they change the new parameters by a constant factor
which will be absorbed in Zu;. Therefore we can reduce
the amount of computations by defining the sets of indices
of the factors that depend on Xj given

pP

Similar derivation can be made when

Q is a Markov net­

The main difference is that in Markov networks

there is a global constraint (defined by the partition func­
tion) rather then local ones for each conditional distribu­
tion. Due to space considerations we omit the details, and
refer the interested reader to

3

[11].

Chain Graph Approximations

As is well known, the classes of distributions that can
be represented by Markov networks and by Bayesian net­
works are not equivalent. Therefore, for some distributions
the best tractable approximations might be represented by
Bayesian networks while for other distributions the best ap­
proximation is a Markov network. We can gain more flex­
ibility in choosing an approximating distribution by using
a more general class of probability models that can cap­
ture the dependency models implied by Bayesian networks,
Markov networks and dependency models that can be cap­
tured by neither of them.
To consider a concrete example, suppose that

P

is a

Bayesian network. W hat is the form of the posterior P(T

o)?

I

For a concrete example, consider the network of Fig­

ure l(a). When, we observe the value of 01, we create de­
pendencies among the variables T1, T2, and T3. The poste­

rior distribution is neither a Bayesian network nor a Markov
network (because of the v-structure in the parents of

T5).

Instead, we can write this posterior in the form:

Uj as follows:

{i: Q � Ind(Xj;Di I Uj)}
F�
{j' =/; j: Q � Ind(Xj;Xj', uj' I Uj)}
----"
'------"
J

FJ includes only potentials that in­
Fl is empty.

=

Note that the term log Q(uj) in EsN (xj, Uj) can be ignored,

1

since it is absorbed by the normalizing constant. We include it

above to simplify the decomposition of EBN ( xh Uj).

where

'f/I(TI . T2,T3 )

=

P(�1 )P(ol

I T�,T2,T3) is a poten­

tial that is induced by the observation of

o1.

A natural class of models that has this general form are
chain graphs [3].

Such a model factorizes to a product

of conditional distributions and potentials. Formally, we

UAI2001

EL-HAY & FRIEDMAN

define a chain graph to have for each variable a (possibly
empty) set of parents, and in addition to have a set of po­

139

Lemma 3.1: IJQ is a chain graph overT, then

tentials on some subsets of variables.
When we represent

Q as a chain graph, we will have the

general form:

Ui are the directed parents of Xi. In
'lj!k are potential functions on subsets ofT, and
l::t fli Q(xi I Uj) flk 'lj!k(Ck) is a normalizing

where, as before,
addition,

ZQ

=

function that ensures that the distribution sums to 1. Fig­

ure l(b) shows the chain graph that represents this factor­
ization.
It is easy to check that if

P(T I o)

P

is a Bayesian network, then

can be represented as a chain graph (for each

variable Xi in

0, add a potential over the parents of Xj).

.F[Q] we get two terms.
I Xj, Uj)] as before, and the other is :F[Q].
since .F[Q] does not depend on the value of Xj,

Note that when we differentiate

The first, is ..F[Q
However,

it is a absorbed in the normalizing constant

Zu; .

Thus,

the general structure of the solution remains similar to the
simpler case of Bayesian networks:

In contrast, it is easy to build examples where the posterior
distribution cannot be represented by a Bayesian network

=

without introducing unnecessary dependencies. Thus, this
class of models is, in some sense, a natural representation

l

e E co

l

eE co (co )

_
_

Zu;
_
_

Zq

of conditional distributions in Bayesian networks.

(x ; u; )
,

This argument suggests that by considering chain graphs
we can represent approximate distributions that are more
tractable than the original distribution, yet are closer to
the posterior we want to approximate. For example, Fig­
ure l(c) shows a simple example for a possible approxi­
mate network for representing the posterior of the network
of Figure l(a). In this network there are two potentials with
two variables each, rather than one with three variables.
Given the structure of the approximating chain graph, we
wish to find the set of parameters that maximizes

F[Q],

the lower bound on the log-likelihood. As usual, we need
to define a Lagrangian that capture the constraints on the
model. These constraints contain the constraints that ap­
peared in the Bayesian network case, and, in addition, we

where

Ecc(xj, ui)
Eca(ck)

F[Q I Xj, Uj]-logQ(xj, Uj ) + logBx; lu;
:F[Q I ck]-logQ(ck) + log'lj!k(Ck)

To get an explicit form of these equations, we simply
write the chain-graph analogue of Lemma

2.1 which has

similar form but includes additional terms. As in the case
of Bayesian network, we can easily identify terms that can
depend on the value of

Xj, and focus the computation only

on these. This is a straightforward extension of the ideas in
Bayesian networks, and so we omit the details.

4

require that each potential sums up to one:

=

Adding Hidden Variables

Structured approximations were the first method proposed
for improving the mean field approximation. Jaakkola and
Jordan [5] proposed another way of improving the mean
To understand this constraint, note that the each potential

field approximation: to use mixture distributions, where

can be scaled without changing

each mixture component is represented by a factorized dis­

stant is absorbed

Q,

since the scaling con­

in ZQ. Thus, without constraining the

scale of each potential there is a continuum of solutions,
and the magnitude of values in the potentials can explode.
Putting these together, the Lagrangian has the form:

tribution.

The motivation for using mixture distribution

emerges from the fact that in many cases the posterior dis­
tribution is multi-modal, i.e. there are several distinct re­
gions in the domain of the distribution with relatively high
probability values. If the location of the different modes of
the distribution depends on the values of several variables
than the mean field approximation can not capture more
than one mode.

The main difference from the Bayesian network approxi­
mation is in the form of the analogue of Lemma

2.2. In the

case of chain graphs, we also have to differentiate
so we get slightly more complex derivatives.

ZQ, and

Recal1 that the mean field approximation uses a graphical
model in which all the variables in

T

are independent of

each other. Thus, we can think of it as a Bayesian network
without edges. The mixture distribution approximation can

140

EL-HAY & FRIEDMAN

(a)

UAI2001

(b)

(c)

Figure 2: (a) A simple dynamic Bayesian network that describe a temporal process. Time progresses to the right. Each
vertical "slice" describe variables that exist in the same instance. (b) and (c) are two approximating networks for the
distribution represented by the network (a) with extra hidden variables. (b) Edges within a time slice are maintained.
Correlations between time slice are modeled through the introduction of the hidden variable set
(c) Edges
between time slices are maintained. Correlations between the three chains are modeled through the hidden variables
and

{Vn};;'=l·

V2

V1,

V3•

be viewed as one that uses a Bayesian network over the
variables T and an extra variable
such that
is the
parent node of each Xj E T. As before, the parameters
of the mixture distribution could be found by maximizing
the lower bound of the log likelihood as presented in Sec­
tion 2. Unfortunately, using this technique in a straightfor­
ward manner would not help us since the extra hidden vari­
ables introduces correlations, which leave us with an opti­
mization problem whose complexity is at least as great as
this of the original inference problem. Jaakkola and Jordan
overcame this problem by introducing another variational
transformation resulting in another lower bound to the log
likelihood [5].
In this section, we generalize the ideas of Jaakkola and
Jordan, and show a method where we can perform struc­
tured approximation with distributions Q that are defined
over TUV, where V is a set of hidden variables that did not
appear in the original distribution. (For clarity, we focus on
the case of Bayesian networks, although similar extension
can be applied to chain graphs as well.)
Given the distribution P(X) and evidence owe shall ap­
proximate the posterior P(T I o) with another distribution
Q(T) = l:v Q(T, v ) . This distribution is defined over
the variable set T U V where V is a set of extra hidden
variables. Our task is to find the parameters of Q that will
maximize the lower bound F[Q].
Figure 2(b) and (c) are two examples of possible approx­
imations for the distribution that is represented by the net­
work in Figure 2(a). Recall the structured approximation
for this network modeled the approximating distribution by
a network with three independent chains. In the networks
presented here, the correlations are maintained through the
hidden variables. In Figure 2(a) we added an extra hid­
den variable for every time slice. The correlations between
time slices are maintained through those hidden variables.
The edges within a time slice are maintained in order to
preserve intra-time dependencies. In Figure 2(b) we main-

V,

V

tained the edges between the time slices and added extra
hidden variables for every chain. Correlations among the
chains are maintained by the connections between the hid­
den variables.
Another perspective of the potential of extra hidden vari­
ables was suggested by Jaakkola and Jordan [5]. We can
easily extend it to our case. This is done by reexamination
of the lower bound F[Q].

Lenuna 4.1: Let Q(T)

F[Q]

l:v Q(T, v ), then

EQ(V) [F[Q I V]] + l(T; V)

[ g Qb�J..�l]

EQ to
T and V.

where I(T; V)
tion between

=

=

=

is the mutual informa­

The first term is an average on lower bounds that are gained
without introducing extra hidden variables. The improve­
ment arises from the second term. Given the structure of
the approximating network without extra hidden variables,
the lower bound can be improved if there are several dif­
ferent configurations of the parameters of the sub-network
defined on T that achieve lower bounds that are near opti­
mal. Using an extra hidden variable set to combine these
configuration, will improve the lower bound by the amount
of the mutual information between T and V.
As described above, in the presence of hidden variable,
the optimization of the functional F[Q] is more complex.
The source of these complications is the fact that
Q(t)
does not decompose. Therefore we shall relax the lower
bound. We start by rewriting F[Q] as

log

F[Q]

=

EQ

[log�(�:�)]- H(V IT)

This first term does decompose. The remaining term is the

conditional entropy

UAI2001

EL-HAY & FRIEDMAN

Instead of decomposing this term, we can calculate a lower
bound for it by introducing extra variational parameters.
The new parameters are based on the convexity bound [6)
-

l og(x) � -Ax+ log( ).) + 1

(6)

We can use the convexity bound by adding an extra vari­
ational parameter R(t, v) for every assignment toT U V.
Applying Equation 6 for every term in the summation of
the conditional entropy, we get a lower bound for the con­
ditional entropy:

the expression £ H is similar to the one obtained for the sim­
pler structural approximation, except for the last two terms
that arise from the lower bound on the negative conditional
entropy. To evaluate the term Lv Eq(Tix;,u;) [R (T, v)]
we perform variable-elimination like dynamic program­
ming algorithm.
To complete the story, we need to consider the update
equations for the parameters of R. Simple differentiation
results in the equation

Px;,u;

-H(V I T)
>

Eq -R(t, v)

=

-l:R(t, v)Q(t)

[

Q����)

+

logR(t, v)

+

Eq[logR(t, v)]

+

+1

Obviously, if we add a distinct variational parameter for ev­
ery assignment t, v, the conditional entropy can be recov­
ered accurately. Unfortunately, this setting leaves us with
an intractable computation. In order to reduce the compu­
tational complexity of the lower bound, we assume that R
has a similar structure to that of Q
=

IJ Px;,u;
j

We define the lower bound on F as a new functional:

g[Q,R I c]

EQ ( · Ic}

=

[log �i�'�0�)]

-2: Eq(Tic) (R(T, v) ]

v
+EQ(-Ic) [logR(T, V)]

=

g[Q,RJ-l:l:Au;l:Ox;lu;
x;
j u;

Using Lemma 2.2, and then applying the constraints, we
get the update equations for 8x 11 u; :
ex; I
u; =

1

-.

zu;

e£H(x;,u;)

(7)

Where

£H(Xj, ui )

=

g[Q,R I Xj, Uj]- log(uj)

As usual, we can decompose this term to a sum of terms:

2: EQ ( · Ix; ,u ) [logQ(Xj'

I Uj')]

2: EQ(· I x ,u; ) [logR(Xj'

I uj' )]

;

j'#-j
+

j'

;

L Eq(Tix;,u;) [R(T , v)]
v

Px;,u;
Q XJ, UJ
Lt,vt=x;,u; R(t, v)Q(t) ( . )

(S)

does not appear in the right hand side (since it cancels out
in the fraction). Again, we can efficiently compute such
equations using dynamic programming.
The Lagrangian is a convex function of both Bx;lu; and
Px;,u; . Therefore, asynchronous iterations of Equation 8
and Equation 7 improve the lower bound and will eventu­
ally converge to a stationary point.

5

Examples

To evaluate our methods we performed a preliminary test
with synthetic data. We created dynamic Bayesian net­
works with the general architecture shown in Figure 2(a).
All the variables in these networks are binary. We con­
trolled two parameters: the number of time slices ex­
panded, and the number of variables in each slice. The pa­
rameters of networks were sampled from a Dirichlet prior
with hyperparameter
Thus, there was some bias toward
skewed distributions. Our aim was to compute the like­
lihood of the observation in which all observed variables
were set to be 0. We repeated these tests for sets of 20 net­
works sampled for each combination of the two parameters
(number of time slices and number of variables per slice).
We performed variational approximation to the posterior
distribution using three types of networks with hidden vari­
ables: The first two types are based on the "vertical" and
"horizontal" architectures shown in Figure 2(b) and (c). We
considered networks with 1, 2, and 3 values for the hidden
variable. (Note that when we consider a hidden variable
with one value, we essentially apply the Bayesian network
structured approximation.) The third type are networks that
represent mixture of mean field approximations. For this
type we considered networks with 1, 4 and 6 mixture com­
ponents (When there is one mixture component the approx­
imation is simply mean field). We run each procedure for
10 iterations of asynchronous updates. This seems to con­
verge on most runs. To avoid local maxima, we tried 10
different random starting points in each run and returned
the best scoring one.
The figure of merit for our approximations is the reported
upper-bound on the KL-divergence between the approxi­
mation and the true posterior. This is simply log P{o ) -

t.

+ l

We now can define the Lagrangian with the desired con­
straints:

JH

=

Where the term t, v f= x1,u1 denotes assignments to
{T, V} that are consistent with Xj, Uj. Note that Px;,u;

1]

t,v

R(t, v)

141

EL-HAY & FRIEDMAN

142

Mixture of mean fields

UAI2001

"Vertical" approximation

"Horizontal" approximation

7

Figure 3: Comparison of the two appro x imating structures of Figure 2 and mixture of mean fields. The figures on the left
column report results for the mixture of mean fields approximation, with 1, 4, and 6 mixture components. The figures
on the middle column report results for the network structure containing additional hidden variable for each time slice
(Figure 2(b)) with hidden variables with 1, 2, and 3 values. The figures on the right column report results for the network
structure containing additional hidden variable for each temporal chain (Figure 2(c)) with hidden variables with 1, 2, and
3 values. The figures on the top row report on approximation to networks with 3 va riab les per time slice and the figures
on the bottom row report on networks with 4 variables per time slice. The x-axis corresponds to the number of time slices
in the network. They-axis corresponds to the upper-bound on the KL-divergence log P(o) - 9Q [Q, R] normalized by the
number of time slices in the network. Lines describe to median performance among 20 inference problems, and error bars
describe 25-75 perc entile s.

9Q[Q, R]. (The examples are sufficient ly small, so that we
can compute log P( o). ) We need to examine this quantity
since

different random networks have different values of

P(o) and so we cannot compare lower bounds.

Figure 3 describes the results of these runs. As we can see
the differences grow with the number of time slices. This
is expected as the problem becomes harder with additional
slices. The general trend we see is that runs with more
hidden values perform better. These differences are mostly

pronounced in the larger networks. This is probably due to
the higher complexity of these networks.
The comparison to mixture of mean fi el ds approximation
shows that simple mean field ( 1 component) is much worse
than all the other methods. Second, we see that although
mixtures of mean field improve with larger number of com­
ponents, they are still worse th an the structured approxima­
tions on the network with 3 variables per slices. We believe
that these toy examples are not sufficiently large to high­
light the differences between the different methods. For
e x ample , differences start to emerge when we examine 6

and 7 time slices.
Our implementation of these variational methods is not
optimized and thus we do not believe that running times
are informative on these small examples. Nonetheless, we
note that running mixtures of mean fields with 6 compo ­
nents took roughly the same time as running the structured
approximations with hidden variables of cardinality 3.
One caveat of this experiment is that it is based on random
networks, for which the depenencies between variables is
often quite weak. As such it is hard to gauge how hard
is inference in this networks. We are currently starting to
apply these m ethod s to real-life problems, where we expect
to improvement over mean field type methods to be more
pronounced.
6

Discussion

this paper we presented two extensions of structured
variational methods-based on chain graphs and additional
hidden variables. Each extension exploits a representaIn

UAI2001

EL-HAY & FRIEDMAN

tiona! feature that allows to better match a tractable ap­
proximating network to the posterior.

In Michael I. Jordan, editor, Learning in Graphical

Models. Kluwer, 1997.

By perusing such

extensions we hope to find better tradeoffs between net­
work complexity on one hand and the approximation of the

[6] M. I. Jordan,

methods for graphical models. In M. I.Jordan, editor,

We demonstrated the effect of using hidden variables in

Learning in Graphical Models. Kluwer, 1998.

synthetic examples and showed that learning non-trivial

[7] J. P earl. Probabilistic Reasoning in Intelligent Sys­
tems. Morgan Kauffman, San Francisco, 1988.

We are currently starting larger scale experiments on hard
real-life problems.
We put emphasis on presenting uniform machinery in the

[8] C. Peterson and J.R. Anderson. A mean field theory
learning algorithm for neural networks. Complex Sys­

derivations of the three variants we considered. This uni­
form presentation allows for better insights into the work­
ings of such approximations and simplifies the process of

tems, 1:995-1019, 1987.

[9] L. Saul, T. Jaakkola, and M. Jordan. Mean field the­

ory for sigmoid belief networks. Journal of Artificial

deriving new variants for other representations.

Intelligence Research, 4:61-76, 1996.

One issue that we did not address here for lack of space is
efficient computations on the network Q. The usual analy­

z. Ghahramani, T. Jaakkola, and L. K.

Saul. An introduction to variational approximations

posterior distribution on the other.

network with hidden variables helps the approximation.

143

[10] L. K. Saul and M. I. Jordan.

Exploiting tractable

sis focuses on the maximal tree width of the network. How­

substructures in intractable networks. In Advances

ever, much computation (up to a quadratic factor) can be

in Neural Information Processing Systems, volume 8,

saved by conscious planning of order of asynchronous up­

pp.486-492, 1996.

dates and the propagation of messages in Q's join tree.
The grand challenge for applications of such variational
methods is to build automatic procedures that can deter­
mine what structure matches best a given network with
a given query.

This is a non-trivial problem.

We hope

that some of the insights we got from our derivations can
provide initial clues that will lead to development of such
methods.

Acknowledgements
We thank Gal Elidan, Tommy Kaplan, Iftach Nachman,
Matan Ninio, and the anonymous referees for comments on
earlier drafts of this paper. This work was supported in part
by Israeli Science Foundation grant number 224/99-1. Nir
Friedman was also supported by an Alon fellowship and by
the Abe

& Harry Sherman Senior Lectureship in Computer

Science. Experiments reported here were run on equipment
funded by an ISF Basic Equipment Grant.

References
[1] D. Barber and

W. Wiegerinck. Tractable variational

structures for approximating graphical models.

In

Advances in Neural Information Processing Systems

(NIPS), volume 11, pp. 183-189, 1999.
[2] T.M. Cover and J. A Thomas. Elements of Informa­
tion Theory. John Wiley

& sons, 1991.

[3) R. G. Cowell, A. P. Dawid, S. L. Lauritzen, and D. J.
Speigelhalter. Probabilistic Networks and Expert Sys­
tems. Springer, 1999.

[4]

Z. Ghahramani and M. I. Jordan.

Markov models.

1997.

[5]

Factorial hidden

Machine Learning, 29:245-274,

T. S. Jaakkola and M. I. Jordan. Improving the mean
field approximation via the use of mixture models.

[11]

W. Wiegerinck. Variational approximations between

mean field theory and the junction tree algorithm. In

Sixteenth Conference on Uncertainty in Artificial In­
telligence (VAl),

2000.

