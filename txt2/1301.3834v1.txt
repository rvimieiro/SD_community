to Z. The success of graphical models stems in part
from the fact that vertex separation and conditional in­
dependence share key properties which render graphs
an effective language for specifying independence con­
straints.
In this paper we show that when graphical models
are trees and distributions are from specific classes,
then the relationship between vertex separation and
conditional independence is much more pronounced.
More specifically, we show that if a strictly positive
*Part of this work was done w hile
sabbatical at Microsoft research.

the

author was on

Christopher Meek
Microsoft Research
Redmond, WA, 98052, USA
meek@microsoft.com

joint probability distribution for a set of binary ran­
dom variables factors according to a tree, then vertex
separation represents all and only the independence
relations encoded in the distribution. The same result
is shown to hold also for multivariate strictly positive
normal distributions.
The class of Markov trees has been studied in several
contexts. Practical algorithms for learning Markov
trees from data have been used for pattern recogni­
tion (Chow and Liu, 1968). Geometrical properties of
families of tree-like distributions have been studied in
(Settimi and Smith, 1999). F inally, the property of
perfectness, when a graphical model represents all and
only the conditional independence facts encoded in a
distribution, is a key assumption in learning causal
relationships from observational data (Glymour and
Cooper, 1999).
2

Preliminaries

Throughout this article we use lowercase letters for sin­
gle random variables (e.g., x, y, z ) and boldfaced low­
ercase letters (e.g., x, y, z ) for specific values for these
random variables. Set of random variables are denoted
by capital letters (e.g., X, Y, Z), and their values are
denoted by boldfaced capital letters (e.g., X, Y, Z).
For example, if Z = { x, y} then Z stands for { x, y}
where x is a value of x and y is a value of y. We use
P(X) as a short hand notation for P(X =X). We say
that P(X) is strictly positive if VX P(X) > 0. We use
X y as a short hand notation for X U {y}.
Let X, Y and Z be three disjoint sets of ran­
dom variables having a joint probability distribution
P(X, Y, Z). Then, X and Y are conditionally inde­
pendent given Z, denoted by X l_p Y I Z, if and only
if
VXVYVZ P(X, Y, Z)P(Z) = P(X, Z)P(Y, Z).
When P is strictly positive an equivalent definition is
that X l_p Y I Z holds if and only if
VXVYVZ P(X IZ) = P(XIY, Z).
When P(X, Y, Z) is a strictly positive joint normal
distribution, then X and Y are conditionally indepen­
dent given Z if and only if Pxy.Z = 0 for every x EX

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

20

and y E Y where Pxy.Z is the partial correlation coef­
ficient of x an d y given Z (Cramer, 1946).
The ternary relation X ..lp Y I Z was introduced in
(Dawid, 1979) and further studied in (e.g., Spohn
1980; Pearl and Paz 1987; Pearl 1988; Geiger and
Pearl 1993; Studeny 1992) . The ternary relation
X..lp Y I Z satisfies the following five properties which
are called the graphoid axioms (Pearl and Paz, 1987).
•

•

•

•

•

•

Decomposable transitivity:
aB ..lp De I c 1\ a ..lp e I BD =>
a ..lp c I B V c ..lp e I D

(8)

Symmetry:
X..lp y I z =} y ..lp X I z

(1)

3

Decomposition:
X..lp YW I z =}X..lp y I z

(2)

Weak Union:
X..lp YW I z =}X..lp y I zw

(3)

We now prove that decomposable transitivity holds for
strictly positive joint probability distributions of bi­
nary random variables and for strictly positive normal
distributions. We then show that decomposable tran­
sitivity holds also for vertex separation in undirected
graphs.

Contraction:
X ..lp y I z 1\ X ..lp

I ZY =}
X..lp YWIZ

(4)

If P is strictly positive, then
Intersection:
X ..lp y I zw 1\ X..lp w I ZY =}
X ..lp YW I z

(5)

w

The following property holds for joint normal distri­
butions P(X,Y, Z, c) (Pearl, 1988). It also holds for
discrete random variables if Z = 0 and c is a binary
random variable.
•

The main result in this paper is a converse to Eq. 7
under suitable conditions. W hen the converse holds
we say that G is a perfect representation of P. To
facilitate our argument we must first introduce a new
property for conditional independence.

Weak Transitivity:
X ..lp Y I Z 1\ X ..lp Y I Zc =>
X ..lp c I z v c ..lp y I z

(6)

A Markov network of a probability distribution
=
P(x1, . . . ,xn) is an undirected graph G (V, E) where
V = { x1,. . . , xn} is a set of vertices, one for each ran­
dom variable x;, and E is a set of edges each repre­
sented as (x;,xj) such that (x;,xj) E E if and only
if
•X; ..lp Xj I {x1, ... , Xn } \ {x;, Xj}·
A Markov tree is a Markov network where G is a tree.
A key property of Markov networks is the following.
Let A ..lc B I C stand for the assertion that every path
in G between a vertex in A and a vertex in B passes
through a vertex in C, where A, B, and C are mu­
tually disjoint sets of vertices. Note that whenever
A ..lc B I C holds in G, A and B are (vertex) sepa­
rated by C. The ternary relation A ..lc B I C satisfies
all the properties we listed for A ..lp B I C and some
additional properties that do not hold for A ..lp B I C
(Pearl, 1988).

Theorem 1 (Pearl and Paz, 1987; Pearl, 88)
Let G be a Markov network of P(x1,... , Xn ) , and sup­
pose Intersection holds for P. Then
A ..la B I C implies A ..lp B I C
(7)
for every disjoint set of vertices A, B, and C of G and
their corresponding random variables in {x1,. . . , Xn } ·

New property of conditional
independence

Theorem 2 Let a, c, e be binary random variables, B
and D be {possibly empty) sets of binary random vari­
ables, and P(a, c, e,B,D) be a strictly-positive joint
probability distribution for these random variables.
Then
aB ..lp De I c 1\ a ..lp e I BD =>
a ..lp c I B V c ..lp e I D
holds for P.
Proof: We use a to denote a value for a, B to denote a
value for a set of variables B, and a0 and a1 to denote
the two values of a binary random variable a.
Due to aB ..lp De I c it follows that
P(a,B,c,D,e)· P(c) = P(a,B,c)· P(c, D,e) (9)
for every value a,c, e,B,D of the corresponding ran­
dom variables. Due to a ..lp e I BD it follows that
P(a0,B,D,e0) ·P(a1,B,D,e1) =
P (al, B, D, e0) ·P(a0, B, D, e1). (10)
for every value B,D of B, D. Since c is a binary vari­
able
P(a,B,D,e)= P(a,B,c0,D,e)+ P(a,B,cl,D,e)
(11)
Now, substituting Eq. 9 into Eq. 11, then substitut­
ing the result into Eq. 10, yields using some divisions,
which are allowed because P is strictly positive, that
1+ a(B),B(D) = a(B)+ ,B(D)
where

and

P(c1,D,e0) ·P(c0,D,e1)
P(c0,D,e0) ·P(c1,D,el)
Consequently, either a(B) = 1 or ,B(D) = 1. Further­
more, since B and D are arbitrary values of B and
D, respectively, we have \fB\fD [a(B) = 1 V ,B(D) = 1]
which is equivalent to [\fB a(B) = 1) V ['v'D ,B(D) = 1]
which is equivalent to a ..lp c I B V c ..lp e I D.
0
,B(D)

=

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

Theorem 3 Let a,c, and e be continuous random
variables, B and D be {possibly empty) sets of con­
tinuous random variables, and let P(a, c, e,B, D) be
a strictly positive joint normal probability distribution
for these random variables. Then,
aB l_p De I c 1\ a l_p e I BD
a l_p c I B V

=>
c

l_p e I D (12)

21

2000

on the path 11 between a and c, or that a vertex b E B
resides on the path 12 between c and e. In the first
case vertices a and d are connected and the path that
connects them does not include c, and in the second
case vertex b and e are connected and the path that
connects them does not include c. Thus, in both cases,
aB l_a De I c does not hold in G, contradicting our
�umpt�n.
0

holds for P.

Proof: We use a formal logical deduction style to em­
phasize that the only properties of normal distribu­
tions being used are the ones encoded in Symmetry,
Decomposition, Intersection, Weak union, and Weak
transitivity. Recall that weak transitivity holds for
every normal distribution and that intersection holds
for strictly positive normal distributions. The other
properties hold for every probability distribution.
We now derive the conclusion of Eq. 12 from its an­
tecedents.

1. aB l_p De I c
( Given)
2. a l_p e I BD
( Given)
3. a l_p D I cB
( W. union, Decomposition, and Symmetry on (1))
4. B l_p e I cD
( W. union, Decomposition, and Symmetry on (1))
5. a l_p e I BDc
( Weak union and Symmetry on (1))
6. a l_p c I BD V c l_p e I BD
( Weak transitivity on (2) and (5))
7. a l_p cD I B V Be l_p e I D
( Intersection and Symmetry on (3), (4) and (6))
8. a l_p c I B V c l_p e I D
( Symmetry and Decomposition on (7))
0

Theorem 4 Let a,c, and e be distinct vertices of an
undirected graph G, and let B and D be two (possibly
empty) disjoint sets of vertices of G that do not include
a, c or d. Then,
aB l_a De I c 1\ a l_a e I BD
a l_a c I B V

=>
c

l_a e I D (13)

holds for G.

Proof: Assume the conclusion of Eq. 13 does not hold
in G but its antecedents hold. Then, there exists a
path 11 in G between a and c such that no vertices
from B reside on 11, and there exists a path /2 in G
between c and e such that no vertices from D reside
on 11. If B and D are empty, then the concatenated
path /1/2 contradicts a l_a e I BD which is assumed
to hold in G. Thus, we can assume either B or D are
not empty. The concatenated path /1/2 contains a
vertex from B or D ( or both) because a l_a e I BD is
assumed to hold in G. Assume a vertex d E D resides

4

Perfect Markovian trees

We are ready to prove the main result.

Theorem 5 Let G be a Markov tree for a probabil­
ity distribution P(x1,...,xn). If x1, ...,xn are bi­
nary random variables and P is a strictly-positive joint
probability distribution, or if x1,..., Xn are continuous
random variables and P is a strictly positive joint nor­
mal distribution then, in both cases,
A

l_a B I C if and only if

A

l_p B I C

(14)

for every disjoint set of vertices A, B, and C of G and
their corresponding random variables in { x1,...,xn}.

Proof: Theorem 1 proves one direction of Eq. 14, and
so it remains to prove that
A

l_p B I C implies

A

l_a B I C

(15)

To prove Eq. 15 it is sufficient to show that
a l_p b I C implies a l_a b I C
for every pair of vertices a E A and b E B because
A l_p B I C implies VaVb a l_p b I C and VaVb a l_a
b I C is equivalent by definition to A l_a B I C.
We proceed by contradiction. Let x and y be a pair
of vertices for which there exists a set of vertices Z
satisfying
(16)
x l_p y I Z 1\ •x l_a y I Z
and such that x and y are connected with the shortest
path among all pairs x',y' for which there exists a set
Z' satisfying x' l_p y' I Z' 1\ •x' l_a y' I Z'.
Suppose first that the path between x and y is merely
an edge connecting the two vertices. We will now reach
a contradiction by showing that G cannot be a Markov
network of P. In particular, we show that P satisfies
x l_p y I Uxy where Uxy are all vertices except x and
y. Let Ux be all the vertices on the x side of the edge
(x, y) and Uy be the rest of the vertices. ( Namely, Ux
are the vertices in the component of x after removing
the edge (x,y)). Let B = Ux n Z and D = Uy n Z.
We proceed by a formal deduction using properties of
conditional independence.
1. B l_a Dy I x
( By definition of B and D in G)
2. B l_p Dy I X
( From (1) and since G is a Markov network of P)
3. B l_p y I xD
( Weak union on (2))
4. X l_p y I BD
(Z = BD and x l_p y I Z is assumed)

22

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

5. xB l_p y I D
(Intersection and Symmetry on (3) and (4))
6. X l_p y I D
(Decomposition on (5))
7. X l_G D I y
(By definition of D in G)
8. X l_p D I y
(From (7) and since G is a Markov network of P)
9. X l_p yD 1 0
(Intersection on (6) and (8))
10. X l_p Y 1 0
(Decomposition on (9))
11. X l_G Uy I y
(Definition of Uy)
12. X l_p Uy I y
(From (11) and since G is a Markov network of P)
13. X l_p yUy 10
(Contraction on (10) and (12))
14. Ux l_c yUy I x
(Definition of Ux and Uy)
15. Ux l_p yUy I X
(From (14) and since G is a Markov network of P)
16. xUx l_p yUy 10
(Contraction and Symmetry on (13) and (15))
17. x l_p yiUxUy
(Weak union and Symmetry on (16))
Now suppose the path between x andy has more than
one edge and that c is a vertex on this path. We reach
a contradiction by showing that the pair x, y is not
the closest pair of vertices that satisfy Eq. 16 for some
set Z', contrary to our selection of these vertices. Let
B, D be a partition of Z such that B are the vertices
in Z on the x side of c and D = Z \ B. The rest of
the derivation is a formal deduction using properties
of conditional independence.
1. xB l_c Dy I c
(By definition of B and D in G)
2. xB l_p Dy I c
(From (1) and since G is a Markov network of P)
�3. x l_p yiBD
(Z = BD and x l_p y I Z is assumed)
4. X l_p c I B v c l_p y I D
(Decomposable transitivity on (2) and (3))
5. -,x l_c c I B 1\ -,c l_p y I D
(By definition of B and D in G)
v
6. [x l_p c I B 1\ -,x l_c c I B]
((4) and (5))
[ c l_p y I D 1\ -,c l_c y I D]
Each disjunct in Step (6) exhibits a pair of vertices that
are closer to each other than x and y and yet satisfy
Eq. 16 for some set Z'. Note that Step (4) uses De­
composable transitivity which holds if Xt, . . . , Xn are
binary random variables and P is a strictly-positive
joint probability distribution, or if Xt, . . . , Xn are con­
tinuous random variables and P is a strictly positive
joint normal distribution, as assumed.
D

5

Discussion

Our proof uses a new property of conditional indepen­
dence that holds for the two classes of probability dis­
tributions we have focused on. The approach of using
logical properties of conditional independence as a way
of reasoning follows the approach taken by (Pearl and
Paz, 1987) who analyzed the logical properties shared
by vertex separation and conditional independence.
The algorithmic consequence of Theorem 5 is that in
order to check whether a Markov tree of P represents
all the conditional independence statements that hold
in P, assuming P satisfies Intersection and Decom­
posable transitivity, requires one to check whether for
each edge (x, y) in G, x l_p y 1 0 holds in P. Note that
this test is more reliable and simpler than checking
for each edge (x, y) in G, whether x l_p y I Uxy holds,
as the definition of a Markov tree requires. An open
question remains as to what is the minimal computa­
tion needed to ensure that a general Markov network
represents all the conditional independence statements
that hold in P and what properties P needs to satisfy
to accommodate these computations.
A straightforward attempt to extend our results with­
out changing the tests or the assumptions on P is quite
limited because we have counter examples to Theo­
rem 5 when G is a polytree (a directed graph with
no underlying undirected cycles) and when P does
not satisfy Intersection or Decomposable transitivity.
These counter examples, together with the proof of
Theorem 5, show that if G is a Markov tree of a prob­
ability distribution P, then G is a perfect represen­
tation of P if and only if P satisfies Intersection and
Decomposable transitivity.

References
Chow, C., & Liu, C. (1968). Approximating discrete
probability distributions with dependence trees.
IEEE Transaction on information theory, 14,
462-467.
Cramer, H. (1946). Mathematical methods of statistics.
Princeton university press.
Dawid, A. P. (1979). Conditional independence in sta­
tistical theory (with discussion). Journal of the
royal statistical society, Series B, 41, 1-31.
Geiger, D., & Pearl, J. (1993). Logical and algorith­
mic properties of conditional independence and
graphical models. Annals of Statistics, 21, 200121.
Glymour, C., & Cooper, G. (1999). Computation,
Causation, Discovery. AAAI press/The MIT
press, Menlo Park.
Lauritzen, S. (1989). Lectures on contingency tables
{3rd edition). Aalborg University.
Lauritzen, S., & Spiegelhalter, D. (1988). Local com­
putations with probabilities on graphical struc­
tures and their application to expert systems
(with discussion). Journal Royal Statistical So­
ciety, B, 50, 157-224.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

(1988). Probabilistic reasoning in intelligent
systems: Networks of plausible inference. Mor­

Pearl, J.

gan Kaufmann, San Mateo, California.
Pearl, J., & Paz, A. (1987). Graphoids: a graph based
logic for reasoning about relevancy relations. In
Boulay, B., Hogg, D., & Steel, L. ( Eds.) , Ad­
vances in artificial intelligence-//, pp. 357-363.
North-Holland, Amsterdam.
Settimi, R., & Smith, J. (1999). Geometry, moments
and bayesian networks with hidden variables. In

Proceedings of the seventh international work­
shop on Artificial Intelligence and Statitics, pp.
293-298 San Francisco. Morgan Kaufmann.
Spohn, W. (1980). Stochastic independence, causal
independence, and shieldability. Journal philo­
sophical logic, 9, 73-99.
Studeny, M. (1992). Conditional independence have
no finite complete characterization. In Trans­
actions of the 11th Prague conference on infor­
mation theory, statistical decision functions and
random processes, pp. 3-16 Prague. Academia.
Whittaker, J. (1990). Graphical models in applied mul­
tivariate statistics. John Wiley and Sons, Chich­
ester.

23

