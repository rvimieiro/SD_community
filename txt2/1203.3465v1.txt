The compilation of Bayesian networks is always considered as an important area. Recently, researchers

Salem Benferhat
CRIL-CNRS
University of Artois
France, 62307
benferhat@cril.univ-artois.fr

Rolf Haenni
RISIS
Bern University
Switzerland, CH-2501
rolf.haenni@bfh.ch

have been interested in various kinds of exact and approximate Bayesian networks inference algorithms using compilation techniques (Darwiche, 2003) (Chavira
and Darwiche, 2005) (Wachter and Haenni, 2007), etc.
Despite the importance of possibility theory, there is
no compilation that has been proposed for possibilistic networks. This paper analyzes this issue by first
adapting well-known compilation-based probabilistic
inference approaches, namely the arithmetic circuit
method (Darwiche, 2003) and the logical compilation
of Bayesian Networks (Wachter and Haenni, 2007).
Both of them are based on a networkâ€™s encoding into a
logical representation and a compilation into a target
compilation language, namely Î -DNNF. From there,
all possible queries are answered in polynomial time.
The third method exploits results obtained on one
hand in (Benferhat et al., 2002) that transforms a minbased possibilistic network into a possibilistic knowledge base, and on the other hand results obtained regarding compilation of possibilistic bases (Benferhat
et al., 2007) in order to assure inference in polytime.
This method that is purely possibilistic is flexible since
it permits to exploit efficiently all the existing propositional compilers.
The rest of this paper is organized as follows: Section 2 gives a briefly background on possibility theory, possibilistic logic, possibilistic networks and introduces some compilation concepts. Section 3 is dedicated to possibilistic adaptations of compilation-based
probabilistic inference methods. Section 4 presents a
new inference method in possibilistic networks using
compiled possibilistic knowledge bases. Experimental
study is presented in Section 5.

2
2.1

BASIC CONCEPTS
POSSIBILITY THEORY

This subsection briefly recalls some elements of possibility theory, for more details we refer to (Dubois and
Prade, 1988). Let V = {X1 , X2 , ..., XN } be a set of

variables. We denote by DXi = {x1 , .., xn } the domain
associated with the variable Xi . By xi we denote any
instance of Xi . â„¦ denotes the universe of discourse,
which is the Cartesian product of all variable domains
in V . Each element Ï‰ âˆˆ â„¦ is called a state of â„¦.
The notion of possibility distribution denoted by Ï€ is
a mapping from the universe of discourse to the unit
interval [0, 1]. To this scale, two interpretations can be
attributed, a quantitative one when values have a real
sense and a qualitative one when values reflect only an
order between the different states of the world. This
paper focuses on the qualitative interpretation of possibility theory.
Given a possibility distribution Ï€, we can define a mapping grading the possibility measure of an event Ï† âŠ† â„¦
by Î (Ï†) = maxÏ‰âˆˆÏ† Ï€(Ï‰). Î  has a dual measure which
is the necessity measure N (Ï†) = 1 âˆ’ Î (Â¬Ï†).
Conditioning consists in modifying our initial knowledge, encoded by a possibility distribution Ï€, by the
arrival of a new certain piece of information Ï† âŠ† â„¦.
The qualitative interpretation of the scale [0, 1] leads
to the well known definition of min-conditioning (Hisdal, 1978), (Dubois and Prade, 1988):

Î (Ïˆ âˆ§ Ï†) if Î (Ïˆ âˆ§ Ï†) < Î (Ï†)
Î (Ïˆ | Ï†) =
(1)
1
otherwise
2.2

POSSIBILISTIC LOGIC

Possibilistic logic (Dubois et al., 1994) handles qualitative uncertainty in a logical setting. A possibilistic
logic formula is a pair (p, a) where p is a propositional
formula and a its uncertainty degree which estimates
to what extent it is certain that p is true. The higher
is the weight, the more certain is the formula. A possibilistic knowledge base Î£ is made up of a finite set
of weighted formulas, i.e.,
Î£ = {(pi , ai ), i = 1, .., n}

(2)

where ai is the lower bound on N (pi ).
Each possibilistic knowledge base induces a unique
possibility distribution such that âˆ€ Ï‰ âˆˆ â„¦ and âˆ€
(pi , ai ) âˆˆ Î£:

1
if Ï‰ |= pi
Ï€Î£ (Ï‰) =
(3)
1 âˆ’ max {ai : Ï‰ 2 pi } otherwise
where |= is propositional logic entailment.
2.3

POSSIBILISTIC NETWORKS

A min-based possibilistic network over a set of variables V , denoted by Î Gmin is composed of:
- a graphical component that is a DAG (Directed

Acyclic Graph) where nodes represent variables and
edges encode the links between the variables. The
parent set of a node Xi is denoted by Ui =
{Ui1 , Ui2 , ..., Uim }. For any ui of Ui we have ui =
{ui1 , ui2 , ..., uim } where m is the number of parents of
Xi . In what follows, we use xi , ui , uij to denote, respectively, possible instances of Xi , Ui and Uij .
- a numerical component that quantifies different links.
For every root node Xi (Ui = âˆ…), uncertainty is represented by the a priori possibility degree Î (xi ) of each
instance xi âˆˆ DXi , such that maxxi Î (xi ) = 1. For the
rest of the nodes (Ui 6= âˆ…) uncertainty is represented
by the conditional possibility degree Î (xi |ui ) of each
instances xi âˆˆ DXi and ui âˆˆ DUi . These conditional
distributions satisfy the following normalization condition: maxxi Î (xi |ui ) = 1, for any ui .
The set of a priori and conditional possibility degrees
in a min-based possibilistic network induce a unique
joint possibility distribution defined by the following
chain rule:
Ï€min (X1 , .., XN ) = min

i=1..N

2.4

Î (Xi | Ui )

(4)

COMPILATION CONCEPTS

A target compilation language is a class of formulas
which is tractable for a set of transformations and
queries. Compilation languages are compared in terms
of their spatial efficiency via the succinctness criteria and also in terms of the set of logical queries and
transformations they support in polynomial time (see
(Darwiche and Marquis, 2002) for more details).
Within the most effective target compilation languages, we cite the Decomposable Negation Normal
Form (DNNF) (Darwiche, 2001). This language is universal and presents a number of properties (determinism, smoothness, etc.) that makes it of a great interest.
It supports a rich set of polynomial-time logical operations. To define DNNF, the starting point is Negation
Normal Form (NNF) which is a set of propositional
formulas where possible connectives are conjunctions,
disjunctions and negations. A set of important properties may be imposed to NNF, such that:
- Decomposability: the conjuncts of any conjunction in
NNF do not share variables.
- Determinism: two disjuncts of any disjunction in
NNF are logically contradictory.
- Smoothness: the disjunct of any disjunction in NNF
mentions the same variables.
These properties lead to a number of interesting subsets of NNF. Within these subsets, the language
DNNF (Darwiche, 2001) is one of the most effective
target compilation languages that supports the decomposability. We can also mention, the d-DNNF sat-

isfying determinism, sd-DNNF satisfying smoothness
and determinism, etc. Each compilation language supports some queries and transformations in polynomial
time. In what follows we are in particular interested
by conditioning and forgetting transformations (Darwiche and Marquis, 2002).

âˆ¨ and âˆ§ as max and min operators, respectively). A
sentence in Î -sd-DNNF is a sentence in Î -DNNF satisfying decomposability, determinism and smoothness.

3

In (Darwiche, 2003), authors have focused on inference in compiled Bayesian networks. The main idea is
based on representing the network using a polynomial
and then retrieving answers to probabilistic queries by
evaluating and differentiating the polynomial. This
latter itself is exponential in size, so it has been represented efficiently using an arithmetic circuit that can
be evaluated and differentiated in time and space linear in the circuit size. In what follows, we propose
a direct adaptation of this method in the possibilistic setting. Given a min-based possibilistic network,
we first encode it using a possibilistic function fmin
defined by two types of variables:

POSSIBILISTIC ADAPTATIONS
OF COMPILATION-BASED
PROBABILISTIC INFERENCE
METHODS

There are several compilation methods which handle
the inference problem in probabilistic graphical models. In this section, we first propose an adaptation
of the arithmetic circuit method of (Darwiche, 2003).
Then we will study one of its variants proposed in
(Wachter and Haenni, 2007), namely the logical compilation of Bayesian Networks.
DNNF has been introduced for propositional language.
Recall that in qualitative possibility theory, we basically manipulate two main operators Max and Min.
These operators fully make sense when we deal with
qualitative plausibility ordering. Therefore, we propose to define concepts of Î -DNNF (resp. Î -d-DNNF,
Î -sd-DNNF) as adaptations of the DNNF language
(resp. d-DNNF, sd-DNNF) (Darwiche, 2001) in the
possibilistic setting (definition 1).
Definition 1. A sentence in Î -DNNF is a rooted
DAG where each leaf node is labeled with true, false
or variableâ€™s instances and each internal node is labeled with max or min operators and can have arbitrarily several children. Roughly speaking, Î -DNNF is
the same as the classical DNNF although its operators
are max and min instead of âˆ¨ and âˆ§, respectively.
Example 1. Figure 1 depicts a sentence in Î -DNNF.
Consider the Min-node (root) in this figure. This node has
two children, the first contains variables A, B while the
second contains variables C, D. This node is decomposable
since its two children do not share variables.

3.1

INFERENCE USING POSSIBILISTIC
CIRCUITS

â€¢ Evidence indicators: for each variable Xi in the
network , we have a variable Î»xi for each instance
xi âˆˆ DXi .
â€¢ Network parameters: for each variable Xi and its
parents Ui in the network, we have a variable
Î¸xi |ui for each instance xi âˆˆ DXi and ui âˆˆ DUi .
fmin = max
x

min
(xi ,ui )âˆ¼x

Î»xi Î¸xi |ui

(5)

where x represents instantiations of all network variables and ui âˆ¼ x denotes the compatibility relationship among ui and x. The possibilistic function fmin
of a possibilistic network represents the possibility distribution and allows to compute possibility degrees of
variables of interest. Namely, for any piece of evidence
e which is an instantiation of some variables E in the
network, we can instantiate fmin as it returns the possibility of e, Î (e) (Definition 2 and Proposition 1).
Definition 2. The value of the possibilistic function
fmin at evidence e, denoted by fmin (e), is the result of
replacing each evidence indicator Î»xi in fmin with 1 if
xi is consistent with e, and with 0 otherwise.
Proposition 1. Let Î Gmin be a possibilistic network
representing the possibility distribution Ï€ and having
the possibilistic function fmin . For any evidence e, we
have fmin (e) = Ï€(e).

Figure 1: A sentence in Î -DNNF.

A sentence in Î -d-DNNF is a sentence in Î -DNNF
satisfying decomposability and determinism (viewing

Let figure 2 be the min-based possibilistic network
used throughout the paper.
The possibilistic function of the network in figure 2
has 8 terms corresponding to the 8 instantiations of
variables F, B, D. Two of these terms are as follows:

is outlined by algorithm 1. Note that the suffix P F is
added to signify that this method uses a possibilistic
function (fmin ) before ensuring the CNF encoding.
Algorithm 1: Inference using Î -DNNF (Î -DNNFP F )

Figure 2: Example of Î Gmin .

fmin = max(min(Î»d1 , Î»f1 , Î»b1 , Î¸d1 |f1 ,b1 , Î¸f1 , Î¸b1 ); min
(Î»d1 , Î»f2 , Î»b1 , Î¸d1 |f2 ,b1 , Î¸f2 , Î¸b1 ); Â· Â· Â· )
If the evidence e = (d1 , b1 ) then fmin (d1 , b1 ) is obtained by applying the following substitutions to fmin :
Î»d1 = 1, Î»d2 = 0, Î»b1 = 1, Î»b2 = 0, Î»f1 = Î»f2 = 1. This
leads to Î (e) = 0.7.
The possibilistic function fmin is then encoded on a
propositional theory (CNF) using Î»xi and Î¸xi |ui . For
each network variable Xi , the encoding contains the
following clauses:
Î»xi âˆ¨ Î»xj
(6)
Â¬Î»xi âˆ¨ Â¬Î»xj , i 6= j

(7)

Moreover, for each propositional variable Î¸xi |ui , the
encoding contains the clause:
Î»xi âˆ§ Î»ui1 âˆ§ . . . âˆ§ Î»uim â†” Î¸xi |ui

(8)

The CNF encoding, denoted by Kfmin recovers the
min-joint possibility distribution (proposition 2).
Proposition 2. The CNF encoding Kfmin of a possibilistic network encodes the joint distribution of given
network.
Once the CNF encoding is accomplished, it is then
compiled into a Î -DNNF, from which we extract the
possibilistic circuit Î¶p (definition 3) that implements
the encoded fmin .
Definition 3. A possibilistic circuit Î¶p encoded by a
Î -DNNF sentence Î¾ c is a DAG in which leaf nodes
correspond to circuit inputs, internal nodes correspond
to max and min operators, and the root corresponds to
the circuit output.
As in the probabilistic case (Darwiche, 2003), this circuit can be used for linear-time inference. More precisely, computing the possibility degree of an event
consists on evaluating Î¶p by setting each evidence indicator Î»x to 1 if the event is consistent with x, to 0
otherwise and applying operators in a bottom-up way.
This possibility degree corresponds exactly to the one
computed from the min-joint possibility distribution
(proposition 3). This method referred to Î -DNNFP F

Data: Î Gmin , instance of interest x, evidence e
Result: Î (x|e)
begin
Compilation into Î -DNNF
Encode Î Gmin into fmin using equation 5
EncodeCNF of Î Gmin into Î¾ using equations 6, 7, 8
Compile Î¾ into Î¾ c
Î¶p â† Possibilistic Circuit of Î¾ c
Inference
Applying Operators on Î¶p
Î (x, e) â† Root Value (Î¶p ; (x,e))
Î (e) â† Root Value (Î¶p ; e)
if Î (x, e) â‰º Î (e) then Î (x|e) â† Î (x, e)
else Î (x|e) â† 1
return Î (x|e)
end

Proposition 3. Let Î Gmin be a possibilistic network.
Let Ï€min be a joint distribution obtained by chain rule.
Then for any a âˆˆ Da and e âˆˆ DE , we have Î (A =
a|E = e) = Î min (A = a|E = e) where Î min (A =
a|E = e) is obtained from Ï€min using equation 1 and
Î (A = a|E = e) is obtained from algorithm 1.
The key point to observe here is that this approach
can handle possibilistic circuits of manageable size as
in the probabilistic case since some possibility values
may have some specific values; for instance, whether
they are equal to 0 or 1, and whether some possibilities are equal. In this case, we can say that the
network exhibit some local structure. By exploiting
it, the produced circuits can be smaller. In fact, the
normalization constraint relative to the initial network
will mean that we will have several values equal to 1.
Thus the idea is to make an advantage from such a
local structure which has a particular behavior with
the max operator in order to construct more compact
possibilistic circuits w.r.t. standard ones as stated by
the following proposition:
Proposition 4. Let N bposs and N bproba be the number of clauses in the possibilistic and probabilistic
cases, respectively. Then N bposs â‰¤ N bproba .
Note that for particular situations where probability
values are 1 or 0, we have N bposs = N bproba , otherwise
N bposs â‰º N bproba .
Example 2. To illustrate algorithm 1 we will consider
the min-based possibilistic network represented in figure 2.
We are looking for Î (f2 |d1 ) with f2 as instance of interest and d1 as evidence. First, we encode the network as
a possibilistic function and encode it on CNF. This latter is then compiled into Î -DNNF from which a possibilistic circuit is extracted. The possibility degree Î (f2 |d1 ) is
computed using this circuit in polynomial time. For instance, Î (f2 , d1 ) is computed using Î¶p by just replacing

Î»f2 = Î»d1 = Î»b1 = Î»b2 = 1 and applying possibilistic
operators in a bottom-up way as shown in figure 3. Hence,
Î (f2 |d1 ) = Î (f2 , d1 ) = 0.4 since Î (f2 , d1 ) = 0.4 â‰º 1.

from a function fÏˆ encoding the CNF. Then, we have
Ï€min (xi , ..., xj ) = Î (xi , ..., xj ), i.e. fÏˆ recovers the
min-joint possibility distribution Ï€min .
Comparing theoretically the probabilistic and the possibilistic case allows us to deduce the following proposition:
Proposition 6. The possibilistic encoding of a possibilistic network given by KÏˆ (equation 10) is more
compact than the probabilistic encoding given in
(Wachter and Haenni, 2007).
In fact, the number of variables used in KÏˆ is less than
the one used in (Wachter and Haenni, 2007). In particular for parameters, our approach uses one variable
per different weight, while in the probabilistic encoding
one variable per parameter. For each clause in KÏˆ
there exists a clause of the same size in the probabilistic encoding. The converse is false.

Figure 3: Inference using the possibilistic circuit (Î¶p ).

3.2

INFERENCE USING POSSIBILISTIC
COMPILED REPRESENTATIONS

DNNF plays an interesting role in compiling propositional knowledge bases. It has been used to compile
probabilistic networks. More precisely in (Wachter
and Haenni, 2007), authors have been interested in
performing a CNF logical encoding of the probability distribution induced by a bayesian network, then
a compilation phase from CNF to d-DNNF. In this
section, we propose to adapt this encoding in the possibilistic setting by taking into consideration the local
structure aspect. This allows to reduce the number
of additional variables comparing to the probabilistic
encoding. Let âˆ† be propositions linked to networkâ€™s
variables and let Î¸ be propositions linked to the possibility distribution entries (equal to 1). We start by
looking at the possibility distribution encoding. The
logical representation of a network variable Xi is defined by: ÏˆXi =

^
^

ui1 âˆ§ Â· Â· Â· âˆ§ uim âˆ§ Î¸xi |ui â†’ xi
(9)
ui

Î¸xi |ui âˆˆâ„¦Î¸X

i

|ui

By taking the conjunction of all logical representations
of variables, we obtain the networkâ€™s representation Ïˆ
as follows:
^
Ïˆ=
ÏˆXi
(10)
Xi âˆˆâˆ†

The CNF encoding, denoted by KÏˆ indeed recovers
the min-joint possibility distribution (proposition 5).
Proposition 5. Let Ï€min be the joint possibility distribution obtained using the chain rule with the minimum operator and Î  be the possibility degree computed

Once the qualitative network is encoded by KÏˆ , it is
compiled into a compilation language that supports
the transformations conditioning and forgetting and
the query possibilistic computation. This language is
Î -DNNF (proposition 7). Therefore, the CNF encoding is first compiled, and the resulting Î -DNNF is then
used to compute efficiently, i.e. in polynomial time
a-posteriori possibility degrees (proposition 8). This
method referred to Î -DNNF is outlined by algo. 2.
Proposition 7. Î -DNNF supports conditioning, forgetting and possibilistic computation.
Algorithm 2: Inference using Î -DNNF
Data: Î Gmin , instance of interest x, evidence e
Result: Î (x|e)
begin
Compilation into Î -DNNF
EncodeCNF of Î Gmin into Ïˆ using equation 10
Compile Ïˆ into Ïˆpc
Inference
v1 â† Explore Î -DNNF(x âˆ§ e, Ïˆpc )
v2 â† Explore Î -DNNF(e, Ïˆpc )
if v1 â‰º v2 then Î (x|e) â† v1 else Î (x|e) â† 1
return Î (x|e)
end

Proposition 8. Let Î Gmin be a possibilistic network.
Let Ï€min be a joint distribution obtained by chain rule.
Then for any a âˆˆ Da and e âˆˆ DE , we have Î (A =
a|E = e) = Î min (A = a|E = e) where Î min (A =
a|E = e) is obtained from Ï€min using equation 1 and
Î (A = a|E = e) is obtained from algorithm 2.
Example 3. Let us illustrate algorithm 2. In fact, Ïˆ of
the network of figure 2 is : Ïˆ = ÏˆF âˆ§ ÏˆB âˆ§ ÏˆD = {(Î¸1 âˆ¨
f2 ) âˆ§ (Î¸2 âˆ¨ b1 ) âˆ§ (f2 âˆ¨ b2 âˆ¨ Î¸2 âˆ¨ d1 ) âˆ§ (f2 âˆ¨ b1 âˆ¨ Î¸1 âˆ¨ d2 ) âˆ§
(f1 âˆ¨ b2 âˆ¨ Î¸3 âˆ¨ d2 ) âˆ§ (f1 âˆ¨ b1 âˆ¨ Î¸4 âˆ¨ d2 )} such as Î¸1 , Î¸2 , Î¸3
and Î¸4 correspond respectively to 0.8, 0.7, 0.4 and 0.2.
To compute Î (f2 |d1 ), we should first compute Î (f2 , d1 ) using algorithm 3. The first step is to check if we have at least

Algorithm 3: Explore Î -DNNF
Data: a set of instances x, compiled representation Ïˆpc
Result: Î (x)
begin
if âˆ€ xi âˆˆ x, Î¸xi |Ui is not a leaf node then
Î (x) â† 1
else
y= {xi | âˆ€, Î¸xi |Ui is a leaf node âˆ€ Ui âŠ† x}
c
Ïˆp|y
â† Condition Ïˆpc on y
c
Ïˆpc â†“|y â† Forget âˆ† from Ïˆp|y
c
Applying Operators on Ïˆp â†“|y
Î (x) â† Root Value of Ïˆpc â†“|y
return Î (x)
end

one Î¸ as a leaf node. In this example, we have Î¸d1 |f2 ,b1
and Î¸d1 |f2 ,b2 as leaf nodes, hence conditioning should be
performed. Then, a computation step is required by applying in a bottom-up way Min and Max operators on the
forgotten Î -DNNF. Therefore, Î (f2 |d1 ) = Î (f2 , d1 ) = 0.4.

4

NEW POSSIBILISTIC
INFERENCE ALGORITHM

In (Benferhat et al., 2002), authors have been interested in the transition of possibilistic networks into
possibilistic logic bases. The starting point is that the
possibilistic base associated to a possibilistic network
is the result of the fusion of elementary bases. Definition 4 presents the transformation of a min-based possibilistic network into a possibilistic knowledge base.
Definition 4. A binary variable Xi of a possibilistic network can be expressed by a local possibilistic knowledge base as follows:
Î£Xi
=
{(Â¬xi âˆ¨ Â¬ui , Î±i ) : Î±i = 1 âˆ’ Ï€(xi |ui ) 6= 0}. The possibilistic knowledge base of the whole network is: Î£min =
Î£X1 âˆª Î£X2 âˆª Â· Â· Â· âˆª Î£Xn .
In another angle, researchers in (Benferhat et al., 2007)
have focused on the compilation of bases under the
possibilistic logic policy in order to be able to process
inference from it in a polynomial time. The combination of these methods allows us to propose a new
alternative approach to possibilistic inference. This is
justified by the fact that the possibilistic logic reasoning machinery can be applied to directed possibilistic
networks (Benferhat et al., 2002).
The idea is to encode the possibilistic knowledge base
Î£min into a classical propositional base (CNF). Let
A = {a1 , ..., an } with a1  ...  an the different
weights used in Î£min . A set of additional propositional variables, denoted by Ai , which correspond exactly to the number of different weights, are incorporated and for each formula Ï†i , ai will correspond the
propositional formula Ï†i âˆ¨Ai . Hence, the propositional

encoding of Î£min , denoted by KÎ£ is defined by:
KÎ£ = {Ï†i âˆ¨ Ai : (Ï†i , ai ) âˆˆ Î£min }

(11)

The following proposition shows that the CNF encoding KÎ£ recovers the min-joint possibility distribution.
Proposition 9. Let Ï€min be the joint possibility
distribution obtained using the chain rule with the
minimum-based conditioning and let KÎ£ be the propositional base associated with the possibilistic network
given by equation 11. Let Ï†i be a propositional formula associated with a degree ai . Then âˆ€Ï‰ âˆˆ â„¦,
Î (Ï‰) = 1 iff {Â¬A1 , ..., Â¬An } âˆ§ Ï‰ âˆ§ KÎ£ is consistent.
Î (Ï‰) = ai iff {Â¬A1 , ..., Â¬Ai } âˆ§ Ï‰ âˆ§ KÎ£ is inconsistent
and {Â¬A1 , ..., Â¬Aiâˆ’1 } âˆ§ Ï‰ âˆ§ KÎ£ is consistent.
The CNF encoding KÎ£ is then compiled into a target
compilation language in order to compute a-posteriori
possibility degrees in an efficient way. Here, we are
interested in a particular query useful for possibilistic networks, namely what is the possibility degree of
an event A = a given an evidence E = e? Therefore, we propose to adapt the algorithm given in (Benferhat et al., 2007) in order to respond to this query
as shown by algorithm 4. Proposition 10 shows that
the possibility degree computed using algorithm 4 and
the one computed using the min-based joint possibility distribution are equal. Note that this approach is
qualified to be flexible since it takes advantage of existing propositional knowledge bases compilation methods (Benferhat et al., 2007). This method referred to
DNNF-PKB is outlined by algorithm 4.
Algorithm 4: Inference using DNNF
Data: Î Gmin , instance of interest x, evidence e
Result: Î (x|e)
begin
Transformation into KÎ£
Transform Î Gmin into Î£min using definition 4
Transform Î£min into KÎ£ using equation 11
Inference
c
KÎ£
â† T arget(KÎ£ )
c
K â† KÎ£
StopCompute â† false
iâ†1
Î (x|e) â† 1
while (K 2 Ai âˆ¨ Â¬e) and (i â‰¤ k) and (StopCompute=false) do
K â† condition (K, Â¬Ai )
if K Â¬x then
StopComputeâ† true
Î (x|e) â† 1-degree(i)
else
iâ†i+1
return Î (x|e)
end

Proposition 10. Let Î Gmin be a possibilistic network. Let Ï€min be a joint distribution obtained by

chain rule. Then for any a âˆˆ Da and e âˆˆ DE , we
have Î (A = a|E = e) = Î min (A = a|E = e) where
Î min (A = a|E = e) is obtained from Ï€min using equation 1 and Î (A = a|E = e) is obtained from Algo. 4.

Indeed, in Î -DNNFP F , we associate propositional
variables not only to possibility degrees (parameters),
but also to each value xi of Xi . While in DNNF-PKB
only m new variables are added (one variable per different degree).

Example 4. To illustrate algorithm 4 we will consider

Let us now analyze these three approaches from experimental points of view. Our experimentation is
performed on random possibilistic networks. More
precisely, we have compared DNNF-PKB and Î DNNFP F on 100 possibilistic networks having from 10
to 50 nodes. As mentioned that the approaches focus
mainly on encoding the possibilistic network as a CNF
then compile it into the appropriate language, hence,
it should be interesting to compare the CNF parameters (the number of variables and clauses) and the
DNNF parameters (the number of nodes and edges)
for the two methods.

the min-based possibilistic network represented in figure 2.
The CNF encoding is as follows :
KÎ£
=
(d2 âˆ¨ f1 âˆ¨ b2 âˆ¨ A1 ) , (b1 âˆ¨ A2 ) , (d1 âˆ¨ f2 âˆ¨ b2 âˆ¨ A2 ) ,
(f2 âˆ¨ A3 ) , (d2 âˆ¨ f2 âˆ¨ b1 âˆ¨ A3 ) , (d2 âˆ¨ f1 âˆ¨ b1 âˆ¨ A4 )
such
as A1 (0.8), A2 (0.6), A3 (0.3) and A4 (0.2) are
propositional variables followed by their weights under
c
brackets. Compiling KÎ£ into DNNF results in: KÎ£
=
((b2 âˆ§ A2 ) âˆ§ [(A3 âˆ§ f1 ) âˆ¨ (f2 âˆ§ [d2 âˆ¨ (A4 âˆ§ d1 )])]) âˆ¨ (b1 âˆ§
[[f2 âˆ§ (d2 âˆ¨ (A1 âˆ¨ d1 ))] âˆ¨ [(f 1 âˆ§ A3 ) âˆ§ (d1 âˆ¨ (A2 âˆ§ d2 ))]]).
c
The computation of Î (f2 |d1 ) using KÎ£
requires two
iterations. Therefore, Î (f2 |d1 ) = 1 âˆ’ degree(2) = 0.4.

Due to the compilation step, this algorithm runs in
polynomial time. Moreover, the number of additional
variables is low since it corresponds exactly to the
number of priority levels existing in the base.

5

COMPARATIVE AND
EXPERIMENTAL STUDIES

The paper analyzes three compilation-based methods,
namely DNNF-PKB, Î -DNNF and Î -DNNFP F . The
first dimension that differentiates the three approaches
proposed in this paper is the CNF encoding. It consists
of specifying the number of variables and clauses per
approach.
The CNF of DNNF-PKB is based on encoding Â¬x
where x is an instance of interest having a possibility degree different from 1. In Î -DNNF, we write
implications relative to instances having 1 as possibility degree. We can notice that the local structure
in both methods is exploited in semantically different
ways. In DNNF-PKB, the encoding uses the number
of different weights as the number of additional variables while the Î -DNNF encoding uses the number of
the non-redundant possibility degrees different from 1
in the distributions. Regarding the number of clauses,
both methods handle possibility degrees different from
1. This leads us to the following proposition:

5.1

CNF PARAMETERS

First we propose to test the CNF encodings characterized by the number of variables and the number of
clauses. Regarding DNNF-PKB, the number of additional variables correspond to the number of weights
which are different. While in Î -DNNFP F , variables
are both those associated to the possibility degrees
of each distribution and those to variableâ€™s instances.
The number of clauses for each method is related to
the CNF encoding itself. Figure 4 shows the results of
this experimentation. Each approach is characterized
by a curve for the average number of variables and a
curve for the average number of clauses. It is clear
that the higher the number of nodes considered in the
possibilistic network, the higher the number of variables and clauses. Figure 4 shows that DNNF-PKB
has the lower number of variables and clauses comparing to Î -DNNFP F , which confirms the theoretical
results detailed above.

Proposition 11. The CNF encodings of DNNF-PKB
and Î -DNNF have the same number of variables and
clauses.
The CNF encoding of Î -DNNFP F is different from
the ones of DNNF-PKB and Î -DNNF. Proposition 12
shows the difference between Î -DNNFP F and DNNFPKB in terms of number of variables and clauses.
Proposition 12. The number of variables and clauses
in Î -DNNFP F is more important than those in
DNNF-PKB.

Figure 4: CNF parameters.

5.2

DNNF PARAMETERS

Once we obtain the CNF encodings, it is important
to compare the number of nodes and edges for each
compiled base. Figure 5 represents the average size of
the compiled bases for the two methods in terms of
nodes and edges numbers. We remark that the number of nodes and edges depends deeply on CNF parameters. More precisely, the number of nodes and
edges in DNNF-PKB is considered narrow comparing
to Î -DNNFP F . This can be explained by the lower
number of variables and clauses on CNFs and the local structure which shrinks the sizes of compiled bases.
Comparing DNNF-PKB to Î -DNNFP F , the behavior
of DNNF-PKB is important.

(Pearl, 2000) (Benferhat and Smaoui, 2007).
Acknowledgements
We thank the anonymous reviewers for many interesting
comments and suggestions. Also, we wish to thank Mark
Chavira for our valuable discussions on this subject. The
third author would like to thank the project ANR Placid.

References
Benferhat, S., Dubois, D., Garcia, L., and Prade, H.
(2002). On the transformation between possibilistic
logic bases and possibilistic causal networks. International Journal of Approximate Reasoning, 29(2):135â€“
173.
Benferhat, S. and Smaoui, S. (2007). Possibilistic causal
networks for handling interventions: A new propagation algorithm. In AAAI, pages 373â€“378.
Benferhat, S., Yahi, S., and Drias, H. (2007). On the compilation of stratified belief bases under linear and possibilistic logic policies. In International Journal of
Approximate Reasoning, pages 2425â€“2430.
Chavira, M. and Darwiche, A. (2005). Compiling bayesian
networks with local structure. In Proceedings of the
19th International Joint Conference on Artificial Intelligence (IJCAI), pages 1306â€“1312.
Cooper, G. F. (1990). The computational complexity of
probabilistic inference using bayesian belief networks
(research note). Artif. Intell., 42(2-3):393â€“405.

Figure 5: DNNF parameters.

6

CONCLUSION

This paper proposes algorithms that ensure inference
in possibilistic networks using compilation techniques.
First, we have proposed possibilistic adaptations of
two compilation-based probabilistic methods, namely
Î -DNNFP F and Î -DNNF. Then we have developed a
new possibilistic inference method DNNF-PKB based
on a transformation phase from a possibilistic network
into a compiled possibilistic knowledge base. We theoretically show that DNNF-PKB and Î -DNNF share
the same number of variables and clauses even they
are based on different computations in their inference
process since the first is based on necessity degrees and
the second on possibility degrees. We have also shown
that DNNF-PKB is more compact than Î -DNNFP F
which proves the importance of the possibilistic setting
versus the probabilistic setting. All these results were
confirmed by experimental results. A future work will
be to compare these algorithms with the well-known
junction tree propagation algorithm. Another future
work is to exploit results of this paper in order to infer
efficiently interventions in possibilistic causal networks

Darwiche, A. (2001). Decomposable negation normal form.
Journal of the ACM, 48(4):608â€“647.
Darwiche, A. (2003). A differential approach to inference
in bayesian networks. Journal of the ACM, 50(3):280â€“
305.
Darwiche, A. and Marquis, P. (2002). A knowledge compilation map. Journal of Artificial Intelligence Research,
17:229â€“264.
Dubois, D., Lang, J., and Prade, H. (1994). Possibilistic logic. In Handbook on Logic in Artificial Intelligence and Logic Programming, volume 3, pages 439â€“
513. Oxford University press.
Dubois, D. and Prade, H. (1988). Possibility theory:An
approach to computerized, Processing of uncertainty.
Plenium Press, New York.
Hisdal, E. (1978). Conditional possibilities independence
and non interaction. Fuzzy Sets and Systems, 1.
Pearl, J. (2000). Causality: Models, reasonning and inference. Cambridge University Press.
Wachter, M. and Haenni, R. (2007). Logical compilation
of bayesian networks with discrete variables. In European Conf. on Symbolic and Quantitative Approaches
to Reasoning with Uncertainty, pages 536â€“547.

