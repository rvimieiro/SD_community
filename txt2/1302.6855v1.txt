OR-gate model have been utilized to reduce the
complexity of knowledge acquisition (Pearl 1988, Hen­
rion 1987) as well as the complexity of inference (Kim
and Pearl 1983). Beckerman (1993) is the first re­
searcher to try to formally define intercausal indepen­
dence. His definition is temporal in nature. Based
on this definition, a graph-theoretic representation of
intercausal independence has been proposed.

noisy

This paper attempts a constructive definition. Our
definition is based on the following intuition about in­
tercausal independence: a number of causes contribute
independently to an effect and the total contribution
is a combination of the individual contributions. The

definition allows us to represent intercausal indepen­
dence by factorization of conditional probability, in a
way similar to that conditional independence can be
represented by factorization of joint probability.
The advantages of our factorization-of-conditional­
probability representation of intercausal independence
over Beckerman's graph-theoretic representation are
twofold. Firstly, the symmetric nature of intercausal
independence is retained in our representation. Sec­
ondly and more importantly, our representation allows
one to make full use of intercausal independence to re­
duce inference complexity.
While Heckerman uses intercausal independencies to
alter the topologies of Bayesian networks, we follow
Pearl (1988) (section 4.3.2) to exploit intercausal in­
dependencies in inference. While Pearl only deals with
the case of singly connected networks, we deal with the
general case.

The rest of this paper is organized as follows. A
constructive definition of intercausal independence is
given in Section 2. Section 3 discusses factorization of
a joint probability into a multiplication of conditional
probabilities, and points out intercausal independence
allows one to further factorize conditional probabili­
ties into "even-smaller" factors. The fact that those
"even-smaller" factors might be combined by opera­
tors other than multiplication leads to the concept of
heterogeneous factorization (HF). After some technical
preparations (Sections 4 and 5), the formal definition
of HF is given in section 6. Section 7 discusses how
to sum out variables from an HF. An algorithm for
computing marginals from an HF is given in Section
8, which is illustrated through an example in Section
9. Related work is discussed in Section 10.
2

CONSTRUCTIVE INTERCAUSAL
INDEPENDENCE

This sections gives a constructive definition of inter­
causal independence. This definition is based on the
following intuition: a number of causes c1, c2, . . . , Cm
contribute independently to an effect e and the total

607

Intercausal Independence and Heterogeneous Factorization

contribution is a combination of the individual contri­
butions.
Let us begin with an example - the noisy OR-gate
model (Good 1961, Pearl 1988, Beckerman 1993). In
this model, there is a random binary variable �i in
correspondence to each c;, which is also binary. �;
depends on c; and is conditionally independent of any
other �i given the c; 's. e is 0 if and only if all the �; 's
are 0, and is 1 otherwise. In formula, e=6 V . . . V �m·
Consider the case when m=2 and consider the condi­
tional probability P(eict, c2). For any value /3; of c;
(i::::1:: , 2), we have

P(e=Oic1 =f3t, c2=fJ2)
= P(�t V�2=0ict:::::/31, c2=/12)
= P(6 :::::Oict =f31)P(6=0ic2=f32),
and

Define ft(e=a:bct=f3t)=deJP(6=a:tict=f3t) and de­
fine !2(e:::::a:2, c2=.B2)=d•J P(6=a:2Jc2=.62). We can
rewrite the above two equations in a more compact
form as follows:
P(e=a:lcl=f3t, c2=.62) =
ft(e=o:t, Ct=f3dh(e=a:2, c2=.B2),

(1)

a1va-,=a

where o:, a:11 and a:2 can be either 0 or
motivates the following definitions.

1.

This example

Let e be a discrete variable and let *• be an
commutative and associative binary operator over
the frame n. - the set of possible values of e. In the previous example, *• is the logic
OR operator V. Let f(e, x1, . .. ,xr,Yt, ... ,y,) and
g( e, Xt, . . . , Xr, Zt, ... , Zt) be two functions, where the
Yi 's are different from the Zj 's. Then, the combination
!®.g off and g is defined as follows: for any value a:
of e,
/(i)eg(e:::::a:, XI, ... , Xr, Yt, . .. , y,, Zt, ..., Zt)
= dej L [f(e=o:t,XJ1 . .. ,Xr,YI, .. . ,y,)

where ®e is the *e-induced combination operator. The
right hand of the equation makes sense because ®e
is commutative and associative. When c1, . . . , em
contribute independently to e, we call e a bastard
variable1• A non-bastard variable is said to be nor­
mal. We also say that f;(e, c;) is (a description of) the
contribution by c; to e.
Intuitively, the base combination operator (e.g. V)
determines how contributions from different sources
are combined, while the induced combination operator
is the reflection of the base operator at the level of
conditional probability.

P(e=llcFf3t, c2=P2)
P(€1 V6=1!c1 =f3t, c2=f32)
P(6=1 let=f3t)P(6=0Jcz=f32)
+P(�t =OJct =f3t)P(6=lic2=f32)
+P(6=1ict=f3t)P(6=llc:�=f3:�).

2:::

Here is our constructive definition of intercausal in­
dependence. We say that c1, ... , Cm contribute in­
dependently to e or e receives contributions indepen­
dently from Ct, ••• , Cm if there exists a commutative
and associative binary operator *• over the frame of
e and real-valued non-negative functions It (e, c1 ), ...,
f m (e, Cm) such that

X

Because of equation (1), the noisy-OR gate model is
an example of constructive intercausal independence,
with the logic OR Vas the base combination operator.
As another example, consider the noisy adder model
(Beckerman 1993). In this model, there is a random
Variable �i in correspondence to each Cj j e; depends
On Cj and is Conditionally independent of any other ej
given the c,'s. The �;'s are combined by the addition
operator "+" to result in e, i.e. e =6 + .. . +em.
To see that e is a bastard variable in this model, let
the base combination operator *e be simply "+" and
let the description of individual contribution /;(e, c;)
be as follows: for any value o: of e and any value f3 of
C; '

/; (e==o:, c;=f3)=def P(e;=aJc;=,B).
Then it is easy to verify

that

equation (3) is

satisfied.

It is interesting to notice the similarity between equa­
tion (3) and the following property of conditional in­
dependence: if a variable x is independent of another
variable z given a third variable y, then there exist
non-negative functions f(x, y) and g(y, z) such that
the joint probability P(x, y, z ) is given by
P(x,y, z)=f(x, y)g(y, z).

(4)

1Those who are familiar with clique tree propagation

We shall refer to *• as the base combination operator
and ®e as the *e-induced combination operator. We
would like to alert the reader that *e combines values
of e, while ®e combines functions of e. It is easy to
see that the induced operator ®e is also commutative
and associative.

may remember that the :first thing to do in constructing

a clique tree from a Bayesian network is to "marry" the

(

parents of each node variable)

1988).

( Lauritzen and Spiegehalter

As implies by the word "bastard", the parents of

a bastard node will not be married. This is because the
conditional probability of a bastard node is factorized into
a bunch of factors, each involving only one parent.

608

Zhang and Poole

In (4) conditional independence allows us to factorize
a joint probability into factors that involve less vari­
ables, while in (3) intercausal independence allows us
to factorize a conditional probability into a bunch of
factors that involve less variables. The only difference
lies in the way the factors are combined.
Conditional independence has been used to reduce in­
ference complexity in Bayesian networks. The rest of
this paper investigates how to use intercausal indepen­
dence for the same purpose.
3

FACTORIZATION OF JOINT
PROBABILITIES

This section discusses factorization of joint probabili­
ties and introduces the concept of heterogeneous fac­
torization (HF).
A fundamental assumption under the theory of proba­
bilistic reasoning is that a joint probability is adequate
for capturing experts' knowledge and beliefs relevant
to a reasoning-under-uncertainty task. Factorization
and Bayesian networks come into play because joint
probability is difficult, if not impossible, to directly
assess, store, and reason with.

Let P(xb x2, ..., xn) be a joint probability over variables x1, x2, ... , Xn. By the chain rule of probabilities,
we have
P(x1, X2,
, Xn)
=P(xi)P(x2lxt) ... P(xnlxl, ..., Xn-1)·
.

•

.

(5)

For any i, there might be a subset 71'; � {x1, ..., :r;_I}
such that X& is conditionally independent of all the
other variables in {x1, ... , X&-d given the variables in
11';, i.e P(x;lxb ..., x;_I)=P(x;l7r;). Equation (5) can
hence be rewritten as
n

P(x1,x2, . . . , Xn)= IT P(x&l7r;).

(6)

&=1

Equation
(6)
factorizes
the
joint proba­
bility P(z 1 , x2, ..., z,. ) into a multiplication of factors
P(xd1Ti)· While the joint probability involves all then
variables, the factors usually involves less than n vari­
ables. This fact implies savings in assessing, storing,
and reasoning with probabilities.
A Bayesian network is constructed from the factoriza­
tion as follows: construct a directed graph with nodes
x1, x2, . . . , :r,. such that there is an arc from :rj to
x; if and only if Xj E 71';, and associate the conditional
probability P(x;l7r;) with the node x;. P(x1,...,Xn) is
said to be the joint probability of the Bayesian network
so constructed. Also nodes in 11'& are called parents of
:r;.

The above factorization is homogeneous in the sense
that all the factors are combined in the same way, i.e
by multiplication.

Figure 1: A Bayesian network, where e1 and e2 re­
ceive contribution independently from their respective
parents.
Let x;1, ... , x;m, be the parents of x;. If x; is a bastard
variable with base combination operator *i, then the
conditional probability P(xd7r;) can be further factor­
ized by

where®; is the *;-induced combination operator. The
fact that ®; might be other than multiplication leads
to the concept of heterogeneous factorization (HF).
The word heterogeneous reflects the fact that differ­
ent factors might be combined in different manners.
As an example, consider the Bayesian network in Fig­
ure 1. The network indicates that P(a, b, c, e1,e2,ea, y)
can be factorized into a multiplication of P(a),
P(b), P(c), P(e1la,b), P(e2la, b, c), P ( eale1 , e2) , and
P(ylea).
Now if the e;'s are bastard variables, then there exist
base combination operators *i (i=l, 2, 3) such that
the conditional probabilities of the e; 's can be further
factorized as follows:
P(e1la, b)
P(e2la,b,c)
P(ealet, e2)

fu(el, a)®dt2(el> b)
/21(e2, a)®2!n(e2, b)®2/2a(e2, c)
!at(ea,el)®a!a2(e3,el)

where fu(et, a), for instance, denotes the contribution
by a to e1, and where the ®i's are the combination
operators respectively induced by the *i's.
The factorization of P(a, b, c, et, e2 , e3, y) into the
factors:
P(a), P(b), P(c), P(ylea), fu(el, a),
(
,
b),
e2, a), /22(e2, b), ha(e2 , c), fat(ea, et),
h1(
!t2 e1
and !a2 (e3 , e2) is called the HF in correspondence to
the Bayesian network in Figure 1. We shall call the
fii 's heterogeneous factors since they might be com­
bined by operators other than multiplication. On the
other hand, we shall say that the factors P(a), P(b),
P(c), and P(yle3) are normal.

Intercausal Independence and Heterogeneous Factorization

609

To prevent I(e1, eD from being mistaken to be the con­
tribution by ei to e1, we shall always make it explicit
that I(e1, e�) is a normal factor, not a heterogeneous
factor.

COMBININ G FACTORS THAT

5

IN V OLVE M OR E THAN ONE
BAS TARD VARIABLE
Even though deputation guarantees that every hetero­

Figure 2: The Bayesian network in Figure
deputation of bastard nodes.

4

1

after the

geneous factor involves only one bastard variable at
the beginning, inference may give rise to factors that
involve more than one bastard variable. In Figure 2,
for instance, summing out the variable a results in a
factor that involves both e1 and e2. This section in­
troduces an operator for combining such factors.
Suppose e1, ... , e�o are bastard variables with base
opera­
combination
tor *t, . . , *k· Let f(et, ...,e,.,xt, ...,xr, Yl, ..., y.)
and g ( et,... ,e�:,xt,····x.
, ,zt, .. . ,zt ) be two func­
tions, where the xi's are normal variables and the yj's

DEPUTATION OF BAS TARD

.

NODES

'

Consider the heterogeneous factor h1( es, e1) from the
previous example. It contains two bastard variables e1
to e3. As we shall see later, it is desirable for every
heterogeneous factor to contain at most one bastard
variable. The concept of deputation is introduced to
guarantee this.

are different from the zr s (they can be bastard as well
as normal variables). Then, the combination f®g of
f and g is defined as follows: for any particular value
a; of e; ,

f®g(el =a:1, .. . ,e�o=O:i:, Xt,.. ., Xr,
Yl,- . . , y,, Zt, . . . , Zt )

Let e be a bastard node in a Bayesian network. The
deputation of e is the following operation: make a copy
e' of e, make the children of e to be children of e',
'
make e a child of e, and set the conditional probability
P(e'le) as follows:

P(e'Ie ) =

{0
1

if e = e'
otherwise

(8)

We shall call e' the deputy of e. We shall also call
P(ele') the deputing function, and rewrite it as I(e, e')
since P(ele') ensures that e and e' be the same.
The Bayesian network in Figure 1 becomes the one in
Figure 2 after the deputation of aU the bastard nodes.
We shall call the latter a a deputation Bayesian net­

work.

Proposition 1 Let N' be a Bayesian network, and let
N' is the Bayesian network obtained from N' by the

deputation of all bastard nodes. Then the joint proba­
bility of N can be obtained from that of N' by summing
out all the deputy variables. 0

In Figure 1, we have the heterogeneous factors
h1(es,el) and f32(es,e2), which involves two bastard
variables. This may cause confusions and is undersir­
able for other reasons, as we shall see soon. After dep­
utation, each heterogeneous factor involves only one
bastard variable. As a matter of fact, fst(es, et) and
fs2(es,e2) have become fst(es,eD and fs2(es,e�).

[f(et =au, . . . , e�o=akl, x1,..., Xr, Yl, , Ys) X
g(e1 =0:121 , ek:::0:1:21 Xt, .. ., Xr1 Zt 1 • • • 1 Zt)]. (9)
.

•

•

.

•

•

A few notes are in order. First, fixing a list of bas­
tard variables and their base combination operators,
one can use the operator® to combined two arbitrary
functions. In the following, we shall always work im­
plicitly with a fixed list of bastard variables, and we
shall refer to ® as the general combination operator.
Second, when

(2).

k

=

1 th is definition reduces to equation

Third, since the base combination operators are com­
mutative and associative, the operator ® is also com­
mutative and associative.
Fourth, when
off and g.

5.1

k

==

0, f®g

is simply the multiplication

Combining all the Heterogeneous Factors
in a Bayesian networks

Equipped with the general combination operator ®,
we now consider combining all the heterogeneous fac­
tors of the Bayesian network in F igure 2. Because of
the third note above, we can combine them in any
order. Let us first combine fu(et, a ) with !t2(e2, b),
!21(e2, a) with h2(e2, b) and hs(e2, c), and fst(es, eD

610

Zhang and Poole

:F

In the following, we shall also say that the
of the function F(X).

We now combine the resulting conditional probabili­
ties. Because of the fourth note, the combination of
P(etia, b), P(e2 la , b, c) , and P(e3lei,e2) is their multi­
plication. So, the combination of all the heterogeneous
factors of the Bayesian network in Figure 2 is simply
the multiplication ofthe conditional probabilities of all
the bastard variables. This is true in general.

Suppose N is a deputation Bayesian network. Sup­
pose :F is the HF that corresponds to N. :F has two
interesting properties.

In a deputation Bayesian network,
the multiplication of the conditional probabilities of all
the bastard variables is the same as the result of com­
bining of all the heterogeneous factors. D

Proposition 2

6.1

is an

HF

with /32(e3, e2). Because of the second note, we have
/H®/t2(et,a,b ) = P(eda,b),
P(e2la,b, c) ,
ht®h2®!23(e2,a,b, c)
P(e31e2,eD.
ht®/32(e3,e2,eD

HF's in Correspondence to Deputation
Bayesian Networks

First, according to Proposition 2 the combination of all
the heterogeneous factors is the multiplication of the
conditional probabilities of all the bastard variables.
Thus, the joint of :F is simply the joint probability of

N.

The joint of the HF that corresponds
to a deputation Bayesian network N is the same as
the joint probability of N.

Proposition 3

Note that in Figure 1, since ht(e3,e1) and h2(e3, e2)
involve two bastard variables, the combination
fn(et,a) ® ... ® f23(e2,c) ® ht(e3,et) ® /a2(e3 ,e2)
would not the same as the multiplication of the condi­
tional probabilities of the bastard variables.

To reveal the second interesting property, let us first
define the concept of tidness. An HF is tidy if for each
bastard variable e, there exists at most one normal
factor that involves e. Moreover, this factor, if exists,
involves only one other variable in addition to e itself.

This is why we need deputation; deputation allows us
to combine the heterogeneous factors by a single com­
bination operator ®, which opens up the possibility of
combining the heterogeneous factors in any order we
choose. This flexibility turns out to be the key to the
method of utilizing intercausal independence we are
proposing in this paper.

An HF that corresponds to a deputation Bayesian net­
work is tidy. For each bastard variable e, I(e, e') is the
only one normal factor that involves e, and this factor
involves only one other variable, namely e'.

6

HETEROGENEOUS
FACTORIZATION

We now formally define the concept of heterogeneous
factorization. Let X be a set of discrete variables. A
heterogeneous factorization (HF) F over X consists of
1. A list e1, .. . , em of variables in X that are said
to be bastard variables. Associated with each bas­
tard variable ei is a base combination operator *i,
which is commutative and associative,
2. A set :Fo of heterogeneous factors, and
3. A set :F1 of normal factors.
We shall write an HF as a quadruplet :F =(X,
{(e1, *t ) , . . . , (em, *m ) } , :Fo, Ft). Variables that are
not bastard are called normal.
In an HF, the combination F0 of all the heterogeneous
factors is given by
(10)
The joint F(X) of an HF is the multiplication of Fa
and all the normal factors. In formula
F=deJ(®Je:F0f)

IT
g€1"1

g.

(11)

Tidy HF's do not have to be in correspondence to a
deputation Bayesian network. As a matter of fact, we
shall start with a tidy HF that corresponds to a dep­
utation Bayesian network, and then sum out variables
from the HF. We shall sum out variables in such a
way such that the tidness is retained. Even though
the HF we start out with corresponds to a deputation
Bayesian network, after summing out some variables,
the resulting tidy HF might no longer correspond to
any deputation Bayesian network.
However, we shall continue to use the terms deputy
variable and deputing function.
7

SUMM ING OUT VARIABLES
FROM TIDY HF'S

Let F(X) be a function. Suppose A is a subset of X.
The projection F(A) of F(X) onto A is obtained from
F(X) by summing out all the variables in X -A. In
formula
(12)
F(A)=d•J I: F(X).
X-A

When F(X)
probability.

is

a joint probability, F(A) is a marginal

Summing variables out directly from F(X) usually re­
quire too many additions. Suppose X contains n vari­
ables and suppose all variables are binary. One needs
to perform 2n - 1 additions to sum out one variable.

Intercausal Independence and Heterogeneous Factorization

A better idea is to sum out variables from an factoriza­
tion of F(X) if there is one. This section investigates
how to sum out variables from tidy HF's. The follow­
ing two lemmas are of fundamental importance, and
they readily follow the definition of the general com­
bination operator @.
Both m'llltiplication and @ are distributive
w. r. t summation.
More specifically, s'llppose f and g
are two functions and variable x appears in f and not
in g . Then

Lemma 1

1.

'Er(fg) = ('Er f)g, and

summing out z does not affect the deputing functions.
Therefore, :F' remains tidy.
When z is a bastard variable, summing out z will not
affect the deputing functions of any other bastard vari­
ables. Therefore, :F' also remains tidy. 0
general, a variable can appear in more than one nor­
mal and heterogeneous factors. The next proposition
reduces the general case to the case where the variable
appear in at most two factors, one normal and one
heterogeneous.

In

F(X), and let z
be a variable in X. Let It , ... , fm be all the heteroge­
neous factors that involve z and let 91, . . , Un be all
the normal factors that involve z. Define

Proposition 5 Let :F be an HF of

2. E., (I®g)= CEr f)® g.

.

0

f=aej 0?;1 /; ,

The following lemma spells out two conditions under
which multiplication and ® are associative with each
other.
Lemma 2
1.

h{f®g}={hf} ®g.

n

g=aeJ IT Ui·
j=l

Let f and g be two functions.

If h is a function that involves no bastard vari­
ables, then

(13)

Let :F' be the HF obtained from :F by removing the fi 's
and the Ui 's, and by adding a new heterogeneous factor
f and a new normal factor g. Then
1. :F' is also an HF of F(X), and f and

g are the
only two factors that involve z. In particular,
when either m=O or n=O, there is only one factor
in :F' that involves z.

2. If h is a function such that all the bastard variables
in

h

f and not in g,
h{f®g } ={hf}®g.

appear only in

then

(14)

0

We now proceed to consider the problem of summing

out variables from a tidy HF in such a way that the tid­
ness is retained. First of all the following proposition
deals with the case when the variable to be summed
out appears in only one factor.
4 Let :F be an HF of F(X) and is tidy.
Suppose z is a variable that appears only in one factor
!(A), normal or heterogeneous. Define h

Proposition

h(A - {z} )=d�J Lf(A).
z

Let :F' be the HF obtained from :F by replacing

f with

h 2• Then, :F' is a HF of F(X- { z } )
the projection
of F(X) onto X-{z} . Moreover if z is not a dep'llty
-

variable, then :F' remains tidy.

Proof:

611

The first part of proposition follows from

Lemma 1.

For the second part, since z is not a deputy variable, it
can be either a non-deputy normal variable or a bas­
tard variable. When z is a non-deputy normal variable,
2The factor h is heterogeneous or normal if and only if
f is.

2.

If z

is not a dep'llty variable, then when :F is tidy,
so is :F'.

Proof: The first part of the proposition follows from

the commutativity and associativity of multiplication
and of the general combination operator @.
For the second part, since z is not a deputy variable, it
can either be a non-deputy normal variable or a bas­
tard variable. When z is a non-deputy normal vari­
ables, the operations performed by the proposition do
not affect the deputing functions. Thus, :F' remains
tidy.
When z is a bastard variable, the deputing functions
are not affect either. Because for each bastard variable
e, its deputing functions is the only normal factor that
involves e. So, :F' also remains tidy. D.
The following proposition merges a normal factor into
a heterogeneous factor.
Proposition 6 Let :F be an HF of F(X) and is tidy.

Suppose z is a variable that appears in only one normal
factor g and only one heterogeneous factor f. Define
h by

h=aeJfg.
Let :F' be the HF obtained from :F by removing g and
f, and by adding a heterogeneous judor h. If z is not

612

Zhang and Poole

a deputy variable, then the joint of :F' is also F(X)
and :F' is tidy. Moreover, h is only one factor in :F'

that involves

z.

Proof: We first consider the case when z is a non­
deputy normal variable. Because the tidness of :F, g
involves no bastard variables. According to Lemmas 2

(1), the joint ofF' is also F.

Since g is not a deputing function, the operation of
combining f and g into one factor does not affect the
deputing functions. Hence, :F' remains tidy.
Let us now consider the case when z is a bastard vari­
able. Since :F is tidy, g must be the deputing function
of z. Since f is the only heterogeneous factor that in­
volves z, all other heterogeneous factors do not involve
z. According Lemma 2 (2), the joint of :F' is also F.
After combining f and g into a heterogeneous factor,
there is no normal factor that involve z. Also, the
deputing functions of the other bastard variables are
not affected. Hence, :F' remains tidy. D.
The

in Bayesian networks. To this end, we need only con­

sider deputing functions I(e, e') such that I(e, e') = 1
if e = e' and I(e, e') = 0 otherwise. Let us say such
deputing functions are identifying. Since for any func­
tion f(e, e', x1, ... , xn ) ,

LI(e, e')f(e, e',

Procedure PROJECTION ( :F, A,
•

8

AN ALGORITHM

This section presents an algorithm for computing pro­
jections of a function F(X) by summing variables from
a tidy HF of F(X). Because of Proposition 3, the al­
gorithm can be used to compute marginal probabili­
ties, and hence posterior probabilities, in Bayesian net­
works.
To sum out the variables in X- A, an ordering needs
to be specified (Lauritzen and Spiegehalter 1988). In
the literature, such an ordering is called an elimi­
nation ordering, which can be found by heuristics
such as the maximum cardinality search (Tarjan and
Yannakakis 1984) or the maximal intersection search
(Zhang 1993).
At the end of the last section, we said that a deputy
variable should be summed out only after the corre­
sponding bastard variable has been summed out. If e
is a bastard variable in A, what should we do with its
deputy variable e'?
The paper is concerned with intercausal independence

p)

Input:
1. :F - A tidy HF of a certain func­
tion F(X) such that all the deputing
functions are identifying,

a tidy HF, bastard variables and non-deputy normal
variables. You may ask: how about deputy variables?
As it turns out, after summing out a bastard variable
e, its deputy e1 becomes a non-deputy normal variable.
So, we can also sum out deputy variables; we just have
to make sure to sum out a deputy variable after the
corresponding bastard variable has been summed out.
variable e' needs to be summed out after the corre­
sponding bastard variable e. As a matter of fact, sum­
ming out e ' before e is the inverse of the deputation of
e. But we have shown at the end the Section 5 that
deputation is necessary.

) = f(e, e, x 1 , .. . , Xn),

we can handle the deputies of bastard variables in A as
follows: wait till after all the other variables outside
A have been summed out and all the heterogeneous
factors have been combined, then simply remove all
the deputing functions, replace each occurrence of a
deputy variable with the corresponding bastard vari­
able. This operation can be viewed as the inverse of
deputation.

above three propositions allow us to sum out, from

It is possible to intuitively understand why a deputy

Xt, ... , Xn

e'

2. A- A subset of X,

3. p - An elimination ordering consist­
ing all the variables other than the
variables A and their deputies. In
p, a deputy variable e� comes right
after the corresponding bastard vari­
able e;.

•

Output:
onto A.

F(A) - The projection of F

1. If p is empty, combine all the het­
erogeneous factors by using the gen­
eral combination operator ®, resulting
in f; remove all the deputing functions
and replace each occurrence of a deputy
variable with the corresponding bastard
variable; multiply f together with all the
normal factors; output the resulting fac­
tion; and exit.
2. Remove the first variable

z

from the or­

dering p.

3. Remove from :F all the heterogeneous
factors ft, .. . , fl� that involve z, and
set

f=dej

®f=l k

Let B be the set of all the variables that
appear in f.
4. Remove from :F, all the normal factors
91 , . .., Om that involve z, and set

m

D= deJ

IT 9j·

j= l

Let C be the set of all the variables that
appear in g.

Intercausal Independence and Heterogeneous Factorization

5. If k=O, define a function

The bastard variable e3 appears in heterogeneous fac­
tors /31 (e3ei) and /a2(e 3, e�), and in the normal factor
I3(e3,e �). After summing out eg the factors become:

h by

h(C-{z})=def Lg(C),
Add h into F

as

613

{T,bt(eL e�,e�), fu(el,a),
Fo
!21(e2,a), /22(e2, b), ha(e2,c)};
F1={P(a), P(b), P(c), P( yie�),
h(e2, e�)},

•

a normal factor,

6. Else if m=O, define a function h by

•

h(B-{z})=deJ L f(B),

/t 2(e1, b),
l1(et,ei),

where
Add h into F as a heterogeneous factor,

7. Else define a function h by

h(BUC-{z} )=de/ L f(B)g(C),
Add h into F as a heterogeneous factor.
Endif
8. Recursively call PROJECTION(F,

A, p)

The correctness of PROJECTION is guaranteed by
Propositions 4, 5, and 6.

summing out a variable re­
quires combinin g only the factors that involve the vari­
able. This is why PROJECTION allows one to ex­

Note that in the algorithm

ploit intercausal independencies for efficiency gains. If
one ignores intercausal independencies, to sum out one
variable one needs combine all the conditional proba­
bilities that involve the variable. There is a gain in effi­
ciency by using PROJECTION because intercausal in­
dependence allows one to further factorize conditional
probabilities into factors that involve less variables. In
Figure 1, for instance, summing out a requires com­
bining P(e tla, b) and P(e2la, b, c) when intercausal in­
dependencies are ignored; there are five variables in­
volved here. By using PROJECTION, one needs to
combine f11 (e 1 , a) and !21(e2,a); there are only three
variables involved in the case.
Finally, we would like to remark that the algorithm
is an extension to a simple algorithm for computing
marginal probabilities from a homogeneous factoriza­
tion (Zhang and Poole 1994).

tPt(e�, e�,e3)=def L(/31 (ea,eD®faz(e3, e� ))I3(e3, e3).
"�

Now e� is the next to sum out. e� appears in the
heterogeneous factor t/J1 and the normal factor P(yle�),
After summing out e�, the factors become:
•

•

where

?f>2(e�, e�, y)=def L if>1 (e� ,e�,e3)P(yle3).
e;

Next, summing out
•

•

Suppose the elimination ordering pis:

e�, c.
•

•

e3, e�, a, b, e1,

Initially, the factors are as follows:

:Fo = {/u(et,a), /t2(e1,b), ht(e2, a ) , /22(e2,b)
h3(e2,c), h1(e3,eD, h2(e3,e�)};
:F1 = {P(a), P(b), P(c), P(yle�), h(el,eD,
l2(e2,e�), l3(ea, e�)}.

tfi2(e�,e�,y),
Fa={ tP3(e1 ,e2),
h2(e2, b), /23(e2,c)};

!t2(e1,b),

F1={P(b), P(c), I1(e11 ei), I2(e2, e�)},

a

Then, summing out

•

work N shown in Figure 2. Since P(e2ly=O) can
be readily obtained from the marginal probability
P(e2,y), we shall show how PROJECTION computes
the latter.

gives us:

a

•

To illustrate PROJECTION, consider computing the
conditional probability P(e2ly=O ) in the Bayesian net­

a

where

An example

9

:Fo=N2(eL e�,y), /u(et,a), !t2(e1,b), !21(e2,a),
!22(e2,b), h3(e2, c)};
:F1= {P(a), P(b), P(c), It(et, ei), I2(e2,e�)},

b gives

us:

Fo={ 'tj.>4(e1, e2),
!23(e2,c)};
Ft= {P(c), lt(et,eD, I2(e2,e�)},

where

if>4(et,e2)

=def
=

L P(b)[/t2(et,b)®f22(e2,b)]
L P(b)!t2(e1,b)h2(e2, b).
b

The next variable on p is e1, which appears in hetero­
geneous factors 'tj.>3 (e1,e2) and 'tj.>4 (e 1, e2) and normal
factor h (e1, ei). After summing out e1 the factors be­
come:

614

Zhang and Poole

together with conditional independencies, to further
reduce inference complexity.

where

t/Js(e2, e�)=deJ

2: It (et, eD[t/Ja(e1, e2)®t/J4(e1, e2)].
"1

Due to space limit, we have to discontinue the example
here. Hopefully, the following two points shoul be be
clear now. F irst, in summig out one variable, PRO­
JECTION combines only the factors that involve the
variable.
Second, since

not have

e1

is a bastard variable, we usually do

Acknowledgement
The authors are grateful for the three anonymous re­
viewers for their valuable comments and suggestions.
Research is supported by NSERC Grant OGP0044121
and by a travel grant from Hong Kong University of
Science and Technology.

References
[1] P. Dagum and A. Galper (1993), Additive BeliefNetwork Models, in Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence,

E t/J2(e�, e�, y)[E h(et, ei)(tP4(et, e2)® '!fa(el, e2)) ] =
ei

e1

EeJEe• '!f2(ei,e�,y)ft(e1, ei)]®'!f4(e1, e2)0'1f3(e1,e2)·
1

This is why
sumed out.

10

e�

must be summed out after e1 has been

RELATED WORK

[2]

PP· 91-98·
I. Good (1961), A causal calculus (I). British
Journal of Philosophy of Science, 11, pp. 305-318

[3] D. Heckerman (1993), Causal independence for
knowledge acquisition and inference, in Proceed­
ings of the Ninth Conference on Uncertainty in

Artificial Intelligence, pp. 122-127.

The paper is closely related to Heckerman (1993).
However, the relationship between Beckerman's tem­
poral definition of intercausal independence and our
constructive definition remains to be clarified.

[4] M. Henrion (1987), Some practical issues in con­
structing belief networks, in L. Kana!, T. Levitt,
and J. Lemmer (eds. ) Uncertainty in Artificial In­
telligence, 3, pp. 161-174, North-Holland.

Our constructive definition is a special case of the gen­
eralization of the noisy-0 R model proposed by Srinivas
(1993). While we consider only binary operators for
combining contributions from different sources, Srini­
vas considers general mappings from contributions to
effect.

[5] J. Kim and J. Pearl (1983), A computational
model for causal and diagnostic reasoning in infer­
ence engines, in Proceedings of the Eighth Interna­
tional Joint Conference on Artificial Intelligence,
Karlsruhe, Germany, pp. 190-193.

In the

additive belief-network model proposed by

Dagum and Galpher (1993), a conditional probability
.. , xn ) is decomposed into a linear combina­
tion of the P(ylxs) 's. Although this may appear to be
a special case of constructive equation (3), they are
actually very different. For example, Lemma 2 would
not hold if0 were addition.

P(ylxl> .

11

CONCLUSION

A constructive definition of intercausal independence
has been given. The definition is based one an intuitive
picture of intercausal independence where a number of
causes contribute independently to an effect and the
total contribution is a combination of the individual
contributions. Noisy OR-gates and noisy adders are
examples of constructive intercausal independence.
It is well known that conditional independence implies
factorization of joint probability, which enables one to
make use of conditional independencies to reduce in­
ference complexity in Bayesian networks. Under the
constructive definition, intercausal independence im­
plies factorization of conditional probability, which al­
lows one to make use of intercausal independencies,

[6] S. L. Lauritzen and D. J. Spiegehalter (1988), Lo­
cal computations with probabilities on graphical
structures and their applications to expert sys­
tems, Journal of Royal Statistical Society B, 50:
2, pp. 157 - 224.
[7] J. Pearl (1988), Probabilistic Reasoning in Intel­
ligence Systems: Networks of Plausible Inference,
Morgan Kaufmann Publishers, Los Altos, CA.
[8] S. Srinivas (1993), A generalization of the Noisy­
Or model, in Proceedings of the Ninth Conference
on Uncertainty in Artificial Intelligence, pp. 208215.
[9] R. E. Tarjan and M. Yannakakis {1984), Simple
linear time algorithm to test chordality of graphs,
test acyclicity of hypergraphs, and selectively re­
duce acyclic hypergraphs, SIAM J. Comput., 13,
pp. 566-579.
(10] L. Zhang (1993), Studies on hy pergraphs 1: hy­
perforest, Discrete Applied Mathematics, 42, pp.
95-112.

[11]

N. L. Zhang and D. Poole (1994), A simple ap­
proach to Bayesian network computations, to ap­
pear in Proceedings of the Tenth Canadian Con­
ference on Artificial Intelligence.

