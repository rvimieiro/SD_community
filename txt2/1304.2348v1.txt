temporally-dependent information, (ii) a space and
time efficient method for refining probabilistic causal
rules.

Predicting the future is an essential compo­
nent of decision making.

In most situations,

however, there is not enough information to
make accurate predictions.

In this paper, we

develop a theory of causal reasoning for pre­
dictive inference under uncertainty.

We em­

phasize a common type of prediction that in­
volves reasoning about persistence:

whether

or not a proposition once made true remains
true at some later time. We provide a decision
procedure with a polynomial-time algorithm
for determining the probability of the possible
consequences of a set events and initial con­
ditions.

The integration of simple probabil­

ity theory with temporal projection enables us
to circumvent problems in dealing with per­
.
sistence by nonmonotonic temporal reasoning

·

schemes. The ideas in this paper have been �m­
plemented in a prototype system that refines
a database ofcausal rules in the course of ap­
plying those rules to construct and carry out
plans in a manufacturing domain.

I.

II.

In order to explore some of the issues that arise in
causal reasoning, we will consider some examples in­

Introductipn

We are interested in the design of robust inference
systems for generating and executing plans in rou­
tine manufacturing situations. We hope to build
autonomous agents capable of dealing with a fairly
circumscribed set of possibilities in a manner that
demonstrates both strategic reasoning (the ability to
antici pate i!ond plan for possible futures) and adap­
tive reasoning (the ability to recognize and react to
unanticipated conditions). In this paper, we develop
a computational theory for temporal reasoning un1 This

work was supported in part by the National Science

Foundation under grant IRI-8612644 and by
development award.
sented at CSCSI-88.

an

Causal Theories

IBM faculty

A version of this paper has been
pre'

73

volving a robot foreman that directs activity in a

factory. The robot has a plan of action that it is
continually executing and revising. Among its tasks
is the loading of trucks for clients. If our robot learns
that a truck is more likely to leave than it previously
believed, then it should consider revising its plans so
that this truck will be loaded earlier. If, on the other
hand, it predicts that all trucks will be loaded ahead
of schedule; then it should take advantage of the op­
portunity to take care of other tasks which it did not
previously consider possible in the available time.
In order to construct and revise its plan of ac­
tion, the robot makes use of a fairly simple model
of the world: a special-purpose theory about the
cause-and-effect relationships that govern processes

at work in the world (referred to as a causal theory).
The robot's causal theory consists of two distinct
types of rules which we will refer to as

projection
rules and pers�stence rules. We will defer discussion

of persistence rule� for just a bit.

quisition of the data to dictate the limitations of our
inference mechanism.
Our inference system needs to deal with the im­

precision of most temporal information. Even if a

As an example of a projection rule, the robot

robot is able to consult a clock in order to verify the
exact time of occurrence of an observed event, most

in an order, then, with some likelihood, the client's

information the robot is given is imprecise (e.g., a
client states that a truck will pick. up an order at

might have a rule that states that if a client calls.
truck will eventually arrive to pick up the order.
The consequent prediction, in this case the· arrival
of a client's truck, is conditioned on two things: an
event referred to as the

triggering event, in this case

the client calling in the order, and an enabling condi­
tion corresponding to propositions that must be true
at the time the triggering event occurs. For exam­
ple, the rule

just

mentioned might be conditioned

on propositions about the type of items ordered,
whether or not the caller has an account with the re­
tailer, or the time of day. The simplest form of a pro­
jection rule is PROJECT(P1 A P2 . .. A Pn, E, R, .��: ) .

This says that R will be true with probability .11:
immediately following the event E given that P1
through Pn are true at the time E occurs. Let (P,t)

(E, t)
time t.

indicate that the fluent Pis true at timet, arid
indicate that an event of type

E

occurs at

Restated as a conditional probability, this would be:

around noon, or a delivery is scheduled to arrive
sometime in the next 20 minutes). One of the most
important sources of uncertainty involves

predicting

how long a condition lasts once it becomes true (i.e..
how long an observed or predicted fact is likely to

persist). In most planning systems (e.g., [14]) there
is a single (often implicit) default rule of persistence
[6] that corresponds more or less to the intuition that
a proposition once made true will remain so until

something makes it false. The problem with using
this rule is that it is necessary to predict a contra­

vening proposition in order to get rid of a lingering
or persistent proposition: a feat that often proves
difficult in nontrivial domains. If a commuter leaves

his newspaper on a train, it is not hard to predict
that the paper is not likely to be there the next time
he rides on that train; however, it is quite unlikely
that he will be able to predict what caused it to be
removed or when the removal occurred.
When McDermott first proposed the notion

In this paper, we will assume for simplicity that
P1 through Pn are independent. In [4] we discuss
methods by which this restriction can be removed.

of persistence as a framework for reasoning about
change [12], he noted that persistence might be given
a probabilistic interpretation. That is exactly what
We replace the single default rule of

Projection rules are applied in a purely antecedent

we do here.

fashion (as in a production system) by the inference

persistence used in most planning systems with a

engine we will be discussing.

The objective is to

set of (probabilistic) rules: one or more -for each flu­

obtain an accurate picture of the future in order to
support reasoning about plans [2] [1].

use a persistence rule to reason about the likelihood

ent that the system is aware of.

Our robot might

Our approach, as described up to this point, is

that a truck driver will still be waiting at various

fairly traditional and might conceivably be handled

times following his arrival at the factory. The in­

[7].

What distin­

formation derived from applying such a rule might

guishes our approach from that of other probabilistic
reasoning approaches is that we are very much con­

be used to decide which truck to help next or how
to cope when a large number of trucks are waiting

by some existing approach [13)

cerned with the role

of

time and in particular the

simultaneously. Each persistence rule has the form

tendency of certain propositions (often referred to

PERSIST(P, p),

as

tion of time referred to as a

fiuents [11]) to change with the passage of time.

By adding time as a parameter to our causal rules,

where Pis a fluent and pis a func­

survivor function [17].

In our implementation, we consider only two types

�e have complicated both the inference task and the

of survivor functions: exponential decay functions

knowledge acquisition task. Complications notwith­

and piecewise linear functions. The former are de­

standing, the capability to reason about change in an

scribed in Section IV., and the latter, requiring a

uncertain environment remains an important prereq­

slightly more complex analysis, are described in [5].

uisite to robust performance in most domains. We

Exponential decay functions are of the form e-.\t

simply have to be careful to circumscribe a useful

where J\ is the constant of decay. Persistence rules

and yet tractable set of operations. In our case, we

referring to exponential decay functions are notated

have allowed the computational complexity of the

simply

reasoning tasks and the availability and ease of ac-

example, to indicate that the probability

74

PERSIST(P, .X).

Such functions are used, for.

of

a truck

I
I
I
I
I

I

I
I
I
I
I
I
I
I
I
I
I
I
I

I

I

PltOTECT(AL WAYS.ARRIVE(I""""l.ATOOCX(I""""),1)
PROTECT(ATDOC IC(Iruckl.LEAVE(I""""),NOT(ATDOCIC('""")" ),1)
PERSIST(ATOOCIC(truckJ.f)

OCCURS(ARJUVE(TRUCICl4),11) •1111 13 s tl s 15
OCCURS(CWSECSTOR£1),12) •u 16.5SI2S17.5
1.0

I

I

I

I

I

I
I

I
I
I
I
I
I
I

I

I

p(OCCURS(ARRIVE(TRUCICl4),l))

\

0.5

hol3

Figure 1: A simple causal theory illustrating the use

of survivor functions

5% every 15 min­
PERSIST(P, .\) encodes

remaining at the dock decreases by
utes. The persistence rule
the fact that:

p((P, t} I (P, t-�))

=

e-A(t-�)

insensitive to changes in the time of occurrence of
events that cause such propositions to become true,
and, hence, are easy to handle efficiently.
There are a number of issues that every compu­
tational approach to reasoning about causality must
deal with. One such issue· involves reasoning about

(13] (e. g

.,

the application of two

probabilistic causal rules t�at have the same conse­
quent effects, both of which appear to apply in a
given set of circumstances but whose conditions are
correlated). Another issue concerns handling other
forms of incompleteness and nonmonotonic inference

(9] [3] (e.g.,

the robot might have a general rule for

reasoning about the patience (persistence) of truck
drivers waiting to be served and a special rule for
how they behave right around lunch time or late in
the day). While we agree that these problems are
important, we do not claim to have any startling
new insights into their solution. There is one area,
however, in which our theory does offer some new
insights, and that concerns the form of probability
functions used in causal rules and how they can be
used to efficiently predict the causal consequences.

17

A set of basic facts and their probabilistic

the world changing over time as a consequence of ob­
served and predicted events. This picture is formed

These governing rules are collectively referred to as

causal theory.
Figure 1 depicts a simple causal theory. Predi­
cates (ATDOCK), and constants (TRUCK14) are in
upper case, while functions (p, g) and variables (t,
truck) are in lower case. We refer to an instance
a

of a fact (type) being true over some interval of

time token, or simply token. For exam­
ARRIVE(TRUCK14) denotes a general type of
event whereas (ARRIVE(TRUCK14), t) denotes a
particular instance of ARRIVE(TRUCK14) becom­
ing true. The predicate ALWAYS is timelessly true
(i.e., 'Vt (ALWAYS, t}). The function p, a survivor
time as a
ple,

function, describes how certain types of propositions
are likely to persist in lieu of further supporting or
contravening information.
Figur� 2 shows a set of basic facts correspond­
ing to two events assumed in our example to occur
with probability

1.0

within the indicated intervals.

The system assumes that there is a distribution de­
scribing the probability of each event occurring at
various times, and uses some default distribution if
no distribution is provided.
Evidence concerned

with the

occurrence

of

events and the persistence of propositions is com­
bined to obtain a probability function

Probabilistic Projection

Q

1r

for a propo­

being true at various times in the future by

convolving the density function

f

for an appropriate

triggering event with the survivor function p associ­

In this section, we will try to provide some intu­
ition concerning the process of reasoning about per­
sistence, which we will refer to as

16

interpretation

sition

III.

2:

15

to govern objects and agents in a particular domain.

where� is a positive number indicating the length of

dependent causes

Figure

14

by extrapolating from certain observed events (re­
ferred to as basic facts) on the basis of rules believed

an interval of time. Exponential decay functions are

tion.

\

p(OCCURS(CUJSIUSTOREl)J))

1.0

probabilistic projec­

A planner is assumed to maintain a picture of

75

ated with

Q:
1r(t)

=

1100 f(z)p(t- z)dz

(1)

I

I

I

p(HOLDSIATDOCI:ITRUCJ:lf),l))

\

1.0

�f

-

p(OCCURSIARRIVECTRUC1:14)-"J)f(t·zlb
p(HOLDSIA TDOClCI11'VCkl.IJJ

p
1r.tp(HOLDS!ATDOClCII....Jrl.l- 6))

o.5

I

p!OCCUII.S!AlUUVE(tnu:k),lJI

14

Figure

3:

15

16

-·-

11

- ...

Figure

An example of simple probabilistic infer­

3 illustrates

sition of interest became true. All that is required

a simple instance of this kind of

is to multiply the last value by the constant decay

inference. Note that the range of the resulting prob­

rate, and add it to any contribution from the causal

ability function is restricted; after the point in time
labeled

17, the persistence

is said to be

Computing the convolution integral incre­

mentally

ence about persistence

Figure

4:

I

distribution for that time step. The process is illus­
trated graphically in figure 4.

of ATDOCK(TRUCK14)

clipped, and thereafter its probability is

There are many details concerned with index­

represented by another function described below.

ing and applying projection rules that will not be

All probability computations are performed in­
crementally in our system.

mentioned in this paper (but see [6]). The details

Each token has associ­

ated with it a vector which is referred to as its

of probabilistic projection using exponential decay

ex­

Our up­
functions are described in Section IV..
date algorithm is polynomial in the product of the

pectation vector that records the expected probabil­
ity that the proposition corresponding to the token's

number of causal rules, the size of the set of basic

type will be true at various times in the future.

fact-s, and the size of the mesh used in approximating

The system updates the expectation vectors ev­

the integrals. For many practical situations, perfor­

ery time new propositions are added to the database,

mance is closer to linear in the size of the set of basic

and also at regular intervals as time passes. In the

facts.

update, a single pass sweep forward in time is made

The convolution equation can be easily ex­

through the database. There is, according to the do­
main and granularity of data, a fixed

tended to handle the case of clipping.

time step, or

(1)

a quantum by which we partition time. Starting at

a term, the function g, corresponding to the dis­

tribution of an event which clips the state. of a fact

the "present time," we compute for each proposition

being true.

its expected probability for the time step according
to the causal theory governing that type of propo­

1r(t)

sition, and record it in the expectation vector. We
compute the probability for all propositions, before
moving on to the next time step.

We add to

=

1'00

f(z)e-A(t..oz)[l-

The cumulative distribution of

The process is

1

t

g(w)dw]dz

(2)

g defines the degree

to which it becomes unlikely that the fact repre­

repeated for some finite number of time steps.

sented by

For event causation, the update is straightfor­

11"

remains true in the world. We see that

under certain conditions,

ward; in the simplest cases, it is just a table lookup

we desire.

and copying of the density function into the vector.

(2)

describes exactly what

Unfortunately, there will be a tendencv

g to count the same ef­
[4] we address methods by which this

for the decay function and

For the convolution, it is necessary to take steps to

fects twice. In

avoid computing the convolution integral afresh at

problem can be attacked in a different framework.

each time step. We compute the convolution as a
Riemann sum, successively summing over the time

IV.

axis with a mesh of fixed size (the time step). By us­
ing the exponential decay form of survivor functions,

The Algorithm

Probabilistic causal theories are composed of two

it is possible to compute the convolution at a time

types of rules, projection rules:

step by looking only at the value for the last time
step, independent of the time at which the propo-

PROJECT(Pt

76

/1.

P2 .. .

/1.

Pn, E, R, x: )

I

I

I

I
I
1
I
t·

I

I

I

I

I

I

I

I

I

I

I
I
I
I
I
I
I
I
I
I
I
I
I

and persistence rules:
PERSIST(Q, ..\)
where P1 through Pn, R, and Q are all fact types,
and E is an event type. We assume independence
of fact types so that, if we are interested in the con­
junction P1/\ P2 ... I\ Pn, we can assume that
n
p( (P11\P2···1\Pn,t)) = IIP((P;,t))

(3)

i=l

We define a relation -<c on fact types so that
Q -<c R just in case there exists a rule of the form
PROJECT(P1 1\ P2 . .. I\ Pn, E, R, �)where P; = Q
for some i. For any given set of causal rules, the
graph g-<c whose vertices correspond to fact types
and whose arcs are defined by -<c is likely to have
cycles; this will be the cause of a small complica­
tion that we will have to resolve later. In this paper,
we distinguish between fact types corresponding to
propositions that hold over intervals and event types
corresponding .to instantaneous (point) events. For
each occurrence (token) of a point event of type E,
we will need its density function p((E, t) ) . Proba­
bilistic projection takes as input a set of initial events
and their corresponding density functions. Given
the restricted format for projection rules, the only
additional point events are generated by the system
in response to the creation of new instances of fact
types. For each token of fact type P, we identify a
point event of type E p corresponding to the partic­
ular instance of that fact becoming true. In the pro­
cess of probabilistic projection, we will want to com­
pute the corresponding density function p( (Ep, t)).
In addition to computing density functions, we will
also want to compute the mass functions p( (P, t))
for instances of facts.
In order to describe the process of probabilistic
projection, we will divide the process into two differ­
ent stages: deterministic causal projection and prob­
abilistic causal refinement. The actual algorithms
are more integrated to take advantage of various
pruning techniques, but this simpler, staged, pro­
cess is somewhat easier to understand. Determinis­
tic causal projection starts with a set of tokens and a
set of projection rules and generates a set of new to­
kens T by scanning forward in time and applying the
rules without regard for the indicated probabilities.
This stage can be carried out using any number of
simple polynomial algorithms (see (6] (10]) and will
not be further detailed here. Probabilistic causal
refinement is concerned with computing density and
mass functions for tokens generated by deterministic
causal projection. In the following, all density and

77

mass functions are approximated by step (i.e., piece­
wise constant) functions. vVe represent these func­
tions of time using vectors (e.g., mass(T) denotes
the mass function for the token T and mass;(T) de­
notes the value of the function at t = i). For each
fact token Tp, we create a corresponding event to­
ken TEp and define a vector mass(Tp). For each
event token TE, we define a vector density(TE)· We
define an upper bound Q on projection and assume
that each mass and density vector is of length n.
. Initially, we assume that
'VT

E T: 1 :S i :S n: density;(T) = 0 1\ mass; (T) =

0

Event tokens are supplied by the user in the form
�=

i

lst

est

p( (E,t))dt

where est and 1st correspond (respectively) to the
earliest and latest start time for the token and x; is
the probability that the event will occur at all. We
assume that the density function for such an event
is defined by a Gaussian distribution over the inter­
val from est to 1st. For a token TE corresponding
to a user-supplied initial event, it is straightforward
to fill in density(TE ). Probabilistic causal refine­
ment is concerned with computing mass;(Tp) and
density;(TEp) for all fact tokens Tp and all event
tokens TEp. We partition the set of tokens T into
fact tokens TF and event tokens TE. Probabilistic
causal refinement can be defined as follows:

Procedure: refine( T)
for i= 1 to n:
forT E T E:
forT E T F:

i);
i);

density-update(T,
mass-update(T,

Of course, all of the real work is done by density­
and mass-update. Each token has associated
with it a specific derivation that is used in computing
its mass or density. For a token TER, this derivation
corresponds to a rule of the form
update

PROJECT(Pt

1\

Pz .. . I\ Pn, E, R, x: )

and a set of antecedent tokens {TE , Tp1, Tp2
Tp,.}
used to instantiate the rule and generate the conse­
quent token TR. Given that
•

p((ER, t) = � * p((E, t)) * p((P1

1\

•

•

P2 . .. 1\ Pn, t)

and, assuming independence (3), we have

Procedure:

density-update(TE R,

density;(TER) x:
*
den si ty; (TE )

*

i)

f17=l

mass;(TP;)

I
There is one problem with this formulation: it
relies on all the mass and density functions for the

I

We will take advantage of the fact that

. antecedent conditions being already computed for
the instant i. In the present algorithm,
no care in ordering the tokens in

T.

refine

number of ways of ensuring that the updates are
performed in the correct order.

and

p(sk+l - x)

The easiest is to

partially order T according to -<c and insist that
g-<c be acyclic, but this would preclude the use of
most interesting causal theories.

I

takes

There are a

A more realistic

where 6

=

=

e->.o p(sk- x)

Sk+t - sk.

Making appropriate substitutions, we have

T with respect to an instant
that are open and those· that are

method is to partition
i into those tokens

closed.

Deterministic causal projection defines an

earliest start time

(est)

kens a latest start time

for each token; for event to­

(1st)

is specified. An event

token is open throughout the interval

est to 1st and

closed otherwise. For fact tokens, we modify prob­
abilistic causal refinement so that it closes a fact
token

Tp as

soon as

mass; (Tp )

drops below

threshold. A fact token is open from its

est

a

fixed

until it

is closed. All we require then is that for any i the
set of tokens that are open define an acyclic causal
dependency graph using -<c.

This restriction still

allows for a wide range of causal theories.

To get

refine to do the right thing, we would have to apply
refine only to open tokens and either sort the tokens
using �c, or (as is actually done) define refine so
that if, in the course of updating a consequent to­
ken,

refine

finds an antecedent token that hasn't yet

It should be clear that updates depending upon
such simple survivor functions can be performed
quite quickly.

been updated, it applies itself recursively.

Tp corresponds to a
PERSIST(P, ..\)where ..\is the con­
stant of decay for the fact type P, and an event token
TsP. The procedure mass-update is a bit more dif­
ficult to define than density-update since it depends
The derivation of a token

rule of the form

upon the type of decay functions used in persistence

Integration is approximated using

Riemann sums with a mesh of fixed size (roughly)
corresponding to 6. We define the procedure

update

mass­

as follows:

Procedure: mass-update(Tp, i)
mass;(Tp) ·e>-P6 mass;_1(Tp )

+

density;(Tsp)

rules. In the case of exponential decay functions, the

The actual algorithms are complicated some­

density-update is reasonably straightfor­

what by the fact that the choice of mesh size may not

operation of
ward.

coincide precisely with the steps in the step functions

Recall the basic combination rule for probabilis­
tic projection:

11'(t)

We compensate for this by using a somewhat finer
mesh in the update algorithms.

=

ltoo f(x)p(t- x)dx

and suppose that p

is of the

form

e->-z

the accuracy of the resulting mass and density func­
tions, but these errors can be controlled. We have
tried to make a reasonable tradeoff, taking into ac­
where

..\ is

mated by a step function as in

{

The fact that we

employ a fixed mesh size still causes small errors in

some constant of decay, and that f can be approxi­

f(x) =

approximating survivor functions and distributions.

count that the finer the mesh the larger the mass
and density vectors. Given that the step functions
used for encoding survivor functions and distribu­
tions are only approximations, the re is a point past
which employing a finer mesh affords no additional
information. We have found that a mesh size of half

Ct

...

C,.

the smallest step in any step function works quite
Sn-1

:5

X <

well in practice.

Sn

78

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

I

I

I

I
I
I
I
I

I.
I
I
I
I
I
I
I
I
I
I

V.

Acquiring Rules

Statistical methods have not seen particularly wide
application in AI. This is largely due to problems
concerning the availability of the data necessary to
employ such methods. Data provided from experts
has been labeled as unreliable, and the use of priors
in Bayesian inference has been much maligned. An
alternative to expert judgements and estimating pri­
ors, is to integrate the data acquisition process into
your system: have it gather its own data. In such
a scheme, all predictions made by the system are
conditioned only upon what the system has directly
observed. Of course, this is unrealistic in many cases
( e.g., diagnostic systems whose decisions could im­
pact on the health or safety of humans). In the in­
dustrial automation applications considered in this
paper, however, not only is it practical, but it ap­
pears to be crucial if we are to build systems capable
of adapting to new situations.
In this section, we describe a system for con­
tinually refining a database of probabilistic causal
rules in the course of routine planning and execu­
tion. Given the focus of this paper, we will concern
ourselves exclusively with the acquisition (or refine­
ment) of persistence rules. Our warehouse planner
keeps· track of how long trucks stay around and uses
this information to construct survivor functions for
various classes of trucks. The system must be told
which quantities it is to track and how to distinguish
different classes of trucks, but given that, the rules
it acquires are demonstrably useful and statistically
valid.
The survivor function for a given class of trucks
is computed from a set of data points corresponding
to instances of trucks observed arriving and then
obser"ed leaving without being loaded. It should be
clear that, in general, a collection of data points will
not define a survivor function uniquely. There are
many ways in which to derive a reasonable approxi­
mation for such a function. For example, we might
employ some form of curve fitting based on an ex­
pected type of function and the sample data. While
such methods may yield more accurate approxima­
tions in some cases, for our application, there are
simpler and more efficient methods. With both of
the simple classes of functions we have considered,
the exponential decay and the linear decay functions,
computing, respectively, the persistence parameter
(.A) and the slope is trivial. In the case of an expo­
nential decay, we use the mean as the half-life of the
function.
We can now sketch the simple algorithm uti­
lized in our system. As noted, we need to collect

79

data for each class of interest. The data for each
class is collected in a data structure along with var­
ious intermediate quantities used by the update al­
gorithm (e.g., since the algorithm calls for the arith­
metic mean of the data points it is convenient to in­
crementally compute the sum of the elements of the
collection). The class data type has the following ac­
cessor functions associated with it (c is an instance
of class):
type(c): the type of the associated survivor
function: linear or exponential
lambda(c): the rate or slope
insts(c)_: the number of data points in the col­
lection
sum (c): the sum of the items in the collection

Assuming that c is an instance of class and p is
a new data point, the acquisition algorithm can be
described as follows:

Procedure: acquire(c, p)

insts(c) <- insts(c) + 1;
sum(c) <- sum(c) + p;
lambda(c) <- rate(c,sum(c)linsts(c));

The function rate depends on the type of survivor
function used:

Function: rate(c, J.l)
it J.l = 0
then +oo
else i:f type(c) = linear
then 0.5 I J.l
else i:f type(c) = exponential
then (ln 2) I J.l
Although we have tested our approach exten­
sively in simulations and have found the acquired
persistence data to converge very rapidly to the cor­
rect values, we do not claim that the above methods
have any wider application. The simplicity of the
algorithm and its incremental nature are attractive,
but the most compelling reason for using it is that
the algorithm works well in practice. Probabilistic
projection does not rely upon a particular method
for coming up with persistence rules. As an alter­
native, the data might be integrated off line, using
more complex (and possibly more accurate) meth­
ods.
It should be noted that our system is given the
general form of the rules it is to refine. It cannot, on
the basis of observing a large set of trucks, infer that
trucks from one company are more impatient than
.those from another company, and then proceed to
create two new persistence rules where before there
was only one. The general problem of generating

I
causal rules from experience is very difficult. We are
currently exploring methods for distinguishing dif­
ferent classes of trucks based on statistical clustering
techniques ([8] [15]). Using such methods, it appears
to be relatively straightforward to determine that a
given data set corresponds to more than one class,
and even to suggest candidate survivor functions for
the different classes. However, figuring out how to
distinguish between the classes in order to apply the
different survivor functions is considerably harder.

VI.

Conclusions

In this paper, we have sketched a theory of reason­
ing about change that extends previous theories [12]
[16]. In particular, we have shown how persistence
can be modeled in probabilistic terms. Probabilistic
projection is a special case of reasoning about contin­
uously changing quantities involving partial orders
and other sorts of �ncomplete information, and as
such it represents an intractable problem. We have
tried to identify a tractable core in the inferences
performed by probabilistic projection.
In [5], we describe a planning system capable
of continually refining its causal rules. The system
makes predictions, observes whether or not those
predictions come to pass, and modifies its rules ac­
cordingly. It is capable of routine data acquisition
and updates its probabilitistic rules in the course
of everyday operation. Initial experiments with the
prototype system have been very encouraging. We
believe that the inferential and causal rule refine­
ment capabilities designed into our system. are essen­
tial for robots to perform robustly in routine manu­
facturing situations. We hope that our current inves­
tigations will yield a new view of strategic planning
and decision making under uncertainty based on the
idea of continuous probabilistic projection.

References
[1] Thomas Dean. Large-scale temporal data bases
for planning in complex domains. In Proceed­
ings IJCAI 10. IJCAI, 1987.
[2] Thomas Dean. An approach to reasoning
about the effects of actions for automated plan­
ning systems. Annals of Operations Research,
12:147-167, 1988.
[3] Thomas Dean and Mark Boddy. Incremen­
tal causal reasoning. In Proceedings AAAI-87,
pages 196-201. AAAI, 1987.
[4] Thomas Dean and Keiji Kanazawa. Prdbabilis­
tic temporal reasoning. In Proceedings AAAI88. AAAI, 1988.

80

[5] Thomas Dean and Keiji Kanazawa. Persistence
and probabilistic inference. IEEE Transactions
on Systems, Man, and Cybernetics, forthcom­
ing.
[6] Thomas Dean and Drew V. McDermott. Tem­
poral data base management. Artificial Intelli­
gence, 32:1-55, 1987.
[7] R.O. Duda, P.E. Hart, and Nilsson N.J. Subjec­
tive bayesian methods for rule-based inference
systems. In B.W . Webber and N.J. Nilsson, ed­
itors, Readings in Artificial Intelligence. Tioga,
Palo Alto, CA, 1981.
[8] Olive Jean Dunn and Virginia A. Clark. Ap­
plied Statistics: Analysis of Variance and Re­
gression. John Wiley and Sons, 1974.
[9] M.L. Ginsberg. Does probability have a place
in non-monotonic reasoning? In Proceedings
IJCAI 9. IJCAI, 1985.
[10] Steve Hanks and Drew V. McDermott. Default
reasoning, nonmonotonic logics, and the frame
problem. In Proceedings AAAI-86, pages 328333. AAAI, 1986.
[11] John McCarthy and Patrick J. Hayes. Some
philosophical problems from the standpoint of
artificial intelligence. Machine Intelligence, 4,
1969.
[12) Drew V. McDermott. A temporal logic for rea­
soning about processes and plans. Cognitive
Science, 6:101-155, 1982.
[13] Judea Pearl. A constraint propagation ap­
proach to probabilistic reasoning. In Proceed­
ings of the 1985 AAAI/IEEE Sponsored Wark­
shop on Uncertainty and Probability in Artifi­
cial Intelligence, 1985.
[14] Earl Sacerdoti. A Structure for Plans and Be­
havior. American Elsevier Publishing Com­
pany, Inc., 1977.
[15] S. S. Shapiro and M. B. Wilk. An analysis of
variance test for normality. Biometrika, 52:591612, 1965.
[16] Yoav Shoham and Thomas Dean. Temporal
notation and causal terminology. In Proceed­
ings Seventh Annual Conference of the Cogni­
tive Science Society. Cognitive Science Society,
1985.
[17] Ryszard Syski. Random Processes. Marcel
Dekker, New York, 1979.
.

I
I
I
I
I
I
I
.I
I
I
I
I
I
I
I
I
I
I

