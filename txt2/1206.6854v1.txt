message passing in an elimination tree using the arcreversal operation of Shachter & Kenley [14] (referred
to as the EXCHANGE operation). By performing message passing in an elimination tree the need for complex matrix operations is eliminated.
We introduce a new architecture for belief update
in CLG BNs. The architecture is an extension of
lazy propagation [10] based on operations introduced
by Lauritzen & Jensen [6] and Cowell [2]. Belief update proceeds by message passing in a strong junction tree structure where messages are computed using arc-reversal and EXCHANGE operations. The EXCHANGE operation is extended to eliminate discrete
variables by arc-reversal. Variables are eliminated using a sequence of EXCHANGE operations and barren
variable removals. Posterior marginal distributions are
computed using EXCHANGE and PUSH [6] operations.
We investigate the computational efficiency of the proposed architecture by comparing its performance on a

number of randomly generated CLG BNs with the performance of the Lauritzen & Jensen [6] architecture as
implemented in the HUGIN Decision Engine, i.e., the
inference engine of the HUGIN tools. Furthermore,
we analyze the performance of various steps of belief
update such as computing posterior distributions.

2

PRELIMINARIES AND NOTATION

Y |Z1 , . . . , Zn ∼

2.1 CLG BAYESIAN NETWORK
A CLG BN N = (X, G, P, F) over variables X consists of
an acyclic, directed graph (DAG) G = (V, E), a set of
conditional probability distributions P = {P(X| π(X)) :
X ∈ ∆}, and a set of CLG density functions F =
{f(Y | π(Y)) : Y ∈ Γ } where ∆ is the set of discrete
variables and Γ is the set of continuous variables such
that X = ∆ ∪ Γ . The vertices V of G correspond one to
one with the variables of X. CLG BN N induces a multivariate normal mixture density over X on the form:
Y
Y
P(∆) · f(Γ |∆) =
P(X| π(X)) ·
f(Y | π(Y)),
X∈∆

Y∈Γ

where π(X) is the set of variables corresponding to the
parents of the vertex representing X in G.
Let Y ∈ Γ with I = π(Y) ∩ ∆ and Z = π(Y) ∩ Γ , then Y
has a CLG distribution if:
2
L(Y |I = i, Z = z) = N(α(i) + β(i)z, σ (i)),

(1)

where the mean value of Y depends linearly on the
values of the continuous parent variables Z, while the
variance is independent of Z. In (1), α(i) is a table
of real numbers, β(i) is a table of |Z|-dimensional vectors, and σ2 (i) is a table of non-negative values.
Evidence on a variable X ∈ ∆ is assumed to be an
instantiation, i.e., X = x. Evidence on a variable Y ∈ Γ
is an assignment of a value y to Y, i.e., Y = y. We
let ǫ∆ and ǫΓ denote the set of evidence on variables
of ∆ and Γ , respectively, such that ǫ = ǫ∆ ∪ ǫΓ .
Definition 2.1 [Barren Variable]
A variable X is a barren variable w.r.t. a set of variables T , evidence ǫ, and DAG G, if X 6∈ T , X 6∈ ǫ and X
only has barren descendants in G (if any).
2.2 THE

EXCHANGE

OPERATION

Let Y, Z ∈ Γ with parent sets π(Z) = {Z1 , . . . , Zn } ⊆ Γ
and π(Y) = {Z, Z1 , . . . , Zn } ⊆ Γ such that:
Y |Z, Z1 , . . . , Zn ∼ N(αY + βZ Z +

n
X

βi Zi , σ2Y ),

i=1

Z|Z1 , . . . , Zn ∼ N(αZ +

n
X
i=1

The EXCHANGE operation is essentially Bayes’ theorem.
It converts the above pair of distributions such that Y
becomes a parent of Z in the domain graph spanned by
the two distributions maintaining the same joint probability density function of the original pair [2]. Graphically speaking the EXCHANGE operation corresponds
to arc-reversal in the domain graph. The distribution
of Y after EXCHANGE is:

δi Zi , σ2Z ).

N(αY + βZ αZ +

n
X

(βi + βZ δi )Zi , σ2Y + β2Z σ2Z ),

i=1

while the distribution of Z is (Cowell [2] considers
three different cases depending on the values of σ2Y
and σ2Z that are mathematical limits of this case):


σ2Z σ2Y
ρ
,
,
Z|Y, Z1 , . . . , Zn ∼ N
σ2Y + β2Z σ2Z σ2Y + β2Z σ2Z
where
ρ = αZ σ2Y −αY βZ σ2Z +

n
X

(δi σ2Y −βi βZ σ2Z )Zi +βZ σ2Z Y.

i=1

It is straightforward to extend the EXCHANGE operation to handle discrete variables. Let Xi , Xj ∈ ∆ with
parent sets π(Xi ) = {X1 , . . . , Xn } ⊆ ∆ and π(Xj ) =
{Xi , X1 , . . . , Xn } ⊆ ∆ such that p(Xj |Xi , X1 , . . . , Xn )
and p(Xi |X1 , . . . , Xn ) are the corresponding probability potentials of Xi and Xj , respectively. In the discrete
case the EXCHANGE operation is also essentially Bayes’
theorem. That is, the EXCHANGE operation converts
the above pair of potentials such that Xj becomes a
parent of Xi in the domain graph spanned by the two
potentials maintaining the same joint probability potential of the original pair:
p(Xj |X1 , . . . , Xn ) =
X
p(Xj |Xi , X1 , . . . , Xn )p(Xi |X1 , . . . , Xn ),
Xi

p(Xi |Xj , X1 , . . . , Xn ) =
p(Xj |Xi , X1 , . . . , Xn )p(Xi |X1 , . . . , Xn )
.
p(Xj |X1 , . . . , Xn )
Graphically speaking the EXCHANGE operation corresponds to arc-reversal in the domain graph.
By construction it is never necessary to apply the EXCHANGE operation to a pair of mixed variables (i.e., a
continuous and a discrete variable). Also, prior to applying the EXCHANGE operation on a pair of adjacent
variables we make sure that the two variables share
the same set of parents. This is achieved by straightforward domain extensions.

2.3 THE STRONG JUNCTION TREE
Belief update is performed by message passing in a
strong junction tree T = (C, S) with cliques C, separators S and strong root R ∈ C. T has the property
that for all adjacent cliques A and B with A closer to R
than B, it holds that S = A ∩ B ⊆ ∆ or B \ A ⊆ Γ . Let A
and B be adjacent cliques with A closer to R than B
and such that S = A ∩ B. Then, A is referred to as the
parent clique of B (denoted πC (B)) and S is referred to
as the parent separator of B (denoted πS (B)).
A clique C ∈ C is referred to as a boundary clique if C ∩
Γ 6= ∅ and either B ⊆ ∆ or B ∩ Γ is instantiated by
evidence where B = πC (C). Let bd(C) denote the set
of boundary cliques.

We define the contraction of π∅ as c(π∅ ) = 1.
Definition 3.4 [Projection]
The projection of a potential πW = (PW , FW ) to a
subset U ⊆ W denotes the potential πU = π↓U
W =
(PU , FU ) on U obtained by performing a sequence
of EXCHANGE operations and barren variable removals
eliminating variables of W \ U.
In projection continuous variables are eliminated before discrete variables. Notice that the head of any
factor or density will consists of a single variable or
a single piece of evidence. If a variable X is barren,
then X and its factor or density may be removed without further computation.
3.2 INITIALIZATION

3

LAZY PROPAGATION

A junction tree for a discrete BN is by construction
wide enough to support the computation of any posterior marginal given any subset of evidence. The junction tree is, however, often too wide to take advantage
of independence properties induced by evidence. Lazy
propagation aims at taking advantage of independence and irrelevance properties induced by evidence
in a Shenoy-Shafer message passing scheme [10]. In
Lazy propagation belief update proceeds by message
passing in a junction tree maintaining decompositions
of clique and separator potentials.

The first step in initialization of T = (C, S) is to associate π∅ with each clique C ∈ C. Next, for each X ∈ ∆,
we assign P(X| π(X)) ∈ P, to the clique C closest to R
such that fa(X) ⊆ C where fa(X) = π(X) ∪ {X}. Similarly, for each Y ∈ Γ . After initialization each clique C
holds a potential πC = (P, F). The joint potential πX
on T = (C, S) is therefore:
!
O
[
[
πX =
πC =
{P(X| π(X))},
{f(Y | π(Y))} .
C∈C

X∈∆

The contraction of the joint potential πX is:
O
Y
Y
c(πX ) = c(
πC ) =
P(X| π(X)) ·
f(Y | π(Y)).
C∈C

3.1 POTENTIALS AND OPERATIONS
Definition 3.1 [Potential]
A potential on W ⊆ X is a pair πW = (P, F) where P
is a set of (discrete) probability potentials on subsets
of W and F is a set of probability density functions on
subsets of W ∩ Γ conditional on subsets of W ∩ ∆.
Elements of P are referred to as factors and elements
of F as density functions (or densities). Furthermore,
we call a potential πW vacuous if πW = (∅, ∅). The
vacuous potential is denoted π∅ .
Definition 3.2 [Combination]
The combination of two potentials πW1 = (P1 , F1 )
and πW2 = (P2 , F2 ) denotes the potential on W1 ∪ W2
given by πW1 ⊗ πW2 = (P1 ∪ P2 , F1 ∪ F2 ).

Y∈Γ

X∈∆

Y∈Γ

Evidence ǫ∆ is inserted as part of initialization while
evidence ǫΓ is inserted during message passing.
C1 X1 X2 X3 Y1
X2 X3 Y1
X1

Y1

X2

Y2

X3

Y3

C2 X2 X3 Y1 Y2
Y4

X3 Y1 Y2
C3 X3 Y1 Y2 Y3
Y1 Y2 Y3

Notice that potential combination is set union.
Definition 3.3 [Contraction]
The contraction c(πW ) of a potential πW = (P, F) is
the non-negative function on W given by:
c(πW ) =

Y
p∈P

p·

Y
f∈F

f.

C4 Y1 Y2 Y3 Y4
Figure 1: A CLG BN and junction tree
Example 3.5
Figure 1 shows a CLG BN over variables Yi ∈ Γ for i =

1, . . . , 4 and Xj ∈ ∆ for j = 1, 2, 3 and its strong junction
tree T. After initialization the clique potentials are:
πC1
πC2

= ({P(X1 ), P(X2 ), P(X3 )}, {f(Y1 |X1 )}),
= (∅, {f(Y2 |X2 )}),

πC3

= (∅, {f(Y3 |X3 )}),

πC4

= (∅, {f(Y4 |Y1 , Y2 , Y3 )}).

The domain of each factor in any clique potential consists of a single variable. This is contrary to both
the Lauritzen & Jensen [6] and Cowell [2] architectures
where each clique has a probability potential over all
discrete variables in the clique. This representation is
storage demanding when Y4 has additional parents each
having a single discrete variable as parent and when the
discrete variables have many states.
The above example illustrates the structure of a set
of CLG BNs used in production by a commercial customer. In this application a large part of the discrete
variables are observed making the present inference
scheme very efficient on this type of network.
3.3 PROPAGATION
Propagation of information in T proceeds by message
passing via the separators S. The separator S = A ∩
B between two adjacent cliques A and B stores the
messages passed between A and B, see Figure 2.

R

···

B

S

A

..
.

where πC→ A is the message passed from C to A and ↓
is the projection operation based on EXCHANGE operations and barren variable removals.
3.5 THE

PUSH

OPERATION

A strong junction tree representation T of a CLG BN
is not always wide enough to support the insertion of
evidence on any continuous variable or the calculation
of any posterior marginal density function. If the junction tree is not wide-enough to support a calculation,
then the PUSH operation is used [6].
The marginal density of a variable Y ∈ Γ is, in general,
a mixture of Gaussian distributions. In order to compute the marginal mixture of Y, it may be necessary
to (temporarily) rearrange the content of cliques and
separators of T. The PUSH operation is applied in order
to rearrange T such that Y becomes part of a boundary clique. This is achieved by extending cliques and
separators to include Y and collecting Y towards R.
Assume A is the clique closest to R such that Y ∈ A,
A 6∈ bd(C), B = πC (A), and S = πS (A), see Figure 2.
The PUSH operation extends S and B to include Y. In
the process any continuous variable Z ∈ T(f) such
that Z 6∈ S is eliminated from the density f of Y
where T(f) is the tail of f, i.e., the set of conditioning variables. The process of eliminating tail variables
not in S is repeated recursively until T(f) ⊆ S. The
resulting density f is associated with πB and πA→ B .
The PUSH operation is applied recursively on the parent clique until Y becomes part of a boundary clique.
3.6 INSERTION OF CONTINUOUS EVIDENCE

Figure 2: A junction tree with root clique R
Messages are passed from leaf cliques toward R by recursively letting each clique A pass a message to its
parent B whenever A has received a message from
each C ∈ adj(A) \ {B} (C OLLECT). Messages are,
subsequently, passed in the opposite direction (D IS TRIBUTE). D ISTRIBUTE is performed from the root to
boundary cliques.
3.4 MESSAGES
The message πA→ B is passed from A ∈ C to B ∈
adj(A) by absorption. Absorption from A to B involves
eliminating the variables A \ B from the combination
of the potential associated with A and the messages
passed to A from each neighbor except B. The message πA→ B is computed as:
↓B
,
πA→ B = πA ⊗ ⊗C∈adj(A)\{B} πC→ A

Let Y ∈ Γ be instantiated by evidence ǫY = {Y = y},
let f(Y | π(Y)) be the density function for Y given π(Y)
and let C be the clique to which f(Y | π(Y)) is associated. Assume Y has only discrete parents, if any,
i.e., I = π(Y) ⊆ ∆. Insertion of evidence ǫY produces
a factor p(y|I) such that:

exp −(y − αY (i))2 /(2σ2 (i))
p
,
p(y|I = i) =
2πσ2 (i)

where we assume σ2Y (i) > 0 for all i [6, 2]. The clique
potential πC = (P, F) is updated such that π∗C = (P ∪
{p}, F \ {f}). If σ2 (i) = 0, insertion of evidence may be
undefined, see [2] who cites [6].

If π(Y) 6⊆ ∆, then a sequence of PUSH operations are
performed in order to compute the marginal density
function for Y. The density f of Y is pushed to the
boundary clique. Subsequently, evidence ǫY is inserted as described above. This implies that the insertion of evidence on a continuous variable may change

the content of clique and separator potentials. This occurs when it is necessary to apply the PUSH operation
in order to insert ǫY . Finally, Y is instantiated in all
density functions where Y is a tail variable.
Notice that bd(C) may change when ǫY is inserted.
Example 3.6
Consider the simple CLG BN and its corresponding junction tree T shown in Figure 3. After initialization the
clique potentials are:
=
=

πC1
πC2

({P(X)}, {f(Y1 |X)}),
(∅, {f(Y2 |Y1 }).

Assume evidence ǫ = {Y2 = y2 }. Since the tail
of f(Y2 |Y1 ) is continuous and a subset of the parent separator, it is necessary to apply the PUSH operation on Y2
in order to insert ǫ into T.
X

XY1

Y1

Y1

Y2

Y1 Y2

C1

C2

Figure 3: Insertion of evidence on Y2 requires a
operation

PUSH

3. Insert evidence ǫΓ using the PUSH operation.
4. Perform in sequence a C OLLECT and a D ISTRIBUTE
operation on R.
During the C OLLECT operation performed in step 4
messages are passed from the boundary cliques to R.
Thus, the effect of steps 2 and 4 is that two messages have been passed between each pair of adjacent
cliques on any path between the root R and a boundary clique. No messages are passed from boundary
cliques to leave cliques.
The architectures described in [2], [6], and [9] each
does a full propagation over all the nodes of the computation tree prior to inserting ǫ whereas we do only
a partial C OLLECT prior to inserting ǫ∆ .
3.8 POSTERIOR MARGINALS
The posterior marginal P(X|ǫ) for X ∈ ∆ may be
computed from any clique or separator containing X.
Since ǫΓ is incorporated using PUSH operations, no Y ∈
Γ needs to be eliminated in the process of computing P(X|ǫ). If X ∈ C, then P(X|ǫ) is computed as:
Y
X Y
X
c(πC ) =
f
p·
P(X|ǫ) ∝
=

First the density f(Y2 |Y1 ) is pushed to the parent clique,
next an EXCHANGE operation is performed on the arc
(Y1 , Y2 ). Next, densities including Y2 in the domain are
instantiated to reflect the evidence. Once the PUSH operation completes the revised clique potentials are:
π∗C1
π∗C2

=
=

({P(X), p(y2 |X)}, {f(Y1 |X, y2 )}),
(∅, ∅).

1. Initialization including insertion of evidence ǫ∆ .
2. At each A ∈ bd(C) C OLLECT from every B ∈
adj(A) \ {πC (A)}.

f∈FC

p,

where πC = (PC , FC ) is the clique potential for C. On
the other hand, if S is a separator containing X with
adjacent cliques A and B, then P(X|ǫ) is computed as:
X
c(πA→ B ⊗ πB→ A )
P(X|ǫ) ∝
S\{X}

=
=

3.7 PROPAGATION OF CONTINUOUS EVIDENCE

Let T = (C, S) be a strong junction tree representation
and let ǫ = ǫ∆ ∪ ǫΓ be the evidence to propagate.
The evidence ǫ is propagated in T by performing the
following sequence of steps:

X Y

C\{X} p∈PC

This completes the insertion of evidence ǫ.

Section 3.3 describes the propagation scheme used
when ǫΓ = ∅. When ǫΓ 6= ∅, the recursive message
passing scheme is terminated at each boundary clique.
Once each boundary clique A ∈ bd(C) has received
messages from each C ∈ adj(A) \ {πC (A)}, continuous
evidence is inserted using the PUSH operation.

C\{X} p∈PC

C\{X}

X

Y

S\{X} p∈PA→ B ∪PB→ A

X

Y

p·
p,

Y

f

f∈FA→ B ∪FB→ A

S\{X} p∈PA→ B ∪PB→ A

where potential πA→ B = (PA→ B , FA→ B ) and potential πB→ A = (PB→ A , FB→ A ) are the potentials passed
from A to B and from B to A, respectively.
The posterior mixture for Y ∈ Γ is computed using
PUSH operations followed by a projection of the relevant boundary clique to Y and a contraction.
Example 3.7
The prior mixture densities of Y1 and Y2 of the CLG BN
shown in Figure 4 are:
X
f(Y1 ) =
P(x1 ) · f(Y1 |x1 ),
x1 ∈X1

f(Y2 )

=

X

x1 ∈X1 ,x2 ∈X2

P(x1 )P(x2 |x1 ) · f(Y2 |x1 , x2 ).

X1
X1 X2 Y1
X2

Y1

X2 Y1
X2 Y1 Y2

Y2
Figure 4: Prior density for Y1 has ||X1 || components

The density for Y1 has only kX1 k components. This
is a reduction compared to the Lauritzen & Jensenand Cowell architectures where the marginal density
will have kX1 k · kX2 k components. The reduction is due
to the decomposition of clique and separator potentials.
Example 3.8
Consider again Figure 1 of Example 3.5. The number of components in the mixture marginal for Y4
is kX1 k · kX2 k · kX3 k whereas the number of components in the mixture marginal for Yi is equal to kXi k.
This is a reduction compared to the Lauritzen & Jensenand Cowell architectures where the number of components is kX1 k · kX2 k · kX3 k. Hence, in the case of a larger
number of variables (and same structure), the storage
and time reduction can be significant.

4

COMPARISON

separator has a CG potential over its variables. This
implies that complex matrix operations are required
during belief update.
Initialization plays an important role in the Lauritzen
& Jensen [6] architecture. It produces a Lauritzen
& Spiegelhalter-like junction tree representation [7]
where clique potentials are conditioned on the continuous variables of the parent separator. This ensures that a variable Y ∈ Γ is only propagated when
inserting evidence on Y or when computing the mixture marginal for Y. Furthermore, a complex recursive
combination operator may be required during initialization in order to combine CG potentials. The need
for conditioning, recursive combination, and complex
matrix operations is eliminated in both the Cowell [2]
and the present architectures.
Example 4.1
Figure 5 shows a CLG BN and its junction tree T. The
initial clique potentials are:
πC1
πC2

= ({P(X1 )}, {f(Y1 |X1 ), f(Y3 |X1 , Y1 , Y2 )}),
= (∅, {f(Y2 |Y1 , Y4 ), f(Y4 )}).

In the Lauritzen & Jensen [6] architecture initialization
of T requires a recursive combination operation.
C1 X1 Y1 Y2 Y3
X1

Y1

Y2
Y1 Y2

4.1 COWELL
Y3
Cowell [2] presents an algorithm for belief update
where message passing proceeds on an elimination
tree rather than a (strong) junction tree. This produces a local propagation scheme in which all calculations involving continuous variables are performed by
manipulating univariate regressions (avoiding matrix
operations) such that continuous variables are eliminated using EXCHANGE operations.

Y4

C2 Y1 Y2 Y4

Figure 5: Initialization requires recursive combination
in the Lauritzen & Jensen architecture
In the proposed architecture initialization amounts to
associating probability distributions and densities with
cliques of T. The prior distribution of each variable is
readily computed using the EXCHANGE operation.

The three main differences between the present propagation scheme and Cowell [2] are: use of a strong
junction tree as opposed to an elimination tree, use of
EXCHANGE to eliminate both continuous and discrete
variables and a single round of message passing.

The Lauritzen & Jensen [6] architecture calculates
weak marginals during D ISTRIBUTE. This is not the
case for the Cowell [2] nor the present architecture.

4.2 LAURITZEN AND JENSEN

The present architecture is quite different from the architecture proposed by Madsen [9]. The latter architecture is an extension of Madsen & Jensen [11] to
the case of CLG BNs based on the propagation scheme
of Lauritzen & Jensen [6]. This implies a number
of differences when compared to the present scheme.
First, the architecture is based solely on the operations
of Lauritzen & Jensen [6] whereas the present scheme
is based on operations of both Lauritzen & Jensen [6]

The architecture of Lauritzen & Jensen [6] performs
belief update by message passing in a strong junction
tree architecture. A CG potential [8] is associated with
each clique and separator. A CG potential consists of
a probability potential over discrete variables and a
probability density function over continuous variables
conditional on the discrete variables. Each clique and

4.3 MADSEN

and Cowell [2]. Second, a Lauritzen & Spiegelhalterlike junction tree representation is achieved as the result of initialization, i.e., during the initial C OLLECT
operation, the sender clique is conditioned on the
continuous variables of the parent separator. Finally,
in the present scheme variable eliminations are performed using EXCHANGE operations and barren variable removals.

6

5

1

PERFORMANCE ANALYSIS

A preliminary performance analysis on a set of randomly generated CLG BNs has been made. Networks
with 25, 50, 75, 100, 125, and 150 variables with different fractions of continuous variables (0, 0.25, 0.5,
0.75, 1) were randomly generated (ten networks of
each size). For each network, evidence sets of 0 to 20
instantiated variables were generated and 40 sets of
evidence were generated for each size.
We compared the performance of the present architecture with the performance of the commercial implementation of the Lauritzen & Jensen [6] architecture
in the HUGIN Decision Engine. Table 1 shows statisTable 1: Statistics On CLG BN net50-4
Network
net50-4-0
net50-4-0.25
net50-4-0.5
net50-4-0.75
net50-4-1

|X|
50
50
50
50
50

|C|
42
39
38
39
40

maxC∈C s(C)
3, 888
186, 624
165, 888
1, 728
1

s(C)
18, 084
231, 309
218, 656
2, 444
40

HUGIN 0,
HUGIN 0.25,
HUGIN 0.5,
HUGIN 0.75,
HUGIN 1,
LAZY 0,
LAZY 0.25,
LAZY 0.5,
LAZY 0.75,
LAZY 1,

5
4
3
2

0
0

5

10

15

Figure 6: Average time in sec. for belief update
sets. Figure 7 illustrates how the average largest discrete domain size decreases as |ǫ| increases. Notice
that the average largest size is significantly smaller
than the size of the largest clique in the strong junction
tree.
For networks with only discrete or only continuous
variables the Lauritzen & Jensen implementation is
faster than the implementation of the proposed architecture. However, for some networks with a fraction
of 0.25 or 0.5 continuous variables Lauritzen & Jensen
is significantly slower than the proposed architecture.
70000

LAZY 0,
LAZY 0.25,
LAZY 0.5,
LAZY 0.75,
LAZY 1,

60000
50000

tics on one of the
P(net50-4)
Qnetworks used in the tests
where s(C) = X∈∆∩C kXk and s(C) = C∈C s(C).
Figure 6 shows the average time cost of belief update
in net50-4 whereas Figure 7 shows the average size of
the largest discrete configuration. A discrete configuration is either the domain of a factor or the discrete
conditioning set of a density. This is an example where
the proposed architecture is most efficient.
The present architecture maintains a factorization of
clique and separator potentials into sets of factors and
densities. This decomposition implies that the largest
discrete domain size considered during belief update
is often significantly smaller than the discrete domain
size of the largest clique in the strong junction tree.
This insight is supported by the experimental analysis,
which indicates that the Lauritzen & Jensen [6] implementation runs out of memory on most networks with
75 or more variables for a large fraction of the evidence sets whereas the present architecture runs out
of memory on a much smaller fraction of the evidence

20

Number of instantiations

40000
30000
20000
10000
0
0

5

10

15

20

Number of instantiations

Figure 7: Average size in numbers
The typical decrease in average time cost as |ǫ| increases for lazy propagation is not as significant on
CLG BNs. The reason is that computing marginal densities is a dominant and a non-constant factor in the
time cost of belief update. A significant amount of the
total time for propagating evidence is spent on computing posterior mixture marginals. In the proposed
architecture the number and the computational cost

of PUSH operations is reduced by a decomposition of
clique and separator potentials. The significance of the
decrease depends on the ratio of continuous variables.
Figure 8 shows the average time cost of computing
marginals in net50-4. Notice that a significant amount
of the time cost originates from computing marginals.
5

4
3.5
3
2.5

[4] F. V. Jensen. Bayesian Networks and Decision
Graphs. Springer-Verlag, 2001.
[5] S. L. Lauritzen. Propagation of probabilities,
means and variances in mixed graphical association models. Journal of the American Statistical
Association, 87(420):1098–1108, 1992.

2
1.5
1
0.5
0
0

5

10

15

20

Number of instantiations

Figure 8: Average time in sec. for marginals
On most of the networks considered in the test —
where belief update is feasible — the commercial implementation of Lauritzen & Jensen [6] is most efficient (typically networks with less than 75 variables).
The HUGIN Decision Engine has significantly more efficient data structures and operations than the implementation of the proposed architecture though.
The experiments were performed using a Java implementation on a desktop computer with a 2.2 GHz AMD
AthlonTM CPU and 768 MB RAM running Redhat 8.

6

[2] R. G. Cowell. Local Propagation In Conditional
Gaussian Bayesian Networks. Journal of Machine
Learning Research, 6:1517–1550, 2005.
[3] R. G. Cowell, A. P. Dawid, S. L. Lauritzen, and
D. J. Spiegelhalter. Probabilistic Networks and Expert Systems. Springer-Verlag, 1999.

HUGIN 0,
HUGIN 0.25,
HUGIN 0.5,
HUGIN 0.75,
HUGIN 1,
LAZY 0,
LAZY 0.25,
LAZY 0.5,
LAZY 0.75,
LAZY 1,

4.5

variables. IEEE Transactions on Systems, Man.
and Cybernetics, 25(6):910–916, 1995.

CONCLUSION

An architecture for belief update in CLG BNs based on
lazy propagation where messages are computed using EXCHANGE operations and barren variable eliminations has been presented. The architecture is based
on extended versions of operations introduced by Lauritzen & Jensen [6] and Cowell [2].
Despite a significant difference in the efficiency of table operations the proposed architecture is — in some
cases — more efficient than a commercial implementation of the Lauritzen & Jensen [6] architecture. The
results of the performance evaluation indicate a significant potential of the proposed architecture.

References
[1] K. C. Chang and R. Fung. Symbolic probabilistic inference with both discrete and continuous

[6] S. L. Lauritzen and F. Jensen. Stable Local
Computation with Mixed Gaussian Distributions.
Statistics and Computing, 11(2):191–203, 2001.
[7] S. L. Lauritzen and D. J. Spiegelhalter. Local computations with probabilities on graphical structures and their application to expert systems. Journal of the Royal Statistical Society, B.,
50(2):157–224, 1988.
[8] S. L. Lauritzen and N. Wermuth. Graphical models for associations between variables, some of
which are qualitative and some quantitative. The
Annals of Statistics, 17:31–57, 1989.
[9] A. L. Madsen. All Good Things Come to Those
Who Are Lazy - Efficient Inference in Bayesian Networks and Influence Diagrams Based on Lazy Evaluation. PhD thesis, Department of Computer Science, Aalborg University, 1999.
[10] A. L. Madsen. Variations Over the Message Computation Algorithm of Lazy Propagation. IEEE
Transactions on Systems, Man. and Cybernetics
Part B, 2006. To appear.
[11] A. L. Madsen and F. V. Jensen. Lazy propagation: A junction tree inference algorithm based
on lazy evaluation. Artificial Intelligence, 113(12):203–245, 1999.
[12] J. Pearl. Probabilistic Reasoning in Intelligence
Systems. Morgan Kaufmann Publishers, 1988.
[13] R. Shachter, B. D’Ambrosio, and B. DelFavero.
Symbolic probabilistic inference in belief networks. In Proceedings Eighth National Conference
on AI, pages 126–131, 1990.
[14] R. D. Shachter and C. R. Kenley. Gaussian influence diagrams. Management Science, 35(5):527–
549, 1989.

