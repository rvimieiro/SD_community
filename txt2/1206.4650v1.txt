From the analyses we are aware of, such as (Gretton
et al., 2009) on the confidence bound of the RND by
KMM, (Kanamori et al., 2012) on the convergence rate
of the least-squares estimate of the RND, and (Cortes
et al., 2008) on the distributional stability, they all
assume that certain functions lie in the reproducing
kernel Hilbert space (RKHS) induced by some user selected kernel. Since this assumption is impossible to
verify (even worse, almost certainly violated in practice), one naturally wonders if we can replace it with
something more reasonable. Such goal is pursued in
this paper and constitutes our main contribution.
We consider the following simple problem: Given the
tr
training sample {(Xitr , Yitr )}ni=1
and the test sample
te nte
{Xi }i=1 , how well can we estimate the expected value
EY te , provided covariate shift has happened? Note
that we do not observe the output Yite on the test
sample. This problem, at a first glance, ought to be

Analysis of Kernel Mean Matching under Covariate Shift

‚Äúeasy‚Äù, after all we are humbly asking for estimating a scalar. Indeed, under usual assumptions, plus
the nearly impossible assumption that the regression
function lies in the RKHS, we prove a parametric rate,
‚àí1
‚àí1
that is O(ntr 2 + nte 2 ), for the KMM estimator in Theorem 1 below (to fix ideas, we focus exclusively on
KMM in this paper). For a more realistic assumption on the regression function that we borrow from
learning theory (Cucker & Zhou, 2007), the convergence rate, proved in Theorem 2, degrades gracefully
‚àí

Œ∏

‚àí

Œ∏

to O(ntr 2(Œ∏+2) +nte 2(Œ∏+2) ), where Œ∏ > 0 is a smoothness
parameter measuring certain regularity of the regression function (in terms of the kernel). Observe that in
the limit when Œ∏ ‚Üí ‚àû, the regression function eventually lies in the RKHS and we recover the previous
parametric rate. In this regard our bound in Theorem 2 is asymptotically optimal. A very nice feature
we discovered for the KMM estimator is that it does
not require knowledge of the smoothness parameter Œ∏,
thus, it is in some sense adaptive.
On the negative side, we show that, if the chosen kernel does not interact very well with the unknown regression function, the convergence rate of the
KMM estimator could be exceedingly slow, roughly
¬∑nte
), where s > 0 again measures certain
O(log‚àís nntrtr+n
te
regularity of the regression function. This unfortunate
result should draw attention to the importance of selecting which kernel to be used in practice. A thorough
comparison between the KMM estimator and the natural plug-in estimator, conducted in Section 4.3, also
reveals the superiority of the former.
We point out that our results are far from giving a
complete picture even for the simple problem we consider here, for instance, it is unclear to us whether or
not the rate in Theorem 2 can be improved, eventually,
to the parametric rate in Theorem 1? Nevertheless, we
hope that our paper will convince others about the importance and possibility to work with more reasonable
assumptions under covariate shift, and as an example,
suggest relevant tools which can be used to achieve
that goal.

2. Preliminaries
In this section we formally state the covariate shift
problem under our consideration, followed by some relevant discussions.
2.1. Problem Setup
Consider the familiar supervised learning setting,
where we are given independent and identically disntr
tributed (i.i.d.) training samples {(Xitr , Yitr )}i=1
from

the joint (Borel) probability measure Ptr (dx, dy) on
the (topological) domain X √ó Y, and i.i.d. test samte
from the joint probability measure
ples {Xite }ni=1
Pte (dx, dy) on the same domain. Notice that we do
not observe the output Yite on the test sample, and
more importantly, we do not necessarily assume that
the training and test sample are drawn from the same
probability measure. The problem we consider in this
paper is to estimate the expected value EY te from the
tr
and the test sample
training sample {(Xitr , Yitr )}ni=1
te nte
{Xi }i=1 . In particular, we would like to determine
how fast, say, the 1 ‚àí Œ¥ confidence interval for our estimate shrinks to 0 when the sample sizes ntr and nte
increase to infinity.
This problem, in its full generality, cannot be solved
simply because the training probability measure can
be completely irrelevant to the test probability measure that we are interested in. However, if the two
probability measures are indeed related in a nontrivial way, our problem becomes solvable. One particular
example, which we focus on hereafter, is known in the
literature as covariate shift (Shimodaira, 2000):
Assumption 1 (Covariate shift assumption)
Ptr (dy|x) = Pte (dy|x).

(1)

We use the same notation for the joint, conditional and
marginal probability measures, which should cause no
confusion as the arguments would reveal which measure is being referred to. Note that the equality
P(dx, dy) = P(dy|x)¬∑P(dx) holds from the definition of
the conditional probability measure, whose existence
can be confirmed under very mild assumptions.
Under the covariate shift assumption, the difficulty of
our problem, of course, lies entirely on the potential
mismatch between the marginal probability measures
Ptr (dx) and Pte (dx). But the Bayes rule already suggests a straightforward approach:
Pte (dx, dy) = Pte (dy|x)¬∑Pte (dx) = Ptr (dx, dy)¬∑

dPte
(x),
dPtr

where the three quantities on the right-hand side can
all be estimated from the given samples. However, in
order for the above equation to make sense, we need
Assumption 2 (Continuity assumption) The
te
Radon-Nikodym derivative Œ≤(x) := dP
dPtr (x) is welldefined and bounded from above by B < ‚àû.
Note
that B ‚â• 1 due to the normalization constraint
R
Œ≤(x)P
tr (dx) = 1. The Radon-Nikodym derivative
X
(RND) is also called the importance weight or the
density ratio in the literature. Evidently, if Œ≤(x) is

Analysis of Kernel Mean Matching under Covariate Shift

not well-defined, i.e., there exists some measurable
set A such that Pte (A) > 0 and Ptr (A) = 0, then
in general we cannot infer Pte (dx, dy) from merely
Ptr (dx), Pte (dx) and Ptr (dy|x), even under the covariate shift assumption. The bounded from above assumption is more artificial. Recently, in a different
setting, (Cortes et al., 2010) managed to replace this
assumption with a bounded second moment assumption, at the expense of sacrificing the rate a bit. For us,
since the domain X will be assumed to be compact, the
bounded from above assumption is not too restrictive
(automatically holds when Œ≤(x) is, say, continuous).
Once we have the RND Œ≤(x), it becomes easy to correct the sampling bias caused by the mismatch between Ptr (dx) and Pte (dx), hence solving our problem.
Formally, let
Z
m(x) :=
y Pte (dy|x)
(2)
Y

be the regression function, then
Z
Z
EY te =
m(x) Pte (dx) =
m(x)Œ≤(x) Ptr (dx).
X

X

By the i.i.d. assumption, P
a reasonable estimator for
ntr
Œ≤(Xitr ) ¬∑ Yitr . Hence,
EY te would then be n1tr i=1
similarly to most publications on covariate shift, our
problem boils down to estimating the RND Œ≤(x).

KMM tries to match the mean elements in a feature
space induced by a kernel k(¬∑, ¬∑) on the domain X √ó X :
)

(

ntr
nte
1 X
1 X
min LÃÇ(Œ≤ÃÇ) :=
Œ≤ÃÇi Œ¶(Xitr ) ‚àí
Œ¶(Xite )
ntr i=1
nte i=1
Œ≤ÃÇi

s.t. 0 ‚â§ Œ≤ÃÇi ‚â§ B,

H

(3)

where Œ¶ : X 7‚Üí H denotes the canonical feature map,
H is the reproducing kernel Hilbert space1 (RKHS) induced by the kernel k and k ¬∑ kH stands for the norm in
H. To simplify later analysis, we have chosen to omit
Pntr
Œ≤ÃÇi ‚àí 1 ‚â§ ,
the normalization constraint n1tr i=1
where  is a small positive number, mainly to reflect the fluctuation caused by random samples. It
is not hard to verify that (3) is in fact an instance
of quadratic programming, hence can be efficiently
solved. More details can be found in the paper of
Gretton et al. (2009).
A finite sample 1‚àíŒ¥ confidence bound for LÃÇ(Œ≤) (similar
as (10) below) is established in Gretton et al. (2009).
This bound is further transferred into a confidence
bound for the generalization error of some family of
loss minimization algorithms in Cortes et al. (2008),
under the notion of distributional stability. However,
neither results can provide a direct answer to our problem: a finite sample confidence bound on the estimate
of EY te .

2.2. A Naive Estimator?
2.4. Plug-in Estimator
An immediate solution for estimating Œ≤(x) is to estimate the two marginal measures from the training sample {Xitr } and the test sample {Xite }, respectively. For instance, if we know a third (Borel) measure
Q(dx) (usually the Lebesgue measure on Rd ) such that
dPtr
te
both dP
dQ (x) and dQ (x) exist, we can employ standard density estimators to estimate them and then set
dPtr
te
Œ≤ÃÇ(x) = dP
dQ (x)/ dQ (x). However, this naive approach
is known to be inferior since density estimation in high
dimensions is hard, and moreover, small estimation ertr
ror in dP
dQ (x) could change Œ≤ÃÇ(x) significantly. To our
knowledge, there is little theoretical analysis on this
seemingly naive approach.
2.3. A Better Estimator?
It seems more appealing to directly estimate the RND
Œ≤(x). Indeed, a large body of work has been devoted to this line of research (Zadrozny, 2004; Huang
et al., 2007; Sugiyama et al., 2008; Cortes et al., 2008;
Bickel et al., 2009; Kanamori et al., 2009). From the
many references, we single out the kernel mean matching (KMM) algorithm, first proposed by Huang et al.
(2007) and is also the basis of this paper.

Another natural approach is to estimate the regression
function from the training sample and then plug into
the test set. We postpone the discussion and comparison with respect to this estimator until section 4.3.

3. Motivation
We motivate the relevance of our problem in this section.
Suppose we have an ensemble of classifiers, say,
{fj }N
all trained on the training sample
j=1 ,
tr
{(Xitr , Yitr )}ni=1
.
A useful task is to compare,
hence rank, the classifiers by their generalization
errors. This is usually done by assessing the classifiers
te
on some hold out test sample {(Xite , Yite )}ni=1
. It is
not uncommon that the test sample is drawn from
some different probability measure than the training
sample, i.e. covariate shift has happened. Since it
could be too costly to re-train the classifiers when the
test sample is available, we nevertheless still like to
1
A thorough background on the theory of reproducing
kernels can be found in Aronszajn (1950).

Analysis of Kernel Mean Matching under Covariate Shift

where recall that m(x) is the regression function defined in (2) and Œ≤(x) is the true RND.

have a principled way to rank the classifiers.
Let `(¬∑, ¬∑) be the user‚Äôs favourite loss function, and set
tr
te
Zij
= `(fj (Xitr ), Yitr ), Zij
= `(fj (Xite ), Yite ), then we
te nte
}i=1 to estimate
can use the empirical average of {Zij
te
the generalization error, that is E(Zij
), of classifier
fj . But what if we do not have access to Yite hence
te
consequently Zij
? Can we still accomplish the ranking
job?

The equality in (5) indeed holds under at least two conditions (respectively). First, if the regression function
m ‚àà H, then taking inner products with m in (4) and
applying the reproducing property we get (5). Second,
if the kernel k is characteristicR(Sriperumbudur et al.,
2010), meaning that the map X Œ¶(x)P(dx) from the
space of probability measures to the RKHS H is injective, then we conclude Œ≤ÃÇ ‚àó = Œ≤ from (4) hence follows
(5).

The answer is yes, and it is precisely the covariate
shift problem under our consideration. To see that,
tr ntr
te
. Under
consider the pair {Xitr , Zij
}i=1 and {Xite }ni=1
the covariate shift assumption, that is Ptr (dy|x) =
Pte (dy|x), it is not hard to see that Ptr (dz|x) =
Pte (dz|x), hence the covariate shift assumption holds
for the ranking problem, therefore the confidence
bounds derived in the next section provide an effective solution.

The above two cases suggest the possibility of solving our problem by KMM. Of course, in reality one
only has finite samples from the underlying probability
measures, thus calls for a thorough study of the empirical KMM, i.e. (3). Interestingly, our analysis reveals
that in the first case above, we indeed can have a parametric rate while in the second case the rate becomes
nonparametric, hence inferior (but does not seem to
rely on the characteristic property of the kernel).

We do not report numerical experiments in this paper
for two reasons: 1). Our main interest is on theoretical
analysis; 2). Exhaustive experimental results on KMM
can already be found in Gretton et al. (2009).

4.2. The empirical version
In this subsection we analyze KMM in details. The
following assumption will be needed:

4. Theoretical Analysis
This section contains our main contribution, i.e., a
theoretical analysis of the KMM estimator for EY te .

Assumption 3 (Compactness assumption) X is
a compact metrizable space, Y ‚äÜ [0, 1], and the kernel k is continuous, whence kkk‚àû ‚â§ C 2 < ‚àû.

4.1. The population version
Let us first take a look at the population version of
KMM2 , which is much easier to analyze and provides
valuable insights:
Z
Œ¶(x)Œ≤ÃÇ(x)Ptr (dx) ‚àí
Œ¶(x)Pte (dx)
Œ≤ÃÇ
X
X
Z
s.t. 0 ‚â§ Œ≤ÃÇ ‚â§ B,
Œ≤ÃÇ(x)Ptr (dx) = 1.

Œ≤ÃÇ ‚àó ‚àà arg min

Z

X

The minimum value is 0 since the true RND Œ≤(x) is
apparently feasible, hence at optimum we always have
Z
Z
Œ¶(x)Œ≤ÃÇ ‚àó (x)Ptr (dx) =
Œ¶(x)Pte (dx).
(4)
X

X

The
question is whether the natural estimator
R
Œ≤ÃÇ ‚àó (x)y Ptr (dx, dy) is consistent? In other words,
X √óY
is
Z
Z
?
m(x)Œ≤ÃÇ ‚àó (x)Ptr (dx) = EY te = m(x)Œ≤(x)Ptr (dx),
X

X

(5)
2
All Hilbert space valued integrals in this paper are to
be understood as the Bochner integral (Yosida, 1980).

H

We use k¬∑k‚àû for the supremum norm. Under the above
assumption, the feature map Œ¶ is continuous hence
measurable (with respect to the Borel œÉ-fields), and
the RKHS is separable, therefore the Bochner integrals
in the previous subsection are well-defined. Moreover,
the conditional probability measure indeed exists under our assumption.
We are now ready to deriveP
a finite sample confidence
ntr
Œ≤ÃÇi Yitr ‚àí EY te |, where
bound for our estimate | n1tr i=1
Œ≤ÃÇi is a minimizer of (3). We start by splitting the sum:
ntr
1 X
Œ≤ÃÇi Yitr ‚àí EY te
ntr i=1

=

ntr
1 X
Œ≤ÃÇi (Yitr ‚àí m(Xitr ))
ntr i=1

+

ntr
1 X
(Œ≤ÃÇi ‚àí Œ≤i )(m(Xitr ) ‚àí h(Xitr ))
ntr i=1

+

ntr
1 X
(Œ≤ÃÇi ‚àí Œ≤i )h(Xitr )
ntr i=1

ntr
1 X
Œ≤i m(Xitr ) ‚àí EY te ,
+
ntr i=1

(6)

where Œ≤i := Œ≤(Xitr ) and h ‚àà H is to be specified later.

Analysis of Kernel Mean Matching under Covariate Shift

We bound each term individually. For the last term
in (6), we can apply Hoeffding‚Äôs inequality (Hoeffding,
1963) to conclude that with probability at least 1 ‚àí Œ¥,
ntr
1 X
Œ≤i m(Xitr ) ‚àí EY te ‚â§ B
ntr i=1

r

1
2
log .
2ntr
Œ¥

(7)

The first term in (6) can be bounded similarly. Conditioned on {Xitr } and {Xite }, we apply again Hoeffding‚Äôs inequality. Note that Œ≤ÃÇi (Yitr ‚àí m(Xitr )) ‚àà
[‚àíŒ≤ÃÇi m(Xitr ), Œ≤ÃÇi (1 ‚àí m(Xitr ))], therefore its range is of
size Œ≤ÃÇi . With probability at least 1 ‚àí Œ¥,
v
r
u
ntr
ntr
u 1 X
1 X
1
tr
tr
2
t
Œ≤ÃÇi (Yi ‚àí m(Xi )) ‚â§
Œ≤ÃÇi ¬∑
log
ntr i=1
ntr i=1
2ntr
r
1
2
(8)
‚â§B
log .
2ntr
Œ¥
The second and third terms in (6) require more work.
Consider first the third term:

expectation, and then bound the expectation straightforwardly. In general, Pinelis‚Äôs inequality will lead to
(slightly) tighter bounds due to its known optimality
(in certain sense).
Finally, we come to the second term left in (6), which
is roughly the approximation error in learning theory (Cucker & Zhou, 2007). Note that all confidence
bounds we p
have derived so far shrink at the parametric rate O( 1/ntr + 1/nte ). However, from here on
we will have to tolerate nonparametric rates. Since
we are going to apply different approximation error
bounds to the second term in (6), it seems more convenient to collect the results separately. We start with
2 an encouraging result:
Œ¥
Theorem 1 Under Assumptions 1-3, if the regression
function m ‚àà H (the RKHS induced by the kernel k),
then with probability at least3 1 ‚àí Œ¥,
s 

ntr
B2
1
6
1 X
tr
te
Œ≤ÃÇi Yi ‚àí EY
‚â§M¬∑ 2
+
log ,
ntr i=1
ntr
nte
Œ¥

ntr
ntr
1 X
1 X
(Œ≤ÃÇi ‚àí Œ≤i )h(Xitr ) =
(Œ≤ÃÇi ‚àí Œ≤i )hh, Œ¶(Xitr )i
ntr i=1
ntr i=1

‚â§ khkH ¬∑

ntr
1 X
(Œ≤ÃÇi ‚àí Œ≤i )Œ¶(Xitr )
ntr i=1

H

‚â§ khkH ¬∑ [LÃÇ(Œ≤ÃÇ) + LÃÇ(Œ≤1:ntr )]
‚â§ khkH ¬∑ 2LÃÇ(Œ≤1:ntr ),

(9)

where Œ≤1:ntr denotes the restriction of Œ≤ to the training
sample {Xitr }, LÃÇ(¬∑) is defined in (3), and the equality
is because h ‚àà H (and the reproducing property of
the canonical feature map), the first inequality is by
the Cauchy-Schwarz inequality, the second inequality
is due to the triangle inequality, and the last inequality
is by the optimality of Œ≤ÃÇ and the feasibility of Œ≤1:ntr in
problem (3). Next, we bound LÃÇ(Œ≤1:ntr ):
ntr
nte
1 X
1 X
Œ≤i Œ¶(Xitr ) ‚àí
Œ¶(Xite )
ntr i=1
nte i=1
H
s 

B2
1
2
‚â§C 2
+
log
(10)
ntr
nte
Œ¥

LÃÇ(Œ≤1:ntr ) :=

with probability at least 1 ‚àí Œ¥, where the inequality
follows from the Hilbert space valued Hoeffding inequality in (Pinelis, 1994, Theorem 3.5). Note that
Pinelis proved his inequality for martingales in any
2-smooth separable Banach space (Hilbert spaces are
bona fide 2-smooth). We remark that another way, see
for instance (Gretton et al., 2009, Lemma 1.5), is to
use McDiarmid‚Äôs inequality to bound LÃÇ(Œ≤1:ntr ) by its

where M := 1+2CkmkH and Œ≤ÃÇi is computed from (3).
Proof: By assumption, setting h = m zeros out the
second term in (6). A standard union bound combining (7)-(10) completes the proof (and we simplified the
bound by slightly worsening the constant).
The confidence bound shrinks at the parametric rate,
although the constant depends on kmkH , which in general is not computable, but can be estimated from the
training sample {(Xitr , Yitr )} at a rate worse than parametric. Since this estimate inevitably introduces other
uncomputable quantities, we omit the relevant discussion. On the other hand, our bound suggests that if
a priori information about m is indeed available, one
should choose a kernel that minimizes its induced norm
on m.
The case when m 6‚àà H is less satisfactory, despite of its
practicality. We point out that a denseness argument
cannot resolve this difficulty. To be more precise, let
us assume for a moment m ‚àà C (X ) (the space of continuous functions on X ) and k be a universal kernel
(Steinwart, 2002), meaning that the RKHS induced by
k is dense in (C (X ), k ¬∑ k‚àû ). By the assumed universal property of the kernel, there exists suitable h ‚àà H
that makes the second term in (6) arbitrarily small (in
fact, can be made vanishing), however, on the other
hand, recall that the bound (9) on the third term in
(6) depends on khkH hence could blow up. If we trade
3
Throughout this paper, the confidence parameter Œ¥ is
always taken arbitrarily in (0, 1).

Analysis of Kernel Mean Matching under Covariate Shift

off the two terms appropriately, we might get a rate
that is acceptable (but worse than parametric). The
next theorem concretizes this idea.

since 0 ‚â§ m ‚â§ 1 by Assumption 3. The quantity A2 (m, R) is called the approximation error in
learning theory and its polynomial decay is known
Œ∏

Theorem 2 Under Assumptions 1-3, if A2 (m, R) :=
inf km ‚àí gkLP2 ‚â§ C2 R‚àíŒ∏/2 for some Œ∏ > 0 and
kgkH ‚â§R

tr

constant C2 ‚â• 0, then with probability at least 1 ‚àí Œ¥,
ntr
1 X
‚â§
Œ≤ÃÇi Yitr ‚àí EY te
ntr i=1
r
Œ∏
2
9
8
B
log + CŒ∏ (BC2 ) Œ∏+2 D2Œ∏+2 ,
2ntr
Œ¥
r 

q
2
where D2 := 2C 2 nBtr + n1te log 8Œ¥ +BC 2n1tr log 8Œ¥ ,
 2
CŒ∏ := (1 + 2/Œ∏) Œ∏2 Œ∏+2 and Œ≤ÃÇi is computed from (3).

Proof: By the triangle inequality,
ntr
1 X
(Œ≤ÃÇi ‚àí Œ≤i )(m(Xitr ) ‚àí h(Xitr ))
ntr i=1

‚â§B¬∑

ntr
1 X
|m(Xitr ) ‚àí h(Xitr )|.
ntr i=1

to be (almost) equivalent to m ‚àà Range(Tk2Œ∏+4 ),
see for instance Theorem 4.1 of Cucker & Zhou
operator (Tk f )(x0 ) =
R(2007).0 Here Tk is the integral
2
k(x , x)f (x)Ptr (dx) on LPtr . The smoothness paX
rameter Œ∏ > 0 measures the regularity of the regression function, and as it increases, the range space of
Œ∏

Tk2Œ∏+4 becomes smaller, hence our decay assumption
on A2 (m, R) becomes more stringent. Note that the
Œ∏
is necessarily smaller than 1/2 (but apexponent 2Œ∏+4
proaches 1/2 when Œ∏ ‚Üí ‚àû) because by Mercer‚Äôs theo1

rem Tk2 is onto H (in which case the range assumption
would bring us back to Theorem 1).
Theorem 2 shows that the confidence boundŒ∏ now
‚àí
shrinks at a slower rate, roughly O(ntr 2(Œ∏+2) +
‚àí

Œ∏

nte 2(Œ∏+2) ), which, as Œ∏ ‚Üí ‚àû, approaches the paramet‚àí1

‚àí1

ric rate O(ntr 2 + nte 2 ) derived in Theorem 1 where
we assume m ‚àà H. We point out that the source of
this slower rate comes from the irregular nature of the
regression function (in the eye of the kernel k).

The polynomial decay assumption on A2 (m, R) is not
always satisfied, for instance, it is shown in Theorem
Not surprisingly, we apply yet again Hoeffding‚Äôs in6.2 of Cucker & Zhou (2007) that for C ‚àû (indefinite
equality to relate the last term above to its expectatimes differentiable) kernels (such as the popular Gaustion. Since
sian kernel), polynomial decay implies that the regression function m ‚àà C ‚àû (X ) (under mild assumptions on
km ‚àí hk‚àû ‚â§ 1 + k hh, Œ¶(¬∑)i k‚àû ‚â§ 1 + CkhkH ,
X and Ptr (dx)). Therefore, as long as one works with
we have with probability at least 1 ‚àí Œ¥,
smooth kernels but nonsmooth regression functions,
the approximation error has to decay logarithmically
r
ntr
1 X
1
2
slowly. We give a logarithmic bound for such cases.
tr
tr
|m(Xi )‚àíh(Xi )| ‚â§ (1+CR)
log +A2 (m, R),
ntr i=1
2ntr
Œ¥
Theorem 3 Under Assumptions 1-3, if A‚àû (m, R) :=
inf km ‚àí gk‚àû ‚â§ C‚àû (log R)‚àís for some s > 0 and
where R := khkH . Combining this bound with (7)kgkH ‚â§R
(10) and applying our assumption on A2 (m, R):
constant C‚àû ‚â• 0 (assuming R ‚â• 1), then (for ntr and
ntr
1 X
(Œ≤ÃÇi ‚àí Œ≤i )(m(Xitr ) ‚àí h(Xitr ))
ntr i=1
s 
r

2
8
B2
1
8
‚â§B
log + 2RC 2
+
log
ntr
Œ¥
ntr
nte
Œ¥
r
1
8
+ BC2 R‚àíŒ∏/2 + B(1 + CR)
log .
2ntr
Œ¥

Setting R =



Œ∏BC2
2D2

2
 Œ∏+2

nte larger than some constant),

s

‚àís
ntr
1 X
1
sBC‚àû
tr
te
BC‚àû log
Œ≤ÃÇi Yi ‚àí EY
‚â§ 1+
ntr i=1
s
D‚àû
r
1
s
2
6
s+1
+B
log + (sBC‚àû ) s+1 D‚àû
ntr
Œ¥
holds
probability at least 1 ‚àí Œ¥, where D‚àû =
r with


B2
2C 2 ntr + n1te log 6Œ¥ and Œ≤ÃÇi is computed from (3).

completes the proof.

In Theorem 2 we do not even assume m ‚àà C (X ); all
we need is m ‚àà LP2tr , the space of Ptr (dx) square integrable functions. The latter condition always holds

The proof is similar as
that of Theorem 2 except that
s
‚àû s+1
we set R = ( sBC
)
.
D‚àû
Theorem 3 shows that in such unfavourable cases,
the confidence bound shrinks at an exceedingly slow

Analysis of Kernel Mean Matching under Covariate Shift
¬∑nte
rate, roughly, O(log‚àís nntrtr+n
). The reason, of course,
te
is due to the slow decay of the approximation error
A‚àû (m, R). It is proved in Theorem 6.1 of Cucker &
Zhou (2007) that for the Gaussian kernel k(x0 , x) =
exp(‚àíkx ‚àí x0 k22 /œÉ 2 ), if X ‚äÜ Rd has smooth boundary and the regression function m ‚àà H s (X ) with index s > d/2, then the logarithmic decay assumed in
Theorem 3 holds. Here H s (X ) is the Sobolev space
(the completion of C ‚àû (X ) under the inner product
R P
Œ±
Œ±
hf, gis := X |Œ±|‚â§s ddxf ddxg , assuming s ‚àà N). Similar bounds also hold for the inverse multiquadrics kernel k(x0 , x) = (c2 + kx ‚àí x0 k22 )‚àíŒ± with Œ± > 0. We
remark that in this regard Theorem 3 disrespects the
popular Gaussian kernel used ubiquitously in practice
and should draw the attention of researchers.

4.3. Discussion
It seems worthwhile to devote a subsection to discussing a very natural question that the reader might
already have: why not estimate the regression function
m on the training set and then plug into the test set,
after all m does not change under the covariate shift
assumption? Algorithmically, this is perfectly doable,
perhaps conceptually even simpler since the algorithm
does not need to see the test data beforehand. We note
that estimating the regression function from i.i.d. samples has been well studied in the learning theory literature, see for instance, Chapter 8 of Cucker & Zhou
(2007) and the many references therein.
The difficulty, though, lies in the appropriate error
metric on the estimate. Recall that when estimating
the regression function from i.i.d. training samples,
one usually measures the progress (i.e. the discrepancy between the estimate mÃÇ and m) by the L 2 norm
under the training probability measure Ptr (dx), while
what we really want is a confidence bound on the term
nte
1 X
mÃÇ(Xite ) ‚àí EY te .
nte i=1

(11)

Since Ptr 6= Pte , there is evidently a probability measure mismatch between the bound we have from estimating m and the true interested quantity. Indeed,
conditioned on the training sample {(Xitr , Yitr )}, using
the triangle inequality we can bound (11) by :
Z
nte
1 X
te
mÃÇ(Xi ) ‚àí mÃÇ(x)Pte (dx) + kmÃÇ ‚àí mkLP2 .
te
nte i=1
The first term above can be bounded again through
Hoeffding‚Äôs inequality, while the second term is
close to what we usually have from estimating m: the only difference being that the L 2

norm is now under the test probability measure
Pte (dx). Fortunately, since the norm of the identity
map id : ([‚àí1, 1]X , k ¬∑ kLP2 ) 7‚Üí ([‚àí1, 1]X , k ¬∑ kLP2 ) is
te
tr
‚àö
bounded by B (see Assumption 2), we can deduce
a bound for (11) based upon results from estimating
m, though less appealingly, a much looser bound than
the one given in Theorem 2. We record such a result
for the purpose of comparison:
Theorem 4 Under Assumptions 1-3, if the regression
Œ∏

function m ‚àà Range(Tk2Œ∏+4 ) for some Œ∏ > 0, then with
probability at least 1 ‚àí Œ¥,
nte
1 X
mÃÇ(Yite ) ‚àí EY te ‚â§
nte i=1

r

4 ‚àö
1
‚àí 3Œ∏
log + BC1 ntr 12Œ∏+16 ,
2nte
Œ¥

where C1 is some constant that does not depend on
ntr , nte , and mÃÇ is the (regularized least-squares) estimate of m in Smale & Zhou (2007).
The theorem follows from the bound on kmÃÇ ‚àí mkLP2
tr
in Corollary 3.2 of Sun & Wu (2009), which is an improvement over Smale & Zhou (2007).
Carefully comparing the current theorem with Theorem 2, we observe: 1). Theorem 4, which is based
on the regularized least-squares estimate of the regression function, needs to know in advance the parameter
Œ∏ (in order to tune the regularization constant) while
Theorem 2, derived for KMM, does not require any
such information, hence in some sense KMM is ‚Äúadaptive‚Äù; 2). Theorem 4 has much worse dependence on
the training sample size ntr ; it does not recover the
parametric rate even when the smoothness parameter
‚àí1/4
‚àí1/2
Œ∏ goes to ‚àû (we get ntr , instead of ntr ). On the
other hand, Theorem 4 has better dependence on the
test sample size nte , which is, however, probably not so
important since usually one has much more test samples than training samples because the lack of labels
make the former much easier to acquire; 3). Theorem
4 seems to have better dependence on the parameter
B; 4). Given the fact that KMM utilizes both the
training data and the test data in the learning phase,
it is not entirely a surprise that KMM wins in terms of
convergence rate, nevertheless, we find it quite stunning that by sacrificing the rate slightly on nte , KMM
is able to improve the rate on ntr so significantly.

5. Conclusion
For estimating the expected value of the output on
the test set where covariate shift has happened, we
have derived high probability confidence bounds for
the kernel mean matching (KMM) estimator, which

Analysis of Kernel Mean Matching under Covariate Shift
‚àí1

‚àí1

converges, roughly O(ntr 2 + nte 2 ) when the regression function lies in the RKHS, and more generally
‚àí

Œ∏

‚àí

Heckman, James J. Sample selection bias as a specification error. Econometrica, 47(1):153‚Äì161, 1979.

Œ∏

O(ntr 2(Œ∏+2) + nte 2(Œ∏+2) ) when the regression function
exhibits certain regularity measured by Œ∏. An ex¬∑nte
), is also
tremely slow rate, roughly O(log‚àís nntrtr+n
te
provided, calling attention of choosing the right kernel. From the comparison of the bounds, KMM proves
to be much more superior than the plug-in estimator hence provides concrete evidence/understanding to
the effectiveness of KMM under covariate shift.

Hoeffding, Wassily. Probability inequalities for sums of
bounded random variables. Journal of the American
Statistical Association, 58(301):13‚Äì30, 1963.

Although it is unclear to us if it is possible to avoid
approximating the regression function, we suspect the
bound in Theorem 2 is in some sense optimal and we
are currently investigating it. We also plan to generalize our results to the least-squares estimation problem.

Kanamori, Takafumi, Hido, Shohei, and Sugiyama,
Masashi. A least-squares approach to direct importance estimation. JMLR, 10:1391‚Äì1445, 2009.

Acknowledgements

Huang, Jiayuan, Smola, Alexander J., Gretton,
Arthur, Borgwardt, Karsten M., and SchoÃàlkopf,
Bernhard. Correcting sample selection bias by unlabeled data. In NIPS, pp. 601‚Äì608. 2007.

Kanamori, Takafumi, Suzuki, Taiji, and Sugiyama,
Masashi. Statistical analysis of kernel-based leastsquares density-ratio estimation. Machine Learning,
86:335‚Äì367, 2012.

This work was supported by Alberta Innovates Technology Futures and NSERC.

Pinelis, Iosif. Optimum bounds for the distributions of
martingales in Banach spaces. The Annals of Probability, 22(4):1679‚Äì1706, 1994.

References

Shimodaira, Hidetoshi. Improving predictive inference
under covariate shift by weighting the log-likelihood
function. Journal of Statistical Planning and Inference, 90(2):227‚Äì244, 2000.

Aronszajn, Nachman. Theory of reproducing kernels.
Transactions of the American Mathematical Sociery,
68:337‚Äì404, 1950.
Ben-David, Shai, Blitzer, John, Crammer, Koby, and
Pereira, Fernando. Analysis of representations for
domain adaptation. In NIPS, pp. 137‚Äì144. 2007.
Bickel, Steffen, BruÃàckner, Michael, and Scheffer, Tobias. Discriminative learning under covariate shift.
JMLR, 10:2137‚Äì2155, 2009.
Blitzer, John, Crammer, Koby, Kulesza, Alex, Pereira,
Fernando, and Wortman, Jennifer. Learning bounds
for domain adaptation. In NIPS, pp. 129‚Äì136. 2008.
Cortes, Corinna, Mohri, Mehryar, Riley, Michael, and
Rostamizadeh, Afshin. Sample selection bias correction theory. In ALT, pp. 38‚Äì53. 2008.
Cortes, Corinna, Mansour, Yishay, and Mohri,
Mehryar. Learning bounds for importance weighting. In NIPS, pp. 442‚Äì450. 2010.
Cucker, Felipe and Zhou, Ding-Xuan. Learning theory:
an approximation theory viewpoint. Cambridge University Press, 2007.
Gretton, Arthur, Smola, Alexander J., Huang, Jiayuan, Schmittfull, Marcel, Borgwardt, Karsten M.,
and SchoÃàlkopf, Bernhard. Covariate Shift by Kernel
Mean Matching, pp. 131‚Äì160. MIT Press, 2009.

Smale, Steve and Zhou, Ding-Xuan. Learning theory
estimates via integral operators and their approximations. Constructive Approximation, 26:153‚Äì172,
2007.
Sriperumbudur, Bharath K., Gretton, Arthur, Fukumizu, Kenji, SchoÃàlkopf, Bernhard, and Lanckriet,
Gert R. G. Hilbert space embeddings and metrics on
probability measures. JMLR, 11:1517‚Äì1561, 2010.
Steinwart, Ingo. On the influence of the kernel on the
consistency of support vector machines. JMLR, 2:
67‚Äì93, 2002.
Sugiyama, Masashi, Nakajima, Shinichi, Kashima,
Hisashi, Buenau, Paul Von, and Kawanabe, Motoaki. Direct importance estimation with model selection and its application to covariate shift adaptation. In NIPS, pp. 1433‚Äì1440. 2008.
Sun, Hongwei and Wu, Qiang. A note on application of integral operator in learning theory. Applied
and Computational Harmonic Analysis, 26:416‚Äì421,
2009.
Yosida, KoÃÇsaku. Functional Analysis. Springer, 6th
edition, 1980.
Zadrozny, Bianca. Learning and evaluating classifiers
under sample selection bias. In ICML. 2004.

