When modeling the real world, we often encounter situations in which multiple causes bear on a single eﬀect.
A typical interaction of this sort can be modeled with
the belief network shown in Figure 1. In the figure,
the variable e represents an eﬀect and the variables
c1 , . . . , cn represent n causes of that eﬀect. This representation is inadequate, because it fails to represent the independence of causal interactions—or causal
independence—that so often applies in this situation.
Consequently, the representation imposes intractable
demands on both knowledge acquisition and inference.
To overcome this inadequacy, knowledge engineers have used belief networks of the form shown in
Figure 2 to represent causal independence (Kim and
Pearl, 1983; Henrion, 1987; Srinivas, 1992). As in
Figure 1, the variables c1 , . . . , cn , and e represent the
causes and eﬀect, respectively. In addition, the intermediate variables i1 , . . . , in represent the independent
contributions of each cause on the eﬀect. That is, the
eﬀect e is some deterministic function of these intermediate variables. This belief network encodes causal
independence via the absence of arcs between pairs of
i variables and via the absence of any arc between a c
and i variable. As a result, this representation avoid-

e = f (i 1, ... , i n )
Figure 2: An explicit representation of causal independence.
s one limitation of the representation of Figure 1. In
particular, the representation requires only O(n) probability assessments.
Like the representation in Figure 1, however, this representation leads to intractable inference computations. In addition, the representation introduces a diﬃculty not present in the naive representation of multiple causes shown in Figure 1: The variables i1 , . . . , in
are not observable. In my experience, assessments are
easier to elicit (and presumably more reliable) when a
person makes them in terms of observable variables.
In this paper, we examine a definition of causal independence that explicitly involves temporal considerations. From this definition, we derive a belief-network

representation of causal independence. The representation facilitates tractable inference, and does not require the use of unobservable variables.

c1

cn

c2
• • •

2

A TEMPORAL DEFINITION OF
CAUSAL INDEPENDENCE

In this section, we examine a temporal definition of
causal independence, which will form the basis of the
belief-network representation of causal independence
presented in the next section. In this definition, we
associate a set of variables indexed by time with each
cause and with the eﬀect. We use cjt to denote the
variable associated with cause cj at time t, and et to
denote the variable associated with the eﬀect at time
t. For all times t and t0 , we require the variables cjt and
cjt0 to have the same set of instances. To simplify the
definition, we assume that each variable has discrete
instances. The generalization to continuous variables
is straightforward. As will be clear from the discussion,
there is no need to generalize to continuous time.
Under these assumptions, we can define causal independence to be the set of conditional-independent assertions
∀t, cj (et+1 ⊥ c1t , . . . , cj−1
, cj+1
, . . . , cnt |
t
t
et , cjt , cjt+1 , ckt = ckt+1 for k 6= j)

(1)

where (X ⊥ Y |Z) denotes the conditionalindependence assertion “the sets of variables X and
Y are independent, given Z.” Note that Assertion 1
is somewhat unusual, in that independence in conditioned, in part, on the knowledge that the instances
of variables are equal, but otherwise undetermined
(ckt = ckt+1 for k 6= j). In words, Assertion 1 states
that if cause cj makes a transition from one instance
to another between t and t + 1, and if no other causes
makes a transition during this time interval, then the
probability distribution over the eﬀect at time t + 1
depends only on the state of the eﬀect at time t and
on the transition made by cj ; the distribution does not
depend on the other causation variables.

3

A BELIEF-NETWORK
REPRESENTATION OF CAUSAL
INDEPENDENCE

As mentioned in the previous section, we can derive a
belief-network representation of causal independence
from this definition. First, for each cause, designate
some instance of its associated variables to be distinguished. For most real-world models, this instance will
represent the state of the cause in which that cause has
no bearing on the eﬀect—that is, the “oﬀ” state—but
we do not require this association. Second, construct a belief network consisting of nodes c1 , . . . , cn , and
e0 , . . . , en , as shown in Figure 3. In this belief network, node e0 represents the eﬀect when all causes

e0

e1

e2

en=e

Figure 3: A temporal belief-network representation of
causal independence.
take on their distinguished instance. Node c1 represents the state of cause c1 after it has made a transition from its distinguished instance (a transition may
be the trivial transition, wherein the cause maintains its distinguished instance). Node e1 represents the
eﬀect after only c1 has made the transition. In general, node cj represents the state of cause cj after it
has made a (possibly trivial) transition from its distinguished instance. Node ej represents the eﬀect after
causes c1 , . . . , cj have made their transitions. In particular, node en represents the eﬀect after all causes
have made transitions. Thus, node en corresponds to
node e in the atemporal representation of causal independence (Figure 2).
The conditional independencies represented in the belief network of Figure 3 follow from the temporal definition of casual independence. The belief network,
however, does not encode all of the conditional independencies associated with the definition. For example, from the temporal definition, we can obtain a belief network identical to the one in Figure 3, except
for the exchange of nodes cj and ej with ck and ek ,
respectively (for any j 6= k); we cannot obtain such
a belief network directly from the belief network in
Figure 3. Nonetheless, this belief-network representation of causal independence retains most of the advantages of the temporal definition. In particular, (1)
probability assessment is tractable, (2) if we use any
of the standard belief-network inference algorithms
(e.g., Shachter (1986), Pearl (1985), or Lauritzen and
Spiegelhalter (1988)), then inference is tractable, and
(3) all probability assessments in this representation
involve observable variables. The atemporal beliefnetwork representation of causal independence does
not have the latter two advantages.
Let us illustrate this representation with two examples. First, consider the most commonly used form of
causal independence: the noisy OR-gate (Good, 1961;
Suppes, 1970; Habbema, 1976; Kim and Pearl, 1983).
This model, expressed in the atemporal representation of causal independence, consists of binary variables c1 , . . . , cn , i1 , . . . , in , and e (i.e., each variable
has “true” and “false” as its only instances). Also, we
require
f (i1 , . . . , in ) = i1 ∨ . . . ∨ in

and

j

j

p(i = true|c = false) = 0

(2)

for j = 1, . . . , n. The adjustable parameters of the
model are the probabilities
p(ij = true|cj = true) ≡ qj

(3)

Finally, to allow for the possibility that e is true when
all causes are absent—sometimes called a “leak” (Henrion, 1987)—we add a variable c0 to the belief network
in Figure 2, and instantiate c0 to true.
The noisy OR-gate, expressed in the temporal beliefnetwork representation consists of binary variables
c1 , . . . , cn , and e0 , . . . , en . The variable en in this belief
network corresponds to the variable e in the atemporal representation. The distinguished instances of the
variables cj are the instances “false.” We require
p(ej = true|ej−1 = true) = 1
p(ej = true|ej−1 = false, cj = false) = 0
which corresponds to Equation 2. The adjustable parameters of the model are the probabilities
j

p(ej = true|ej−1 = false, c = true) = qj
the same parameters as those in Equation 3. The probability p(e0 = true) corresponds to the leak probability
q0 in the atemporal representation.
As mentioned, the advantages of the temporal
representation are twofold.
First, most inference computations—for example, the computation of
p(c1 |e = true)—using standard belief-network algorithms have time complexity O(n) in the temporal representation, but O(2n ) in the atemporal
representation.1 Second, the temporal representation
requires probability assessments involving only observable variables. In contrast, the atemporal representation requires the assessments p(ij = true|cj = true),
where the ij are unobservable variables.
Let us consider another model of a common causeand-eﬀect interaction: the noisy adder. This model,
expressed in the atemporal representation of causal independence, consists of binary variables c1 , . . . , cn , and
integer-valued variables i1 , . . . , in . Each ij can take on
values ranging from -l to +l. Also, we require
f (i1 , . . . , in ) = i1 + . . . + in
p(ij = 0|cj = false) = 1

(4)

for i = 1, . . . , n. The adjustable parameters of the
model are the probabilities
p(ij = k|cj = true) ≡ qjk
1

The noisy adder, expressed in the temporal representation consists of binary variables c1 , . . . , cn , and integervalued variables e0 , . . . , en = e. The distinguished instances of the variables cj are the instances “false.”
We require
p(ej = k|ej−1 = k, cj = false) = 1
which corresponds to the requirement of Equation 4.
The adjustable parameters of the model are the probabilities
p(ej = k|ej−1 = 0, cj = true) = qjk
the same parameters as those in Equation 5. We can
derive the remaining probabilities from the additive
model. In particular, we obtain

which implements the OR function, and

and

To allow for a leak, we add a variable c0 to the belief
network in Figure 2, and instantiate c0 to true. Note
that e can take on values ranging from -(n + 1)l to
+(n + 1)l.

(5)

Pearl (1988) developed an O(n) inference algorithm
for the noisy OR-gate interaction. The temporal representation eliminates the need for this special-case algorithm.

∀m p(ej = k + m|ej−1 = m, cj = true) = qjk
Note that the values of variable ej range from -(j + 1)l
to +(j + 1)l. Consequently, inference using any standard belief-network algorithm has computational complexity O(n3 l2 ), a significant improvement over the
intractable computations dictated by the atemporal
representation.2

4

A REAL-WORLD EXAMPLE

Although the computational benefits of the temporal
belief-network representation of causal independence
are substantial, the new representation was inspired
by the fact that it does not require probability assessments over unobservable variables. In particular,
while I was developing a normative expert system for
the morphologic diagnosis of blood disorders with the
expert hematopathologist Patrick Ward, we encountered an interesting interaction between the possible
diseases of the blood, a patient’s white-blood-cell (WBC) count, and various drugs that the patient may
be taking as treatments for nonblood diseases. We
attempted to model the interaction as a noisy adder,
using the atemporal representation of causal independence, as shown in Figure 4(a). The pathologist did
not understand clearly the definition of the intermediate variables, and could not provide the assessments
required by the model. When I developed the alternative representation, and explained the probability
assessments in terms of the belief network shown in
Figure 4(b), the pathologist provided the assessments
without diﬃculty.
2

If we limit the integer values of e to the range [0, +l],
then the computational complexity of inference becomes
O(nl2 ).

DISEASE

i

0

DISEASE

DRUG 1

...

DRUG n

i1

...

in

WBC = i0+...+i

n

...

DRUG 1

WBC 0

...

WBC 1

(a)

DRUG n

WBC n = WBC

(b)

Figure 4: (a) A portion of a belief network for the morphologic diagnosis of blood disorders. The model uses an
atemporal representation of causal independence. The variable DISEASE represents the mutually exclusive and
exhaustive blood disorders. The variable WBC represents the patient’s white-blood-cell count. The variables
DRUGi represent various drug treatments for nonblood disorders. The variable DRUG0 , which is instantiated
to true, is not shown. (b) A model of the same relationships using the temporal representation of causal
independence.

5

IMPROVEMENTS IN THE
REPRESENTATION FOR
INFERENCE

As we discussed, the temporal belief-network representation does not represent all of the conditional independencies corresponding to the temporal definition
of causal independence. In particular, the representation imposes a particular ordering over the causation
variables. This order specification does not appear to
impose limitations on knowledge acquisition, but situations may arise wherein this order specification may
make inference ineﬃcient. For example, suppose we
know that the blood-disorder system described in the
previous section is going to process a series of cases in
which only the variables WBC, DRUG2 , and DRUG7
will be instantiated. We would like the inference algorithm to recognize the irrelevance of the order of
the causation variables, and rearrange the variables
in the belief network so as to increase the eﬃciency
of inference. In particular, the algorithm can rearrange the variables so that DRUG2 , WBC2 , DRUG7 ,
and WBC7 =WBC appear last in the chain. Consequently, for the entire series of cases, the algorithm
need sum over the other drug and WBC variables only once.
To accomplish this goal, we can transform the belief
network in Figure 4(b) to that in Figure 5. In particular, we reintroduce intermediate variables, and make
the addition function explicit, using deterministic variables. This transformation can be done algorithmi-

DISEASE

WBC 0

DRUG 1

...

i1

...

+

...

DRUG

n

in

+ = WBC

Figure 5: A modified version of the belief network in
Figure 4. An inference algorithm may be able to use
the additional information in this belief network to
increase the eﬃciency of inference.

cally. Recognizing that addition is commutative, the
inference algorithm now can rearrange WBCj –DRUGj
variable pairs so as to increase the eﬃciency of inference.

c1

c2

• • •

a

c1

c2

a0

a1

a2

b0

b1

b2

cn

cn
• • •

an

b

(a)

bn
(b)

Figure 6: (a) Two eﬀects share the same causes. (b) The same relations depicted in the temporal representation.
Undirected cycles exist in both representations.

6

A LIMITATION OF THE
REPRESENTATION FOR
INFERENCE

Roughly speaking, two situations cause belief-network
inference to become intractable: (1) a large parent set
for one or more nodes, and (2) undirected cycles in
the belief network. The use of the temporal representation for causal independence eliminates the first
problem in many situations, but does not eliminate
the second problem. To understand this point, consider the belief network shown in Figure 6(a), wherein
n causes bear on two eﬀects a and b. The transformation to the temporal representation of this situation, shown in Figure 6(b), does not eliminate undirected cycles. For example, Figure 6(b) contains the
undirected cycle b1 —c1 —a1 —a2 —c2 —b2 —b1 . Consequently, the transformation does not produce tractable
inference.

7

AN OBSERVATION ABOUT
GENERALITY

The model in Figure 5 is a special case of the atemporal representation of causal independence, where the
function f is a nested series of added terms. That is,
f (WBC0 , i1 , . . . , in−1 , in ) =
(((. . . (WBC0 + i1 ) + . . .) + in−1 ) + in )
Indeed, if we make the variables ij and function f sufficiently complex, then any interaction encoded using
the temporal belief-network representation of causal
independence can be encoded using the atemporal representation of causal independence.
Despite its less general nature, however, the temporal
representation appears to be useful for many practical
applications. Over the last decade, I have participated

in the construction of dozens of normative expert systems. I have reexamined the belief networks for these
systems, and have found many instances in which the
use of the temporal representation described in this
paper would have simplified knowledge acquisition and
inference. The added generality of the atemporal model would not have been useful in any of these instances.

References
Good, I. (1961). A causal calculus (I). British Journal of Philosophy of Science, 11:305–318. Also in
I.J. Good, Good Thinking: The Foundations of
Probability and Its Applications, pages 197–217.
University of Minnesota Press, Minneapolis, MN,
1983.
Habbema, J. (1976). Models for diagnosis and detection of combinations of diseases. In de Dombal, F. and Gremy, F., editors, Decision Making
and Medical Care, pages 399–411. North-Holland,
New York.
Henrion, M. (1987). Some practical issues in constructing belief networks. In Proceedings of the
Third Workshop on Uncertainty in Artificial Intelligence, Seattle, WA, pages 132–139. Association for Uncertainty in Artificial Intelligence,
Mountain View, CA. Also in Kanal, L., Levitt,
T., and Lemmer, J., editors, Uncertainty in Artificial Intelligence 3, pages 161–174. North-Holland,
New York, 1989.
Kim, J. and Pearl, J. (1983). A computational model for causal and diagnostic reasoning in inference engines. In Proceedings Eighth International
Joint Conference on Artificial Intelligence, Karlsruhe, West Germany, pages 190–193. International Joint Conference on Artificial Intelligence.
Lauritzen, S. and Spiegelhalter, D. (1988). Local computations with probabilities on graphical struc-

tures and their application to expert systems. J.
Royal Statistical Society B, 50:157–224.
Pearl, J. (1985). A constraint propagation approach
to probabilistic reasoning. In Proceedings of the
Workshop on Uncertainty and Probability in Artificial Intelligence, Los Angeles, CA, pages 31–
42. Association for Uncertainty in Artificial Intelligence, Mountain View, CA. Also in Kanal, L.
and Lemmer, J., editors, Uncertainty in Artificial
Intelligence, pages 357–369. North-Holland, New
York, 1986.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo, CA.
Shachter, R. (1986). Evaluating influence diagrams.
Operations Research, 34:871–882.
Srinivas, S. (1992). Generalizing the noisy OR model
to n-ary variables. Also in this proceedings.
Suppes, P., editor (1970). A Probabilistic Theory of
Causality. North-Holland, New York.

