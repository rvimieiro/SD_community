cial intelligence (AI) have used Bayesian belief networks
to build models of expert opinion. Using standard methods
drawn from the theory of computational complexity, work­
ers in the field have shown that the problem of exact prob­
abilistic inference on belief networks almost certainly
requires exponential computation in the worst case [3]. We
have previously described a randomized approximation
scheme, called BN-RAS, for computation on belief net­
works [1, 2, 4]. We gave precise analytic bounds on the
convergence of BN-RAS and showed how to trade running
time for accuracy in the evaluation of posterior marginal
probabilities. We now extend our previous results and dem­
onstrate the generality of our framework by applying sim­
ilar mathematical techniques to the analysis of convergence
for logic sampling [7], an alternative simulation algorithm
for probabilistic inference.

1.0

Introduction

Given truth assignments for a set E of random variables in a
belief network, an algorithm for Probabilistic Inference in

Belief NETworks (PIBNET) computes the posterior prob­
abilities for the outcomes of a specified node X. PIBNET is
hard for NP, by reduction from 3-satisfiability in the prop­
ositional calculus [3]. That classification has focused re­
search on approximate methods, special-case techniques,
heuristics, and analyses of average-case behavior.
There now exists a number of algorithms for exact prob­
abilistic inference in belief networks: the message-passing
algorithm of Pearl [12], the triangulation method of Lau­
ritzen and Spiegelhalter [10], and others. Previous approx­
imation algorithms include the Markov-simulation scheme
of Pearl [13, 14], Henrion's logic sampling [7], and the ran­
domized approximation scheme (ras), known as BN-RAS,
which we have previously demonstrated [1]. Heckerman
has proposed a special-case algorithm for certain kinds of
two-level belief networks [6]. Each algorithm has compu­
tational properties that render it attractive for inference on
certain kinds of networks. The NP-hard classification sug­
gests, however, that no algorithm can provide a definitive
efficient solution for all inference problems.
A randomized approximation scheme (ras) is, by defini­
tion, an algorithm that computes, with low probability of

13 1

I

Methods and Procedures

I
failure, a result that lies arbitrarily close to the true answer
[8, 9]. Moreover, a ras must terminate within time that is a
linear function of the reciprocol of the error and the re­
ciprocol of the failure probability. We can define approxi­
mation schemes that guarantee either interval or relative
error bounds, with high probability.
BN-RAS derives from Markov-simulation algorithms orig­
inally proposed by Pearl. We now introduce a new ras, BN­
RAS-LS, based on logic sampling. We do not modify the
original algorithm for logic sampling; rather, we specify a
convergence analysis that transforms the logic-sampling
method into a ras.

2.0

Methods and Procedures

grows. Trial samples that are consistent with F are called
successful trials. Samples that do not correspond to the
findings in F must be discarded, and the algorithm's per­
formance deteriorates as a result (See [7] for additional de­
tails.)
We view logic sampling over a belief network with a set of
findings F to be a Bernoulli process with a binomial dis­
tribution
N

= 1- L, � �njq"-j

(1)

The detailed argument, based on Chebyshev's inequality
[9], reveals that
(2)

guarantees the (a, �) convergence criterion, whereN is the
total number of simulation trials. Each trial corresponds to
the choice of a joint instantiation, consistent with known
evidence, for all the nodes in the belief network. Inasmuch
as N is a polynomial in 1 I� and 1 Ia, our sampling scheme
with its convergence criterion defines a ras for PIDNET.
An algorithm for logic sampling simulates a value for every
variable in the model, in graphical order, at each trial.
When a variable X is simulated, the values of its condi­
tioning variables have already been select�d; logic sam­
pling chooses a new value for X based on its conditional­
probability matrix. The forward-propagation technique of
logic sampling suffers as the size of the set of findings F

I
I

(3)

I

where K is the number of successful logic-sampling trials,

I

sure (a, �) convergence, n is the total number of samples
(including successes and failures}, p=P (F) is the prior
probability of the set of findings (i.e., the probability of a
successful trial}, and q= 1- p.

I

P(K>N}

i=O

(

l

T

N= 1 I [ 4 ( 1- �) a2] is the number of trials needed to en­

Suppose that we wish to compute all posterior probabilities
in a belief network to within an interval error a. Suppose,
in addition, that we are willing to tolerate a small proba­
bility 1 - � that the algorithm fails to converge within the a
bound. Let J.1. represent the true posterior marginal proba­
bility of the node under consideration; f denotes the ap­
proximate probability computed by the algorithm. The
(a, �) convergence criterion may be written

I

If K>N, then we have performed more thanN successful
t r i a l s , a n d t h e r e f o r e P<if-�1 �a) ��- T h u s , i f
P (K>N) �a, then b y substitution o f P <it-�� �a) ��
for K>N, we obtain that P (P<lf-���a) ��) � a .
We now introduce an intermediate result:

I
I

LEMMA 1.

Let I= I P' (zl e) - P (zl e) 1. where P' (zl e) is an unbiased
estimator of P (zl e) , and where z and e are random vari­
ables. If P (P (I�b) �c) > d, then P (I � b) > cd, for arbi­
trary constants b, c, and d.
Proof. First, by Markov's inequality:

E (X)
P (X�c) <--.
c

I

(4)

Let X=P (I�b) . Then, by applying (4) to the antecedent
of the theorem, we have that P (X�c) � ( (E (X)) Ic)
Note that the theorem also specifies that d < P (X�c) .
Combining, we see that d < P (X�c) � (£(X) ) Ic . Thus,
E(X) >cd. Now, E(X) =X=P (l�b) , so P (l�b) >cd
Q.E.D.
and the lemma is proved.
•

I
I
I
I
I
I
I
I

132

I

Methods and Procedures

I
I

If P(K>N) C!:o, then
by Lemma 1:

P (Pclf-J.Li Sa)

(5)

Now define a function

I
I
I
I
I

I
I
I
I
I
I
I

1 -0.5 [ 1.0 + (0.115194) i
-4

(11)

which carries a maximum interval error of 2.5xl0-4[11,
page 51]. (We give approximations, with interval errors,
because tables are not available for the wide range of val­
ues that we may require.) If we let

(6)
f'(n,p,N) = c!l,

(

N- np +

r,.;npq

4) r- 4)
,

-c!l

np -

�
,.;npq

,

(12)

In addition, let
g (a,p,N)

=

min {n: 1-f(n,p,N) �a}.

(7)

If we perform g ( o, p, N) trials, then P (K>N) C!: o and re­
lation 5 is satisfied. We therefore require an efficient algo­
rithm for computing an upper bound on g ( o, p,N)

then li<n,p,N) -f'(n,p,N)I S5.0x10-4, and we have
g(a,p,N)

s

min {n: f'(n,p,N)

S

1

•

Consider the following normal approximation to the bino­
mial distribution f(n,p, N) :

I
I

=

+ 0.000344i- 0.019527x4]

I
I

42>' (x)

C!:i;) <!: a and thus

where 4!l is the standard normal distribution. In [11, page
170], it is shown that
0.14
1'2v(n,p,N) -f(n,p,N ) < r--

I

-

(9)

,.Jnpq

-

0 14

a - �-5.0 x10-4 } . (13)
,.Jnpq

To find an appropriate value for n, we perform a binary
search in the range [N,N2lp] and look for values that sat­
isfy the inequality in (13). (The approximation f' ( n,p, N)
is a monotonically decreasing function· on n .) Clearly,
n C!:N, because we require at least N successful trials. The
choice of til/p is purely arbitrary. Notice that as long as

�

2
0
o S 0.99954- ·1 -/'(l!._,p,N),
p
N ,.;q
n =

(14)

til/p will satisfy (13) and will therefore ensure that

P (K > N) C!: o. Hence, for most o of interest, we have given

a binary-search algorithm that y ields an upper bound
g,. ( o, p,N) on g ( o, p,N) We can accommodate o closer
to 1 by choosing finer approximations to the normal dis­
tribution, or by expanding the upper end of the search in­
terval beyond tilIp.
.

Therefore,
g(a,p,N)

.

S mm

0.14

{n: }(n,p,N ) < l -a·
r--: (10)
,.Jnpq

Now consider the approximation

_

Note that, in general, p = P (F) is not known exactly. We
can, however, calculate a lower bound p' on p as follows:
p �p' =

IT min"x [ P (XI1tx) l,

XeF

(15)

133
Discussion

I
I
I

where the X are nodes in the finding set F, and the "x de­

note the parents of X. If we perform g., (a, p', N) trials, then
P (K >N) � a. Poor lower bounds will, of course, cause the

Figure 1. u= 0.5

7
10 �����������
------�
t
I
I
I
-N•IOO

performance of the algorithm to deteriorate.
Our arguments prove the following theorem:

<It-�� �a) � �a = Cll for logic sampling,

--·N•IOOO
----·N•IOOOO

10d

1:

108

r-

THEOREM 1.
To obtain P

1:

1\,

...,
_

g ., (a,p',N) trials using t h e standard protocol for logic

sampling, and score the outcomes in the usual fashion.
103

As trials are scored, we can incrementally reapply a similar
analysis to compute a new value for g., (a,p', N) in light of

�

po s s i bl e t o c o m p u t e a l o w e r b o u nd g 1(a,p,N) o n

---

-

'--.._

min {n: f'(n,p,N) :S 1-a+

�

0

..

___

---o-

E

I

0.1

g (a,p, N) , derived from the relation

g(CJ,p,N) �

-;

..•____

� ""'- ..___

102
0.0

the trial successes already encountered. Moreover, it is also

'·

'

�\
4
10 r

with 5 a n d a s u c h that a s a t i sf i e s (14 ) , p e r f o r m

_

____ ... ___ _--·
_
·-----

"'!
__

_ D---

....___
I

0.2

--

p

�>-- - -

�:

-- I
I

0.3

0.4

0.5

+ 5.0x10 }. (16)
proximating the binomial distribution, we avoid making as­
sumptions about asymptotic behavior, such as convergence

Also,

to the normal distribution. Without such an analysis, a

p �p" =

TI

XeF

max11
X

[P (XI "x))

.

method for computing probability estimates cannot be clas­

(17)

sified as a ras.

Initially, we expect to have to perform at least g 1 (a,p",N)

Figure 1 plots the number of trials suggested by our meth­
odology, for various values of the probability of the set of

trials in order to achieve the convergence criterion. Know­

findings and for several values of N, with a held to 0.5.

ing the lower bound can be very useful in indicating wheth­

Figures 2 and 3 plot corresponding values for a = 0.9 and

er simulation for a given set of parameters is worth doing.

a = 0.99, respectively. On log-linear scales, the value of a

Note that estimate-based approaches to error analysis [7]

makes little difference; the probability p, on the other hand,

cannot yield such a priori lower bounds, because those ap­

strongly determines the expected number of trials.

proaches require several successful trials; that requirement,
in tum, defeats the objective of avoiding simulation in cas­
es where successful trials are rare.

3.0

Other workers in the field have previously given proce­

We have shown how to compute a priori bounds on the ran­
domized computation of marginal posterior probabilities in

dures for estimating the accuracy of logic-sampling esti­
mates. In particular, Henrion [7] uses a sample to compute

an estimate of the standard error of a probability of interesL
Our approach differs in that we use Chebyshev's and Mark­
ov's inequalities, as applied to the probabilities of the belief
network, to specify a priori upper and lower bounds on the
expected number of trials required for convergence. By ap-

I
I
I
I
I

4

-.�npq

I

Discussion

belief networks. We have applied our analytic techniques
both to Markov simulation [I] and, in this paper, to logic
sampling. To characterize the performance of logic sam­
pling in analytic terms, we have employed the area-esti­
mation technique of Karp a n d Luby, C h e b y s h e v ' s
inequality, Markov's inequality, a n d normal approxima-

I
I
I
I
I
I
I
I
I
I

134

I

Acknowledgments

I
I

fies the expected number of trials needed for convergence
as a function of the interval error, the failure probability,
and the prior probability of the evidence set.

Figure 2. u=0.9

I
''
'

I
I
I
I

1011
s::

'

\,

--.+-, ___ __ ... _
_

__
__

'

104

'G._._

--

+__

------··---- - -

o-

_ - - a--___ o-- __

...____ ___

_

-----

102 �������
0.0
0.1
0.2
0.3
0.5
0.4
p

I
I
Figure 3. u=0.99

I
I
I
I
I

-

''
1011 ... ---.+-,
:-.\.
'
'
104

-

..,
--

�--:G._._

-- ...

--a.-

...___

-

- -- -

..

...

-- --

_ --o-

...... ..__ _ _..,
_ _
_

.

I
I

The analysis of running time has important practical con­
sequences. The selection of an appropriate algorithm for
probabilistic inference depends crucially on the parameters
of the problem. A meta-algorithm that selects inference
techniques can apply those stochastic algorithms with the
best expected convergence for a given inference task.
The techniques we have described do not apply directly to
modified stochastic algorithms for probabilistic inference,
including importance sampling. We expect that future re­
search will illuminate the running time of those more so­
phisticated algorithms, and thereby further facilitate the
tailoring of belief-network inference techniques to knowl­
edge bases for real-world applications.

4.0

Acknowledgments

Lyn Dupre edited the manuscript.

- -- o- ___

----- � ----�

··�

102 �������
0.0
0.1
0.2
0.3
0.4
0.5
p

I
I

The number of trials specified by gu(n,p', N) is a sufficient
number of trials, such that P <I Y -1..1-l sa) ;::: ro holds true at
the start, before we begin simulation, for every posterior
p r o b a b i l i t y ll in t h e n e t w o r k . A f t e r w e p e rf o r m
g u(n,p', N) trials, however, w e are n o t guaranteed that
P <if-1..1- l s a) ;::: ro still holds, because it is possible that K
is less than N. Note, on the other hand, that we gain infor­
mation about convergence from the number of successful
trials as the simulation proceeds. We view that adaptive be­
havior, which distinguishes BN-RAS-LS from its Markov­
simulation predecessor BN-RAS, as one of the algorithm's
most appealing properties.

tions to the binomial distribution. The resulting ras speci-

This work has been supported by grant IRI-8703710 from
the National Science Foundation, grant P-25514-EL from
the U.S. Army Research Office, the Medical Scientist
Training Program under grant GM07365 from the National
Institutes of Health, and grant LM-07033 from the National
Library of Medicine. Computer facilities were provided by
the SUMEX-AIM resource under grant LM-05208 from
the National Institutes of Health.

135
References

I
I

5.0
1.

2.

their application to expert systems. Journal of the Roy­
al Statistical Society B 50 (19):157-224, 1988.

References
R. M. Chavez and G. F. Cooper. A randomized ap­
proximation scheme for the Bayesian inferencing
problem. Technical Report KSL-88-72, Knowledge
Systems Laboratory, Stanford University, Stanford,
CA, April 1989. To appear inNetworks.
R. M. Chavez and G. F. Cooper. An empirical evalu­
ation of a randomized algorithm for probabilistic in­
ference. In Proceedings of the Fifth Workshop on
Uncertainty in Artificial Intelligence, Windsor, Ontar­
io, August 1989, pages 60-70.

3.

G. F. Cooper. The computational complexity of prob­
abilistic inference using Bayesian belief networks. Ar­
tificial Intelligence 42 (1990), pages 393-405.

4.

R. M. Chavez. Hypermedia and randomized algo­
rithms for medical expert systems. In Proceedings of
the Thirteenth Symposium on Computer Applications
in Medicine, W ashington, DC, November 1989. To
appear in Computer Methods and Programs in Bio­
medicine.

5.

M. R. Garey an D. S Johnson. Computers and Intrac­
tability: A Guide to the Theory ofNP-Completeness.
W. H. Freeman and Company, New York, NY, 1979.

6.

D. E. Heckerman. A tractable inference algorithm for
diagnosing multiple diseases. In Proceedings of the
Fifth Workshop on Uncertainty in Artificial Intelli­
gence, W indsor, Ontario, August 1989, pages 174-

180.
7.

8.

M. Henrion. Propagating uncertainty in Bayesian net­
works by probabilistic logic sampling. In J. F. Lemmer
and L. N. Kanal, eds., Uncertainty in Artificial Intel­
ligence 2, North-Holland, 1988, pages 149-163.
M. Jerrum and A. Sinclair. Conductance and the rapid
mixing property for Markov chains: The approxima­
tion of the permanent resolved. In Proceedings of the
Twentieth ACM Symposium on Theory of Computing,

pages 235-244, 1988.
9.

R. M. Karp and M. Luby. Monte Carlo algorithms for
enumeration and reliability problems. In Proceedings
of the Twenty-fourth IEEE Symposium on Foundations
of Computer Science, 1983.
·

10.

S. L. Lauritzen and D. J. Spiegelhalter. Local compu­
tations with probabilities on graphical structures and

11.

J. K. Patel and C. B. Read. Handbook of theNormal
Distribution. Marcel Dekker, New York, NY, 1982.

12.

J. Pearl. Fusion, propagation, and structuring in belief
networks. Artificial Intelligence, 2:241-288, 1986.

13.

J. Pearl. Evidential reasoning using stochastic simu­
lation of causal models. Artificial Inte lligence,
32:245-257, 1987.

14.

J. Pearl. Addendum: Evidential reasoning using sto­
chastic simulation of causal models. Artificial Intelli­
gence, 33:131, 1987.

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

