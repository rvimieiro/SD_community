1

1

2

3

1

2

3

1

2

3

1

2

3

4

5

6

4

5

6

4

5

6

4

5

6

7

8

9

7

8

9

7

8

9

7

8

9

Figure 1: Left: A simple 2d-lattice MRF to illustrate our notation. For node j = 7 we have N (xj ) =
{x4 , x8 }. Centre left: The 1-neighbourhood of the clique q = {x7 , x8 } including additional edges
(dashed lines) present in the marginal over the 1-neighbourhood. Factors of this form are used by
the LAP algorithm of Mizrahi et. al. Centre right: The MRF used by our conditional estimator of
Section 5 when using the same domain as Mizrahi et. al. Right: A smaller neighbourhood which
we show is also sufficient to estimate the clique parameter of q.
verify the assumption of Liu and Ihler, we are able to import their M-estimation results into the LAP
framework directly, bridging the gap between LAP and consensus estimators.

2

Background

Our goal is to estimate the D-dimensional parameter vector θ of an MRF with the following Gibbs
density or mass function:
X
1
p(x | θ) =
exp(−
E(xc | θ c ))
(1)
Z(θ)
c
Here c ∈ C is an index over the cliques of an undirected graph G = (V, E), E(xc | θ c ) is known as
the energy or Gibbs potential, and Z(θ) is a normalizing term known as the partition function.
When E(xc | θ c ) = −θ Tc φc (xc ), where φc (xc ) is a local sufficient statistic derived from the values
of the local data vector xc , this model is known as a maximum entropy or log-linear model. In
this paper we do not restrict ourselves to a specific form for the potentials, leaving them as general
functions; we require only that their parameters are identifiable. Throughout this paper we focus
on the case where the xj ’s are discrete random variables, however generalising our results to the
continuous case is straightforward.
The j-th node of G is associated with the random variable xj for j = 1, . . . , M , and the edge connecting nodes j and k represents the statistical interaction between xj and xk . By the HammersleyClifford Theorem [10], the random vector x satisfies the Markov property with respect to the graph
G, i.e. p(xj |x−j ) = p(xj |xN (xj ) ) for all j where x−j denotes all variables in x excluding xj ,
and xN (xj ) are the variables in the neighbourhood of node j (variables associated with nodes in G
directly connected to node j).
2.1

Centralised estimation

The standard approach to parameter estimation in statistics is through maximum likelihood, which
chooses parameters θ by maximising
LM L (θ) =

N
Y

p(xn | θ)

(2)

n=1

(To keep the notation light, we reserve n to index the data samples. In particular, xn denotes the
n-th |V|-dimensional data vector and xmn refers to the n-th observation of node m.)
This estimator has played a central role in statistics as it is consistent, asymptotically normal, and
efficient, among other desirable properties. However, applying maximum likelihood estimation to
an MRF is generally intractable since computing the value of log LM L and its derivative require
evaluating the partition function, or an expectation over the model respectively. Both of these values
involve a sum over exponentially many terms.
2

To surmount this difficulty it is common to approximate p(x | θ) as a product over more tractable
terms. This approach is known as composite likelihood and leads to an objective of the form
LCL (θ) =

N Y
I
Y

f i (xn , θ i )

(3)

n=1 i=1

where θ i denote the (possibly shared) parameters of each composite likelihood factor f i .
Composite likelihood estimators are and both well studied and widely applied [6, 14, 12, 7, 16, 2, 22,
4, 21]. In practice the f i terms are chosen to be easy to compute, and are typically local functions,
depending only on some local region of the underlying graph G.
An early and influential variant of composite likelihood is pseudo-likelihood (PL) [3], where
f i (x, θ i ) is chosen to be the conditional distribution of xi given its neighbours,
LP L (θ) =

N Y
M
Y

p(xmn | xN (xm )n , θ m )

(4)

n=1 m=1

Since the joint distribution has a Markov structure with respect to the graph G, the conditional distribution for xm depends only on its neighbours, namely xN (xm ) . In general more efficient composite
likelihood estimators can be obtained by blocking, i.e. choosing the f i (x, θ i ) to be conditional or
marginal likelihoods over blocks of variables, which may be allowed to overlap.
Composite likelihood estimators are often divided into conditional and marginal variants, depending
on whether the f i (x, θ i ) are formed from conditional or marginal likelihoods. In machine learning
the conditional variant is quite popular [12, 7, 16, 15, 4] while the marginal variant has received less
attention. In statistics, both the marginal and conditional variants of composite likelihood are well
studied (see the comprehensive review of Varin et. al. [26]).
An unfortunate difficulty with composite likelihood is that the estimators cannot be computed in
parallel, since elements of θ are often shared between the different factors. For a fixed value of θ
the terms of log LCL decouple over data and over blocks of the decomposition; however, if θ is not
fixed then the terms remain coupled.
2.2

Consensus estimation

Seeking greater parallelism, researchers have investigated methods for decoupling the sub-problems
in composite likelihood. This leads to the class of consensus estimators, which perform parameter
estimation independently in each composite likelihood factor. This approach results in parameters
that are shared between factors being estimated multiple times, and a final consensus step is required
to force agreement between the solutions from separate sub-problems [27, 13].
Centralised estimators enforce sub-problem agreement throughout the estimation process, requiring
many rounds of communication in a distributed setting. Consensus estimators allow sub-problems
to disagree during optimisation, enforcing agreement as a post-processing step which requires only
a single round of communication.
Liu and Ihler [13] approach distributed composite likelihood by optimising each term separately
!
N
Y
i
i
θ̂ βi = arg max
f (xAi ,n , θ βi )
(5)
θ βi

n=1

where Ai denotes the group of variables associated with block i, and θ βi is the corresponding set of
parameters. In this setting the sets βi ⊆ V are allowed to overlap, but the optimisations are carried
out independently, so multiple estimates for overlapping parameters are obtained. Following Liu
and Ihler we have used the notation θ i = θ βi to make this interdependence between factors explicit.
i

The analysis of this setting proceeds by embedding each local estimator θ̂ βi into a degenerate estii

i

mator θ̂ for the global parameter vector θ by setting θ̂ c = 0 for c ∈
/ βi . The degenerate estimators
are combined into a single non-degenerate global estimate using different consensus operators, e.g.
i
weighted averages of the θ̂ .
3

The analysis of Liu and Ihler assumes that for each sub-problem i and for each c ∈ βi
i

p

(θ̂ βi )c → θ c
(6)
i.e. that each local estimate for the parameter of clique c is a consistent estimator of the corresponding clique parameter in the joint distribution. This assumption does not hold in general, and one of
the contributions of this work is to give a general condition under which this assumption holds.
The analysis of Liu and Ihler [13] considers the case where the local estimators in Equation 5 are arbitrary M -estimators [25], however their experiments address only the case of pseudo-likelihood. In
Section 5 we prove that the factorisation used by pseudo-likelihood satisfies Equation 6, explaining
the good results in their experiments.
2.3

Distributed estimation

Consensus estimation dramatically increases the parallelism of composite likelihood estimates by
relaxing the requirements on enforcing agreement between coupled sub-problems. Recently Mizrahi
et. al. [19] have shown that if the composite likelihood factorisation is constructed correctly then
consistent parameter estimates can be obtained without requiring a consensus step.
In the LAP algorithm of Mizrahi et al. [19] the domain of each composite likelihood factor (which
they call the auxiliary MRF) is constructed by surrounding each maximal clique q with the variables
in its 1-neighbourhood
[
Aq =
c
c∩q6=∅

which contains all of the variables of q itself as well as the variables with at least one neighbour in
q; see Figure 1 for an example. For MRFs of low degree the sets Aq are small, and consequently
maximum likelihood estimates for parameters of MRFs over these sets can be obtained efficiently.
The parametric form of each factor in LAP is chosen to coincide with the marginal distribution over
Aq .
The factorisation of Mizrahi et al. is essentially the same as in Equation 5, but the domain of each
term is carefully selected, and the LAP theorems are proved only for the case where f i (xAq , θ βq ) =
p(xAq , θ βq ).
As in consensus estimation, parameter estimation in LAP is performed separately and in parallel for
each term; however, agreement between sub-problems is handled differently. Instead of combining
parameter estimates from different sub-problems, LAP designates a specific sub-problem as authoritative for each parameter (in particular the sub-problem with domain Aq is authoritative for the
parameter θ q ). The global solution is constructed by collecting parameters from each sub-problem
for which it is authoritative and discarding the rest.
In order to obtain consistency for LAP, Mizrahi et al. [19] assume that both the joint distribution and
each composite likelihood factor are parametrised using normalized potentials.
Definition 1. A Gibbs potential E(xc |θ c ) is said to be normalised with respect to zero if E(xc |θ c ) =
0 whenever there exists t ∈ c such that xt = 0.
A perhaps under-appreciated existence and uniqueness theorem [9, 5] for MRFs states that there
exists one and only one potential normalized with respect to zero corresponding to a Gibbs distribution. This result ensures a one to one correspondence between Gibbs distributions and normalised
potential representations of an MRF.
The consistency of LAP relies on the following observation. Suppose we have a Gibbs distribution
p(xV | θ) that factors according to the clique system C, and suppose that the parametrisation is
chosen so that the potentials are normalised with respect to zero. For a particular clique of interest
q, the marginal over xAq can be obtained as follows
X
X
X
1
p(xAq | θ) =
p(xV | θ) =
exp(−
E(xc | θ c ))
Z(θ) x
x
V\Aq

V\Aq

1
=
exp(−E(xq | θ q ) −
Z(θ)

4

X
c∈Cq \{q}

c∈C

E(xc | θ V\q ))

(7)

where Cq denotes the clique system of the marginal, which in general includes cliques not present in
the joint. The same distribution can also be written in terms of different parameters α
X
1
p(xAq | α) =
exp(−E(xq | αq ) −
E(xc | αc ))
(8)
Z(α)
c∈Cq \{q}

which are also assumed to be normalised with respect to zero. As shown in Mizrahi et. al. [19], the
uniqueness of normalised potentials can be used to obtain the following result.
Proposition 2 (LAP argument [19]). If the parametrisations of p(xV | θ) and p(xAq | α) are chosen to be normalized with respect to zero, and if the parameters are identifiable with respect to the
potentials, then θ q = αq .
This proposition enables Mizrahi et. al. [19] to obtain consistency for LAP under the standard
smoothness and identifiability assumptions for MRFs [8].

3

Contributions of this paper

The strength of the results of Mizrahi et al. [19] is to show that it is possible to perform parameter
estimation in a completely distributed way without sacrificing global consistency. They prove that
through careful design of a composite likelihood factorisation it is possible to obtain estimates for
each parameter of the joint distribution in isolation, without requiring even a final consensus step
to enforce sub-problem agreement. Their weakness is that the LAP algorithm is very restrictive,
requiring a specific composite likelihood factorisation.
The strength of the results of Liu and Ihler [13] is that they apply in a very general setting (arbitrary
M -estimators) and make no assumptions about the underlying structure of the MRF. On the other
hand they assume the convergence in Equation 6, and do not characterise the conditions under which
this assumption holds.
The key to unifying these works is to notice that the specific decomposition used in LAP is chosen
essentially to ensure the convergence of Equation 6. This leads to our development of the Strong
LAP Condition and an associated Strong LAP Argument, which is a drop in replacement for the LAP
argument of Mizrahi et al. and holds for a much larger range of composite likelihood factorisations
than their original proof allows.
Since the purpose of the Strong LAP Condition is to guarantee the convergence of Equation 6, we
are able to import the results of Liu and Ihler [13] into the LAP framework directly, bridging the
gap between LAP and consensus estimators. The same Strong LAP Condition also provides the
necessary convergence guarantee for the results of Liu and Ihler to apply.
Finally we show how the Strong LAP Condition can lead to the development of new estimators, by
developing a new distributed estimator which subsumes the distributed pseudo-likelihood and gives
estimates that are both consistent and asymptotically normal.

4

Strong LAP argument

In this section we present the Strong LAP Condition, which provides a general condition under
which the convergence of Equation 6 holds. This turns out to be intimately connected to the structure
of the underlying graph.
Definition 3 (Relative Path Connectivity). Let G = (V, E) be an undirected graph, and let A be a
given subset of V. We say that two nodes i, j ∈ A are path connected with respect to V \ A if there
exists a path P = {i, s1 , s2 , . . . , sn , j} =
6 {i, j} with none of the sk ∈ A. Otherwise, we say that
i, j are path disconnected with respect to V \ A.
in
For a given A ⊆ V we partition the clique system of G into two parts, CA
that contains all of the
out
in
cliques that are a subset of A, and CA = C \ CA that contains the remaining cliques of G. Using
this notation we can write the marginal distribution over xA as
X
X
X
1
exp(−
E(xc | θ c ))
exp(−
E(xc | θ c ))
(9)
p(xA | θ) =
Z(θ)
out
in
x
c∈CA

V\A

5

c∈CA

3

j

k

2
1

6

4

5

i

Figure 2: Illustrating the concept of relative path connectivity. Here, A = {i, j, k}. While (k, j)
are path connected via {3, 4} and (k, i) are path connected via {2, 1, 5}, the pair (i, j) are path
disconnected with respect to V \ A.
P
P
Up to a normalisation constant, xV\A exp(− c∈C out E(xc | θ c )) induces a Gibbs density (and
A
therefore an MRF) on A, which we refer to as the induced MRF. (For example, as illustrated in
Figure 1 centre-left, the induced MRF involves all the cliques over the nodes 4, 5 and 9.) By the
Hammersley-Clifford theorem this MRF has a corresponding graph which we refer to as the induced
graph and denote GA . Note that the induced graph does not have the same structure as the marginal,
it contains only edges which are created by summing over xV\A .
Remark 4. To work in the general case, we assume throughout that that if an MRF contains the
path {i, j, k} then summing over j creates the edge (i, k) in the marginal.
Proposition 5. Let A be a subset of V, and let i, j ∈ A. The edge (i, j) exists in the induced graph
GA if and only if i and j are path connected with respect to V \ A.
Proof. If i and j are path connected then there is a path P = {i, s1 , s2 , . . . , sn , j} =
6 {i, j} with
none of the sk ∈ A. Summing over sk forms an edge (sk−1 , sk+1 ). By induction, summing over
s1 , . . . , sn forms the edge (i, j).
If i and j are path disconnected with respect to V \ A then summing over any s ∈ V \ A cannot
form the edge (i, j) or i and j would be path connected through the path {i, s, j}. By induction, if
the edge (i, j) is formed by summing over s1 , . . . , sn this implies that i and j are path connected via
{i, s1 , . . . , sn , j}, contradicting the assumption.
Corollary 6. B ⊆ A is a clique in the induced graph GA if and only if all pairs of nodes in B are
path connected with respect to V \ A.
Definition 7 (Strong LAP condition). Let G = (V, E) be an undirected graph and let q ∈ C be a
clique of interest. We say that a set A such that q ⊆ A ⊆ V satisfies the strong LAP condition for q
if there exist i, j ∈ q such that i and j are path-disconnected with respect to V \ A.
Proposition 8. Let G = (V, E) be an undirected graph and let q ∈ C be a clique of interest. The
joint distribution p(xV | θ) and the marginal p(xAq | θ) share the same normalised potential for q if
Aq satisfies the strong LAP condition for q.
Proof. If Aq satisfies the Strong LAP Condition for q then by Corollary 6 the induced MRF contains
no potential for q. Inspection of Equation 9 reveals that the same E(xq | θ q ) appears as a potential
in both the marginal and the joint distributions. The result follows by uniqueness of the normalised
potential representation.
We now restrict our attention to a set Aq which satisfies the Strong LAP Condition for a clique of
interest q. The marginal over p(xAq | θ) can be written as in Equation 9 in terms of θ, or in terms of
auxiliary parameters α
X
1
p(xAq | α) =
exp(−
E(xc | αc ))
(10)
Z(α)
c∈Cq

Where Cq is the clique system over the marginal. We will assume both parametrisations are normalised with respect to zero.
Theorem 9 (Strong LAP Argument). Let q be a clique in G and let q ⊆ Aq ⊆ V. Suppose p(xV | θ)
and p(xAq | α) are parametrised so that their potentials are normalised with respect to zero and the
parameters are identifiable with respect to the potentials. If Aq satisfies the Strong LAP Condition
for q then θ q = αq .
6

Proof. From Proposition 8 we know that p(xV | θ) and p(xAq | θ) share the same clique potential
for q. Alternatively we can write the marginal distribution as in Equation 10 in terms of auxiliary
variables α. By uniqueness, both parametrisations must have the same normalised potentials.
Since the potentials are equal, we can match terms between the two parametrisations. In particular
since E(xq | θ q ) = E(xq | αq ) we see that θ q = αq by identifiability.
4.1

Efficiency and the choice of decomposition

Theorem 9 implies that distributed composite likelihood is consistent for a wide class of decompositions of the joint distribution; however it does not address the issue of statistical efficiency.
This question has been studied empirically in the work of Meng et. al. [17, 18], who introduce a
distributed algorithm for Gaussian random fields and consider neighbourhoods of different sizes.
Meng et. al. find the larger neighbourhoods produce better empirical results and the following theorem confirms this observation.
Theorem 10. Let A be set of nodes which satisfies the Strong LAP Condition for q. Let θ̂A be the
ML parameter estimate of the marginal over A. If B is a superset of A, and θ̂B is the ML parameter
estimate of the marginal over B. Then (asymptotically):
|θq − (θ̂B )q | ≤ |θq − (θ̂A )q |.
Proof. Suppose that |θq − (θ̂B )q | > |θq − (θ̂A )q |. Then the estimates θ̂A over the various subsets A
of B improve upon the ML estimates of the marginal on B. This contradicts the Cramer-Rao lower
bound achieved the by the ML estimate of the marginal on B.
In general the choice of decomposition implies a trade-off in computational and statistical efficiency.
Larger factors are preferable from a statistical efficiency standpoint, but increase computation and
decrease the degree of parallelism.

5

Conditional LAP

The Strong LAP Argument tells us that if we construct composite likelihood factors using marginal
distributions over domains that satisfy the Strong LAP Condition then the LAP algorithm of Mizrahi
et. al. [19] remains consistent. In this section we show that more can be achieved.
Once we have satisfied the Strong LAP Condition we know it is acceptable to match parameters
between the joint distribution p(xV | θ) and the auxiliary distribution p(xAq | α). To obtain a consistent LAP algorithm from this correspondence all that is required is to have a consistent estimate
of αq . Mizrahi et. al. [19] achieve this by applying maximum likelihood estimation to p(xAq | α),
but any consistent estimator is valid.
We exploit this fact to show how the Strong LAP Argument can be applied to create a consistent
conditional LAP algorithm, where conditional estimation is performed in each auxiliary MRF. This
allows us to apply the LAP methodology to a broader class of models. For some models, such as
large densely connected graphs, we cannot rely on the LAP algorithm of Mizrahi et. al. [19]. For
example, for a restricted Boltzmann machine (RBM) [23], the 1-neighbourhood of any pairwise
clique includes the entire graph. Hence, the complexity of LAP is exponential in the number of
cliques. However, it is linear for conditional LAP, without sacrificing consistency.
Theorem 11. Let q be a clique in G and let xj ∈ q ⊆ Aq ⊆ V. If Aq satisfies the Strong LAP
Condition for q then p(xV | θ) and p(xj | xAq \{xj } , α) share the same normalised potential for q.
Proof. We can write the conditional distribution of xj given Aq \ {xj } as
p(xAq | θ)
p(xj | xAq \{xj } , θ) = P
xj p(xAq | θ)

(11)

Both the numerator and the denominator of Equation 11 are Gibbs distributions, and can therefore
be expressed in terms of potentials over clique systems.
7

Since Aq satisfies the Strong LAP Condition for qPwe know that p(xAq | θ) and p(xV | θ) have the
same potential for q. Moreover, the domain of xj p(xAq | θ) does not include q, so it cannot
contain a potential for q. We conclude that the potential for q in p(xj | xAq \{xj } , θ) must be shared
with p(xV | θ).
Remark 12. There exists a Gibbs representation normalised with respect to zero for
p(xj | xAq \{xj } , θ). Moreover, the clique potential for q is unique in that representation.
The existence in the above remark is an immediate result of the the existence of normalized representation both for the numerator and denominator of Equation 11, and the fact that difference
of normalised potentials is a normalized potential. For uniqueness, first note that p(xAq | θ) =
p(xj | xAq \{xj } , θ)p(xAq \{xj } , θ) The variable xj is not part of p(xAq \{xj } , θ) and hence this distribution does not contain the clique q. Suppose there were two different normalised representations
with respect to zero for the conditional p(xj | xAq \{xj } , θ). This would then imply two normalised
representations with respect to zero for the joint, which contradicts the fact that the joint has a unique
normalized representation.
We can now proceed as in the original LAP construction from Mizrahi et al. [19]. For a clique of
interest q we find a set Aq which satisfies the Strong LAP Condition for q. However, instead of
creating an auxiliary parametrisation of the marginal we create an auxiliary parametrisation of the
conditional in Equation 11.
p(xj | xAq \{xj } , α) =

X
1
exp(−
E(xc | αc ))
Zj (α)

(12)

c∈CAq

From Theorem 11 we know that E(xq | αq ) = E(xq | θ q ). Equality of the parameters is also
obtained, provided they are identifiable.
Corollary 13. If Aq satisfies the Strong LAP Condition for q then any consistent estimator of αq in
p(xj | xAq \{xj } , α) is also a consistent estimator of θ q in p(xV | θ).
5.1

Connection to distributed pseudo-likelihood and composite likelihood

Theorem 11 tells us that if Aq satisfies the Strong LAP Condition for q then to estimate θ q in
p(xV | θ) it is sufficient to have an estimate of αq in p(xj | xAq \{xj } , α) for any xj ∈ q. This tells
us that it is sufficient to use pseudo-likelihood-like conditional factors, provided that their domains
satisfy the Strong LAP Condition. The following remark completes the connection by telling us
that the Strong LAP Condition is satisfied by the specific domains used in the pseudo-likelihood
factorisation.
Remark 14. Let q = {x1 , x2 , .., xm } be a clique of interest, with 1-neighbourhood Aq = q ∪
{N (xi )}xi ∈q . Then for any xj ∈ q, the set q ∪ N (xj ) satisfies the Strong LAP Condition for q.
Moreover, q ∪ N (xj ) satisfies the Strong LAP Condition for all cliques in the graph that contain xj .
Importantly, to estimate every unary clique potential, we need to visit each node in the graph. However, to estimate pairwise clique potentials, visiting all nodes is redundant because the parameters
of each pairwise clique are estimated twice. This observation is important because it takes us back
to the work of Liu and Ihler [13]. If a parameter is estimated more than once, it is reasonable from a
statistical standpoint to apply the consensus operators of Liu and Ihler to obtain a single consensus
estimate. The theory of Liu and Ihler tells us that the consensus estimates are consistent and asymptotically normal, provided Equation 6 is satisfied. In turn, the Strong LAP Condition guarantees the
convergence of Equation 6.
The above remark tell us that the convergence in Equation 6 is satisfied for the distributed pseudolikelihood setting of Liu and Ihler. We can go beyond this and consider either marginal or conditional
factorisations over larger groups of variables. Since the asymptotic results of Liu and Ihler [13]
apply to any distributed composite likelihood estimator where the convergence of Equation 6 holds,
it follows that any distributed composite likelihood estimator where each factor satisfies the Strong
LAP Condition (including LAP and the conditional composite likelihood estimator from Section 5)
immediately gains asymptotic normality and variance guarantees as a result of their work and ours.
8

6

Conclusion

We presented foundational theoretical results for distributed composite likelihood. The results provide us with sufficient conditions to apply the results of Liu and Ihler to a broad class of distributed
estimators. The theory also led us to the construction of new globally consistent estimator, whose
complexity is linear even for many densely connected graphs. We view extending these results to
model selection, tied parameters, models with latent variables, and inference tasks as very important
avenues for future research.

9

References
[1] D. H. Ackley, G. Hinton, and T. Sejnowski. A learning algorithm for Boltzmann machines. Cognitive
Science, 9:147–169, 1985.
[2] A. Asuncion, Q. Liu, A. Ihler, and P. Smyth. Learning with blocks: Composite likelihood and contrastive
divergence. In Artificial Intelligence and Statistics, pages 33–40, 2010.
[3] J. Besag. Spatial interaction and the statistical analysis of lattice systems. Journal of the Royal Statistical
Society, Series B, 36:192–236, 1974.
[4] J. K. Bradley and C. Guestrin. Sample complexity of composite likelihood. In Artificial Intelligence and
Statistics, pages 136–160, 2012.
[5] P. Bremaud. Markov Chains: Gibbs Fields, Monte Carlo Simulation, and Queues. Springer-Verlag, 2001.
[6] B. Cox. Composite likelihood methods. Contemporary Mathematics, 80:221–239, 1988.
[7] J. V. Dillon and G. Lebanon. Stochastic composite likelihood. Journal of Machine Learning Research,
11:2597–2633, 2010.
[8] S. E. Fienberg and A. Rinaldo. Maximum likelihood estimation in log-linear models. The Annals of
Statistics, 40(2):996–1023, 2012.
[9] D. Griffeath. Introduction to random fields. In Denumerable Markov Chains, volume 40 of Graduate
Texts in Mathematics, pages 425–458. Springer, 1976.
[10] J. M. Hammersley and P. Clifford. Markov fields on finite graphs and lattices. 1971.
[11] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press,
2009.
[12] P. Liang and M. I. Jordan. An asymptotic analysis of generative, discriminative, and pseudolikelihood
estimators. In International Conference on Machine Learning, pages 584–591, 2008.
[13] Q. Liu and A. Ihler. Distributed parameter estimation via pseudo-likelihood. In International Conference
on Machine Learning, 2012.
[14] K. V. Mardia, J. T. Kent, G. Hughes, and C. C. Taylor. Maximum likelihood estimation using composite
likelihoods for closed exponential families. Biometrika, 96(4):975–982, 2009.
[15] B. Marlin and N. de Freitas. Asymptotic efficiency of deterministic estimators for discrete energy-based
models: Ratio matching and pseudolikelihood. In Uncertainty in Artificial Intelligence, pages 497–505,
2011.
[16] B. Marlin, K. Swersky, B. Chen, and N. de Freitas. Inductive principles for restricted Boltzmann machine
learning. In Artificial Intelligence and Statistics, pages 509–516, 2010.
[17] Z. Meng, D. Wei, A. Wiesel, and A. O. Hero III. Distributed learning of Gaussian graphical models via
marginal likelihoods. In Artificial Intelligence and Statistics, pages 39–47, 2013.
[18] Z. Meng, D. Wei, A. Wiesel, and A. O. Hero III. Marginal likelihoods for distributed parameter estimation
of Gaussian graphical models. Technical report, arXiv:1303.4756, 2014.
[19] Y. Mizrahi, M. Denil, and N. de Freitas. Linear and parallel learning of Markov random fields. In
International Conference on Machine Learning, 2014.
[20] K. P. Murphy. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012.
[21] S. Nowozin. Constructing composite likelihoods in general random fields. In ICML Workshop on Inferning: Interactions between Inference and Learning, 2013.
[22] S. Okabayashi, L. Johnson, and C. Geyer. Extending pseudo-likelihood for Potts models. Statistica Sinica,
21(1):331–347, 2011.
[23] P. Smolensky. Information processing in dynamical systems: foundations of harmony theory. Parallel
distributed processing: explorations in the microstructure of cognition, 1:194–281, 1986.
[24] D. Strauss and M. Ikeda. Pseudolikelihood estimation for social networks. Journal of the American
Statistical Association, 85(409):204–212, 1990.
[25] A. W. van der Vaart. Asymptotic statistics. Cambridge University Press, 1998.
[26] C. Varin, N. Reid, and D. Firth. An overview of composite likelihood methods. Statistica Sinica, 21:5–42,
2011.
[27] A. Wiesel and A. Hero III. Distributed covariance estimation in Gaussian graphical models. IEEE Transactions on Signal Processing, 60(1):211–220, 2012.

10

