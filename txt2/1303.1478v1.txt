
INTRODUCTION

Explanation, finding causes for observed facts (or evi­
dence), is frequently encountered within Artificial In­
telligence. Research and applications exist in natural
language understanding [10, 1, 19], automated medi­
cal diagnosis [5, 14, 13], vision and image processing
[7, 6], finding commonsense explanations, and other

fields [15]. In general, finding an explanation is charac­
terized as follows: Given world knowledge in the form
of (causal) rules, and observed facts (a formula), de­
termine what needs to be assumed in order to predict
the evidence. 1
One would like to find an explanation that is "optimal"
in some sense. Systems that perform explanation tasks
need to provide criteria for optimality. In related pa­
pers [18, 16, 17], we have argued that plausibility, the
power of predicting the observed facts, and relevance,
are important criteria. We assume a framework that
has causality as a primitive notion, and uses probabil­
ities as the uncertainty formalism. World knowledge
in this framework can be represented in the form of
Bayesian belief networks. Random variables in the
network (also referred to as nodes throughout) are as­
sumed to represent the occurrence of real-world events.
For simplicity, we assume that the nodes are discrete
random variables. Evidence is an assignment of values
to some of the nodes in the network, while an explana­
tion is another such assignment that obeys the plausi­
bility, predictiveness, and relevance criteria. Note that
an assignment here is treated as a sample-space event,
and as such has a probability. For example, if we have
a random variable die-throw, then the assignment die­
throw=3 is the event where the die turns up with a 3,
and has a probability of i, assuming a fair 6 sided die.
With the above assumptions, optimizing plausibility
and predictiveness means maximizing the posterior
probability of the explanation (or assignment). If we
ignore relevance, then just finding the MAP (Maxi­
mum A-Posteriori) assignment to the network is suffi­
cient. The necessity for relevance was shown by exam­
ple in [11], by noting that assigning values to irrelevant
variables leads to anomalous abductive conclusions. It
was suggested that only nodes that are ancestors of
some evidence node ( "evidentially supported" ) be as­
signed.
In [18, 16, 17], we presented a variant of the example
1Thus, by "explanation" we mean "abduction", "ab­
ductive reasoning", or "diagnosis", and "an explanation"
is "abductive conclusions". We do not intend to imply that
such explanations are for human consumption.

Relevant Explanations: Allowing Disjunctive Assignments

P(Intend_to_go) = 10

-8

P(Some_method I Intend_to_go) = 0.01
P(No_method I Intend_to_go) = O.Ql
P(Some_method I -Intend_to_go)- 0
P(No_method I -Intend_to_go)- 1

Some_method =union of all methods exceptNo_method
P(At_tracks I Kidnapped orSome_method)-

I

P(At_tracks I anything else)- 0

Figure 1: Train Tracks Example Network
(the vacation plan problem, omitted here for lack of
space), and called that form of anomaly the "overspec­
ification problem" . We noted that the evidential sup­
port criterion still considered too many nodes as rele­
vant. We then defined explanations as partial MAPs,
i.e. assignments of maximum probability where irrele­
vant nodes are left unassigned. The evidence nodes are
always considered to be relevant. Nodes are considered
irre.levant if they are not ancestors of some evidence
node (evidentially supported), or if a certain statisti­
cal independence criterion holds for all of their relevant
descendants (the Independence-Based (IB) condition):

The IB condition holds at node v w. r. t.
an assignment A iff v is independent of the ancestors
of v that are not assigned by A, given the values as­
signed by A to (the rest of) the ancestors.

Definition 1

By using statistical independence in this way to define
irrelevance, the overspecification problem was partially
alleviated2 . Several problems remained in the solution:
"almost" independent cases (which may be overcome
by using 8-IB MAP explanations 3 [17]), and incapa­
bility of providing disjunctive explanations, even when
the representation is favorable. Consider the following
example (figure 1):
Jack is found at the train tracks (our evidence, which
needs explaining). Suppose that there are two expla­
nations for his being there: getting there of his own ac­
cord, or being kidnapped. For getting there intention2

The idea of using independence (in Bayesian belief
networks) of a particular assignment to a set of variables
(rather than all assignments to the variables) is similar to
that of Bayes multinets [8].
3
8-IB MAP explanations are the same as IB MAP ex­
planations, except that in the IB condition, "independent"
is replaced by "independent within a factor of 1 - 8", i.e.
the ratio between the maximal and minimal conditional
probabilities over all possible assignments to the ancestors
of

v

is greater than 1

-

8.

201

ally, Jack may have used any one of 99 different meth­
ods (such as walking, taking a bus, etc.), all equally
likely given that Jack intended to get to the tracks,
for the sake of this example. The method variable is
represented by a node with 100 possible values, one
for each method, and one for not going by any method
(for the case where he did not intend to go, or could
not go for other reasons). Assume that the prior prob­
ability of getting kidnapped is 50 times less than that
of intending to go.
Since the IB condition does not hold at the at-tracks
node given that kidnapping did not occur (nor does
the 8-IB condition hold, for any reasonable 8), the
system would prefer the kidnapping explanation. Intu­
itively, we should prefer the intend-to-go explanation,
and should just ignore the method node, or state that
Jack went to the train-tracks by some (undetermined)
method. Even if we use a weighted abduction system,
such as [10, 2], the problem still remains. We noted in
[18] that if we allowed the system to collapse all the
methods into a single method, or equivalently allowed
disjunctive assignments, the problem would go away.
Actually, the method of selecting nodes with high pos­
terior probabilities to be part of the explanation also
happens to give the right answer, but we have already
shown in [18, 17], that the posterior node probabili­
ties scheme is undesirable for other reasons (possible
inconsistencies and irrelevant explanations).
Disjunctive assignments in explanations are also use­
ful for handling cases where there are multiple-valued
variables, such that sets of values correspond to nat­
ural types of events in a taxonomic hierarchy. In this
case we might want to aggregate the values into a sin­
gle value on the fly, if the need arises. We formally
develop disjunctive assignments (which we also call
generalized assignments) in section 2, and use them
to define generalized IB (GIB) assignments and ex­
planations. GIB assignments are shown to have lo­
cality properties similar to those that hold for IB as­
signments. Section 3 discusses an algorithm for imple­
menting GIB explanation, based on the locality prop­
erties. Section 4 evaluates GIB explanation, and sug­
gests how to extend the formalism to handle 8 inde­
pendence.
2

GIB EXPLANATION

When forming an explanation, we need to decide
whether certain events are part of the explanation.
Several (perhaps even "most" ) AI programs use a tax­
onomic hierarchy for representing event types (as well
as other kinds of object types). One interesting ques­
tion is that of the specificity of the explanation: should
we prefer an event higher up the hierarchy (more gen­
eral), or lower down (more specific)?
A solution proposed by Goldman and Charniak for
the WIMP natural language understanding program
[1, 9] allows aggregation of node values into a single

202

Shimony

value. The kind of specificity that this scheme han­
dles is specificity of event description w.r.t. some hier­
archical knowledge base of events. For example, sup­
pose that one event type is "shopping" , and that there
are events lower down in the hierarchy, "supermarket
shopping" , "liquor-store shopping" , etc. that are sub­
types of "shopping" . In the belief network representa­
tion, a multiple valued node consisting of all possible
events is used. Posterior probabilities are computed.
If the probabilities of the individual subtypes of shop­
ping events is low, one may still aggregate all these into
a single value that corresponds to "shopping" , and if
that has a high probability, a decision on the shopping
explanation can be made. In this example, the sys­
tem selects a less specific explanation (less specific, at
least, than a particular subtype of shopping), in order
to get a high probability explanation.
This scheme works only if the taxonomic hierarchy is
a strict hierarchy, i.e. each object has only one parent
and there are no "negative" links. We will assume
that this is indeed the case, as is done in WIMP. This
means that the is-a hierarchy does not have multiple
inheritance. The implication of this is that the number
of possible aggregated values for a node with n possible
values is at most 2n.
We have seen in the introduction how allowing aggre­
gation of node values can help us alleviate the over­
specification problem. Rather than actually aggregat­
ing values into a single value, we elect to generalize
assignments. Assignments can now assign a disjunc­
tion of values to a node or variable. The result will be
the same as when aggregating node values into a single
value. We do not want to allow any old disjunction
to be assigned, however. The disjunctions assigned
should correspond to concepts, or to different events
in our hierarchy of event types. The most general event
is the "anything happens" event, which corresponds to
the disjunction of all the values of a node. Assigning
the "anything happens" disjunction to a node, is ex­
actly equivalent to leaving it unassigned. Thus, we see
that allowing the assignment of disjunctions to nodes
in explanations is a generalization of independence­
based explanations.

an explanation to consist only of natural events and
concepts. This is equivalent to assuming that a set of
allowable disjunctions is provided to the system. Sec­
ond, we only assign a disjunction if the probability
of the descendent nodes is statistically independent of
which value (from the disjunction) we condition on.
To get a picture of where this is leading us, consider
the special case where the only higher level concept
is the "any event" concept. In this case, allowing the
assigning of disjunctions under the above constraints is
exactly equivalent to independence based assignments.
That is because the only allowed disjunctions are those
with a single value, or those with all the values of a
node. The second constraint forces us to assign the
disjunction only if independence occurs, exactly as in
the case of independence-based assignments.
We will ignore in this paper the representation issue,
and just assume that for each (multi-valued) node, a
set of all permissible disjunctive assignments is given,
in some form. Thus, for each node v in the belief
network, with a domain Dv, the set of permissible dis­
junctions Mv is given, where Mv � 2D•, as well as
the set of all conditional probabilities of each permis­
sible disjunctive assignment to v given the parents of
v. In what follows, we will usually omit referring to
Mv, assuming its presence implicitly.
One may argue that we do not need to introduce the
first constraint and Mv at all. We could allow any dis­
junction, as long as the second constraint, that condi­
tional independence hold, is obeyed. In fact, this seems
equivalent to an argument of the following form: we
(as intelligent agents) construct our concepts from em­
pirical data. Therefore, if (conditional) independence
occurs, i.e. it does not matter which of a set of values
is assigned, we are justified in creating a new concept
that corresponds to that set of values. The latter ar­
gument seems reasonable, but this issue is beyond the
scope of this paper. Suffice it to say that our defini­
tions require the existence of the set of allowable dis­
junctions Mv, but if we decide that it is not needed,
we can just set Mv = 2D• for every variable in the
network, thereby voiding the first constraint.

We remain with the question: when do we allow a
particular disjunction to be assigned to a node in a
proposed explanation? The answer to this question is
not at all obvious. For example, if we allowed any dis­
junction corresponding to a concept to be used every
time, then all explanations will assign the most gen­
eral disjunction (a disjunction of all the node's values)
to each node. Essentially, this is equivalent to leaving
all non-evidence nodes unassigned, which gives us the
highest probability assignment. This result is, how­
ever, an undesirable trivial explanation, that is com­
pletely independent of our knowledge base.

We begin by formally defining assignments and dis­
junctive (or generalized) assignment. An assignment
A to a set of variables V, each variable v E V having
domain Dv, is a set of pairs ( v, d), where v E V and
d E Dv. If ( v, d) E A we say that A assigns variable v
the value d. We sometimes write v = d instead of ( v, d)
in an assignment. In our example, we might have an
assignment:

Instead, we propose the following criteria: first, the
disjunction has to correspond to a pre-existing con­
cept. The reason for this assumption is that we want

A is complete w.r.t. V if for every v E V there is a
pair ( v, d) E A for some d, i.e. it assigns values to all
of the variables. We call an assignment partial if it is
not necessarily complete.

2.1

GIB EXPLA NATION: DEFINITION

Q

=

{at-tracks = T, method

=

walk}

Relevant Explanations: Allowing Disjunctive Assignments

We define span(A) to be the set of variables assigned
by A, i.e. span(A) = {vl3d (v,d) E A 1\ d E Dv}·
For example, span(Q) = {at-tracks, method}. A is
consistent if each variable in the span of the assign­
ment is assigned a unique value, i.e. if (v,d)E A and
(v,d')E A then d = d'.
A disjunctive (or generalized) assignment A to a set
of variables V is a set of pairs (v,D) where vE V and
D s;; Dv. Each variable is assigned a set of values,
rather than just a single value. A generalized assign­
ment is also a sample space event, the union of the
events comprising its member assignments. In some
cases, we use the notation v = d1 V d2 V ... V dk as
a variant for (v, {d1,d2, ... ,dk}). In our example, we
might have:
g ={at-tracks = T, method = take-taxi V walk}

For generalized assignment (G-assignment) A, we de­
fine span(A) to be the set of variables assigned by
A, i.e. span(A) = {vi3D (v, D) E A 1\ D s;; Dv}·
span- (A), the proper span of A, is the set of vari­
ables v that are assigned a value-set different from Dv.
Formally4: span-(A) = {vi3D (v,D)E AI\D C Dv}·
A G-assignment is consistent if it assigns a unique,
non-empty set to each variable in the span of the as­
signment, i.e. if (v,D) E A and (v,D') E A then
D = D' ::j:. ¢;.
G-assignment B is more refined than G-assignment A
(written B s;; A) iff every value set assigned by A to
each variable is a ( noi\-strict) superset of the value set
assigned by B. Formally:
B s;; A

.....

((v,D)E A-dD' (v,D')E B 1\ D's;; D)

Likewise, G-assignment B is strictly more refined than
G-assignment A (written B C A) iff every value set
assigned by A to each variable is a (non-strict) superset
of the value set assigned by B, except for at least one
variable, where the value set is a strict superset.
An assignment A is included in a G-assignment B
(written AEB) if for every node v assigned some value
set D by B, the node is assigned a value in D by A.
That is, (v,D)E B----+ (3d dE D 1\ (v,d) E A). For
example, both {at-tracks=T,method=take-taxi} and
{at-tracks=T,method=walk} are included in g.
Sometimes we need to refer to an assignment (or G­
assignment) to only certain variables, possibly a subset
of the span of some assignment. We denote such (par­
tial) assignments with a subscript, the set of nodes in
the partial assignment. Thus, for an assignment (or
G-assignment) A:
As

=

{(v,D)l(v,D)E A

1\

vE S}

Definition 2 The

generalized independence-based
condition {GIB condition) holds at node v w. r. t. G-

4
since assigning Dv to v does not restrict the possible
values that v may have, we sometimes would like to say
that v is not "properly" assigned in this case.

203

assignment A iff:

where 1' ( v), denotes the parents (direct predecessors)
of v, and t+ (v) denotes the transitive closure of par­
ents of v (here and throughout this paper).
Intuitively, the GIB condition holds at v if the condi­
tional probability of v given the G-assignment A to the
parents of v is independent of the way we refine the G­
assignment w.r.t. the ancestors of v (i.e. independent
of any further evidence coming from above). We pro­
ceed to define GIB assignments as assignments where
the GIB condition holds at every node. Formally:
Definition 3 A generalized assignment As is GIB iff

for every node vE S, the GIB condition holds.

Finally, we define a GIB MAP as the most probable
GIB assignment where the evidence nodes are assigned
correctly. Formally:
Definition 4 A generalized assignment As is a GIB

MAP w. r. t. evidence £ iff it is a maximum probability
GIB assignment such that £ s;; A.

A GIB explanation is a compact GIB MAP, i.e. a
GIB MAP without the pairs (v,D) such that D = Dv.
Such value-set assignment pairs contribute no infor­
mation, and are thus excluded from the explanation.
Note also that there it is sufficient to maximize the
prior probability As (as defined above), rather than
the conditional probability P(AI£), as the evidence is
constant for each problem instance, and P(£IA) = 1.
In our train-tracks example, the GIB MAP is M =
{at-tracks=T, method=m1 V m2 V ... V m99, intend­
to-go=T}, where each m; is one of the methods M is
a GIB assignment because at-tracks is independent of
the value of the "kidnapped" node, or the assignment
to the method node. It is the GIB-MAP because it is
the most probable amongst the GIB assignments that
have at-tracks=T, with a prior probability of approx­
imately 10-8.
2.2

PROPERTIES OF GIB ASSIGNMENTS

The independence relations that underlie Bayesian be­
lief networks induce certain locality properties on GIB
assignments. These are useful for designing algorithms
that compute GIB explanations. We begin by show­
ing that the bounds on the conditional probability of
a node can be obtained using the bounds of the con­
ditional probability of local complete assignments, i.e.
assignments to the parents (ignoring all the other an­
cestors):
Theorem 1 For positive distributions, the following

equations hold:
min P(A{v}IBt+(v)) =

B�Al(v)

min .

'DECr(v)A'DEAt(v)

P(A{v}IV)

204

Shimony

where Cs is used to denote the set of all complete as­
signments to node set S (throughout this paper), and
thus D ranges over all the complete assignments to the
parents of v that are included in Ar(v), and B ranges
over all G-assignments that are refinements of Al(v)·
For a proof, see appendix A.
From these bounds (theorem 1), and the definition of
the GIB condition (definition 2), it is easy to show
that the GIB condition holds at a node if conditional
independence holds locally:
Theorem 2 For positive distributions, the GIB con­

dition holds at v w. r. t. G-assignment A iff the follow­
ing equation holds:

alizing the concept of hypercubes, on which the IB­
MAP algorithm is based, to allow for disjunctive as­
signments. Generalized hypercubes are generalized
assignments that assign permissible disjunctions to a
node and its parents.
Definition 5 A generalized assignment A is a gen­

eralized hypercube (G-hypercube) based on node v iff
span(A) = {v}U l (v), and if w E span(A) then
A( w) E Mw.
We essentially assume that Mw = Dw, so that the in­
tersection of two value sets (assigned to a variable v
by two different G-hypercubes) is always a permissible
value set for v in a G-hypercube. The latter require­
ment may be satisfied by less restrictive assumptions,
but this issue is beyond the scope of this paper. We de­
fine maximal generalized IB hypercubes, in a manner
similar to IB hypercubes in (16].
Definition 6 A G-hypercube A based on v is a GIB

Thus, checking whether an assignment is GIB is linear
in the size of the span of the assignment, and does not
depend on the size of the graph. Additionally, if A
is a GIB assignment, then its probability is a simple
product, ·computable in time linear in ispan(A)I.
Theorem 3 Let A be a GIB assignment to a (posi­

tive distribution) Bayesian belief network. P(A), the
probability of A, is the product:
P(A) =

IT (

vESpan A)

(1)

This is dn important property, as to compare quality of
GIB explanations, we need to know their probability,
and this theorem allows us to do so efficiently. A proof
outline is discussed in appendix A.
Note that the restriction to positive distributions in
the theorems is only needed so as to ensure that all the
conditional probabilities referred to (in the theorems
and their proofs) are defined. Thus, as long the latter
requirement holds, we do not need the restriction to
positive distributions.
3

GIB-MAP ALGORITHM

An algorithm that uses best-first search is presented
in what follows. The search space is that of partial
generalized assignments (not only GIB assignments),
beginning with the assignment denoting the evidence,
and concluding with a GIB assignment of maximum
probability given the evidence. The next-state gener­
ator selects a node v and generates assignments that
are refinements of the current assignment, by refining
the assignment to the parents of v.
The algorithm is essentially a generalization of the al­
gorithm for finding IB-MAPs (16], achieved by gener-

hypercube (based on v) iff the generalized IB condition
holds at v w.r.t. A.

Definition 7 GIB hypercube A is maximal if it is

minimal w.r.t.• refinement, i.e. there is no GIB hy­
percube B such that A C B.

In our example, the assignment {at-tracks=T,
method=m1 V m2 V .. . V mgg, kidnapped=T} is a
GIB-hypercube based on the node at-tracks, which
is a refinement of the GIB-hypercube {at-tracks=T,
method=m1 V m2 V ... V mgg}. The latter is minimally
refined, and is thus a maximal GIB-hypercube.
The algorithm is shown in figure 2. The termination
condition is that the G-IB condition hold at every node
(it is a weaker condition than the IB-condition). The
GIB condition holds at every expanded node; so there
is no need to check the condition explicitly for every
node in the assignment. It is sufficient that all nodes
are expanded.
States are partial G-assignments, augmented with a
value (approximate probability) and an (integer) index
of the node last expanded. The agenda is kept sorted
(in a heap) by its approximate probability, which for
each state A is determined by:

IT P(A{v}IArcv.))
vES
where S is the set of expanded nodes in A. Theorem
3 ensures that Pa is an admissible heuristic evaluation
function, as it is correct for GIB assignments (all nodes
expanded), and is optimistic for any other assignment
in the agenda.
Pa(A) =

When picking a node, the algorithm selects the unex­
panded node with smallest index in the assignment.
Node indexing is such that each node has a smaller in­
dex than all of its ancestors. Clearly this can be done,
as belief networks are DAGs. The ordering is not nec­
essarily unique, and we just pick some such ordering.

Relevant Explanations: Allowing Disjunctive Assignments

205

events. We do not think, however, that the overspeci­
fication problem is completely overcome by GIB MAP
explanation. That is because slightly changing condi­
tional probabilities may cause an overspecified assign­
ment to variables that are still intuitively irrelevant,
which may in turn cause the wrong explanation to be
preferred.

Queue evidence
onto agenda

This instability problem shown above becomes partic­
ularly acute if the belief network is constructed us­
ing probabilities calculated from real statistical exper­
iments. That can be done either by first constructing
the topology of the network and experimenting to fill
in the conditional probabilities, or by using a method
such as in [4] or as in [12] to get the topology as well
as the conditional probabilities directly from the ex­
periments. In either case, even if exact independence
exists in the real world, the conditional probabilities
computed based on experiments are very unlikely to
be exactly equal.
Figure 2: Computing GIB Explanations
For each assignment, save the number of the node
last expanded.

v

To expand a node v, first check if the GIB condition
holds at v. The condition holds vacuously for root
nodes. Otherwise, if there exists a GIB hypercube 1i
such that Arcv) � 'li, then the GIB condition holds at
v. Set last-expanded(A) to the index of v. If the GlB
condition holds at v, the we consider it expanded, so
evaluate A, and push it back into the agenda.
Otherwise, select all maximal GIB hypercubes 1f. that
are refinements of the assignment A for v and its par­
ents, i.e. such that 1{. � Ar(v)· For each such GIB
hypercube, generate one new assignment B as follows:
B is a (minimally refined) refinement of both 1f. and A.
This is done by looking at the assignment to each node
v. If the node is assigned by only one of the assign­
ments a value set D, then ( v, D) is in B. Otherwise
(if (v,DA) E A and (v,D7t E 11.) for some DA and
D7t), then ( v, DAn D7t) is in B. Evaluate each such
B generated above, and push it into the agenda.
As to the complexity of the algorithm, finding abduc­
tive conclusions is known to be NP-hard in the propo­
sitional case [3, 18], so that any algorithm may be ex­
ponential time in the worst case, as indeed is the case
for our algorithm. However, timing experiments made
for the very similar IB-MAP algorithm suggest that in
practice the running time is reasonable.
4

DISCUSSION

We have shown how generalizing assignments to dis­
junctive assignments, allows us to be more flexible in
defining independence, so as to alleviate the overspec­
ification problem when we have multiple-valued vari­
ables, in which sets of values stand for natural types of

The problem of "almost" independent cases, as well
as a solution that uses 8 independence, is explored in
[18, 17]. It should be possible to apply 8 independence
to generalized assignments as well, as follows:
Definition 8 The generalized delta independence­

based condition (8-GIB condition) holds at node
w. r.t. G-assignment A iff

v

This is a parametric definition: with 8 0 (most re­
strictive), we get the GIB condition. With 8 = 1, the
condition always holds. The correct value for 8 is not
obvious, and it may be desirable to choose its value on
a per-node basis. That may be done, if the distribu­
tions are obtained from empirical data, by estimating
the sampling error bounds. Alternately, we may wish
to bias b based on prior probabilities of the parents of
=

v.

As for properties of GIB assignments, we believe that a
variant of theorem 2, that allows local checking of the
8-GIB condition, holds due to theorem 1. It is clear
that theorem 3 does not hold for 8-GIB assignments,
however.
5

SUMMARY

We have shown that generalizing irrelevance-base ex­
planations to allow a limited assignment of disjunc­
tions (GIB assignments) further alleviates the over­
specification problem. We get the added bonus that
the disjunction allows us to choose a less specific event
(in a particular node) as long as it is irrelevant which
event subtype occurred. GIB assignments were shown
to have certain locality properties.
Based on the locality properties, a best-first search
algorithm for finding GIB MAPs W&'l easy to define

206

Shimony

along the lines of an earlier algorithm, that for com­
puting IB MAPs. It should be possible to show that
the problem is naturally reducible to linear program­
ming (with a . 0-1 solution requirement), as was done
for IB MAPs in [18], which provides another possible
algorithm for computing GIB MAPs. It would also
be interesting to prove the locality property for 8-GIB
assignments, and propose an algorithm for computing
them, perhaps similar to the algorithm for 8-IB MAP
computation, which uses bounds on the probability of
a 8-IB assignment, rather than its exact probability.
Another issue for future research is the following: The
fact that we are using disjunctive assignments rather
than single value assignments may allow us to extend
IB explanations to handle continuous random variables
as well. Events would be ranges of such random vari­
ables where over which conditional independence holds
(i.e. intervals where conditional density function is
constant).
A

of v that are included in A, and for every such V
there exists a G-assignment B that includes exactly
one (complete w.r.t. the ancestors ofv) assignment :F
that is a refinement of V such that equation 4 holds,
then the LHS minimizes over a set that includes all
the cases which are minimized over by the RHS, and
thus we get LHS ::; RHS.
To prove LHS > RHS: let B be any G-assignment
that is more refined than A. Now, using condition­
ing we can write:

But all V are disjoint, and range over all the complete
assignments included in Bt+(v), and thus:

L

P(VjBj+(v))

==

1

VEBr+(v) t\DECr+ (v)

Therefore, equation 5 is a convex sum, and we have:

PROOFS FOR THEOREMS

Theorem 1 For positive distributions, the following

equations hold:

.

max

P(A{v}IV)

>

P(A{v}IBr+(v))

.

min

P(A{v}IV)

<

P(A{v}IBr+(v))

VE Br+(v) I\VECr+ (v)
VEBr+(•J""DECr+(vJ

(6)

Since V is a complete assignment to exactly all the
ancestors of v, then v depends only on the assignment
to its parents:
proof: We prove that the left-hand side of equation 2
(LHS) is less than or equal to the right-hand side of the
equation (RHS) and vice versa. A similar argument
proves equation 3.
To prove LHS < RHS: we note that B ranges over all
refinements ofAj(v)· This includes the G-assignments
where all the ancestors of v are assigned sets of car­
dinality 1. For each of these cases, we have a unique
assignment :F that is complete w.r.t. the ancestors of
v such that FEB.
In Bayesian belief networks, a node is independent of
any (indirect) ancestor given all of its parents, and
thus, we have, for the above cases5:
P(A{v}IF)
P(A{v}IFtcv>)

(4)

Now, since the RHS of equation 2 minimizes
P(A{v}jV) over complete assignments to the parents
5 Actually,

to

v,

this is known to hold only for a value assigned
not for a set of values as here. However, since

P(A{v}!Bt+ (v ))

=

L

P(A'!Bt+ (v))

A'EA{v}AA'EC{v}
and the independence does hold for each A' (since A' it
assigns exactly one value to v) , then it also holds for the
entire sum.

And thus minimizing (or maximizing) over all com­
plete assignments to the ancestors of v is equivalent
to minimizing (or maximizing, respectively) over all
complete assignments to the parents of v, and thus:
.

max P(A{v}IV)

>

P(A{v}IBr+(v))

.

min P(A{v}IV)

<

P(A{v}!Br +(v))

VE Br+(v)I\VECl(v)

VE Br+(v)I\VECl(v)

(7)

Since B in equation 7 is an arbitrary refinement of
A, the equation holds for any such B, in particular
for the B that minimizes P(A{v}IBr+(v))· Now, this
particular B is more refined than A, and thus includes
a (set-wise) smaller set of complete assignments to the
parents of v than does A, and thus:
min P(A{v}IBr+(v)) 2

B�Al(v)

2

.

minP(A{v}IV)

C
min. P(A{v}IV)

VEBr+ ( v "VE l(v)
J
VECl(v)I\VEAl(v)

(8)

Equation 2 follows. Equation 3 likewise follows from
equation 7 (::;), and from equation 4 (2), Q.E.D.
Theorem 3 Let A be a GIB assignment to a (posi­

tive distribution) Bayesian belief network. P(A), the
probability of A is the product:

Relevant Explanations: Allowing Disjunctive Assignments

IT

P(A)

vESpan(A)

P(A{v}IAr(v))

(9)

Proof outline: (complete proof omitted for lack of
space). Assume, without loss of generality, t�at A
assigns some value set to each and every node m the
network. Let B, of cardinality n, be the set of nodes
in the network. Define an integer index from 1 to n
on B such that each node Vi comes before all of its
ancestors (where the subscript is the i�dex). Clear� y
that is possible, as belief networks are directed acyclic
graphs. Since the distribution is positive, it can be
represented as a product of conditional probabilities,
as follows:
n

P(A)

= II P(A{v;}IA{vjln�j>i})
i=I

(10)

It is sufficient to prove that for every n 2: i 2: 1, the
following equation holds:
P(A{vi}IA{vjln?:i>i}) = P(A{v,}IAr(v))
(11)
We can separate out the nodes assigned by the condi­
tioning term on the left-hand side of the above equa­
tion into parents of Vi, other ancestors of Vi, and all the
rest. We then condition on all events that are included
in Al(v) (i.e. write P(A{vi}IA{viln?:j>i}) as a sum of
probability terms). Due to independence, we can drop
some of the conditioning terms, and take some terms
outside the summation, to get:
P(A{v;}IA{viln ;j>i}) = P(A{vi}IAl(v))l:
where :E is a sum of conditional probabilities, which is
shown to be equal to 1.
References

207

[6] Jerome A. Feldman and Yoram Yakimovsky. De­
cision theory and artificial intelligence: I. a
semantics-based region analyzer. Artificial Intel­
ligence, 5:349-371, 1974.
[7] Stuart Geeman and Donald Geeman. Stochastic
relaxation, gibbs distributions and the bayesian
restoration of images. IEEE Transactions on Pat­
tern Analysis and Machine Intelligence, 6:721741, 1984.
[8] Dan Geiger and David Beckerman. Advances in
probabilistic reasoning. In Proceedings of the 7th
Conference on Uncertainty in AI, 1991.
[9] Robert P. Goldman. A Probabilistic Approach to
Language Understanding. PhD thesis, Brown Uni­
versity, 1990. Technical report CS-90-34.
[10] Jerry R. Hobbs, Mark Stickel, Paul Martin, and
Douglas Edwards. Interpretation as abduction. In
Proceedings of the 26th Conference of the ACL,
1988.
[11] Judea Pearl. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Mor­
gan Kaufmann, San Mateo, CA, 1988.
[12] Judea Pearl and T. S. Verma. A theory of inferred
causation. In Knowledge Representation and Rea­
soning: Proceedings of the second International
Conference, pages 441-452, April 1991.
[13] Y. Peng and J. A. Reggia. A probabilistic causal
model for diagnostic problem solving (parts 1 and
2). In IEEE Transactions on Systems, Man and
Cybernetics, pages 146-162 and 395-406, 1987.
[14] R. D. Shachter. Evaluating influence diagrams.
Operations Research, 34 (6):871-882, 1986.

[1] Eugene Charniak and Robert Goldman. A logic
for semantic interpretation. In Proceedings of the
ACL Conference, 1988.

[15) David B. Sher. Towards a normative theory of sci­
entific evidence - a maximum likelihood solution.
In Proceedings of the 6th Conference on Uncer­
tainty in AI, pages 509-515, 1990.

[2] Eugene Charniak and Solomon E. Shimony. Prob­
abilistic semantics for cost-based abduction. In
Proceedings of the 8th National Conference on AI,
August 1990.

[16) Solomon E. Shimony. Algorithms for finding
irrelevance-based map assignments to belief net­
works. In Proceedings of the 7th Conference on
Uncertainty in AI, 1991.

[3] Gregory F. Cooper. The computational complex­
ity of probabilistic inference using bayesian belief
networks. Artificial Intelligence, 42 (2-3):393-405,
1990.

[17] Solomon E. Shimony. Explanation, irrelevance
and statistical independence. In AAAI Proceed­
ings, 1991.

[4] Gregory F. Cooper and Herskovits. Edward. A
bayesian method for the induction of probabilistic
networks from data. Technical Report SMI-91-1,
University of Pittsburgh, January 1991.
[5] Gregory Floyd Cooper. NESTOR: A Computer­
Based Medical Diagnosis Aid that Integrates
Causal and Probabilistic Knowledge. PhD thesis,
Stanford University, 1984.

[18) Solomon E. Shimony. A Pr?babilistic Fra.mew�rk
for Explanation. PhD thesis, Brown Umversity,
1991. Technical report CS-91-57.
[19] Mark E. Stickel. A prolog-like inference system
for computing minimum-cost abductive explana­
tions in natural-language interpretation. Techni­
cal Report 451, Artificial Intelligence Center, SRI,
September 1988.

