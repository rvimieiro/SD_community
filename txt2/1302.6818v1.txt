
Brendan Del Faverol

Gillian Sanders2

1oepartment of Engineering-Economic Systems,
Stanford University, CA 94305
2Section on Medical Informatics
Stanford University, CA 94305

utilities that are difficult to assess and are liable to
judgmental biases. Some people claim that since
human thinking is inherently qualitative, it is
incompatible with quantitative schemes. These
criticisms have fueled interest in alternative
formalisms for reasoning and decision making under
uncertainty that are intended to be easier to use and
more compatible with human cognition. Among these
alternative schemes are: various generalizations of
decision theory [Edwards, 1992]; Dempster-Shafer
belief functions [Shafer, 1976];generalizations of logic,
including default and non-monotonic logics [Ginsberg,
1987]; fuzzy logic [Zadeh, 1983]; possibility theory
[Dubois and Prade, 1988]; and fuzzy probabilities.
If,

however, our goal is simply to provide a qualitative
basis for reasoning and decision making under
uncertainty, there is no need to abandon Bayesian
decision theory. The axioms of decision theory,
indeed, assume only the ability to make qualitative
judgments - that is, to order events by probability or
outcomes by desirability. The quantification of
probabilities and utilities can be based on purely
qualitative judgments. Furthermore, several schemes
have been developed that are purely qualitative, but
are consistent with the axioms of decision theory.
One such scheme is qualitative probabilities, originated
by Wellman [1990; Henrion & Druzdzel 1991;
Wellman & Henrion, 1993]. A second approach to
qualitative probabilities is the kappa-calculus
[Goldszmidt and Pearl, 1992], which represents all
probabilities in a Bayesian belief network by e'K, where
is an integral power of E. The K -calculus is

K

Henrion, Prova n Del Favero, and Sanders

320

,

consistent with the axioms of probability where E---+0.
Events are ranked according to K. Events with larger K
are assumed to be negligible relative to events with
smaller K. The calculus provides a plausible set of
events: those with the smallest (most probable)
consistent with the observed findings. The calculus is
sometimes called

qualitative probability.

To avoid

confusion with other qualitative probability schemes,
we call this representation
Pearl

infinitesimal probabilities.

[1993] has extended this scheme to handle

similar difficulties.
Much current research on
qualitative simulation is directed towards integrating
quantitative information to resolve ambiguities (and
the resultant combinatorial explosions of the search
space).
In this paper, we report the results of an initial
experimental

study

comparing

the

diagnostic

performance on a specific belief network using (1) the
K -calculus or infinitesimal probabilities, and

(2)

qualitative utilities to support decision making.

numerical probabilities. Our goal is to examine how

The K-calculus or infinitesimal probabilities can be

approximation to the numerical representation.

well

the

infinitesimal

scheme performs as

an
We

looked at in two ways: (a) as providing a scheme for

start with a fully assessed numerical representation,

non-monotonic reasoning whose semantics are firmly

convert this into a kappa-representation using finite e

grounded in probability and decision theory; or (b) as

values, and perform inference on a set of test cases.

providing a simplification of belief networks with

We first explain the mappings we used to obtain

numerical probabilities. In this paper, we are focus on

infinitesimal

the second view, and examine the performance of

probabilities, and how we mapped back from the

infinitesimal probabilities as an approximation to
numerical probabilities.

or

K-values

from

the

numerical

pos terior K-values into probabilities for comparison of

From this perspective,

performance. Then, we describe the experimental

proponents of infinitesimal probabilities may claim

design, including the sample network, the set of test

four possible advantages over traditional numerical

cases,

belief networks:

probabilities, the epsilon values used in mapping, and

1. It may be easier to express beliefs by partitioning

and

our

variations

of

the

prior

fault

the number of findings observations per case.

The

number of sets of relative

infinitesimal scheme provides a set of the most

plausibility, that is values, than by assigning

plausible diagnoses for each case. In the results, we

events into a small

each event a precise numerical probabilities.

2. Results from reasoning with infinitesimal

compare these plausible sets with the posterior
probabilities for the diagnoses produced by the

probabilities are more robust and therefore more

numerical

trustworthy since they are based on less specific

implications of these results for the application of the

inputs.

K-calculus as a practical representation.

scheme.

Finally,

we

discuss

the

3. Reasoning with infinitesimal probabilities is
easier to understand and explain.
4. Inference methods with infinitesimal probabilities

can be computationally more efficient.

Initial analysis of the computational complexity of

[1992]

suggests that, in general, it is of the same order as
reasoning with numerical probabilities, that is NP­
hard

[Cooper,

1990].

There

may

be modest

computational savings from doing arithmetic with
small integers instead of floating point numbers.
Most

research

on

qualitative probabilities has

concentrated on developing the formalisms and
efficient algorithms.

AND INFINITESIMAL
PROBABILITIES

Hitherto, these claims have been largely untested.
reasoning infinitesimal probabilities Darwiche

2 MAPPINGS BETWEEN NUMERICAL

There has been little concerted

effort to demonstrate their application to real tasks and
to evaluate their practicality. Initial studies of QPNs
[Henrion and Druzdzel,

1990; Druzdzel and Henrion,
1993; Druzdzel, 1993] suggest that they are often

inconclusive for nontrivial cases. For example, QPNs
give vacuous results in any case with conflicting
evidence. Studies of qualitative simulation have found

In order to be a b le to apply

the

K -calculus to

probabilistic reasoning on a belief network with finite
probabilities, we need to provide a mapping from
probabilities into kappa values. In order to compare
the results we need to map the kappa results back
again into probabilities. Strictly, the K-calculus is only
valid as E---tO.

We use an approximation for finite

values of E. For a finite E, the K-calculus partitions the

real interval

[0,1] into regions identified by integers,

based on the smallest power of in the polynomial. This
mapping is illustrated in Figure 1.
More specifically, consider the real [0,1] interval I,
which is the interval used by probability theory, and a
discretized representation of I, which we call 5. 5 is a
set of non-negative integers which the -calculus uses to
represent probability measures in the interval I.

We

wish to explore the mappings f: I---tS (i.e., from
numerical to infinitesimal probability) and g: S ---t I

Nume rical and Qualitative Probabilistic Reasoning

(i.e., from infinitesimal to

numerical probability).

Note that there is information loss in the mapping f,
since it is not injective. Moreover, the mapping g is
not surjective.

Definition 1

321

[ K"-map] [Spohn 1988] The mapping f

from probability measures to

K"-values takes a

probability 1r and a threshold probability e and

outputs a K"-value K" e S such that

3 APPLICATION DOMAIN: WHY YOUR

CAR DOES NOT START
The task is to troubleshoot why a car is not starting,
given evidence on the status of the lights, battery, fuel,

fan belt, and so on. Figure 2 shows the Bayesian belief

network displaying the causal and conditional
independence relations.

We are grateful to David

Heckerman for providing the original belief network
and to Paul Dagum for lending us his expertise as a
Figure 1 shows an example of a mapping for £

=

0.1.

car mechanic in adjusting some of the probabilities.

All variables are binary (present or absent), except for
battery charge which has three values (high, low,
none). The initial network contains fully quantified,
numerical conditional probability distributions for

Kappa
1C(X)

each influence and prior probabilities for each fault
(source variable).

3

common

effect

Effects of multiple causes of a
are combined

with noisy-ORs,

generalized where necessary.
There are nine explicitly identified faults in this model:
spark plugs bad
distributor bad
fuel line bad
fuel pump bad
gas tank empty

0
0

0.001

0.01

Prd:lability p(X)

0.1

starter bad
battery bad

Figure 1: An example mapping giving kappa as a

fan belt loose

function of probability, for £=0.1.

alternator bad

Figure

2: Bayesian network representing the car diagnosis domain. Leak events represent all the

potential causes of a fault other than those shown explicitly. The number in each origin fault of a leak
node represents its prior probability in the original network. The numbers attached to each influence
arrow represent causal strengths -that is the probability that the successor is broken given that the
predecessor is broken, and all other predecessors are normal.

322

He nrion, Prova n Del Favero, and Sanders
,

We also identified three leaks. Each leak event
represents all possible causes of an event that are not
explicitly identified above. The probability of a leak is
the probability that its associated effect will be
observed even though none of its identified causes are
present.

from 10 to 1000. Table 1 shows the mean and range of

the resulting prior odds we used.

Table 1: The minimum, mean, and maximum prior
fault probabilities. The top line shows the original

engine start other

The

network with larger probabilities. To do this, we
multiplied the prior odds by an odds factor ranging

probabilities. Those below are derived by multiplying

engine tum over other

the odds of each prior by the odds factor and

charging system other

converting back to probabilities.

leaky noisy

or model assigns a probability to each

leak, to handle the fact that the network is inevitably
incomplete. In our adjusted network, the probability

Odds

of each leak was substantially smaller than the sum of

factor

the probabilities of the identified causes for each event.

Minimum

Mean

Maximum

1

0.00001

0.00036

0.00100

10

0.00010

0.00361

0.00991

50

0.00051

0.01750

0.04766

100

0.00103

0.03376

0.09099

300

0.00307

0.08900

0.23095

1000

0 010 17

0.21364

0.50025

There are 10 observable findings in the model_ listed
here in non-decreasing order of expense to test:

1. engine-start
2. gas-gauge
3. engine-tum-over

4. lights
5. radio
6. fan-belt
7. battery-age
8. distributor

.

9. spark-plugs
10. alternator

Note that there are four findings that are also
enumerated faults, namely fan belt, alternator, spark

4.2 Test Cases and quantity of evidence

plugs, and distributor.

We expected that the performance of both numerical

4 EXPERIMENTAL DESIGN

function of the quantity of evidence. We also wished

We wish to investigate the effects of three factors on

relative

the diagnos tic
probabilities:

performance

and infinitesimal schemes would improve as a

of

inf initesimal

to examine the effect of the quantity of evidence on the
performance

of

the

two

schemes.

Accordingly, we needed a representative set of test
cases with varying numbers of findings.

(a) The choice of the value of E on the mapping
between numerical and infinitesimal probabilities.

(b) The range of prior fault probabilities
(c) The quantity of evidence in the test cases.
We have already discussed factor (a). Here, we will
discuss our choice of each of these factors, and the
conduct of the experiment.

We generated a set of 116 test cases, in the following
manner: For each of twelve faults (nine identified
faults plus three leaks), we identified the most likely
(modal) value for each of the ten observable findings.
For each fault, we created a base

case

consisting of all

findings at their modal value. In four cases, the fault is
itself a finding, which we omitted from the base test

case, since including the true fault as observed in the
test case would be trivial. We then generated a second
case for each fault by omitting the most expensive
observation from the base case.

Further cases were

4.1 Range of prior fault probabilities

generated by omitting the next most expensive

The numbers in Figure

finding that the engine does not start. In this way, we
created a series of ten cases for eight faults, and nine

2 are the original prior fault

probabilities. To examine the effect of the magnitude
of the priors on the relative performance of the
infinitesimal calculus, we created versions of the

observation in tum.

In all cases, we retained the

Numerical and Qualitative Probabilistic Reasoning

cases for the four faults that are observable, resulting
in a total of 116 test cases in all.

4.3

323

faults are clearly identifiable, having probabilities at

least an order of magnitude greater than those of all
other faults. We found that this approach, as expected,

gave very similar results to the exact IC-calculus

Computation

inference using CNETS .

To obtain results for the numerical probabilistic

scheme, we employed IDEAL [Srinivas and Breese,

1990], using the clustering algorithm from the I DEAL
library. We applied each of the 116 test cases to the
network using each of the six sets of priors,
performing a total of 696 run. For each run we
computed the posterior probability for each of the
twelve faults resulting in 8352 probabilities.

5 RESULTS
Our first goal was to examine the effect of E values on
the performance of the infinitesimal probability
scheme. We then selected the value of E that gave the
best results and examined the effect of varying the

quantity of evidence on the performance of both

numerical and infinitesimal schemes.

We also converted the original numerical probabilities
into K-values, using the three values e (0.1, 0.01, 0.001),
resulting in a total of 2088 additional runs. We ran

5.1

calculus developed at

we might expect it to perform better for small£, where

each case using CNETS, a full implementation of the K­
the

Rockwell

Palo Alto

Laboratory [Darwiche, 1994], producing posterior K­
values for each fault. For each run, we computed the

plausible set, that is the subset of faults with the
minimal K value.
Definition

[Plausible Set]

2

Consider a set

V:;;{v1,v2, ,vm}representing m possible hypotheses,
•••

Let

vmin

](­

=minvj by the minimum ](-value.
J

probability interval

(0,

1], as shown in Figure 1, and

larger e. To investigate this we analyzed an initial set
of 72 test cases usin g E values of 0.0001, 0.001, 0.01, 0.1,

0.2. Figure 3 shows a graph of average probability
against e. It is interested to note that the average score

identical fore= 0.1 and e

To compare the infinitesimal scheme with the
numerical one, we converted K-values of diagnoses
back to probabilities as follows:
De fi n it ion 3:

original probabilities, with less information lost.

Accordingly, we might expect it to do better with

is identical for E = 0.01 and E

Cll(V)={j:vj =vminl·

[Pro b a bility

setV={v1,v2, ... ,vm}r e presenting
assigned a

the approximation will be mere exact. On the other

hand, a larger E provides rnore partitions to the

score assigned to the true diagnosis for these cases,

The plausible set is given by

hypotheses,

Since the kappa calculus is only strictly correct as E---+0,

consequently, it provides a finer discretization of the

in which each hypothesis has been assigned a
value.

Effect of E values

score]

m

For

=

= 0.001, and also

0.2. Overall, there is an

improvement in performance with increasing E up to

0.2. Accordingly, we selected E

=

0.1 for use in our

remaining experiments.

a

p o s s i b 1 e

in which each hypothes is has been

](-value, the corresponding probability

distribution is given by

ifvj=vmax

(3)

0.3
.,
"'
"' 0.25
..
-<
0.2
I>
., .....
s:: ..
0.15
.. ....
a: "'
.. "
0.1
r;,. ...
.,
"
..
!..
0

��

otherwise

That is, the probability ni= 1/n is assigned to the true
faults if it is in the plausible set of size n. Otherwise,
we assigned p = 0.

�

"

�"'

0.05

0.0001

0.001

0.01

0.1

As an additional test, we also ran IDEAL using the

exact algorithm, but using fault probabilities mapped
to O.OlK for the values obtained from the mapping
using the full set of K values.

subset of 72 test cases.

We applied this to a

In the results, the plausible

Figure 3: Effect of E o n the score (probability

assigned to the true fault) by the infinitesimal scheme

Henrion, Provan, Del Favero, and Sanders

324

5.2 Effect of Number of Findin gs on the

Plausible set

...., 0.5
';
1:1
...

As the quantity of evidence increases, we should
expect the performance of both numerical and
infinitesimal schemes to improve.

Accordingly, we

classified the cases by the number of findings. Figure 4
graphs the average size of the plausible set (number of

Gl
:I
I.
...,
..
Cl
.=

e

Cl,.

0.25

plausible faults) identified by the infinitesimal scheme
as a function of the number of findings. These results
summarize all116 cases fore

=

01
. . As expected, the

average size of the plausible set of faults decreases
with the number of findings, from 7 faults with 1
finding to1. 21 faults for 10 findings. With10 findings,

o �----2
0
6
8
4
10
Number of findings

this scheme provides almost complete specificity that
is, the plausible set usually consists of just a single
diagnosis.
..
.,
Ill

Figure 5: The probability assigned to the true
fault for each scheme as a function of number of
findings

10

.,
...
.Ill
. ..
Ill
:I
"
...
liloo

What is, perhaps, surpnsmg is how closely the
performance of the infinitesimal scheme tracks the
performance of the numerical scheme.

..
0
.,
N

. ..
"'

Indeed the

infinitesimal scheme appears to perform better than
the numerical scheme for intermediate numbers of

5

findings, but this difference is not significant.

Since

the infinitesimal representation is derived from the
numerical one, we could not expect it to do better, on
average.
Note that, even with all ten findings, both schemes

o+-----�----�---r---,--�
0

2

6

Number of fi nd i ngs

8

10

average about 0.5 probability for the true diagnosis.
This relatively poor performance arises because of the
limited scope of the network, which does not provide
the means to differentiate among several classes of

Figure 4: The average size of the plausible set

as a function of the number of findings in each
case.
5.3

Comparing the performance of
infinitesimal and numerical schemes

Next, we compare how the number of findings affects
the diagnostic performance for the infinitesimal and
numerical schemes. Figure 5 graphs the performance
in terms of the average probability each assigns to the
true fault, as a function of the number of findings. For
both schemes, as expected, the average probability
assigned to the true fault increases with increasing
evidence, from about 0.15 with 1 finding, to about 0. 47
with 10 findings.

fault.

5.3 The magnitude of priors and the
performance of infinitesimal

probabilities
The infinitesimal probability scheme appears to
perform very well relative to numerical probabilities
for the original car network, in which the prior fault
probabilities are very small, on average 0.00036

To

examine if it performs equally well for larger priors,
we multiplied the prior odds by five odds factors, as
shown in Table

1.

Figure 6 shows the average

probability assigned to the true diagnosis as a function
of the average priors.

Interestingly, the two schemes

are almost indistinguishable up to an average fault
prior 0. 033. Above that, the performance of the
infinitesimal probability drops off sharply - that is,
for average priors of 0.089 and 0.214.

These results

Numerical and Qualitative Probabilistic Reasoning

confirm our ex pect ation that infinitesimal works well
for small priors, but not so well for large pr i ors.
..
-

=
Cl

...
..
....
c

0.4

ordering of diagnosis. A third, would be to evaluate

even more, the quality of decisions will be less rather
than more sensitive to these differences in
representation.

0.3

While these findings are encouraging for the practical
usefulness of infinitesimal pr oba b il ities, we should

.CI
c
...

a.

them. Another way would be to compare the rank
the quality of decisions based on the diagnosi s. In
general, scoring rules based on ranks of diagnosis or,

....

�
=

325

remember that these initial results are on a single
domain. This car model dom ain is simple, with few

0.2

loops and short chains.

This kind of experiment

should be conducted on a wide range of types of
network to see how far these initial results will hold

0.1-

up.
In the introduction, we distinguished view
infinitesimal

o+-----�----�--+-��o.oo1
0 .01
1
o.oo01
0.1
Aver age prior fault probability
Figure 6: Comparison of the average performance of
infinitesimal and numerical probability schemes as a
function of prior fault probabilities.

probabilities,

as

an

(a)

approach

of
to

nonmonotonic reasoning, from view (b), as an
approximation to numerical probabilities.

We

reiterate that this paper, we focus on (b), and we are
not attempting to evaluate its use as an approach to
nonmonotonic logic.

Conclusions about the former

have limited relevance to the latter.
Infinitesimal pro babi lit ies are quite appealing as an
alternative to numerical probab il ities. They should be

6 CONCLUSIONS

significantly easier to eli ci t from experts. Inference

We find these initial results very encouraging in terms

of the diagnostic performance of the infinitesimal
probability scheme. For this example domain, we

found the best performance occurs using E 0.1 to 0.2.
Performance for E
0.01 was slightly worse.
=

may be more effjcient. And resulting inferences should
be somewhat more robust to changes in probabilities.
Some questions that need further investigation
include:

=

Performance of the infinitesimal scheme relative to the
numerical

scheme

does

not

appear

to

Does the best choice of E vary with the domain?

vary

significantly with the quantity of evi dence. The
performance using infinitesimal probability is not

Does these results hold for larger networks, with
more complex structures?

noticeably worse than the numerical probabilities for
prior fault probabilities up to about 0.03. For larger
average fault probabilities, the relative perform ance of

Can this infinitesimal approximation be extended
to utilities and decision making?

infinitesimal probabilities starts to drop off sharply.

This findings suggests that infinitesimal probabilities

Can we obtain a clearer analytic characterization

are more likely to be reliable for diagnosis tasks with

of when performance

very small prior fault probabilities, such as most
machine and electronic devices. They may also work
for some med ical domains, as long as the
priors are less than

disease

1%.

we have used is very simple.

In

addition,

engineering

The mapping from K-values back to probabilities that
More sophistic ated

mappings are pos sible, making use of higher values.

We should also point out that the scoring methods that
we have used to evaluate performan ce have been
based on posterior probability of the true diagnosis,
which is perhaps the most exacting way to compare

will be or won't be

reliable?
we

methods

need practical knowledge
for

eliciting

infinitesimal

probabilities. We an ticipate that, in the long run, the

best p r actical tools will
quantitative methods.

combine qualitative and

326

Henrion, Provan, Del Favero, and Sanders

Intelligence Conference,
Acknowledgments

M. Goldszmidt and J. Pearl.
causal relations.

This work was supported by the National Science
Institute for Decision Systems Research. We would
like to thank David Beckerman for use of the car

pages 99-110, Vermont, 1992.
entropy approach to nonmonotonic reasoning.

refining some of the probabilities.

M.

Shac�ter,

The Logic of Conditionals.

G.F. Cooper. The Computational Complexity of
Probabilistic Inference Using Belief Networks.

Artificial Intelligence, 42:393-405, 1990.
Darwiche. A symbolic generalization of probability
theory. Ph.D. dissertation, Computer Science Dept.,
Stanford University, Palo Alto, CA, 1992.
M.

&

Goldzmidt.

CNETS:

A

computational environment for generalized causal
networks. 1994, {this volume).

M. Druzdzel and M. Henrion. Efficient reasoning in

Proceedings of
the American Association for Artificial Intelligence
Conference, pages 548-553, Washington D.C., 1993.
J. Druzdzel. Probabilistic Reasoning in Decision
Support Systems: From Computation to Common
Sense. PhD thesis, Department of Engineering and
qualitative probabilistic networks. In

M.

Public

Policy,

Carnegie

Mellon

University,

Pittsburgh, Pa, 1993.

H. Prade. Possibility Theory: an Approach
to Computerized Processing of Uncertainty. Plenum

D. Dubois and

Press, NY, 1988.

Utility Theories: Measurements and
Applications. Kluwer Academic, 1992.
H. A. Geffner. Default Reasoning: Causal and Conditional

W. Edwards.

Theories.
M.

MIT Press,

Henrion.

and

Cambridge, MA, 1992.
M.

Druzdzel

"Qualitative

propagation and scenario-based explanation of
probabilistic reasoning". In M. Henrion and R.
S h achter,

editors,

Intelligence.

6,

Uncertainty in Artificial

Elsevier

Science

B.V.

(North­

Holland), 1991.
M.

Hendon.

"Search-based methods to bound

diagnostic probabilities in very large belief nets".

M.

In Proceedings of Conf on Uncertainty and Artificial
Intelligence, 1991.
Ginsberg. Readings in Nonmonotonic Reasoning.
Morgan Kaufmann, San Mateo, CA, 1987.

M. Goldszmidt and J. Pearl. System Z+ :A formalism
for reasoning with variable strength defaults. In

Proceedings of American Association for Artificial

editors,

Uncertainty in Artificial

Intelhgence, pages 129-138. Elsevier Science B.V.

D. Reidel,

Dordrecht, Netherlands, 1975.

D arwiche

IEEE Transactions on Pattern Analysis and Machine
Intelligence, 15:3:220-232, 1993.
He nrion. An Introduction to Algorithms for
Inference in Belief Networks. In M. Henrion and R.

References

A.

In

M. Goldszmidt, P. Morris, and J. Pearl. A maximum

diagnosis network, and Paul Dagum for help in

A.

Stratified rankings for

Proceedings of the Fourth
International Workshop on Nonmonotonic Reasoning,

Foundation under Grant Project IRI-9120330 to the

E. W. Adams.

pages 399-404, Anaheim,

CA, 1991.

(North-Holland), 1990.

Probability and the Logic of Rational
Belief Wesleyan University Press, Middletown, 1961.

Henry Kyburg.
J.

McCarthy.

Applications of circumscription to

Artificial
Intelligence, 28:89-116, 1986.
Pearl. Probabilistic Reasoning in Intelligent Systems:
Networks of Plausible Inference. Morgan Kaufmann,
formalizing commonsense knowledge.

J.

San Mateo, CA, 1988.
J. PearL From conditional oughts to qualitative

Proceedings of the 9th Conference
on Uncertainty in AI, pages 12-20, Washington

decision theory. In
D.C., 1993.

W. Spohn. Ordinal conditional functions: A dynamic
theory of epistemic states. In W.L. Harper and B.

Causation in Decision, Belief
Change, and Statistics, pages 105-134. Reidel,

Skyrms, editors,

Dordrecht, Netherlands, 1988.
G. Shafer. A

Mathematical Theory of Evidence.

Princeton

University Press, 1976.
S. Srinivas and J. Breese. IDEAL: A Software Package
for Analysis of Influence Diagrams. In

Uncertainty in Artificial Intelligence,

Proc. Conf

pages 212-219,

1990.

Formulation ofTradeoffs in Planning Under
Uncertainty. Pitman, London, 1990.

M. Wellman.

Wellman, Michael P. and Henrion, Max,"Explaining
'Explaining Away"', IEEE Transactions on Pattern
Analysis and Machine Intelligence, val 15, No 3,
March 1993, 287:291.
L. Zadeh. Fuzzy sets. Information and Control, 8:338-353,
1983.

