graph search.
Even with this improvement, the time and space cost
can be prohibitive when N and T are very large; for
example, with a state space defined by 30 Boolean variables, running Viterbi for a million time steps requires
1024 computations. One possible approach to handle
such problems is to use a state abstraction: a mapping œÜ : S0 7‚Üí S1 from the original state space S0 to a
coarser state space S1 . For stochastic models (such as
an HMM), the parameters of the model in S1 are often
chosen to be the maximum of the corresponding constituent parameters in S0 . Although these parameters
do not define a valid probability measure, they serve as
admissible heuristics for an A* search. The same idea
can be applied to produce a a hierarchy of abstractions S0 , S1 , . . . , SL . Coarse-to-fine dynamic programming or CFDP (Raphael, 2001) begins with SL and
iteratively finds the shortest path in the current (abstracted) version of the graph and refines along it until
the current shortest path is completely refined. Several
algorithms‚Äîe.g., hierarchical A* (Holte et al., 1996)
and HA*LD (Felzenszwalb and McAllester, 2007)‚Äîare
able to refine only the necessary part of the hierarchy
tree and compute heuristics only when needed.
The Viterbi algorithm devotes equal effort to every
link in the state‚Äìtime trellis. CFDP and its relatives
can determine that an entire set of states need not
be explored in detail, based on bounding the cost of
paths through that set; but they do so separately for
each time step. In this paper, we show how to use
temporal abstraction to eliminate large sets of states
from consideration over large periods of time.
To motivate our algorithm, consider the following
problem. We observe Judy‚Äôs daily tweets describing
what she eats for lunch, and wish to infer the city in
which she is staying on each day. The state space S0 is
the set of all cities in the world. The abstract space S1
is the set of countries, and S2 is the continents. (Figure 1 shows a small example.) The transition model

TAV refinements
New York

U.S.A.

Cities

Chicago

25

North
America

Montreal

Canada

Toronto

20
15
10
5
10

London

20

30

40

50

Days

England

Manchester

CFDP refinements

Europe
Munich

25

T=1

T=2

T=3

T=4

Cities

Germany

Berlin
T=5

Figure 1: The state‚Äìtime trellis for a small version of
the tracking problem. The links have weights denoting
probabilities of going from a city A to a city B in a day.
The abstract state spaces S1 (countries depicted in green)
and S2 (continents in yellow) are only shown for T=5 to
maintain clarity. The observation links are also omitted
for the same reason.

suggests that on any given day Judy is unlikely to leave
the city she is in, even less likely to leave the country
she is in, and very unlikely indeed to travel to another
continent. Thus, if Judy had Tandoori chicken on a
Thursday but the rest of the week was all hamburgers,
then it is most likely that she was in some American
city for the entire week. However, if she had Tandoori chicken and/or biryani for an entire week, then
it is quite possible that she is in India. Our algorithm,
temporally abstracted Viterbi (henceforth TAV), facilitates reasoning over a temporal interval (like a week
or month or longer) and localized search within those
intervals. Neither of these is possible with Viterbi or
state abstraction algorithms like CFDP. The computational savings of TAV on an instance of this problem
can be seen in Figure 2.
Temporal abstractions have been well-studied in the
context of planning (Sutton et al., 1999) and inference
in dynamic Bayesian networks (Chatterjee and Russell, 2010). An excellent survey of temporal abstraction for dynamical systems can be found in (Pavliotis
and Stuart, 2007). To the best of our knowledge, TAV
is the first algorithm to use temporal abstraction for
general shortest-path problems. TAV is guaranteed to
find the Viterbi path, and does so (for certain problem
instances) several orders of magnitude faster than the
Viterbi algorithm and one to two orders of magnitude
faster than CFDP.
The rest of the paper is organized as follows. Section 2
reviews the Viterbi algorithm and CFDP and introduces the notations and definitions used in the rest of
the paper. Section 3 provides a detailed description
of the main algorithm and establishes its correctness.

20
15
10
5
10

20

30

40

50

Days
Figure 2: A comparison of the performance of CFDP and
TAV on the city tracking problem with 27 cities, 9 countries
and 3 continents over 50 days. The plots indicate portions
of the state‚Äìtime trellis each algorithm explored. Black,
green and yellow squares denote the cities, countries and
continents considered during search. The cyan dotted line
is the optimal trajectory.

Section 4 discusses the computation of temporal abstraction heuristics. Section 5 presents some empirical results to demonstrate the benefits of TAV while
section 6 provides some guidance on how to induce
abstraction hierarchies.

2

Problem Formulation

Consider a hidden Markov model (HMM) whose latent Markovian state X is in one of N discrete states
{1, 2, . . . , N }. Let the actual state at time t be denoted by Xt . The transition matrix A = {aij :
i, j = 1, 2, . . . , N } defines the state transition probabilities where aij = p(Xt+1 = j | Xt = i). The Markov
chain is assumed to be stationary, so aij is independent of t. Let the discrete observation space
be the set {1, 2, . . . , M }. Let Yt be the observation symbol at time t. The observation matrix B =
{bik : i = 1, 2, . . . , N ; k = 1, 2, . . . , M } defines the emission probabilities where bik = p(Yt = k | Xt = i). (We
assume a discrete observation space in this paper,
but our methods naturally extend to the continuous case.) The initial state distribution is given by
Œ† = {œÄ1 , . . . , œÄN } where œÄi = p(X0 = i).
The Viterbi path is the maximum likelihood sequence
of latent states conditioned on the observation sequence. Following Rabiner (1989), we define
Œ¥t (i) = max p(X0:t‚àí1 , Xt = i, Y1:t | A, B, Œ†) ,
X0:t‚àí1

i.e., the likelihood score of the optimal (most likely)
sequence of hidden states (ending in state i) and the

first t observations. By induction on t, we have:
Œ¥t+1 (j) = [max Œ¥t (i)aij ]bjYt+1 .
i

The actual state sequence is retrieved by tracking the
transitions that maximize the Œ¥(.) scores for each t and
j. This is done via an array of back pointers œàt (j). The
complete procedure (Rabiner, 1989) is as follows:
1. Initialization

Œ¥1 (i)
œà1 (i)

=
=

œÄi biY1 ,
0

1‚â§i‚â§N

2. Recursion
Œ¥t (j) =

max1‚â§i‚â§N [Œ¥t‚àí1 (i)aij ]bjYt , 2 ‚â§ t ‚â§ T

œàt (j) =

arg max1‚â§i‚â§N [Œ¥t‚àí1 (i)aij ],

2‚â§t‚â§T

3. Termination
P‚àó

=

XT‚àó

=

max [Œ¥T (i)]

1‚â§i‚â§N

arg max[Œ¥T (i)]
1‚â§i‚â§N

4. Path backtracking
Xt‚àó

=

‚àó
œàt+1 (Xt+1
),

t = T ‚àí 1, T ‚àí 2, . . . , 1

The time complexity of this algorithm is O(N 2 T ) and
the space complexity is O(N 2 + N T ).
In coarse-to-fine (a.k.a. hierarchical) approaches, inference is performed in the coarser models to reduce the amount of computation needed in the
finer models.
Typically, a set of abstract state
spaces S = {S0 , S1 , . . . , SL } and abstract models
M = {M0 , M1 , . . . , ML } are defined where S0 (M0 ) is
the original state space (model) and SL (ML ) is the
coarsest abstract state space (model). Let the parameters of Ml be denoted by the set {Al , B l , Œ†l }.
A state in this hierarchy is denoted by sli , where l is the
abstraction level and i is its index within level l. Nl denotes the number of states in level l. Let œÜ : Sl 7‚Üí Sl+1
denote the mapping from any level l to its immediate
abstract level l + 1. The parameters at level l + 1
are defined by taking the maximum of the component
parameters at level l. Thus, Al+1 = {al+1
ij }, where
l
al+1
ij = max apq
p,q

s.t.

l+1
l
œÜ(slp ) = sl+1
.
i , œÜ(sq ) = sj

B l+1 and Œ†l+1 are defined similarly in terms of B l
and Œ†l . Any transition/emission probability in an abstract model is a tight upper bound on the corresponding probability in its immediate refinement. Hence,
the cost (negative log probability) of an abstract trajectory can serve as an admissible heuristic to guide
search in a more refined state space.
CFDP works by starting with only the coarsest states
sL
1:NL at every time step from 1 to T . The states in t

and t+1 are connected by transition links whose values
are given by AL . B L and Œ†L define the other starting
parameters. It then iterates between computing the
optimal path in the current trellis graph and refining
the states (and thereby the associated links) along the
current optimal path. The algorithm terminates when
the current optimal path contains only completely refined states (i.e., states in S0 ). A graphical depiction
of how CFDP works is shown in Figure 5. The TAV
algorithm, which also has an iterative structure, is described in the next section. We will be reusing notation
and definitions from this section throughout the rest
of the paper.

3

Main algorithm

The distinguishing feature of TAV is its ability to
reason with temporally abstract links. A link in the
Viterbi algorithm and CFDP-like approaches describes
the transition probability between states over a single time step. A temporally abstract link starting in
state s1 at time t1 and ending in state s2 at time
t2 > t1 represents all trajectories having those end
points and is denoted by a 4-tuple‚Äî((s1 , t1 ), (s2 , t2 )).
Links((s, t)) is the set of incoming links to state s at
time t. Children(sl ) = {s0 : s0 ‚àà S l‚àí1 , œÜ(s0 ) = sl } is
the set of children of state sl in the abstraction hierarchy. We define three different kinds of temporally
abstract links:
1. Direct links: d(s, t1 , t2 ) represents the set of trajectories that start at (s, t1 ) and end in (s, t2 ) and
stay within s for the entire time interval (t1 , t2 ).
2. Cross links: c((s1 , t1 ), (s2 , t2 )) represents the set
of trajectories from (s1 , t1 ) to (s2 , t2 ), when s1 6=
s2 .
3. Re‚Äìentry links: r((s, t1 ), (s, t2 )) represents the set
of trajectories that start at (s, t1 ) and end in
(s, t2 ) but move outside s at least once in the
time interval (t1 , t2 ). r((s1 , t1 ), (s1 , t2 )) = ‚àÖ when
t2 ‚àí t1 ‚â§ 1.
The direct and cross links are denoted graphically by
straight lines, whereas the re-entry links are represented by curved lines as shown in Figure 3. A generic
link is denoted by the symbol k. The (heuristic) score
of a temporally abstract link has to be an upper bound
on the probability of all trajectories in the set of trajectories it represents. Computing admissible and monotone heuristics will be discussed in Section 4.
Our algorithm‚Äôs computational savings over spatial abstraction schemes come from two avenues‚Äîfirst, fewer
time points to consider using temporal abstraction;
second, fewer states to reason about by considering
constrained trajectories using direct links. Although

the general flow of the algorithm is similar to CFDP,
the refinement constructions are different. The algorithm descriptions provided omit details about observation matrix computations since they are standard.
However, the issue is revisited in Section 4 to focus
on some subtleties. The correctness of the algorithm
depends only on the admissibility of the heuristics.
3.1

Spatial refinement

When a direct link, d(sl , t1 , t2 ), lies on the optimal
path, the natural thing to do is to refine (partition)
the set of trajectories it represents. The original direct link is replaced with all possible cross, direct and
re-entry links between Children(sl ) at t1 and t2 . This
is depicted graphically in Figure 3. A link is also refined spatially if its time span (t2 ‚àí t1 ) is 1 time step
since temporal refinement is not a possibility. The
pseudocode for spatial refinement (see Algorithm 1)
provides all the necessary details. It is trivial to show
that the new links constitute a partition of the trajectories represented by the original link.
Algorithm 1 Spatial Refinement((p1 , t1 , p2 , t2 ))
C ‚Üê Children(p1 ); D ‚Üê Children(p2 )
if t2 ‚àí t1 > 1 then
Links(p1 , t2 ) ‚Üê Links(p1 , t2 ) \ d(p1 , t1 , t2 )
else
5:
Links(p2 , t2 ) ‚Üê Links(p2 , t2 ) \ k((p1 , t1 ), (p2 , t2 ))
end if
usedStates(t1 ) ‚Üê usedStates(t1 ) ‚à™ C
usedStates(t2 ) ‚Üê usedStates(t2 ) ‚à™ D
for all s ‚àà D do
10:
Links(s, t2 ) ‚Üê Links(s, t2 ) ‚à™ d(s, t1 , t2 )
for all s0 ‚àà C do
if s = s0 then
Links(s, t2 ) ‚Üê Links(s, t2 ) ‚à™ r((s, t1 ), (s, t2 ))
else
15:
Links(s, t2 ) ‚Üê Links(s, t2 ) ‚à™ c((s0 , t1 ), (s, t2 ))
end if
end for
end for

3.1.2

Temporal refinement

When a cross link c((s1 , t1 ), (s2 , t2 )) or a re-entry link
r((s1 , t1 ), (s1 , t2 )) is selected for refinement, we are
faced with the task of refining a set of trajectories that
do not stay in the same state for the abstraction interval. This is a case where temporal abstraction is not
helping (not at the current resolution at least).

s1l

s2l

s2l

s1l

Spatial
Refinement

Refinement constructions

A refinement of a temporally abstract link replaces
the original link with a set of refined links that represent a partition‚Äîa mutually exclusive and exhaustive
decomposition‚Äîof the set of trajectories represented
by the original link. The refinement allows us to reason about subsets of the original set of trajectories
separately and thereby potentially narrow down on a
single optimal trajectory. There are two different kinds
of refinement constructions.
3.1.1

s1l

s1l

T1
s2l
T1

T2

s1l-1

s1l-1

s2l-1

s2l-1

s2l
T2

Figure 3: Spatial refinement: The optimal link, shown in
bright red, is a direct link and is replaced with all possible
links between its children.

An example of temporal refinement, which is only invoked when t2 ‚àí t1 > 1, is shown in Figure 4. It
results in splitting the time interval (t1 , t2 ) into two
sub-intervals that together span the original interval.
Let us assume that we select the (rounded off) midpoint of the interval. When a link is temporally refined , we temporally split all cross, re-entry and direct links spanning the interval (t1 , t2 ) between states
in Children(œÜ(s1 )). We will show later that for any
link longer than 1 time step, œÜ(s1 ) = œÜ(s2 ). It should
be noted that re-entry links are only added when the
sub‚Äìinterval length is longer than 1 time step. Also, if
t2 ‚àí t1 = 1, then a cross link is spatially refined (analogous to CFDP).
One possibility is that some of the direct links for
states in Children(œÜ(s1 )) between (t1 , t2 ) were already
spatially refined. In that case, we apply temporal refinement recursively to the spatially refined links of
those direct links. The choice of the splitting point
does not affect the correctness of the algorithm as long
as the split is replicated in the instantiated portion of
the state space tree rooted at œÜ(s1 ). The pseudocode
(shown in Algorithm 2) provides details of this procedure.
Lemma 3.1 The sets of trajectories represented by
links before and after any spatial or temporal refinement are the same. Also, every trajectory is represented by exactly one temporally abstract path.
Lemma 3.2 Any link created by TAV will always be
between two states at the same level of abstraction. If
the time span of the link is greater than 1 time step,
then those two states will also have the same parent at
all coarser levels of abstraction.
Proof The original links are all between states of level
L. Both refinement constructions add links only between states at the same abstraction level. This proves

Algorithm 2 Temporal Refinement((parent, t1 , t2 ))

Algorithm 3 BestPath(Links, usedStates, usedT imes)

nT ‚Üê d(t1 + t2 )/2e
if parent ‚àà usedStates(nT ) then
return
end if
usedT imes ‚Üê usedT imes ‚à™ nT
C ‚Üê Children(parent)
usedStates(nT ) ‚Üê usedStates(nT ) ‚à™ C
for all s ‚àà C do
if d(s, t1 , t2 ) ‚àà
/ Links(s, t2 ) then
T emporal Ref inement(s, t1 , t2 )
Links(s, t2 ) ‚Üê ‚àÖ
Links(s, nT ) ‚Üê ‚àÖ
else
Links(s, t2 ) ‚Üê {d(s, nT, t2 )}
Links(s, nT ) ‚Üê {d(s, t1 , nT )}
end if
for all s0 ‚àà C do
Links(s, t2 ) ‚Üê Links(s, t2 ) \ k((s0 , t1 ), (s, t2 ))
Links(s, t2 ) ‚Üê Links(s, t2 ) ‚à™ k((s0 , nT ), (s, t2 ))
Links(s, nT ) ‚Üê Links(s, nT ) ‚à™ k((s0 , t1 ), (s, nT ))
end for
end for

curT ime ‚Üê 1
while curT ime < T do
curT ime ‚Üê nextU sedT ime(curT ime, usedT imes)
for all s ‚àà U sedStates(curT ime) do
Œ¥t (s) ‚Üê M axOverLinks(Links((s, curT ime)), Œ¥)
œàt (s) ‚Üê ArgM axOverLinks(Links((s, curT ime)), Œ¥)
end for
for level = L ‚àí 1 to 0 do
for all s ‚àà S level && s ‚àà usedStates(curT ime) do
if Œ¥t (œÜ(s)) > Œ¥t (s) then
Œ¥t (s) ‚Üê Œ¥t (œÜ(s))
œàt (s) ‚Üê œàt (œÜ(s))
end if
end for
end for
for level = 0 to L-1 do
for all s ‚àà S level && s ‚àà usedStates(curT ime) do
if Œ¥t (œÜ(s)) ‚â§ Œ¥t (s) then
Œ¥t (œÜ(s)) ‚Üê Œ¥t (s)
œàt (œÜ(s)) ‚Üê œàt (s)
end if
end for
end for
end while
s‚àó ‚Üê arg maxs‚ààusedStates(T ) Œ¥T (s)
curT ime ‚Üê 1; P ath ‚Üê ‚àÖ
while curT ime > 1 do
P ath ‚Üê P ath ‚à™ œàcurT ime (s‚àó )
(s‚àó , curT ime) ‚Üê œàcurT ime (s‚àó )
end while
return P ath

5:

10:

15:

20:

5:

10:

15:

20:

25:

s1l

s1l

s1l

s1l

s1l

30:

going links. The score computation algorithm moves
forward in time like the normal Viterbi algorithm. The
score computation (at each used time step t) is done
in 3 phases. The pseudocode is given in Algorithm 3.

Temporal
Refinement

s2l
T1

s2l

s2l
T2

s2l
T1

s2l
T

1. Œ¥t (s) is computed using the best of its incoming
links, Links(s, t) and œàt (s) points to that link.

T2

Figure 4: Temporal refinement: When refining a cross or
re-entry link, refine all links between nodes that have the
same parent as the nodes of the selected link.

the first statement. Moreover, upon initialization,
there is no coarser level of abstraction‚Äîhence the second part of the statement is vacuously true. Temporal
refinement always considers links between descendants
of the parent node. Spatial refinement also adds links
between Children(sl ) of a state sl except when the
time step is 1. This proves the second statement. 
3.2

Modified Viterbi algorithm

It is possible to have links to a state s and to its
abstraction œÜ(s) at the same time step t (see Figure 5(b)). This was not possible in CFDP. Hence,
we need a slightly modified scoring and backtracking
scheme. Œ¥t (s) is the best score of a trajectory ending
in state s at time t and œàt (s) contains the temporally
abstract link‚Äôs information which connects (s, t) to its
predecessor. usedT imes is a sorted list of time steps
which have links to or from it. usedStates(t) is the
set of nodes at time t which have incoming or out-

2. Starting at level L ‚àí 1 and going down to level 0,
a state s gets its parent‚Äôs (œÜ(s)) score and backpointer if œÜ(s) has a higher score.
3. Starting from level 0 and going up to level L ‚àí 1,
a state s‚Äôs parent œÜ(s) gets its child‚Äôs score and
backpointer if s has a higher score
Theorem 3.3 The score Œ¥t (s) computed by the
BestP ath procedure is a strict upper bound on all trajectories ending in state s at time t given the current
abstracted version of the state-time trellis.
Proof Any trajectory ending in a state sÃÇ, which is
neither an ancestor nor a descendant of s, does not include any trajectory to s. Hence, BestP ath computes
an upper bound on the score of the best trajectory
ending in state s at time t.
For the bound to be strict, it is sufficient to show that
each phase of BestP ath only considers scores of such
nodes where every incoming link includes at least one
trajectory to s. The first phase accounts for all the
incoming links to node s itself. Let œÜ‚àó (s) denote an
ancestor of s. Any incoming link (direct, cross or re‚Äì
entry) to œÜ‚àó (s) includes at least one trajectory to s.

This necessitates taking the maximum over Œ¥t (œÜ‚àó (s))
(step 2). Finally, any trajectory ending in state s0 ,
where s0 is a descendant of s, is by definition, a trajectory ending in s. Hence, the upper bound is strict.

The ordering of the phases is important to perform the
desired computation correctly and efficiently.
3.3

Complete algorithm

The algorithmic structure of TAV and CFDP are quite
similar. The complete specification of TAV is presented in Algorithm 4. CFDP has a different initialization and refinement is node‚Äìbased (TAV is link‚Äìbased)
which introduces links between states at different levels of abstraction. The two initializations are shown in
Figure 5(a). CFDP‚Äôs initial configuration has no temporally abstract links. The algorithm iterates between
two stages: computing the optimal path in the current graph and refining links along the current optimal
path. A few steps of execution of the two algorithms
are shown on an example in Figure 5.

The heuristic score of a direct link is very easy to compute. We do not have to select between possible state
transitions. Thus, the heuristic for a link spanning
the interval (t1 , t2 ) can be done in O(t2 ‚àí t1 ), since we
still need to account for all the observations in that
interval. If the score is cached, the score for any subinterval is computable in O(1) time.
Cross links and re‚Äìentry links require further consideration. A somewhat expensive option is to compute
the Viterbi path in the restricted scope. As Lemma 3.2
shows, a cross or a re‚Äìentry link represents trajectories
that can switch between sibling states (the ones which
map to the same parent via œÜ). In an abstraction hierarchy, if the cardinality of Children(s) for any state s
is restricted to some constant C, then computing this
heuristic will require O(C 2 (t2 ‚àí t1 )) time.
A computationally cheaper but relatively loose heuristic is the following:
h((si , t1 ), (sj , t2 ))

=

t

Algorithm 4 TAV(A, B, Œ†, œÜ, Y1:T )

5:

10:

15:

20:

25:

Œ¥1 (.) ‚Üê ScoreInitialization(Œ†)
usedStates(1) = usedStates(T ) = S L
usedT imes = {1, T }
for all s ‚àà S L do
Links(s, T ) ‚Üê d(s, 1, T )
for all s0 ‚àà S L do
Links(s, T ) ‚Üê Links(s, T ) ‚à™ k((s0 , 1), (s, T ))
end for
end for
V iterbiP athF ound ‚Üê 0
while V iterbiP athF ound = 0 do
P ath = BestP ath(Links, usedStates, usedT imes)
V iterbiP athF ound = 1
for all k ‚àà P ath do
((s1 , t1 ), (s2 , t2 )) ‚Üê details(k)
if level(s1 ) > 1 || ¬¨isDirect(k) then
V iterbiP athF ound = 0
end if
if isDirect(k) || t2 ‚àí t1 = 1 then
Spatial Ref inement(k)
else
T emporal Ref inement(k)
end if
end for
end while

The correctness of the algorithm follows from the optimality of A* search and Lemma 3.1 and Theorem 3.3.

4

Heuristics for temporal abstraction

In hierarchical state abstraction schemes, computing
heuristics involves taking the maximum of a set of single time step transition probabilities. As mentioned
in Section 2, this can be done by hierarchically constructing Al , Bl and Œ†l . For temporal abstractions
however, there are more design choices to be made
when it comes to computing heuristic scores of links.
There is a more significant tradeoff between cost of
computation and quality of heuristic.

t2 ‚àít1 ‚àí2
max AÃÇik max AÃÇpq
max AÃÇkj
p,q
k
k
Y
max BÃÇkYt
k

AÃÇ and BÃÇ represent the transition matrices for the set
Children(œÜ(si )). This heuristic chooses the best possible transition at every time step other than the two end
points and also the best possible observation probability. Its computational complexity is O(C 2 + (t2 ‚àí t1 )).
Caching values help in both cases. The Viterbi heuristic, being tighter, leads to fewer iterations but needs
more computation time. We will compare the two
heuristics in our experiments.

5

Experiments

The simulations we performed were aimed at showing
the benefit of TAV over Viterbi and CFDP. The benefits are magnified in systems where variables evolve
at widely varying timescales. The timescale of a random variable is the expected number of time steps in
which it changes state. A person‚Äôs continental location
would have a very large timescale, whereas his zip code
location would have a much smaller timescale.
A natural way to generate transition matrices with
timescale separation is to use a dynamic Bayesian network (DBN). We consider a DBN with n variables,
each of which has a cardinality of k. Hence, the state
space size N is k n . We used fully connected DBNs
in our simulations. Our observation matrix was multimodal (hence somewhat informative). A DBN with
a parameter  means that the timescales of successive variables have a ratio of . The fastest variable‚Äôs
timescale is 1/ and the slowest variable‚Äôs (1/)k .
The state space hierarchy at the most abstract level
arises from the branching of the slowest variable. In

(a)
(b)
(c)
(d)
Figure 5: Sample run: TAV: (a) Initialization. The optimal path is a direct link‚Äîhence spatial refinement. The new
additions are shadowed. (b) A re-entry link is optimal‚Äîhence temporal refinement. Since one direct link among siblings
was already refined in Step 1, we also temporally refine the spatially refined component. (c) The optimal path has links at
different levels of abstraction. Such scenarios necessitate the BestP ath procedure. (d) More recursive temporal refinement
is performed. Note the difference in the numbers of links in the two graphs after 3 iterations.

each subtree, we branch on the next slowest variable.
In the experiments in this section we assume that the
abstraction hierarchy is given to us.
5.1

Varying T, N and 

To study the effect of increasing T on the computation time, we generated 2 sequences of length 100000
with  = .1 (case 1) and .05 (case 2). N was 256
and the abstraction hierarchy was a binary tree. For
each sequence, we found the Viterbi path for the first
T timesteps using TAV, CFDP and the Viterbi algorithm. The results are shown in Figure 6(a). TAV‚Äôs
computational complexity is marginally super-linear.
This is because TAV might need to search in the interval [0, T ] even if it had found the Viterbi sequence in
that interval as new observations (after time T ) come
in. For the  values used, TAV is more than 2 orders
of magnitude faster than Viterbi and 1 order of magnitude faster than CFDP.
It is intuitive that TAV will benefit more from a
smaller  (i.e. a larger timescale). As Figure 6(a)
shows, CFDP also benefits from the timescale artifact.
However, TAV‚Äôs gains are larger and also grow more
quickly with diminishing . This set of experiments
had N = 256 and T = 10000.

Finally, to check the effect of the state space size, we
chose  = .5 (case 1) and .25 (case 2). We chose fast
timescales to show the limitations of TAV (having already demonstrated its benefits for small ). As Figure 6(a) shows, TAV is 3x to 10x faster than CFDP
for N < 1024. However, CFDP is about 6x faster than
TAV for N = 2048. In this case, TAV performs poorly
because of its initialization as a single interval. For
fast timescales, the first few refinements in this setting
are invariably temporal and these refinements can be
computationally very expensive.
5.2

A priori temporal refinement

If T is comparable to the timescale of the slowest variable, then one or more temporal refinements at the
very outset of TAV is very likely. Performing these
refinements a priori will be beneficial if TAV actually
had to perform those refinements. The benefit would
be proportional to the cost of deciding whether to refine or not. This decision cost increases with T and N
(but does not depend on ).
Figure 6(b) shows the computation time for cases
where we initialized TAV with 20 equal segments. The
a priori refinement time was also included in the ‚ÄúPreSegmentation‚Äù time. Speedups of 2x to 6x were ob-

Computation Time (sec)

6

6

10

6

10
TAV‚àí1
CFDP‚àí1
TAV‚àí2
CFDP‚àí2
Viterbi

4

10

10

4

4

10

2

10

2

10

2

10

0

10

0

10

0

10
3

10

4

10

TAV‚àí1
CFDP‚àí1
TAV‚àí2
CFDP‚àí2
Viterbi

TAV
CFDP
Viterbi

5

10

10
‚àí3

10

‚àí2

‚àí1

10

Time Horizon

64

10

256

1024

State space size

Œµ

Computation Time (sec)

(a)
4

4

10

10
TAV
Pre‚àísegmentation
TAV‚àíViterbi

3

10

TAV
Pre‚àísegmentation
TAV‚àíViterbi

2

TAV
Pre‚àísegmentation
TAV‚àíViterbi

10

3

10

2

10

1

10

1

10

2

10

0

10

0

3

10

4

10

5

10

10 ‚àí3
10

Time Horizon

‚àí2

‚àí1

10

10

64

Œµ

256

1024

State space size

(b)
Figure 6: Simulation results: (a) The computation time of Viterbi, CFDP and TAV with varying T (left),  (middle)
and N (right). (b) The computation time of TAV and its two extensions‚Äîpre-segmentation and using the Viterbi
heuristic‚Äîwith varying T (left),  (middle) and N (right).

tained for varying values of N and T . When only
 was varied, the benefit was approximately constant
(between 3 to 6 seconds of computation time). This resulted in effective speedups only for the smaller values
of , which had small computation times.
5.3

Impact of heuristics

As discussed in Section 4, there is a trade-off in computing heuristics between accuracy and computation
time. Figure 6(b) compares the effect of using the
Viterbi heuristic instead of the cheap heuristic described previously. With increasing T , there was a
small improvement in computation time, although the
speedup was never greater than 2x. The two computation times were virtually the same with increasing .
For large state spaces, the Viterbi heuristic produced
more than 5x speedup (which made it comparable to
CFDP).
The main reason for the lack of improvement (in computation time) is the randomness of the data generation process. The Viterbi heuristic can significantly
outperform the cheap heuristic only if the most likely
state sequences according to the transition model receive very poor support from the observations. In that
case, the cheap heuristic will provide very inaccurate
bounds and mislead the search. In randomly generated models however, the two heuristics demonstrate

comparable performance.

6

Hierarchy induction

Till this point, we have assumed that the abstraction
hierarchy will be an input to the algorithm. However, in many cases, we might have to construct our
own hierarchy. Spectral clustering (Ng et al., 2001) is
one technique which we have used in our experiments
to successfully induce hierarchies. If the underlying
structure is a DBN of binary variables with timescale
separation between each variable (as discussed in Section 5), there will be a gap in the eigenvalue spectrum
after the two largest values. The first two eigenvectors
will be analogous to indicator functions for branching
on the slowest variable. We can then apply the method
recursively within each cluster. The main drawback is
the O(N 3 ) computational cost. It can be argued that
this is an offline and one-time cost‚Äînonetheless it is
quite expensive. It should be noted that spatial hierarchy induction is also a hard problem.
Let 28 denote a 8-variable DBN where each variable is
binary. The slowest variables are placed at the left‚Äî
hence 42 24 denotes a DBN whose two slowest variables
are 4-valued. As we change the underlying model from
28 to 44 to 162 , is there a particular abstraction hierarchy which performs well for all the models?

Computation Time (sec)

8

4

2 data model

4

10

TAV‚àí1
CFDP‚àí1
TAV‚àí2
CFDP‚àí2

3

10

2

4

TAV‚àí1
CFDP‚àí1
TAV‚àí2
CFDP‚àí2

3

10

1

1

16^2

3

10

10

1

10
4^4

TAV‚àí1
CFDP‚àí1
TAV‚àí2
CFDP‚àí2

2

10

10

16 data model

10

2

10

2^8

2

44 data model

10

10

2^8

4^4

16^2

2^8

4^4

16^2

Tree Structure

Figure 7: Effect of abstraction hierarchy: For different underlying models (28 , 44 and 162 ), deep hierarchies outperform
shallow hierarchies. Cases 1 and 2 have  = 0.1 and .05 respectively

For the experiments, we generated 3 different data sets
using the following DBNs‚Äî28 (left), 44 (middle) and
162 (right)‚Äîwith N = 256. On each data set, we used
the following abstraction hierarchies (28 ; 24 42 ; 42 24 ;
44 ; 41 82 ; 82 41 ; 162 ). The results in Figure 7 show the
computation time for TAV and CFDP using different
abstraction hierarchies (deepest on the left to shallowest on the right) for two different values of . Both TAV
and CFDP perform better with deeper hierarchies, although the improvement is much more pronounced for
TAV. The trend across all 3 underlying data models
indicates that we could always induce a deep hierarchy. The benefit of lightweight local searches in a deep
hierarchy seems to outweigh the cost of the necessary
additional refinements.

7

Conclusion

We have presented a temporally abstracted Viterbi algorithm, that can reason about sets of trajectories and
uses A* search to provably reach the correct solution.
Direct links provide a way to reason about trajectories within a set of states‚Äîsomething that previous
DP algorithms did not do. For systems with widely
varying timescales, TAV can outperform CFDP handsomely. Our experiments confirm the intuition‚Äîthe
greater the timescale separation, the more the computational benefit.
Another smart feature of our algorithm is that it can
exploit multiple timescales present in a system by
adaptive spatial and temporal refinements. TAV‚Äôs limitations arise when the system has frequent state transitions and in such cases, it is better to fall back on the
conventional Viterbi algorithm (CFDP is often slower
as well in such cases). It might be possible to design
an algorithm that uses temporal abstraction and can
also switch to conventional Viterbi when the heuristic
scores of direct links are low.
Acknowledgements
We would like to acknowledge NSF for their support
(grant no. IIS-0904672). We would also like to thank

Jason Wolfe, Aastha Jain and the anonymous reviewers for their comments and suggestions.

References
Chatterjee, S. and Russell, S. (2010). Why are DBNs
sparse?. Journal of Machine Learning Research Proceedings Track, 9, 81‚Äì88.
Felzenszwalb, P. F. and McAllester, D. (2007). The generalized A* architecture. J. Artif. Int. Res., 29, 153‚Äì
190.
Forney, G.D., J. (1973). The Viterbi algorithm. Proceedings
of the IEEE, 61 (3), 268 ‚Äì 278.
Holte, R. C., Perez, M. B., Zimmer, R. M., and MacDonald, A. J. (1996). Hierarchical A*: Searching
Abstraction Hierarchies Efficiently. In AAAI/IAAI,
Vol. 1, pp. 530‚Äì535.
Klein, D. and Manning, C. D. (2003). A* Parsing: Fast
Exact Viterbi Parse Selection. In HLT-NAACL.
Lytynoja, A. and Milinkovitch, M. C. (2003). A hidden
markov model for progressive multiple alignment.
Bioinformatics, 19 (12), 1505‚Äì1513.
Ng, A. Y., Jordan, M. I., and Weiss, Y. (2001). On spectral
clustering: Analysis and an algorithm. In NIPS, pp.
849‚Äì856.
Pavliotis, G. A. and Stuart, A. M. (2007). Multiscale methods: Averaging and homogenization. Springer.
Rabiner, L. (1989). A tutorial on hidden Markov models and selected applications in speech recognition.
Proceedings of the IEEE, 77 (2), 257 ‚Äì286.
Raphael, C. (2001). Coarse-to-Fine Dynamic Programming. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 23, 1379‚Äì1390.
Sutton, R. S., Precup, D., and Singh, S. P. (1999). Between
MDPs and Semi-MDPs: A Framework for Temporal
Abstraction in Reinforcement Learning. Artif. Intell., 112 (1-2), 181‚Äì211.
Viterbi, A. (1967). Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm.
Information Theory, IEEE Transactions on, 13 (2),
260 ‚Äì 269.

