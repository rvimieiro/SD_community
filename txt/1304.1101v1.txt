162

I
I
I

Approxhnations in Bayesian Belief Universes for
Knowledge-Based Syste1ns

I

Frank Jensen

Institute of Electronic Systems

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

& Stig !(jeer Andersen

Aalborg University
Fr. Bajersvej 71 DI<-9220 Aalborg 01 Denmark

Abstract

When expert systems bas ed on causal pr obabilistic
networks (CPNs) re ach a cer tai n size and complex­
ity, the "combinatori al explos ion monster" tends to
be present. We propose an approximation scheme
that identifies rarely occurring cases and excludes
these from being processed as ordinary cases in
a CPN-based expert system. Depending on the
topology and the probability distributions of the
CPN, the numbers (representing probabilities of
state combinations) in the underlying numerical rep­
resentation can become very small. Annihilating
these numbers and utilizing the resulting sparseness
through data structuring techniques often results in
several orders of mag nitude of improvement in the
consumption of comp uter resources. Bounds on the
errors introduced into a CPN-based expert system
through approximations are established. Finally, re­
ports on empirical studies of appl yi ng the approxi­
mation scheme to a real-world CPN are given.
Keywords: Approximative reasoning, belief net­
work, causal probabilistic network, expert system,
knowledge-based system, influence diagram, junc­
tion tree, probability propagation, reasoning under
uncertainty.
1

Introduction

Expert systems, using causal probabilistic networks
(CPNs)1 for knowledge representation, are reaching
the state where it is feasible to handle domains mod­
eled by large-scale networks (e.g., MUNIN [Andreas­
sen et al., 1987; O lesen et al., 1989]). When building
such large networks, it is (for reasons of practicality)
often necessary to introduce approximations besides
those inherent in the process of modeling a domain.
Two main approaches have been i�westigated: fo­
cusing on the development of an approximative al­
gorithm for propagation of information (e.g., [Hen­
rion, 1989]), and focusing on approximations in the
1 Synonyms:

belief

networks, causal networks, and

probabilistic influence diagrams.

underlying network representation and then using
exact inference algorithm.
The objective of this paper is to p res ent. a.n ap­
proximation scheme that takes t.he latter approach.
T he scheme is tailored to the Bayesian belief uni­
verse approach [Jensen et a/., 19t\9] as used in HUGIN
[ Andersen et al., 1989]. The met hod operates by ap­
proximations in the quantitative part. of t he underly­
ing representation, whereas the qualit.at.ive structure
remains unchanged. Within thi� framewor k , we can
assess the accuracy of the appr o xim ate d probabili­
ties, which is not possible with heuristic methods.
Application of the method ofte n results in a. sub­
stantial decrease in the usage of computer resources;
the amount of decrease depends on domain charac­
teristics, such as network topology and prob ability
distributions.
It is known that, in general, probabilistic infer­
ence in CPNs is NP-hard [Cooper, 1987], and ex­
act calculations will eventually become intractable.
This fact emphasizes the importance of approxima­
tive methods.
A domain model in the causal probabilistic net­
work approach consists of a graph with nodes repre­
senting the domain variables and the (directed) ar cs
representing the causal relatious between the do­
main variables. Conditional probabilities are used to
describe the dependency of domain variables given
their immediate predecessors (parents). Different
inference methods have been developed to propa­
gate information in such a network: If the topology
is simple (singly connected) [Pearl, 1986], propaga­
tion can be done directly in tht> CPN; otherwise, a
secondary structure for topologies, including nondi­
rected loops [Lauritzen and Spiegelhalter, 1988;
Jensen et al., 1989; Shafer and Shenoy, 1988], can
be used. Alternatively, for the btter kind of topolo­
gies, the inference could also take place in a set of
conditioned networks [Suermondt aud Cooper, 1988]
or through manipulation of the uetwork with an arc­
reversing technique [Shachter, 1988].
The method of Bayesian belief universes splits the
inference task into two phases: a compilation phase
and a run-time phase. The proposed approximaan

163

tion scheme adds another phase to this task: The
approximation and compression phase. The phases
are thus
Based on the CPN
• The compil ation phase:
domain model, a secondary structure is con­
structed-a so-called junction tree of belief uni­
verses.
•

The approximation and compression phase:

Small numbers, representing the probabilities
of very rare cases, are annihilated (set to zero),
thereby effectively eliminating these cases from
the domain model. Through use of data struc­
turing techniques for sparse tables, the under­
lying numerical tables (the belief tables) of the
junction tree are compressed.
• The run-time phase: The actual inference takes
place in the junction tree, using the modified
belief tables.
In Section 2, we review the basic belief universe
concepts essential for the proposed approximation
scheme. Section 3 describes how to perform the
approximation and establishes some worst-case er­
ror bounds on probabilities obtained from the ap­
proximated junction tree. Finally, Section 4 reports
empirical results we obtained by applying the pro­
posed approximation scheme to a real-world CPN­
namely, one of the networks of the MUNIN knowledge
base.
2

Belief Universes

This section reviews some of the basi c concepts of
the belief universe approach.
The domain represented by the CPN is divided
into a set of subdomains called belief universes. A
belief universe U consists of two parts: a set o f
nodes2 and a belief table, which contains an assess­
ment of the joint probabilities for the state space
of U (i.e., the Cartesian product of the state sets for
the nodes of U).
The construction of a system of belief universes,
equivalent to the original CPN domain model, con­
sists of the following steps:
• Form the moral graph: For each node in the
network, add links between all of its parents
that are not already li nked. Drop the directions.
• Triangulate3 the moral graph: Add links to the
moral graph until a triangulated graph is ob­
tained.
• Form the system of belief universes: The node
sets are the cliques4 of the triangulated graph.
2We sha.ll use U to denote both the belief universe
itself and its set of nodes.

3 A graph is triangulated if every cycle of length
greater than three has a chord.

4 A clique is a maximal set of nodes, a.ll of which are
pairwise linked.

The initial belief tables are calculated as ap­
propriate products of the conditional probabil­
ity tables [Lauritzen and Spiegelhalter, 1988;
.Jensen et al., 1989].
•

Organize the system as a junction tree: Links

between belief universes are introduced, such
t.bat a tree with the following property results:
For each pair (U, V) of belief universes, each
belief universe on the unique path between U
and V contains the nodes U n V. As shown
in [.Jensen, 1988], a junction tree can be con­
structed by a maximal spanning-tree algorithm.
All steps except the second are deterministic: There
is only one moral graph, and the set of cliques of a
triangulated graph is unique. There may be several
junction trees, but the differences among them are
minor (the major cost of a junction tree is the repre­
sentation of t.he belief tables for the belief universes).
The second step is important: A good triangulation
can save substantial space and time [Kjrerulff, 1990].
Let U he a belief universe with belief table B, and
let S C U. We can obtain the joint probabilities
for S from B by summing up all beliefs in B for S.
This operation is called marginalization. In partic­
ular, the belief in a single node can be obtained by
marginalization of the belief table of any belief uni­
verse containing it.
Let U be a belief universe, and let V � U. A
5
finding on 1/ is a subset of the state space of V.
The finding is entered into U6 through annihilation
of the elements in the belief table of U corresponding
to state combinations not in V.
A set of one or more findings is called a case.
A junction tree is said to be consistent if marginal­
ization of two distinct belief universes U and U' with
respect to some set of nodes V (contained in both U
and U') yield "identical" (i.e., proportional) results.
This property is (re)established through the global
propagation operation. This operation refers to a
local propagation method for transmitting evidence
between neighbors in a junction tree.
Absorption is the local propagation method: If
we have entered evidence into a belief universe V,
then an adjacent belief universe U absorbs from V
through the following steps:
1. Calculate the belief table for U n V by marginal­
ization of the belief table of U.
2. Calculate the belief table for the same intersec­
tion by marginalization of the belief table of V.
3. Multiply the belief table of U by the ratio of the
table achieved by Step 1 and the table achieved
by Step 2.
5Typically, a finding is a statement that a node is
known to be in a particular state.
6We shall also use the phrase "evidence is entered
into U."

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

164

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

When absorbing from several neighbors simultane­
ously, these steps must proceed in "parallel" (imply­
ing use of the same version of the belief table of U
in Step 1).
Global propagation is described in terms of
two operations: Col/ectEvidence and Distribute­
Evidence. CollectEv·idence is used when evidence
from the entire system must be propagated to a sin­
gle belief universe U: U asks neighbors to Collect­
Evidence; when they are done, U absorbs from them.
DistributeEvidence is used when evidence from a sin­
gle belief universe U must propagate to the entire
system: U asks each neighbor to absorb from U and
then DistributcEvidence to its other neighbors.
A global propagation operation consists of Collect­
Evidence operation followed by a DistributeEvidence
operation initiated from an arbitrary belief universe.
CollectEvidence has an important property. As­
sume that we have a consistent and normalized junc­
tion tree, and that. we enter evidence into some of
the belief universes of the junction tree. If we in­
voke CollectEvidence from some belief universe U,
then the normalizing constant for the belief table
of U, after CollectEvidence has terminated, is equal
to the (prior) probability of the evidence.
3

The Approximation Scheme

As described in the previous section, the numbers
in the belief tables of the belief universes repre­
sent probabilities in joint probability distributions.
One might expect that excluding the smallest num­
bers (representing rare state combinations) will lead
to substantial improvements in the requirements of
computer resources. In this section, we shall inves­
tigate some properties of such a scheme.
Assuming we have a consistent junction tree, an
approximation is performed in the following way:
1. For each belief universe in the junction tree, we
select some elements of its belief table and an­
nihilate those; the rest are left unchanged.
2. The junction tree is made consistent again by a
global propagation.
3. (Optional] The belief tables of the belief uni­
verses are compressed in order to take advan­
tage of the introduced zeros. (This step will
not be described here; see [Jensen and Ander­
sen, 1990] for details.)
How Do We Select the Numbers to Be
Annihilated?

As previously mentioned, we are interested in the
small numbers. A simple way to do tl,e selection is to
use a threshold value to separate the numbers to be
annihilated from the numbers to be kept. However,
we cannot choose a global threshold value, as the
size of tables and their distribution of numbers may
vary substantially. So instead we shall use a local
threshold value for each table.

We observe that, annihilating an element of a be­
lief table, corresponds to entering a finding that says
that the state combinations, corresponding to this
element, are "impossible" (or are considered unin­
teresting). Moreover, the sum of the annihilated el­
ements in a given belief table is t.he probability of all
the state combinations (the finding) corresponding
to those elements. This probability is a measure of
the (local) error, we commit.. We can control this
error by choosing a suitable threshold value.
Suppose we want to retain 1 s of the probability
mass of each belief table. Then, a simple method is
to compute a threshold value 6 by repeatedly halv­
ing 5 (using c: as the initial value for li) until the
sum of the elements less than li is no greater than c:;7
these elements will be annihilated (we believe that
either all or no elements with the same value in a
given table should be eliminated) . A more costly
method is to sort the elements of the table and to
repeat. annihilating the smallest. number(s) as long
as the sum of the annihilated numbers does not ex­
ceed c:.
The global errore (the total amount of probability
mass removed) is computed as t = 1- J.l, where J.L is
the normaliza.tion constant. found during the global
propagation step of the approximation algorithm.
Given an arbitrary case, we can determine if it is
one of the cases that have been completely excluded
from consideration by detecting a zero normalization
constant. The probability of such a case occurring
(assuming the assessed conditional probabilities are
correct) is e.
For each remaining case, some of the state com­
binations supporting the case may have been elimi­
nated. The accumulated probability for those state
combinations determines the error on the posterior
probabilities as shown in the following.
-

How Good Is the Approximation?

Assume that we have approximated the belief
universes and have propagated the approximations
throughout the junction tree. We now have a con­
sistent junction tree.
Let A denote the approximation performed, and
let F denote a set of findings to be entered into
the (consistent) approximated junction tree. Enter­
ing such a set of findings is a common operation
when using the junction tree (or rather the under­
lying CPN) as an expert system. After F has been
entered, and the junction tree has been made con­
sistent by propagation, we want to query the sys­
tem for probabilities of the form P(HIF), where
H is some hypothesis. 8 However, the probabil7This method is used in Hugin [Andersen et al.,

1989].

8ln a real application, the CPN might model the re­
lationships between some diseases and the associated

symptoms; F then would be the set of symptoms found,
H typically would be of the form "the patient has dis­
ease X," and

P(HIF)

would denote the probability that

I

165

ity P(H/F) is not available; instead, we get the
probability P(H/F, A) (that is, the probability for H
given the findings F and the appr oximation A).
We therefore want to find an upper bound on

jP(H/F)-P(H/F, A)j:

/P(H/F)- P(H/F, A)j
/P(H/F, A)P(A/F)
+ P(H/F, A)P(A/F)-P(HJF, A)j
\P(H!F, A)[P(A/F)-1] + P(H\F,A)P(A\F)/
P(A/F)/P(H/F, A) - P(HJF, A) I
� P(A\F)
=

=

=

The quantity

P(A/F) can be rewritten as

P(F n A)
P(Fn A)+ P(F n A)
P(F n A)
<
P(F n A)+ P(F\A)P(A) =

4.1

e
-----

e + p.(l-

e

)

where e = P(A) and JJ = P(F\A). These quanti­
ties are known: e is the appr ox imation error found
at approximation time, and JJ is the normalization
constant found during propagation of F. Unfortu­
nately, JJ is almost always small (� e), so this upper
bound is not a good indicator of the approximation
error.
In practice, however, F is almost. always of the
form !I n . . . n fn, where fi (1 � i � n ) states that
"node Xi is in state Yi ." Thus

P(F n A) S min{ P(fl n A), ... , P(fn n A)}
We can compute these quantities for all combina­
tions of nodes and states at approximation time (the
space required to store these quantities is small).
Although this gives us a better upper bound for
the approximation error, it is, however, strictly a
worst-case bound, and we may have to rely on em­
pirical studies to determine the actual errors. In the
next section, we shall investigate this issue for a real
application.
4

electromyographic findings, this model is cap abl e of
diagnosing three local nerve lesions and one diffuse
disorder in the median nerve in the arm. The CPN
contains 57 nodes; the disease nodes each have be­
tween three and five states, and the finding nodes
have from 15 to 21 states.
The specification of the conditional prob ab ili t.y ta­
bles requires 8126 numbers, of which 67.1 pe rc en t. are
assessed as zeros; however, most of these numbers
have been generated by local models from a much
smaller set of parameters, which has been assessed
by domain experts [ Andreassen et al., 1987].
An explanation of the domain concepts, as well
as a description of the medical performance, can
be found in [ Andreassen et al., 1989; Olesen et a/.,
1989].

An Application

We shall use a network from the MUNIN knowledge
base to study the effect of the proposed approxima­
tion scheme on a real-world CPN.
The domain ofMUNIN is electromyography, a tech­
nique for diagnosing peripheral ·muscle and nerve
disorders. We have chosen a network describing dis­
orders in the median nerve. 9 On the basis of four
the patient has disease X given that he/ she exhibits the
symptoms F.
.
9
It is our impression that this network is a "typical"
network, in the sense that the benefits of approximation
are neither negligible nor excessively large

Junction Trees

Based on different triangulations of the median­
nerve CPN, we have created four junction trees,
yielding different starting points for approximation.
We have used a maximum-cardinalit.v seatch [Tar­
jan and Yannakakis, 1984] and two h�uristic search
strategies that minimize the clique cardinality (the
min-size heuristic) and the size of the state space of
the nodes in the cliques (the min-weight heuristic),
respectively; see [ Kjrerulff, 1990] for details.
Triangulation Method
Clique
Size
14
13
10
9
8
7
6
5

MaxCard 1

MaxCard 2

MinSize

lVIin-

Weight

Number of Cliques

I
I
I
I
I
I
I
I
I
I

1
2
1
1
4
4
2
9

6
7
4
2

3
2
5
7

3
2
4
9

I

4849

10.7

1 .6

1.6

Zeros
(Percent)

I

93

71

77

Max Statespace (106)

4.0

0.45

0.54

Total Statespace {106)

Table 1: Statistics of junction trees for the median­
nerve knowledge base generated from different tri­
angulations.
Table 1 summarizes key parameters of junction
trees, based on different triangulations. We have ob­
tained two maximal-cardinality searches using differ-

I

I
I
I
I
I

166

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

ent starting nodes. However (for obvious reasons),
we consider only the second one, referred to as "max­
card," in the following subsections. The data in Ta­
ble 1 apply to the initial consistent (i.e., after ini­
tialization) junction trees before any approximation
or compression has been done.
4.2

Effect on Resources

We shall focus on two aspects of resources: (1) the

propagation time needed to make the junction tree
consistent after a set of findings has been entered,
and (2) the storage space needed to represent the
knowledge base in a suitable compact form (see [.Jen­
sen and Andersen, 1990] for details).
The global error e, defined in Section 3, is used
to characterize the approximation; we shall use the
term total removed probability mass to refer to this
value.
U)
Q)

108

�----�

>.
:3
Q)
(.)

co
a.
(f)

/

/

/

/

/

l>
0
�--

/

/

/

0

:J
rr
Q)
a:

10 5 �---.--.-�,�.---,--.-.,-rorrl
100
10
1
Propagation Time (seconds)

Figure 2: The relation between required storage
space and propagation time for the median-nerve
knowledge base for various approximations. The line
corresponds to a linear relationship between propa­
gation time and storage space.

Min-Weight
Min-Size
Max-Card
Compression

10 6+-����
1
10
Propagation Time (seconds)

Figure 1: The effect of compression on required stor­
age space and propagation time for the median-nerve
knowledge base.
The time and space measurements reported are for
an implementation of HUGIN [Andersen et al., 1989]
in C for a Sun 3 workstation; however, we are only
interested in relative improvements, so the space and
(in particular) time units should be regarded as ar­
bitrary.
Figure 1 illustrates the effect of the initial com­
pression on required storage space and propagation
time for three different junction trees. As expected,
the gain varies according to the different ratio of ze­
ros in the junction trees (see Table 1).
Figure 2 shows the relation between propagation
time and storage space needed for the three different
triangulation methods at different approximations.
The total removed probability mass (e) varied be­
tween 0.001 and 1 percent. At each data point, the
corresponding approximated and compressed run­
time system was created, and the time and space
·

U)

107

Q)

---�
�-

Unapproximated Values

>.
:3
Q)

c.
�
(f)
Q)

106

g>

a

"0
Q)
·=
:J

105

g

a:

-a--

Min-Weight

--

Min-Size

-o--

Max-Card

104
10-4

1 o·3

10"

2

1
10"

10°

10

Total Removed Probability Mass

1

10

2

(%)

Figure 3: The space requirement as a function of
the probability mass removed for different junction
trees. The arrows indicate the storage requirements
for unapproximated but compressed junction trees

I

167

cha.racteristics were measured. \Ve observe a linear
relationship between propagation time and storage
space needed; thus, we cha racterize resource require­
ments in term of storage space only.
The resource requirements for approximatedjunc­
tion trees as a function of the total removed prob­
ability mass is the subject of Figure 3. Each data
point in this figure corresponds to a data point in
Figure 2, except. for points corresponding to e > 2
p ercen t.. The values corresponding to no approxi­

mation

for the compressed
indicated.

junction trees are also

We observe that, fore less than �0.1 percent, the
approximation is equally efficient for the three junc­
tion trees. For each junction tr ee , e = 0.25 percent
yields about one order of magn itu de in reduction of
the required space. However, for a sufficiently large
value of e, the differences between the junction trees
disapp ear.
Table 2 shows the effect. of the method applied to
the different junction trees at e = 0.1 percent.

Triangulation

MaxCard

MinSize

Method
MinWeight

Space
Initial ( Mby t.e s)
Approx. (Mbytes)

Reduction

46

8.5

7.2

0.95
0.989

0.71
0.916

0.60
0.916

Figure 4 di sp lay s the results of entering a typical
case into various approximated junction trees. The
p r obability of the case is 4.1 x 10-4•

The observed error in the beliefs caused by the
approximation is shown as a funct ion of the total
removed probability mass (e). The figure shows ob­
served errors in the beliefs of states representing ex­
act beliefs between 0.9189 and 0.0005. The worst­
case error bound (Section 3) for each approxi mation
and case also has been computed. '�'e ob se rve that
the difference between the worst-case bound and t he

worst measured absolute error is about
of ma gnit ude for e :::; 0.1 percent.

Approx. ( seconds)
Reduction

1100
9.5
0.991

213
7.1
0.967

175
6.0
0.966

Table 2: The effect of approximation and compres­
sion on junction trees generated from the median­
ne rve CPN.
4.3

Effect

on

the Quality

Whenever we commit ourselves to making an ap­
proximation, we want to know the risk that we will
make serious errors. Unfortunately, the basis on
which we calculate the theoretical worst-case error
bounds might be too coarse, and it is highly unlikely
that the worst-case situation will appear in a real ap­
plication. If we had some method that could warn us
when the situation was questionable, we might take
the risk and make approximations beyond the mag­
nitude imposed by a given worst-case error bound.
We shall use our median-nerve knowledge base, and
shall make a diagnosis on the basis of a set of find­
ings, thus showing how our theoretical estimate on
upper bounds on errors compares to practical values.

orders

I
I
I
I

.!!?
Q)
co

1 o·1

.s:
....
0
....
....
w
"'0
Q)
>
....
Q)
!I)
.0

0

I

10·2
10'

I

3

10-4

I

10·5
-a
10

10""

3
1 0"

10"

2

10" 1

10°

101

Total Removed Probability Mass (%)

Time
Initial (seconds)

three

I

Exact Beliefs:

Exact Beliefs:

-a-- 0.9189 -0-- 0.0444
Ell

0.4478

-6---

0.0153

0.2933

11!1

0.0031

--o- 0.0620

Worst-case
Error Bound:

0.0005

Figure 4: The errors observed in the beliefs for var­
ious states of a local nerve lesion given a standard
case. The probability for this case is 4.1 x 10-4•
Figure 5 shows triples of the worst-case bound
(filled square) , maximal observed error ( diamond) ,
and average observed error ( open square) for 18 dif­
ferent randomly generated cases as a function of the
case-specific normalizing constant, J..lcase· The ap­
proximation used corresponds to a decrease in re­
source requirements by a factor of four relative to
an unapproximated but compressed junction tree.
Figure 5 shows that the observed errors on com­
puted beliefs for the displayed cases are much
smaller than that predicted by the worst-case er­
ror bound derived in Section 3. This difference
shows that it is very unlikely, by picking a ran­
domly generated case with a given J..lcase, to get the

I
I
I
I
I
I
I
I
I
I

168

I
I
I
I

10 °

�

10 "

Ci5
!l) 10-2
.!:
....
0
10-3
....
....
w
-c 10-4
Q)

I

10-7

I
I
I
I
I
I
I
I
I
I
I

0

•
•

...

•

For the median-nerve knowledge base and the fo­
cus on the hypothesis of a lesion at the wrist, a de­
mand of 0.01 as the upper limit of error in a state,
would allow us set the alert threshold as low as
J.Lcase = 10-7 for e = 2 X 10-4.

•
•

0
[J

0 �
0

<>

[J [)

[)

[Ji'

c:

I

I

.

1

Q)
5
(/') 10.c

I

-

10 �

10

[J

Average

0

Maximal

•

Worst Case

-B

7
10 "

�
10

5

..

0

Q:>

[)

co

o<>
DE!

0
<>
0
[)

o<lf>

\
5
10-

10-4

10-

3

·2

1o

Normalizing Constant

Figure 5: For e = 2 x 10-4, triples of worst-case er­
ror, maximal observed error, and average observed
error in the beliefs of the states of the disorder nodes
used for the case in Figure 4 are shown for 18 differ­
ent cases.
worst-case configuration. In the present CPN, the
ratio between the worst-case bound and the max­
imal observed error is three orders of magnitude
for J.Lcase ;::: 10-6• Decreasing the normalizing con­
stant (J.Lcase) implies increasing the error in beliefs
for the specific case, as well as for the worst-case er­
ror. W hen J.Lcase approaches zero, the error in beliefs
approaches one, corresponding to excluding the case
from the domain model.
These empirical studies show, that if we have a
specific hypothesis in mind (for example the diag­
nosis of a local nerve lesion at the wrist) and a set
of test cases which provides us with a span of J.Lcase,
we can get empirical values for the actual expected
error in a specific case, given J.Lcase·
Given a specific approximation e, we would have
the following situations: If we insert a set of findings,
and the theoretical worst-case error bound are below
an accepted level, we can use the approximated junc­
tion tree. If we insert a set of findings which already
has been taken out of the domain model by "zeroing
out," the violation on the model will be recognized
by a zero normalizing constant, and we have to use
a less approximated junction tree. If we insert a set
of findings yielding an unacceptably high worst-case
error, we have to rely on empirical studies, such as
those above, to estimate the error based on J.Lcase,
and on basis of this, decide whether to fall back on
a less approximated junction tree or accept the risk
of committing an error. This approach allow us to
obtain a graceful degradation of the quality of diag­
noses as the limit of the approximation is reached.

Conclusion

We have presented a scheme for approximation in
the numerical part of a CPN-based expert. system.
Our approach eliminates the (small) numbers rep­
resenting probabilities of rare combinations of find­
ings, thereby preventing these findings from being
trea.ted as ordinary findings in the expert system.
The approximation has two effects: (1) we may gain
several orders of magnitude in improvement of re­
source usage, and (2) we may lose some accuracy
in the computed beliefs. However, we can estimate
case-specific upper bounds for the errors made on
the computed beliefs, although these bounds may be
too pessimistic, as the studies reported in Section 4
show.
If the case has been completely excluded by the
approximation process, we will detect it by fiudiug
a zero normalizing constant during propagation; if
the case is one of the common cases, we know that
the computed beliefs can be trusted to a large de­
gree. The problematic cases are the ones that have
a nonzero probability outside the "trusted range''
of probabilities (remember that the probability of
a case is equal to the normalization constant found
during propagation). We suggest that, when a prob­
lematic case occur, we should reenter the case into a
less approximated (maybe even a nonapproximated)
junction tree; however, this solution should rarely be
necessary.
It would be nice to find an upper bound on the
error of beliefs that is better (and still easily com­
putable) than is the one presented in Section 3. Cal­
culation of this bound involves the errors made on
individual findings. We might be able to do better if
we considered two or more findings simultaneously;
however, a straightforward approach would require
O(sn) space, where s is the total number of states
in the nodes, and n is the number of findings con­
sidered.
There might be a clever technique to avoid con­
sidering all these combinations of findings and at the
same time to provide a better error bound. We shall
leave this topic for future research.
6

Acknowledgements

We thank Steffen L. Lauritzen, Kristian G. Olesen,
and Finn V. Jensen for valuable comments, sugges­
tions, and inspiring discussions on the subject of this
paper.
We are grateful for the inspiring environment pro­
vided by the Medical Computer Science Group at

169

Stanford University to one of us (SKA) from Au­
gust, 1989, through June, 1990. Computer support
was partly provided by the SUMEX-AIM resource,
under NIH grant LM05208.
We also thank Lyn Dupre, Stanford University, for
the many improvements of the prose she cont.ri bu ted
to this paper.
References

[Andersen et al., 1989] S. K. Andersen, K. G. Ole­
sen, F. V. Jensen, and F. Jensen. "HUGIN-a shell
for building Bayesian belief universes for expert
systems." In Proceedings of the Eleventh Interna­
tional Joint Conference on Artificial Intelligence,
pages 1080-1085, Detroit, Michigan, August 1989.
[Andreassen et a/., 1987] S. Andreassen, .1\tl. Wold­
bye, B. Falck, and S. K. Andersen. "MUNIN-a
causal probabilistic network for interpretation of
electromyographic findings." In Proceedings of t.lte
Tenth International Joint Conference on Artificial
Intelligence, pages 366-372, Milan, Italy, August
1987.
[Andreassen et a/., 1989] S. Andreassen, F. V. Jen­
sen, S. K. Andersen, B. Falck, U. Kj rerul ff, M.
Woldbye, A. R. S¢rensen, A. Rosenfalck, and
F. Jensen. "MUNIN-an expert EMG assistant."
In Computer-Aided Electromyography and Expert
Systems, J. E. Desmedt (editor), Elsevier Science
Publishers, Amsterdam, The Netherlands, 1989.
[Cooper, 1987] G. F. Cooper. "Probabilistic in­
ference using belief networks is NP-hard." Re­
search Report KLS-87-27, Medical Computer Sci­
ence Group, Stanford University, Stanford, Cali­
fornia, 1987.
[Henrion, 1989] M. Henrion. "Towards efficient in­
ference in multiply connected belief networks."
In Influence Diagrams, Belief Nets and Decision
Analysis, R. M. Oliver and J. Q. Smith (editors),
John Wiley & Sons, Chichester, 1989.
[Jensen, 1988] F. V. Jensen. "Junction trees and
decomposable hypergraphs." Judex Research Re­
port, Judex Datasystemer A/S, Aalborg, Den­
mark, 1988.
[Jensen and Andersen, 1990] F. Jensen and S. K.
Andersen. "Compact and efficient representations
of belief tables in HUGIN." Research Report, In­
stitute of Electronic Systems, Aalborg University,
Aalborg, Denmark. Manuscript in preparation.
[Jensen et al., 1988] F. V. Jensen, K. G. Olesen,
and S. K. Andersen. "An algebra of Bayesian be­
lief universes for knowledge based systems." To
appear in Networks.
[Jensen et al., 1989] F. V. Jensen, S. L. Lauritzen,
and K. G. Olesen. "Bayesian updating in causal
probabilistic networks by local computations." To
appear in Computational Statistics Quarterly.

[Kjrerulff, 1990) U. Kjrerulff. "Triangulation of
graphs - algorithms giving small total state
space'' R esearch Report R-90-09, Institute of
Electronic Systems, Aalborg University, Aalborg,
De nm ark, 1990.
[Lauritzen and Spiegelhalter, 1988] S. L. Laurit­
zen and D. J. Spiegelhalter. "Local computa­
tions with probabilities on graphical structures
and their application to expert systems." Journal
of the Royal Statistical Society, Series B (Method­
ological), 50(2):157-224, 1988.
[Olesen et al., 1989] K. G. Olesen, U. Kjrerulff, F.
Jensen, F. V. Jensen, B. Falck, S. Andreassen, and
S. K. Andersen. "A MUNIN network for the me­
dian nerve-a. case study on loops." Applied Ar­
tificial Intelligence, Spec ial Issue on Causal Mod­
eling, 3(2-3):385-403, 1989.
[Pearl, 1 986] J. Pearl. "Fusion, propagation, and
structuring in belief networks." Artificial Intelli­
gence, 29(3):241-288, 1986.
[Shachter, 1986] R. D. Shachter. "Evaluating in­
fluence diagrams." Operations Research, 34(6):
871-882, 1986.
[Shachter, 1988] R. D. Shachter. "Prob�bilistic in­
ference and influence diagrams." Operations Re­
search, 36(4):589-605, 1988.
[Shafer and Shenoy, 1988] G. Shafer and P.
Shenoy. "Bayesian and belief-function propaga­
tion." Working Paper 121, School of Business,
University of Kansas, Lawrence, Kansas, 1988.
[Suermondt and Cooper, 1988] H. J. Suermondt
and G. F. Cooper. "Updating probabilities in
multiply connected networks." In Proceedings of
the Fourth Workshop on Uncertainty in Artificial
Intelligence, pages 335-343, Minneapolis, August
1988.
[Tarjan and Yannakakis, 1984] R. E. Tarjan and
M. Yannakakis. "Simple linear-time algorithms
to test chordality of graphs, test acyclicity of hy­
pergraphs, and selectively reduce acyclic hyper­
graphs." SIAM Journal on Computing, 13(3):
566-579, 1984.
[Yannakakis, 1981] M. Yannakakis. "Computing the
minimum fill-in is NP-complete." SIAM Journal
on Algebraic and Discrete Methods, 2(1):77-79,
1981.

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

