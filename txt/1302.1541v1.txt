190

Algorithm Portfolio Design: Theory vs. Practice

Carla P. Gomes

Bart Selman

Rome Laboratory*
Rome Lab, NY 13441
gomes@ai.rl.af.mil

AT&T Bell Laboratories
Murray Hill, NJ 07974
selman@research.att.com

Abstract
Stochastic algorithms are among the best for
solving computationally hard search and rea­
soning problems. The runtime of such pro­
cedures is characterized by a random vari­
able. Different algorithms give rise to differ­
ent probability distributions. One can take
advantage of such differences by combining
several algorithms into a portfolio, and run­
ning them in parallel or interleaving them
on a single processor.
We provide a de­
tailed evaluation of the portfolio approach
on distributions of hard combinatorial search
problems. We show under what conditions
the portfolio approach can have a dramatic
computational advantage over the best tra­
ditional methods.

1

Introduction

Randomized algorithms are among the best current
algorithms for solving computationally hard problem.
Most local search methods for solving combinatorial
optimization problems have a stochastic component,
both to generate an initial candidate solution, as well
as to choose among good local improvements during
the search. Complete backtrack-style search methods
often also use an element of randomness in their value
and variable selection in case of ties. The runtime of
these algorithms varies per run on the same problem
instance, and therefore can be characterized by a prob­
ability distribution. The performance of algorithms
can also vary dramatically among different problem
instances. In this case, we want to consider the per­
formance profile of the algorithm over a spectrum of
problem instances.
Carla P. Gomes works for Rome Laboratory
search Associate.

as

a Re­

Given the diversity in performance profiles among
algorithms, various approaches have been developed
to combine different algorithms to take into account
the computational resource constraints and to opti­
mize the overall performance. These considerations
led to the development of anytime algorithms (Dean
and Boddy 1988), decision theoretic metareasoning
and related approaches (Horvitz and Zilberstein 1996;
Russell and Norvig 1995), and algorithm portfolio de­
sign (Huberman et al. 1997). Despite the numer­
ous results obtained in these areas, so far they have
not been exploited much by the traditional commu­
nities that study hard computational problems, such
as operations research (OR), constraint satisfaction
(CSP), theorem proving, and the experimental algo­
rithms community.
In order to bridge this gap, we study the possibility of
combining algorithms in the context of the recent re­
sults concerning the inherent complexity of computa­
tionally hard search and reasoning problems. We will
provide a rigorous empirical study of the performance
profiles of several of the state-of-the-art search meth­
ods on a distribution of hard search problems. Our
search problems are based on the so-called quasigroup
completion task, defined below. For this particular
combinatorial search problem, we can vary the compu­
tational difficulty and the amount of inherent problem
structure in a controlled manner. This enables us to
study different aspects of the algorithm performance
profiles.
Our studies reveal that in many cases the performance
of a single algorithm dominates all others, on the prob­
lem class under consideration. This may be due to
the fact that heuristics are often highly tuned for par­
ticular problem domains. Having a single algorithm
that dominates over the whole spectrum of problem in­
stances prevents any possible payoff of combining dif­
ferent algorithms. However, we also identify several in­
teresting problem classes where no single method dom­
inates. We will show that on those problem classes,

Algorithm Portfolio Design: Theory vs. Practice

191

designing a portfolio of several algorithms gives a dra­

to the original problem of finding an arbitrary latin

matic improvement in terms of overall performance.

square.

In addition, we also show that a good strategy for de­

values is as a set of additional problem constraints to

signing a portfolio is to combine many short runs of

the basic structure of the quasigroup.

the same algorithm.

The effectiveness of such port­

folios explains the common practice of "restarts" for
stochastic procedures, where the same algorithm is run
repeatedly with different initial seeds for the random
number generator. (For related work on the effective­
ness of restarts, see e.g., Aldous and Vazirani 1994;
Ertel 1991; Selman and Kirkpatrick 1996.)

Another way to look at these pre-assigned

There is a natural formulation of the problem as a
Constraint Satisfaction Problem. We have a variable

for each of the N2 entries in the multiplication table of
the quasigroup, and we use constraints to capture the
requirement of having no repeated values in any row or
column. All variables have the same domain, namely
the set of elements Q of the quasigroup. Pre-assigned

Our results suggest that the various ideas on flexible

values are captured by fixing the value of some of the

computation can indeed play a significant role in al­

variables.

gorithm design, complementing the more traditional
methods for computationally hard search and reason­
ing problems.

Colbourn (1983) showed the quasigroup completion
problem to be NP-complete.

In previous work, we

identified a clear phase transition phenomenon for the

The paper is organized as follows.

In the next sec­

quasigroup completion problem (Gomes and Selman

tion, we introduce our benchmark problem domain:

1997).

the quasigroup completion problem. We also discuss

observe that the costs peak roughly around the same

See Figures 1 and 2.

From the figures, we

the theoretical complexity of the problem. In section

ratio (approximately 42% pre--assignment) for differ­

3, we give the performance distribution profiles for sev­

ent values of N. (Each data point is generated using

eral complete stochastic search methods on our prob­

1,000 problem instances. The pre-assigned values were

lem domain. Section 4, we design and evaluate various

randomly generated.) This phase transition with the

algorithm portfolios. In section 5, we summarize our

corresponding cost profile allows us to tune the diffi­

results and discuss future directions.

culty of our problem class by varying the percentage
of pre-assigned values.

2

A Structured Hard Search Problem

An interesting application area of latin squares is the
design of statistical experiments. The purpose of latin
squares is to eliminate the effect of certain system­

In order to study the performance profile of differ­

atic dependency among the data (Denes and Keedwell

ent search strategies, we derive generic distributions

197 4). Another interesting application is in scheduling

of hard combinatorial search problems from the do­

and timetabling. For example, latin squares are useful

main of finite algerbra. In particular, we consider the

in determining intricate schedules involving pairwise

quasigroup domain. A quasigroup is an ordered pair

meetings among the members of a group (Anderson

( Q,

·

)

,

where Q is a set and

()

on Q such that the equations

·

a

is a binary operation
·

x =

b and

y

·

a

::: b

a, b in
Q. The order N of the quasigroup is the cardinality of
the set Q. The best way to understand the structure
of a quasigroup is to consider the N by N multipli­

are uniquely solvable for every pair of elements

cation table as defined by its binary operation. The
constraints on

a

quasigroup are such that its multipli­

cation table defines a Latin square. This means that in
each row of the table, each element of the set Q occurs

1985).

The natural perturbation of this problem is

the problem of completing a schedule given a set pre­
assigned meetings.
The quasigroup domain has also been extensively used
in the area of automated theorem proving.

In this

community, the main interest in this domain has been
driven by questions regarding the existence and nonex­
istence of quasigroups with additional mathematical
properties (Fujita et al. 1993; Lam et al. 1989).

exactly once; similarly, in each column, each element
occurs exactly once (Denes and Keedwell 1974).
An incomplete

or

partial latin square

3

Computational Cost Profiles

P is a partially

filled N by N table such that no symbol occurs twice

We will now consider the computational cost of solv­

in a row or a column.

ing the completion problem for different search strate­

The Quasigroup Completion

Problem is the problem of determining whether the

gies. As our basic search procedure, we use a complete

remaining entries of the table can be filled in such a

backtrack-style search method.

way that we obtain a complete latin square, that is, a

such procedures can vary dramatically depending on

full multiplication table of a quasigroup. We view the

the way one selects the next variable to branch on (the

pre-assigned values of the latin square as a perturbation

"variable selection strategy") and in what order the

The performance of

192

Gomes and Selman

crdu- 11-+­
order 12 -+-­
order lJ. ·B··
order U ..��:.­
order lS ..-...•

lOGO

100

'·'
0.5
f.r.action. of prEo-asaigned eluents

Figure

1:

0.7

0.6

The Complexity of Quasigroup Completion

oL-�--�--�--��---L--�_j
0

0 .•

10

lS.
20
25
30
JS
-40
tl.l.Wbollr of backtra-cks for first solution

45

50

.-------.---..--.---r---,

(Log Scale)

.-·

order 12 +­
order 'LJ -+-··

order H ·B··
order 15 ·IC·"''

0.8

.

�

i

0.6

'

�
0

�

�

0.1

o L-----�----L--�--�
0

'
.
nuab.lt:r of ba.ektra.ek$ fQt :fit.:;:t soll.ltiQt\

10

0.2

Figure 3: Finding quasigroups of order 20 (no pre­
0.1

0.2

0.3
0' -4
0. 5
0. �
faction of proe-assiqned el1111ents

0. 7

0'

�

0. 9

assigned values).

1979).

Figure 2: Phase Transition for the Completion Prob­

called the Brelaz heuristics (Brelaz

lem

laz heuristic was originally introduced for graph color­

The Bre­

ing procedures. It provides one of the most powerful
possible values are assigned to a variable (the "value
selection strategy"). There is a large body of work in

graph-coloring and general CSP heuristics (Trick and
Johnson

1996).

both the CSP and OR communities exploring different

The Brelaz heuristic specifies a way for breaking ties in

search strategies.

the First-fail rule: If two variables have equally small

One of the most effective strategies is the so-called
First-Fail heuristic.1

In the First-Fail heuristic, the

next variable to branch on is the one with the small­
est remaining domain (i.e., in choosing a value for the
variable during the backtrack search, the search pro­
cedure has the fewest possible options left to explore
- leading to the smallest branching factor). We con­
sider a popular extension of the First-Fail heuristic,
1It's really a prerequisit for any reasonable bactrack­
In theorem proving and Boolean
style search method.
satisfiability, the rule corresponds to the powerful unit­
propagation heuristic.

remaining domains, the Brelaz heuristic proposes to
select the variable that shares constraints with the
largest number of the remaining unassigned variables.
A natural variation on this tie-breaking rule is what we
call the "reverse Berlaz" heuristic, in which preference
is given to the variable that shares constraints with
the smallest number of unassigned variables. Any re­
maining ties after the (reverse) Brelaz rule are resolved
randomly. One final issue left to specify in our search
procedure is the order in which the values are assigned
to a variable. In the standard Brelaz, value assignment
is done in lexicographical order

(i.e.,

systematic). In

our experiments, we consider four stragies:

193

Algorithm Portfolio Design: Theory vs. Practice

0.9
0.8
o.1

...
o.s

..

�:::=======::·=··�----·---

yrr

brel.tzs .......
brela.z:t
rbrelus
rbrel4zr

-+-··
·�··

·EJ··

0.1
0.1
0.2
0.1

. �--��--�--�
{'I

10

15
20
2S30
JS
-to
nwWer of be.c:hrac:k:s for Ur5t solution

45

1000
')-QO
numbe:r of ba-ck:tra-:ks fQr

SO

0.1

...

first

I

1500
solution

"

/

o.J

0.2

.

'

number of �cktuclcs for hrst sohotic>n

10

0 �--��--�---L--�
'
8
Hl
12
14
U.
!8
20
0
nll;JUlll!r (lf b«cku·ac:ks for first solutiQ:n

F igure 4: Find ing quasigroups of order 20 with 10%

Fi gure 5: Finding quasigroups of order 20 with 20%

pre-assigned values.

pre-assigned values.

•

Berlaz-S

- Berlaz with systematic value selec­

tion,

part of the p rofile .

•

Berlaz-R-- Berlaz with random value selection,

•

R-Berlaz-S

•

the overall profile; the bottom part gives the initial

- Reverse Berlaz with systematic

First, we note that that R-Brelaz-R dominates R­
Brelaz-S over the full profile. In other words, the cu­
mulative relative frequency curve for R-Brelaz-R lies

value selection, and

above that of R-Brelaz-S at every point along the x­

R-berlaz-R- Reverse Brelaz with random value

As we will see below, we often encounter such pat­

selecti on .

terns, where one strategy simply consistently outper­

axis. R-Berlaz-S, in turn, strictly dominates Brelaz-R.

forms strategies.
Figure 3, shows the performance profile of our four

strategies for the problem of finding a quasigroup of
order 20 (no pre-assigned values).

Each curve gives

the cumulative distribution obtained for each strat­

egy by solving the problem 10,000 times.

The cost

(horizontal axis) is measured in number of backtracks,
which is directly proportional to the total runtime of
our strategies. For exampl e, the figure shows that R­
Berlaz-R, finished roughly 80% of the 10,000 runs in 15
b ackt racks or less. The top panel of the figure shows

Unfortunately, this leaves no room
for combining strategies: one simply picks the best
strategy.

This may explain why some of the ideas

about combining algorithms has not received as much
attention in the traditional communities that deal with
hard computational problems.2
From the perspective of combining algorithms, what is
most interesting, however, is that in the initial part of
2There is still the issue of multiple runs with the same
method. We'll retum to this below.

194

Gomes and Selman

showing the inconsistency of a quasigroup comple­
tion problem. The instance in question has 43% pre­
assigned values. Here we again obeserve that Brelaz-S

br-elus ......_
brelaz::r -+-­
rbrel4Z.S ·El··
rbroel.,.-zr ·-M

0.9.0
o.u

is somewhat better at finding inconsistencies quickly
but again R-Brelaz-R dominates for most of the pro­
file. Again, the good initial performance of Brelaz-S

0.9.2

can be exploited by combining many short runs, as we

0.9

will see below.

o0.88
'0.86
0.84
0.82
0.8

brel.'!.%5 +-r-brelazr -t--·

Q.g

,_.

t.......--'---�----1.--�_j

0

500

1000
1500
.2Q{l{l
numbe:r of N-::k.tr-!!1-cks for tint �olution

3000

2500

0.7

g

!
0.7

b-tli!h.n:
brl!laH
rbrehc;s
rbrehzr

0.6

0.!
0.5
0.4

-+­
-+-·
-O·
-)1(�

0.3
0.2

0.1

0.3

nuaber ot backtrae.ks

0.3

1000

0.1
0 '-----�----�--�
10
t
'
0
n>aber of b•cktncks for first solution

1

0.15

.

.<>
�
0

Figure 6: Finding quasigroups of order 10 at the phase
transition.

1

<
.
..,

�

0.1

0.0'5

the profile (see bottom panel, Figure 3), Brelaz-S dom­
inates the R-Brelaz-R. Intuitively, Brelaz-S is better
than R-Berlaz-R at finding solutions quickly.

How­

ever, in the latter part of the cumulative distribution

ntmber of backt:rac:ks

(for more than five backtracks), R-Brelaz-R dominates
Brelaz-S. In a sense, R-Brelaz-R gets relatively better
when the search gets harder. As we will see in the next
section, we can exploit this in our algorithm portfolio

Figure 7: Showing inconsistency of quasigroups com­
pletion (order 10 with 43% preassigned values).

design.
Figure 4, shows the performance profiles for quasi­
groups with 10% pre-assigned values.

We see essen­

4

Portfolio Design

A

portfolio of algorithms is

tially the same pattern as in Figure 3, but the re­
gion where Brelaz-S dominates is relatively smaller.
When we increase the percentage of pre-assigned val­
ues (20% pre-assigned, Figure

5), we see that R-Brelaz­

a collection of different al­

gorithms and/or different copies of the same algorithm
running on different processors.3 Here we consider the

R completely dominates the other strategies over the

case of independent runs without interprocess commu­

whole problem spectrum. This pattern continues for

nication.

the higher numbers of pre-assigned values (Figure 6, at
the phase transition with roughly 40% pre-assigned).
Finally, Figure 7 gives the performance profile for

30ne can also consider the somewhat more general case
of interleaving the execution of algorithms on one or more
processors.

195

Algorithm Portfolio Design: Theory vs. Practice
Portfolio for 2 processors

" " ,-----,---,--.--,---.----T---,----,---.--.---,
2

bnl�zs.

{l

Portfolio for 2{l

0•

processors

., ,------.--,---.--.,--..--,
0

rbu-la r

1400

br�lazs,

20 r-

@'1 zr

0.<

�

1200

O.JS

]
�

lODO

900

�'

'"

��

<00

O.J

O.lS

�

1

iOO

O.i
btelazs,

1 rbrel4zr

0.15

L----L.--'--�---'--�--'---'--�---'--...__j
___
20

o.:t6

lns,

o.38

0 rbnlaz:r

ru.

{l.42

standard deviation {risk!

o.u

Figure 8: Portfolio for two processors combining Bre­

Figure

laz and R-Brelaz-R.

Brelaz and R-Brelaz-R.
Portfolio for

5

\l.t6

o.u

..t•n-dll.rd d"vi�tion

processors5

o.s

o.s�

o.s4

o.s-s

o.ss

lriskJ

10: Portfolio for twenty processors combining

within a set that is the best, both in terms of expected
br�ltJ:u,

r-br� a:z r

value and risk. This set of portfolios corresponds to the
efficient set or efficient frontier, following terminology

'·'

used in the theory of mathematical finance.

Within

this set, in order to minimize the risk, one has to dete­
riorate the expected value or, in order to improve the
expected value of the portfolio, one has to increase the
risk.
In this context, where we characterize a portfolio in
'(l

bt�laz�,

'i

terms of its mean and variance, combining different

rbrdll.zr

algorithms into a portfolio only makes sense if they
exhibit different probability profiles and none of them

2 brel<!�zs, J rbr-elatr
I L--����-�-�-�-�-�-�
0

'SO

lOti

l'i-D

JC.D

250

standar-d deviation

300.

�risk.l

J'SO

400

.t'iO

Figure 9: Portfolio for five processors combining Bre­
laz and R-Brelaz-R.

dominates the others over the whole spectrum of prob­
lem instances.

As noted earlier, algorithm

bution of algorithm

We are considering Las Vegas type algorithms, i.e.,

A domi­

nates algorithm B if the cumulative frequency distri­

A lies above the cumulative fre­

quency distribution of algorithm B for all points.4

stochastic algorithms that always return a model sat­

Let us consider a set of two algorithms, algorithm 1

isfying the constraints of the search problem or demon­

and algorithm 2. Let us associate a random variable

strate that no such model exists (Motwani and Ragha­

with each algorithm: AI -the number of backtracks

van 1995). The computational cost of the portfolio is

that algorithm 1 takes to find the first solution or to

therefore a random variable. The expected computa­

prove that a solution does not exist; A2 -the number

tional cost of the portfolio is simply the expected value

of backtracks that algorithm 2 takes to find the first

of the random variable associated with the portfolio

solution or to prove that a solution does not exist.

and its standard deviation is a measure of the "disper­
sion" of the computational cost obtained when using
the portfolio of algorithms.

In this sense, the stan­

dard deviation is a measure of the risk inherent to the
portfolio.

Let us assume that we have

N processors and that we

design a portfolio using n1 processors with algorithm
1 and n2 processors with algorithm 2. So,

N

=

nl +

n2. Let us define the random variable associ�ted with
this portfolio: X - the number of backtracks that the

The main motivation to combine different algorithms

portfolio takes to find the first solution or to prove that

into a portfolio is to improve on the performance of the

a solution does not exist.

component algorithms, mainly in terms of expected
computational cost but also in terms of the overall risk.
As we will show, some portfolios are strictly preferrable
to others, in the sense that they provide a lower risk
and also a lower expected computational cost.

How­

ever, in some cases, we cannot identify any portfolio

The probability distribution of X is a "weighted" prob­
ability distribution of the probability distributions of
algorithm 1 and algorithm 2.
4 Another

More precisely, the

criterion for combining algorithms into a port­

folio is given by the algorithm covariance.

196

Gomes and Selman

probability that X = x is given by the probability
that one processor takes exactly x backtracks and all
the other ones take x or more backtracks to find a
solution or to prove that a solution does not exist.
Let us assume that we have N processors and our port­
folio consists of N copies of algorithm 1. In this case,
P[X=x] is given by the probability that one proces­
sor take exactly x backtracks and the other N
1
take more than x backtracks, plus the probability that
two processors take exactly x backtracks and the other
-

(N-2) one takes more than x backtracks, etc., plus the
probability that all the processors take exactly x back­
tracks to find a solution or to prove that a solution does
not exist. The following expression gives the probabil­
ity function for such a portfolio.

N and n2

Given N processors, and let nl
P[X=x] is given by

{; ( � ) P[Al
N

=

x]i P[Al

>

0.

x](N-i)

To consider two algorithms, we have to generalize the
above expression, considering that X = x can occur
just within the processors that use algorithm 1, or just
within the processors that use algorithm 2 or within
both. As a result, the probability function for a port­
folio with two algorithms, is given by the following
expressiOn:
Given N processors, n1 such that 0 <= nl <= N ,
and n2
N - nl, P[X=x] is given by
=

�} P[Al
EE ( )
N

nl

( 7,;) P[A2

=

xfP[Al > x](nl-i')x

=

xf P[A2 > x](n2-i")j

The value of i11 is given by i11 = i i', and the term in
the summation is 0 whenever i11 < 0 or i11 > n2.
-

In the case of a portfolio involving two algorithms the
probability distribution of the portfolio is a summation
of a product of two expressions, each one correspond­
ing to one algorithm. In the case of a portfolio com­
prising M different algorithms, this probability func­
tion can be easily generalized, by having a summation
of a product of M expressions, each corresponding to
an algorithm.
Once we derive the probability distribution for the ran­
dom variable associated with the portfolio, the calcu­
lation of the its expected valt.�e and standard deviation
is straightforward.

4.1

Empirical results for portfolio design

We now design different portfolios based on our perfor­
mance profiles from Section 3. We focus on the case of
finding a quasigroup of order 20 with no-preassigned
values. The performance profiles are given in Figure
3. Note that this is an interesting case from the port­
folio design perspective because Brelaz-S dominates in
the initial part of the distribution, whereas R-Brelaz-R
dominates in the latter part.
Figures 8, 9, and 10 give the expected values and the
standard deviations of portfolios for 2, 5, and 20 pro­
cessors, respectively. (Results derived using the for­
mula given above.) We see that for 2 processors (Fig­
ure 8), the portfolio consisting of two copies of the
R-Brelaz-R has the best expected value and the low­
est standard deviation. This portfolio dominates the
two other 2-processor portfolios.
When we increase the number of processors, we ob­
serve an interesting shift in the optimal portfolio mix.
For example, for 5 processors, using 2 Brelaz-S gives a
better expected value at only a slight increase in the
risk (standard deviation) compared to zero Brelaz-S.
In this case, the efficient set comprises three portfo.­
lios. One with 5 R-Brelaz-R, one with 1 Brelaz-S and
4 R-Brelaz-R, and one with 2 Brelaz-S and 3 R-Brelaz­
R. The situation changes even more dramatically if
we go to yet more processors. In particular, with 20
processors (Figure 10), the best portfolio corresponds
to using all processors to run the Brelaz-S strategy
(the lowest expected value and the lowest standard
deviation). The intuitive explanantion for this is that
by running many copies of Brelaz-S, we have a good
chance that at least one of them will find a solution
quickly. This result is consistent with the common
use of "random restarts" in stochastic search methods
in practical applications. Our portfolio analysis also
gives the somewhat counter-intuitive result that, even
when given two stochastic algorithms, where neither
strictly dominates the other, running multiple copies
of a single algorithm is preferrable to a mix of algo­
rithms (Figure 8 and Figure 10).

5

Conclusions and Future Work

We have provided concrete empirical results showing
the computational advantage of a portfolio approach
for dealing with hard combinatorial search and rea­
soning problems as compared to the best more tra­
ditional single algorithm methods. Our analysis also
showed what properties of the problem instance distri­
butions lead to the largest payoff for using a portfolio
approach in practice. Finally, we saw how the use of
random restarts of a good stochastic method is often

Algorithm Portfolio Design: Theory vs. Practice

the optimal strategy. These results suggest that ideas
developed in the flexible computation community can
play a significant role in practical algorithm design.
Acknowledgments

We would like to thank Karen Alguire for developing
an exciting tool for experimenting with the quasigroup
completion problem. We also would like to thank Nort
Fowler for many useful suggestions and discussions,
and Neal Glassman for suggesting the domain of com­
binatorial design as a potential benchmark domain.
The first author is a research associate with Rome
Laboratory and is funded by the Air Force Office of
Scientific Research, under the New World Vistas Ini­
tiative (F30602-97-C-0037 and AFOSR NWV project
2304, LIRL 97RL005N25).

Horvitz, E. and Klein, A. (1995) Reasoning, metareason­
ing, and mathematical truth: studies of theorem prov­
ing under limited resources.
Proc. of the Eleventh
Conference on Uncertainty in Artificial Intelligence
(UAI-95}, August 1995.
Horvitz, E. and Z ilberstein S. (1996) ( Eds. ) Proceedings of
Flexible Computation, AAAI Fall Symposium, Cam­
bridge, MA, 1996.
Huberman, B.A., Lukose, R.M., and Hogg, T. (1997). An
economics approach to hard computational problems.
Science, 265, 51-54.
Hogg, T., Huberman, B.A., and W illiams , C.P. (Eds.)
(1996). Phase Transitions and Complexity. Artificial
Intelligence, 81 (Spec. Issue; 1996)
Kirkpatrick, S. and Selman, B. (1994) Critical Behavior
in the Satisfiability of Random Boolean Expressions.
Science, 264 (May 1994) 1297-1301.
Lam, C., Thiel, L., and Swiercz, S. (1989) Can. J. Math.,
Vol. XLI, 6, 1989, 1117-1123.

References
Aldous, D. and Vazirani, U. (1994). Proc. of the 35th
Symp. on the Found. of Comp. Sci., IEEE Press
(1994) 492-501.
Andersen, L. (1985). Completing Partial Latin Squares.
Mathematisk F ysisk e M eddelelser, 41, 1985, 23-69.
Brelaz, D. (1979). New methods to color the verices of a
graph. Comm. of the ACM (1979) 251-256.
Tayl or,
Cheeseman, Peter and Kanefsky, Bob and
William M. (1991). Where the Really Hard Problems
Are. Proce edings /JCAI-91, 1991, 163-169.
Colbourn, C. (1983). Embedding Partial Steiner Triple
Systems is NP-Complete. 1. Combin.
Theory (A)
35 (1983), 100-105.
Dean, T. and Boddy, M. (1988) An analysis of time­
dependent planning. Proc. AAAI-88, St. Paul, MI
(1988) 49-54.
Dechter, R. (1991) Constraint networks.

197

Encyclopedia of

Artificial Intelligence John Wiley, New York (1991)
276-285.
Denes, J. and Keedwell, A. (1974) Latin Squares and
their Applications. Akademiai J<iado, Budapest, and
English Un iversities Press, London, 1974.
Ertel, W. (1991) Performance analysis of competitive or­
parallel theorem proving.
University of Munchen,
Techn. report FKI-162-91, 1992.
Fujita, M., Slaney, J. , and Bennett, F. (1993).
Automatic Generation of Some Results in F inite Algebra
Proc. IJCAI, 1993.
Freuder, E. and Mackworth, A. (Eds.) . Constra int-based
reasoning. MIT Press, Cambridge, MA, USA, 1994.
Gent, I. and Walsh, T. (1996) The Satisfiability Con­
straint Gap. Artificial Intelligence, 81, 1996.
Gomes, C.P. and Selman, B. (1997) Solution space of
structured problems in the presence of perturbations.
Submitted for publication (1997).

Mitchell, D., Selman, B., and Levesque, H.J. (1989) Hard
and easy distributions of SAT problems. Proc. AAAI92, San Jose, CA (1992) 459-465.
Russell, S and Norvig P. (1995) Artificial Intelligence a
Modern Approach. Prentice Hall, Englewood Cliffs,
NJ. (1995).
Selman, B. and Kirkpatrick, S. (1996) Finite-Size Scaling
of the Computational Cost of Systematic Search. Ar­
tificial Intelligence, Vol. 81, 1996, 273-295.
Smith, B. and Dyer, M. Locating the Phase Transition in
Binary Constraint Satisfaction Problems. Artificial
Intelligence, 81, 1996.
Trick, M. and Johnson, D. (Eds.) (1996) Proc. DIMACS
Challenge on Satisfiability Testing, Graph Coloring,
and Cliques. DIMACS Series on Discr. Math., Am.
Math. Soc. Press (1996).
Williams, C.P. and Hogg, T. (1992) Using deep structure
to locate hard problems. Proc. AAAI-92, San Jose,
CA, July 1992, 472-277.
Zhang, W. and Korf, R. A Study of Complexity Transi­
tions on the Asymmetric Travelling Salesman Prob­
lem. Artificial Intelligence, 81, 1996.

