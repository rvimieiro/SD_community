232

HOOPER ET AL.

UAI 2009

Improved Mean and Variance Approximations
for Belief Net Responses via Network Doubling

Peter Hooper
Yasin Abbasi-Yadkori, Russ Greiner, Bret Hoehn
Dept of Mathematical & Statistical Sciences
Dept of Computing Science
University of Alberta
University of Alberta
Edmonton, AB T6G 2G1 Canada
Edmonton, AB T6G 2H1
hooper@stat.ualberta.ca
{abbasiya, greiner, hoehn}@cs.ualberta.ca

Abstract
A Bayesian belief network models a joint
distribution with an directed acyclic graph
representing dependencies among variables
and network parameters characterizing conditional distributions. The parameters are
viewed as random variables to quantify uncertainty about their values. Belief nets are
used to compute responses to queries; i.e.,
conditional probabilities of interest. A query
is a function of the parameters, hence a random variable. Van Allen et al. (2001, 2008)
showed how to quantify uncertainty about a
query via a delta method approximation of
its variance. We develop more accurate approximations for both query mean and variance. The key idea is to extend the query
mean approximation to a “doubled network”
involving two independent replicates. Our
method assumes complete data and can be
applied to discrete, continuous, and hybrid
networks (provided discrete variables have
only discrete parents). We analyze several
improvements, and provide empirical studies
to demonstrate their effectiveness.

1

INTRODUCTION

Consider a simple example. Suppose A represents
presence/absence of a medical condition while B and
Y are test results. Variables B and Y are conditionally independent given A, with A and B binary
and Y continuous. The conditional independence assumption is represented by the directed acyclic graph
structure in Figure 1(a). Let θa = P (A = a),
θb|a = P (B = b | A = a), and let p(y | βa , σa ) be the
conditional density of Y given A = a, assumed normal with mean βa and variance σa2 . We want to estimate the probability that condition A is present given

specified results from the two tests B and Y . Let Θ
represent all of the parameters. If Θ were known, we
would use the formula:
θa θb|a p(y | βa , σa )
.
a1 θa1 θb|a1 p(y | βa1 , σa1 )

q(Θ) = qa|b,y (Θ) = P

(1)

In the Bayesian paradigm, uncertainty about Θ is
quantified by modeling parameters as random variables. It follows that query probabilities such as (1)
are also random. A query response is usually estimated
by approximating its posterior mean. This approximation is similar to expression (1), but with θa and θb|a
replaced by their posterior means and with the normal
densities replaced by Student’s t densities.
One may want more than just a point estimate. Van
Allen et al. (2001, 2008) showed (for discrete networks)
how one can approximate the variance and posterior
distribution of a query. Their variance derivation employs the delta method; i.e., a first-order Taylor series
expansion of the function q(Θ) about the posterior
mean of Θ. They provide asymptotic theory and empirical experiments supporting this approach. They
also showed how these approximations can be used
to construct a Bayesian credible interval (error bars)
for q(Θ). Guo and Greiner (2005) applied this delta
method approximation as part of a mean squared error (i.e., squared bias + variance) measure designed
to estimate the quality of different belief net structures when seeking a best classifier. Lee et al. (2006)
provide a technique for combining independent belief
net classifiers that involves weighting their respective
mean probability values by their inverse variances, and
they show that this works well in practice.
We propose new approximations for the mean and
variance based on a simple trick. Suppose (A1 , B1 , Y1 )
and (A2 , B2 , Y2 ) are replicates of the network variables,
conditionally independent given Θ. We represent the
paired replicates as nodes in a “doubled network” with
the same structure; see Figure 1. The squared query
q(Θ)2 can be expressed as a query in this doubled net-

UAI 2009

θb1 |a1
θb1 |a2

θb2 |a1
θb2 |a2

HOOPER ET AL.
#$
θa1 θa2
A
!"
! #
!
#
!
#
!
#
!
#
θ 1 1θ 1 1
"
!
$
#
#$ b |a b |a
#$
θb1 |a1 θb1 |a2
(βa1 , σa21 )
Y
B
2
!" (βa2 , σa2 ) !" θb1 |a2 θb1 |a1
θb1 |a2 θb1 |a2

233

θa1 θa1

θb1 |a1 θb2 |a1
θb1 |a1 θb2 |a2
θb1 |a2 θb2 |a1
θb1 |a2 θb2 |a2

θb2 |a1 θb1 |a1
θb2 |a1 θb1 |a2
θb2 |a2 θb1 |a1
θb2 |a2 θb1 |a2

θa1 θa2

θb2 |a1 θb2 |a1
θb2 |a1 θb2 |a2
θb2 |a2 θb2 |a1
θb2 |a2 θb2 |a2

θa2 θa1

θa2 θa2

'(

A1 , A2
%%&
'
%
'
%
'
%
'
%
'(
'(
(βa1 , σa21 )(βa1 , σa21 ) '
&
%
(
'
(βa1 , σa21 )(βa2 , σa22 )
Y1 , Y2
B1 , B2
(βa2 , σ 22 )(βa1 , σ 21 )
%& (β 2 , σa2 )(β 2 , σa2 ) %&
2
2
a
a
a
a

Figure 1: (a) A simple Bayesian network. (b) The corresponding doubled network.
Figure 1: A simple Bayesian net.

work:
P (A1 = A2 = a | B1 = B2 = b, Y1 = Y2 = y, Θ) .
The method used to approximate the mean of q(Θ)
can be extended to the doubled network to approximate the mean of q(Θ)2 and hence to approximate
the variance. Unlike the delta method, our approach
does not rely on approximate local linearity of q(Θ).
It does involve the addition of two incomplete observations to the data set when calculating the posterior
mean of q(Θ)2 . In some situations, this addition results in under-estimation of the desired variance. This
deficiency is largely eliminated by a simple adjustment.
A similar adjustment substantially improves the usual
query mean approximation.
Section 2 reviews pertinent models and methods for
belief networks. The network doubling technique is
described in Section 3 for discrete, continuous, and hybrid networks. Proposed adjustments and numerical
results are presented in Sections 4 and 5 for discrete
networks. Corresponding work for continuous and hybrid networks is ongoing. Computational issues are
discussed in Section 6. Contributions and plans for
further work are summarized in Section 7.

2
2.1

BACKGROUND
NETWORK VARIABLES

We assume network structure is known. Let B denote
a discrete network variable taking values b ∈ DomB .
Let Y denote a continuous network variable taking values y on the real line. Vectors of variables are denoted
by boldface: A for discrete and X for continuous. Let
Θ be a random vector comprising all unknown network
parameters; i.e., Θ determines all conditional distributions of variables given their parents.
We assume that discrete variables have only discrete
parents. Suppose pa(B) = A; i.e., the parents of B are
the variables comprising the vector A. The conditional
probability that B = b given A = a is denoted
θb|a = θB=b|A=a = P {B = b | A = a, Θ}.

1

Variables associated with values will be clear from context. We employ similar abbreviations for other parameters and hyperparameters. The θb|a parameters
are often presented in conditional probability tables
(CPtables) with rows indexed by a and columns by
b; e.g., see Figure 1. Note that we use superscripts
b1 , b2 to list the distinct values in DomB . We use subscripts b1 , b2 to denote arbitrary values in DomB , often
related to replicated variables B1 , B2 .
Continuous variables can have both discrete and continuous parents. Suppose pa(Y ) = hA, Xi with X =
hX1 , . . . , Xd i. The conditional distribution of Y is

(Y | A = a, X = x, Θ) ∼ N (1, xT )β a , σa2 ; (2)
i.e., normally distributed, conditional mean related to
x by a linear regression model with coefficients depending on a. Here xT is the transpose of the ddimensional column vector x while β a is an (d + 1)dimensional column vector of regression coefficients
(the first entry is the constant term).
2.2

PRIOR AND POSTERIOR

The network parameters represented by Θ consist of
CPtable parameters θb|a , regression coefficient vectors
β a , and variances σa2 . We assume the prior distribution for Θ has the following form; e.g., see Gelman et
al. (2003).
• CPtable rows follow Dirichlet distributions:
θB|a := hθb|a , b ∈ DomB i ∼ Dir(αB|a ),
where αB|a := hαb|a , b ∈ DomB i .
• The regression coefficients and variance together
have a normal-(inverse chi-square) distribution:

(β a | σa2 ) ∼ Nd+1 µa , σa2 (νa Ψa )−1 ,
σa−2

∼ (τa2 νa )−1 χ2νa .

I.e., dropping subscripts for a moment, β conditioned on σ 2 is multivariate normal with mean

234

HOOPER ET AL.
vector µ and covariance matrix σ 2 (νΨ)−1 ; and
ντ 2 /σ 2 has a χ2ν distribution with ν > 0 (not necessarily an integer). Note that τ 2 /σ 2 has mean 1
and variance 2/ν.

• Parameters are assumed to be statistically independent except where joint distributions are specified above. In particular, we assume global independence: the parameters determining the conditional distribution of one variable given its parents
are independent of all other parameters.
The prior is conjugate: given a data set D consisting
of n independent replicates of complete tuples of network variables, the prior hyperparameter values are
updated as follows. Let nab and na be the number of
tuples in D with (A, B) = (a, b) and A = a, respectively. Let (xi , yi ) be the observations of (X, Y ) for
the na tuples with A = a. Let X a be the na × (d + 1)
matrix with rows (1, xTi ). Let y a be the column vector
with entries yi . In the five equations below, the prior
hyperparameter values appear on the right-hand side
and are identified with tildes (e.g., α̃).
αb|a = α̃b|a + nab
νa = ν̃a + na
νa Ψa = ν̃a Ψ̃a + X Ta X a
νa Ψa µa = ν̃a Ψ̃a µ̃a + X Ta y a
i
h
 2

νa τa + µTa Ψa µa = ν̃a τ̃a2 + µ̃Ta Ψ̃a µ̃a + y Ta y a
P
P
The values
a,b αb|a and
a νa are called the effective sample sizes for variables B and Y , respectively.
Our adjustments developed in Section 4 are motivated
by large m asymptotics, where m is proportional to
the effective sample size for each of the variables; i.e.,
0
αb|a = mαb|a
and νa = mνa0
0
with (αb|a
, νa0 , Ψa , µa , τa2 ) fixed.

(3)

Large m asymptotics are similar to but not the same
as large n asymptotics. As the sample size n increases,
the posterior mean E{θb|a | D} = αb|a /α·|a varies and
converges to some value. (Here and elsewhere,
the dot
P
subscript indicates summation: α·|a = b αb|a .) Under assumption (3), the posterior mean remains fixed
as m varies.
2.3

APPROXIMATING A QUERY MEAN

Consider a query involving outcomes of hypothesis
variables H given values for evidence variables E.
It is convenient to represent the query in terms of a
function w(H). E.g., suppose H = A, E = (B, Y ),
e = (b, y), and
q(Θ)

= P (A = a | B = b, Y = y, Θ)
= E{w(A) | B = b, Y = y, Θ} ,

UAI 2009

where w(A) = 1 for A = a and w(A) = 0 otherwise.
For discrete networks, query responses q(Θ) are usually estimated by q(Θ̂), where Θ̂ := E{Θ | D} is the
posterior mean of the parameter vector. This plugin estimate usually differs slightly from the posterior query mean E{q(Θ) | D}. Cooper and Herskovits
(1992, expression 19) showed that the plug-in estimate
equals E{q(Θ) | D, e}; i.e., the posterior query mean
given an augmented data set consisting of D and an
additional partial observation of the evidence variables
E = e. Cooper and Herskovits (1991) derived a formula for E{q(Θ) | D, e} that is valid for discrete, continuous, and hybrid networks. This formula provides a
useful approximation of the less tractable E{q(Θ | D}.
The plug-in estimate is a special case of this formula
for discrete networks. The formula is important for
our network doubling technique, so is reviewed here.
In the integral expression below, Z represents all variables not included in (H, E); dh and dz refer to product measures allowing both integration for continuous
variables (Lebesgue measure) and summation for discrete variables (counting measure). Some manipulation yields
E{q(Θ) | D, e} = E{w(H) | E = e, D}
= E [ E{w(H) | E = e, Θ} | D ]
(4)
RR
R
w(h) p(h, e, z | θ)p(θ | D)dθdhdz
RRR
.
=
p(h, e, z | θ)p(θ | D)dθdhdz
Now p(h, e, z | θ) factors as a product of conditional
probabilities and densities, one for each variable in
the network.
Due to global independence, the inteR
gral p(h, e, z | θ)p(θ | D)dθ factors into a product of
integrals, one for each variable. The result is a product
of probabilities and densities described in Section 2.4
below. It follows that E{q(Θ | D, e} can be calculated
in essentially the same manner as the function q(Θ),
but with two modifications.
• For discrete variables, parameters θb|a are replaced by their posterior means. If all network
variables are discrete, then we have the plug-in
estimate:
E{q(Θ) | D, e} = q(E{Θ | D}).

(5)

• For continuous variables, the normal densities are
replaced by the St1 (η, ω 2 , ν) densities described
below. Note that this is not the same as replacing
β and σ 2 parameters with their posterior means.
2.4

PREDICTIVE DISTRIBUTIONS

The predictive distribution of the network variables is
obtained by integrating out their joint conditional dis-

UAI 2009

HOOPER ET AL.

tribution given Θ with respect to the posterior distribution of Θ. Global independence allows this integration to be carried out separately for each conditional
distribution of a variable given its parents.
The predictive distribution for a discrete variable B is
πb|a := P (B = b | A = a, D) = E{θb|a | D} =

αb|a
.
α·|a

The predictive distribution for a continuous variable is
a location-scale version of the Student’s t distribution
with ν degrees of freedom. We need the multivariate
form of this distribution in Section 3, so we define it
here. Suppose
T = η + U −1/2 (Z − η),
where Z and U are independent, Z ∼ Np (η, Ω), U ∼
(1/ν)χ2ν , and Ω is a nonsingular covariance matrix.
It follows that T has the following density function
(Johnson and Kotz, 1972, page 134):

(νπ)p/2 |Ω|1/2

Γ[(ν + p)/2] / Γ(ν/2)

(ν+p)/2 .
1 + ν1 (t − η)T Ω−1 (t − η)

We refer to this as the Stp (η, Ω, ν) distribution. For
p = 1, we write St1 (η, ω 2 , ν). Note that St1 (0, 1, ν) is
Student’s t distribution.
We claim that (Y | A = a, X = x, D) ∼ St1 (η, ω 2 , ν)
with ν = νa , η = (1, xT )µa , and

ω 2 = τa2 (1, xT )(νa Ψa )−1 (1, xT )T + 1 . (6)
To see this, let us suppress subscripts for a moment.
Let Z1 ∼ N (0, 1) be independent of (β, σ). Put Z 2 :=
σ −1 (β − µ) ∼ Nm+1 0, (νΨ)−1 . We then have
(Y | a, x, D) ∼ (1, xT )β + σZ1

∼ η + (σ/τ )τ (1, xT )Z 2 + Z1 .

3

NETWORK DOUBLING

In Section 2.3 we noted that E{q(Θ) | D} is usually
approximated by the more tractable E{q(Θ) | D, e}.
Here we propose approximating Var{q(Θ) | D} by
Var{q(Θ) | D, e, e}; i.e., the posterior variance given
D and additional replicates E 1 and E 2 of the vector
of evidence variables, both having the same value e.
We develop a formula for this latter variance by imagining a doubled network; see Figure 1(b). These mean
and variance approximations can be improved by adjustments described in Section 4.
Consider two replicated tuples of network variables,
conditionally independent and identically distributed
given Θ. Use these to replace each variable in the

235

original network by a pair of variables; e.g., B is replaced by B ∗ := (B1 , B2 ) with possible values b∗ =
(b1 , b2 ) ∈ DomB ∗ = DomB × DomB . If pa(B) = A,
then pa(B ∗ ) = A∗ := (A1 , A2 ). Conditional distributions of doubled variables given parents are obtained
by multiplying probabilities or densities for single variables.
For discrete variables, we have
P (B ∗ = b∗ | A∗ = a∗ , Θ) = θb1 |a1 θb2 |a2 .
E.g., if A = A, DomA = {a1 , a2 }, and DomB =
{b1 , b2 }, then the CPtable for B ∗ is the 4 × 4 array
shown in Figure 1(b). More generally, if a CPtable
in the original network involves dr × dc parameters,
then corresponding table in the doubled network has
d2r × d2c entries. Note that CPtable rows in the doubled network are not independent (local independence
does not hold) and do not have Dirichlet distributions.
Fortunately, these properties are not needed for the
factorization described following (4).
For continuous variables, the conditional density of
Y ∗ = (Y1 , Y2 ) given (A∗ = a∗ , X ∗ = x∗ , Θ) is the
product of the densities for two normal distributions
of the form (2) with subscript i = 1, 2 on a and x.
Put H ∗ = (H 1 , H 2 ), w∗ (H ∗ ) = w(H 1 )w(H 2 ), E ∗ =
(E 1 , E 2 ), and e∗ = (e, e). Some manipulation using
conditional independence yields
q(Θ)2 = E{w∗ (H ∗ ) | E ∗ = e∗ , Θ} ,
q(Θ) = E{w(H1 ) | E ∗ = e∗ , Θ} .
We thus have
Var{q(Θ) | D, e, e}
(7)
2
2
= E{q(Θ) | D, e, e} − [E{q(Θ) | D, e, e}]
= E{w∗ (H ∗ ) | e∗ , D} − [E{w(H1 ) | e∗ , D}]2 .
The doubled network satisfies global independence assumptions, so we can follow the approach of Section
2.3 to evaluate the two expected values in (7). To
accomplish this task, we need bivariate predictive distributions for the doubled network.
For discrete variables, the calculation follows from the
means and covariances of a Dirichlet distribution. Let
δb1 b2 be the Kronecker delta function. We have
πb∗∗ |a∗

:= P {B ∗ = b∗ | A∗ = a∗ , D}
= E{θb1 |a1 θb2 |a2 | D}
=

πb1 |a1 πb2 |a2 + δa1 a2

πb1 |a1 (δb1 b2 − πb2 |a1 )
.
α·|a1 + 1

If all network variables are discrete, then we have an
identity corresponding to (5). Let Θ∗ be the vector

236

HOOPER ET AL.

of all CPtable entries in the doubled network; e.g.,
θb1 |a1 θb2 |a2 appears in row a∗ and column b∗ for the
CPtable of B ∗ . We then have
E{q ∗ (Θ∗ ) | D, e, e} = q ∗ (E{Θ∗ | D})

(8)

with the entries in E{Θ∗ | D} given by the πb∗∗ |a∗ values above. The two expected values in the variance approximation (7) are calculated by applying (8) twice:
with q ∗ (Θ∗ ) = q(Θ)2 and with q ∗ (Θ∗ ) = q(Θ).
For continuous variables, we need the density for
{(Y1 , Y2 ) | a1 , a2 , x1 , x2 , D}. There are two cases to
consider.
• If a1 6= a2 , then the parameters (β a1 , σa2 1 ) and
(β a2 , σa2 2 ) are mutually independent. Consequently, the joint distribution factors as a product
of two St1 (η, ω 2 , ν) densities; see expression (6).
• If a1 = a2 ( = a, say), then the joint distribution
is St2 (η, Ω, ν) with ν = νa , η = X 2 µa , and
o
n
Ω = τa2 X 2 (νa Ψa )−1 X T2 + I 2 ,
where X 2 is the 2 × (1 + d) matrix whose rows
are each (1, xTi ) and I 2 is the 2 × 2 identity matrix. The derivation is similar to that following
(6). Note that (β a , σa2 ) is the same for both Y1
and Y2 in this case.

4

UAI 2009

Table 1: Summary of approximations for µq and σqq .
Means
q̂1 = E{q(Θ) | D, e}
q̂2 = E{q(Θ) | D, e, e}
q̂3 = q̂1 − (q̂2 − q̂1 )
q̂4 = q̂1 − σ̂qr /µr

√
verify that the distribution of m(Q − µq , R − µr )
converges to bivariate normal by modifying the proof
of Theorem 2 in Van Allen et al. (2008). Asymptotic
normality implies that
2
σqqrr − 2σqr
− σqq σrr → 0 at rate m−5/2

T

v̂1 = g Cg ,

E{R − µr | Q} ≈ (Q − µq )

We use approximations for higher moments motivated
by large m asymptotics; i.e., a sequence of posterior
distributions of the form (3) with m → ∞. One may

2σqr σqq (1 − 2µq )
.
µq (1 − µq ) + σqq

(11)

Switching the roles of Q and R gives
σqrr ≈

(9)

For conciseness we suppress D in our expressions; i.e.,
we implicitly assume that expectations are conditioned
on D. Put Q = q(Θ) = P (H = h | E = e, Θ) and R =
P (E = e | Θ). Note that R is an unconditional query,
with hypothesis E = e and no evidence variables. Let
µq , µr , σqq , σrr , and σqr denote the means, variances,
and covariance for (Q, R). We extend this notation to
higher moments; e.g., σqqr = E{(Q − µq )2 (R − µr )}.

σqr
σqq

and hence σqqr ≈ σqqq σqr /σqq . Now σqqq = 0 for normal distributions; however, Van Allen et al. (2008) argue that query distributions are usually better approximated by beta distributions. Substituting the third
moment of a beta distribution for σqqq , we obtain
σqqr ≈

where g is the gradient vector of q(Θ) and C is the
covariance matrix of Θ, both evaluated at E{Θ | D}.
The second variance approximation v̂2 is the doubling
method introduced in Section 3. The simple adjustments (q̂3 , v̂3 ) and more complex adjustments (q̂4 , v̂4 )
are developed in this section.

(10)

while σqrr and σqqr converge to zero at rate m−2 . We
considered approximating σqqr and σqrr by zero but
found that more accurate approximations give better
results. Asymptotic bivariate normality suggests

ADJUSTMENTS

We now narrow our focus to discrete networks and
consider the four mean and variance approximations
in Table 1. The delta method approximation is

Variances
v̂1 = delta method (9)
v̂2 = Var{q(Θ) | D, e, e}
v̂3 = expression (18)
v̂4 = expression (17)

2σqr σrr (1 − 2µr )
.
µr (1 − µr ) + σrr

(12)

Before proceeding, we observe that µr and σrr can
be calculated exactly because R can be expressed as
a sum of products of independent terms. For queries
with this property, all approximations (except v̂1 ) are
exact; i.e., additional observations of evidence variables have no effect on the posterior mean or variance. E.g., given a discrete network
with structure
P
E → B → H, we have q(Θ) = b θh|b θb|e . Since parameters in each product are independent, it follows
that q̂2 = q̂1 = µq and v̂2 = σqq .
We begin with adjustments to improve q̂1 . Bayes rule
and some manipulation yields
q̂1

=

q̂2

=

E(QR)
σqr
= µq +
(13)
E(R)
µr
E(QR2 )
2µr σqr + σqrr
= µq +
.
2
E(R )
µ2r + σrr

We approximate σqqrr using (10), σqqr by (11), σqr by
(14), µq by q̂4 , and replace σqq by v̂4 . Rearranging
terms yields the identity: v̂4 =
2
(µ2r + σrr ){v̂2 + (q̂2 − q̂4 )2 } − 2σ̂qr
. (17)
µ2r + σrr + 4µr σ̂qr (1 − 2q̂4 )/{q̂4 (1 − q̂4 ) + v̂4 }

Notice that v̂4 appears in the denominator of (17). We
initially set this value to v̂2 , then iteratively solve for
v̂4 . The values converge in a few iterations.
We observe that replacing σrr by zero has negligible
effect on (17) as m → ∞. By also replacing q̂4 by q̂3
and σ̂qr /µr by q̂2 − q̂1 , we obtain a simpler identity:
v̂3 =

v̂2 + 2(q̂2 − q̂1 )2
. (18)
1 + 4(q̂2 − q̂1 )(1 − 2q̂3 )/{q̂3 (1 − q̂3 ) + v̂3 }

We again initialize by v̂2 , then iteratively solve for v̂3 .
The approximations q̂3 and v̂3 may be preferred to q̂4
and v̂4 since µr and σrr are not required.
Rates of convergence are summarized in Proposition 1
below. The proof of this result follows easily from Van
Allen et al. (2008) and the development above.
Proposition 1. Assume a discrete network satisfying
(3) and let m → ∞. The query mean µq remains constant while the variance σqq approaches zero at rate
m−1 . The mean approximations have errors q̂j − µq
approaching zero at rate m−1 for j = 1 and 2, and at
the faster rate m−3/2 for j = 3 and 4. All four variance approximations have relative errors (v̂j −σqq )/σqq
approaching zero at rate m−1 .

-0.1
-0.3

Scaled Error
q3

q4

q1

q4

0.2
-0.6

-0.2

Scaled Error

0.0

Scaled Error

q3

(b) Diamond & m = 20

0.5

(a) NB & m = 20

(15)

µ2r σqq + 2µr σqqr + σqqrr
E{(Q − µq )2 R2 }
=
. (16)
E(R2 )
µ2r + σrr

0.0

q1

In trying to improve v̂2 , we began with the idea of
replacing q̂2 with µq :

This suggests an approximation v̂2 +4(q̂2 − q̂1 )2 , which
does help to reduce the under-estimation problem;
however, a greater improvement is obtained by further
analysis of (15):

-0.5

The formula for q̂4 in Table 1 follows from (13). Now
recall that, under condition (3), µr remains fixed while
σrr → 0 as m → ∞. It follows that setting σrr = 0
in (14) will have negligible effect for large m. We thus
obtain σ̂qr ≈ (q̂2 − q̂1 )µr , leading to the simpler q̂3
approximation.

E{(Q − µq )2 | e, e} = v̂2 + (q̂2 − µq )2 .

-0.4
-0.8

(14)

-0.5

(q̂2 − q̂1 )µr (µ2r + σrr ){µr (1 − µr ) + σrr }
.
2
µ3r (1 − µr ) + µr (1 − 2µr )σrr − σrr

Scaled Error

If µr = 1, then set σ̂qr = 0. Otherwise, substituting
(12) for σqrr and solving yields σ̂qr =

237

0.1

HOOPER ET AL.

-1.0

UAI 2009

q1

q3

q4

(c) NB & m = 500

q1

q3

q4

(d) Diamond & m = 500

Figure 2: Boxplots of scaled errors m(q̂j − q̂0 ) for j ∈
{1, 3, 4}, m ∈ {20, 500}, and network structures NB
and Diamond. Each boxplot shows variation in errors
for a set of distinct queries, 22 +24 = 20 for NB and 108
for Diamond. Errors for q̂3 and q̂4 are nearly identical.
Errors for q̂1 are often much larger. Results for q̂2 are
not plotted since q̂2 − q̂0 ≈ 2(q̂1 − q̂0 ).

5

NUMERICAL RESULTS

We evaluated accuracy of approximations q̂j and v̂j using highly accurate empirical estimates of µq and σqq .
These estimates q̂0 and v̂0 were obtained by simulating k = 106 replicates of Θ from the posterior distribution, evaluating q(Θ) for each replicate, then calculating the sample mean and sample variance. Computational
costs
preclude using empirical variance estipaper/R
figures
mates/Users/peterhooper/Documents/Research/Doubling
in practice. When m is large, asymptotic normality of q(Θ) implies that the distribution of v̂0 /σqq
is approximately (1/k)χ2k with variance 2/k.p Consequently v̂0 /σqq varies over the interval 1 ± 2 2/k for
roughly 95% of samples. Since our variance approximations have relative errors of order m−1 , it follows
that k should be of order at least m2 for v̂0 to have
substantially smaller relative error. When comparing
approximate relative errors (v̂j − v̂0 )/v̂0 with k = 106 ,
variation in v̂0 has a noticeable effect for m = 500; see
Figure 3(f).
Our examples differ with respect to network structure, posterior distribution, and query. All variables
are binary. All posterior distributions satisfy BDe
constraints (e.g., see Hooper 2008), so all variables
have the same effective sample size m. Hyperparameters are thus determined by m and the poste-

238

HOOPER ET AL.
3

E = all children of H, e varies over all combinations (22 for NB-2, 24 for NB-4).

0

1

&
• Diamond network with 4 variables .
& . , all 108
distinct queries with one hypothesis variable.

-2

-1

Scaled Relative Error

2

10
5
0
-5

Scaled Relative Error

-3

-10

v1

v2

v3

v4

v1

v3

v4

0
-2
-6

-4

Scaled Relative Error

10
0
-10
-20
-30

Scaled Relative Error

v2

(b) Diamond & m = 20

2

(a) NB & m = 20

v1

v2

v3

v4

v1

v2

v3

v4

(d) Diamond & m = 100

COMPUTATIONAL ISSUES

2
0
-2
-6

-4

Scaled Relative Error

0
-10
-20

Approximations for means are compared in Figure 2
and for variances in Figure 3. The errors and relative errors are multiplied by m in these figures to facilitate comparisons across a range of effective sample sizes. Boxplots for m = 20, 100, and 500 are
shown. Plots for other values of m are similar. By
Proposition 1, relative errors (v̂j − σqq )/σqq should approach zero at rates cj /m, where cj depends implicitly on the network, E(Θ | D), and the query. This
theory is supported by Figure 3 and additional plots
(not shown) comparing the four methods for individual queries. Our results suggest that c3 ≈ c4 while c1
and c2 tend to be further from zero. Relative errors
can be interpreted in terms of variances or standard
deviations. If (v̂j − σqq )/σqq = cj /m, then we have
p
r
v̂j
cj
cj
cj
v̂j
=1+
and √
= 1+
≈1+
.
σqq
m
σqq
m
2m

6

-40

Scaled Relative Error

10

20

(c) NB & m = 100

-30

UAI 2009

v1

v2

v3

(e) NB & m = 500

v4

v1

v2

v3

v4

(f) Diamond & m = 500

Figure 3: Boxplots of relative errors m(v̂j − v̂0 )/v̂0 for
j ∈ {1, 2, 3, 4}, m ∈ {20, 100, 500}, and network structures NB and Diamond. Each boxplot shows variation
among values for a set of distinct queries, 20 for NB
and 108 for Diamond. We observe that: relative errors
tend to be larger for NB compared with Diamond; v̂3
and v̂4 tend to over-estimate σqq for NB and are more
accurate than v̂2 ; the three methods v̂2 , v̂3 ,and v̂4 have
similar accuracy for Diamond; v̂1 is less accurate than
the other methods. The four methods appear to have
paper/R figures
similar
accuracy in (f), but these plots are mislead/Users/peterhooper/Documents/Research/Doubling
ing. Many of the Diamond queries have the property
described following (12), where v̂2 = v̂3 = v̂4 = σqq .
We would therefore expect the Diamond results for
m = 500 to be similar to those for m = 100. It appears
that the variation among relative errors for m = 500
is due in large part to variation in v̂0 .
rior means E{Θ | D}. Our examples are from three
small networks, each with one vector E{Θ | D} and
m ∈ {20, 50, 100, 200, 500}:
• Two naı̈ve Bayes networks (NB-2 and NB-4 with
2 and 4 features plus the root variable); H = root,

Inference in Bayesian networks is in general an NPcomplete problem (Cooper, 1990). For instance, the
complexity of the Variable Elimination (VE) Algorithm is O(dr ), where d is an upper bound on the
number of values that a variable can take and r is
an upper bound on the size of a factor generated by
the VE Algorithm (Koller and Friedman, 2008). Network doubling uses essentially the same technique to
calculate a variance as that used to evaluate a query,
resulting in corresponding computational complexity.
The doubled CPtables are larger (squared number of
rows and columns), so the computational complexity
of VE is increased to O(d2r ). The delta method retains O(dr ) complexity (Van Allen et al., 2008), so is
typically faster in large networks; see Table 2 below.
In some cases, we can exploit the structure of the network or query to achieve a polynomial time inference
algorithm. For poly-tree Bayesian networks (i.e. networks with at most one undirected path between any
pair of nodes), there exist inference algorithms with
linear time complexity. Reduced complexity is also
available when the query can be expressed in terms of
probabilities of hypothesis and evidence nodes conditioned on their Markov blanket; i.e., the parents, the
children and the parents of the children. Once again,
we have a polynomial time inference algorithm. These
techniques translate directly to efficient algorithms for
computing all of the variance approximations in Table 1.

UAI 2009

HOOPER ET AL.

Table 2: Timing results in seconds.
Network
NB-4
Diamond
Alarm

# Queries
100,000
108,000
100

Delta
37.837
50.052
11.390

Doubling
3.969
12.660
282.342

239

Acknowledgements
We are grateful for helpful comments from the anonymous reviewers. We acknowledge support provided by
NSERC, iCORE, and the Alberta Ingenuity Centre for
Machine Learning.
References

We empirically compared timing results for the delta
and network doubling methods using queries from
three networks: 1000 from Naı̈ve Bayes (5 variables)
repeated 100 times, 108 from the Diamond network
(4 variables) repeated 1000 times, and 100 from the
Alarm network (37 variables, Beinlich et al. 1989).
The Alarm queries were randomly generated so that
queries had on average 3 hypothesis variables, 25 evidence variables, and 9 non-specified variables. The
results in Table 2 corroborate the earlier claim that
doubling is faster for simple queries and delta is faster
for complex queries.

7

CONCLUSION

We plan to extend the implementation of the network
doubling and delta methods to continuous and hybrid
networks. This should be easy for the doubling method
when all continuous variables are evidence variables.
We will then compare the accuracies of these methods.
Our main contributions are summarized as follows.
• Development of a network doubling method to
approximate query variances. This technique exploits the Cooper and Herskovits formula (5) for
approximating the query mean and is easily implemented for discrete networks. The technique
is also applicable for continuous and hybrid networks, but implementation may be less straightforward.
• Adjustments to improve accuracy, motivated by
asymptotic theory for discrete networks. This
theory also provides rates of convergence for the
approximations.
• Numerical comparisons of the network doubling
and delta methods, showing superior accuracy of
the former in simple networks.
We do not recommend that network doubling replace
delta in all applications. If the effective sample size
is large, then both approaches may provide adequate
approximations and the choice between them may depend primarily on computational complexity. If the
network is large, then delta may have an advantage.
If the effective sample size is small or the network is
not large, then doubling may be the better choice.

I. Beinlich, H. Suermondt, R. Chavez, and G. Cooper
(1989). The ALARM monitoring system: A case study
with two probabilistic inference techniques for belief
networks. In Second European Conference on Artificial
Intelligence in Medicine, 38: 247–256.
G. Cooper (1990). The computational complexity of
probabilistic inference using Bayesian belief networks.
Artificial Intelligence 42: 393–405.
G. Cooper and E. Herskovits (1991). A Bayesian
method for the induction of probabilistic networks
from data (Report SMI-9M). Medical Informatics,
Univ. of Pittsburgh. (Also available as Report KSL91-02, Stanford Univ., Medical Informatics.)
G. Cooper and E. Herskovits (1992). A Bayesian
method for the induction of probabilistic networks
from data. Machine Learning 9: 309–347.
A. Gelman, J. Carlin, H. Stern and D. Rubin (2003).
Bayesian Data Analysis, Second Edition. New York:
Chapman and Hall.
Y. Guo and R. Greiner (2005). Discriminative model
selection for belief net structures. In Proceedings of
the Twentieth National Conference on Artificial Intelligence (AAAI-05), 770-776.
P. M. Hooper (2008). Exact distribution theory for
belief net responses. Bayesian Analysis 3: 615–624.
N. L. Johnson and S. Kotz (1972). Distributions
in Statistics: Continuous Multivariate Distributions.
New York: John Wiley & Sons.
D. Koller and N. Friedman (2008). Structured Probabilistic Models, in preparation.
C. Lee, R. Greiner, and S. Wang (2006). Using queryspecific variance estimates to combine Bayesian classifiers. In Proceedings of the International Conference
on Machine Learning (ICML-06), 529-536.
T. Van Allen, R. Greiner, and P. M. Hooper (2001).
Bayesian error-bars for belief net inference. In Proceedings of the Seventeenth Conference on Uncertainty in
Artificial Intelligence (UAI-01), 522-529.
T. Van Allen, A. Singh, R. Greiner, and P. Hooper
(2008). Quantifying the uncertainty of a belief net
response: Bayesian error-bars for belief net inference.
Artificial Intelligence 172: 483–513.

