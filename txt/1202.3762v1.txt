Symbolic Dynamic Programming for Discrete and Continuous State MDPs

Scott Sanner
NICTA & the ANU
Canberra, Australia
ssanner@nicta.com.au

Karina Valdivia Delgado
University of Sao Paulo
Sao Paulo, Brazil
kvd@ime.usp.br

Leliane Nunes de Barros
University of Sao Paulo
Sao Paulo, Brazil
leliane@ime.usp.br

Abstract

yond the subset of DC-MDPs which have an optimal hyperrectangular piecewise linear value function [8, 11].

Many real-world decision-theoretic planning
problems can be naturally modeled with discrete
and continuous state Markov decision processes
(DC-MDPs). While previous work has addressed
automated decision-theoretic planning for DCMDPs, optimal solutions have only been defined
so far for limited settings, e.g., DC-MDPs having
hyper-rectangular piecewise linear value functions. In this work, we extend symbolic dynamic
programming (SDP) techniques to provide optimal solutions for a vastly expanded class of DCMDPs. To address the inherent combinatorial aspects of SDP, we introduce the XADD — a continuous variable extension of the algebraic decision diagram (ADD) — that maintains compact
representations of the exact value function. Empirically, we demonstrate an implementation of
SDP with XADDs on various DC-MDPs, showing the first optimal automated solutions to DCMDPs with linear and nonlinear piecewise partitioned value functions and showing the advantages of constraint-based pruning for XADDs.

Yet even simple DC-MDPs may require optimal value
functions that are piecewise functions with non-rectangular
boundaries; as an illustration, we consider K NAPSACK:
Example 1.1 (K NAPSACK). We have three continuous
state variables: k ∈ [0, 100] indicating knapsack weight,
and two sources of knapsack contents: xi ∈ [0, 100] for
i ∈ {1, 2}. We have two actions move i for i ∈ {1, 2} that
can move all of a resource from xi to the knapsack if the
knapsack weight remains below its capacity of 100. We get
an immediate reward for any weight added to the knapsack.
We can formalize the transition and reward for K NAPSACK
action move i (i ∈ {1, 2}) using conditional equations,
where (k, x1 , x2 ) and (k 0 , x01 , x02 ) are respectively the preand post-action state and R is immediate reward:
(
k + xi
k =
k + xi
(
k + xi
R=
k + xi
(
k + xi
x0i =
k + xi
0

≤ 100 :
> 100 :

k + xi
k

≤ 100 :
> 100 :

xi
0

≤ 100 :
> 100 :

0
xi

x0j = xj , (j 6= i)

1

Introduction

Many real-world stochastic planning problems involving
resources, time, or spatial configurations naturally use continuous variables in their state representation. For example,
in the M ARS ROVER problem [6], a rover must manage
bounded continuous resources of battery power and daylight time as it plans scientific discovery tasks for a set of
landmarks on a given day.
While problems such as the M ARS ROVER are naturally
modeled by discrete and continuous state Markov decision processes (DC-MDPs), little progress seems to have
been made in recent years in developing exact solutions
for DC-MDPs with multiple continuous state variables be-

If our objective is to maximize the long-term value V (i.e.,
the sum of rewards received over an infinite horizon of actions), then we can write the optimal value achievable from
a given state in K NAPSACK as a function of state variables:
8
>
>x1 + k > 100 ∧ x2 + k
>
>
>
x1 + k > 100 ∧ x2 + k
>
>
<x + k ≤ 100 ∧ x + k
1
2
V =
>
x
+
k
≤
100
∧
x
+k
1
2
>
>
>
>
x
+
k
≤
100
∧
x
+k
1
2
>
>
:
x1 + x2 + k ≤ 100 :

> 100 :
0
≤ 100 :
x2
> 100 :
x1
≤ 100 ∧ x2 > x1 : x2
≤ 100 ∧ x2 ≤ x1 : x1
x1 + x2

(1)

One will see that this encodes the following rules (in order): (a) if both resources are too large for the knapsack, 0
reward is obtained, (b) otherwise if only one item can fit,

work we introduce techniques for working with arbitrary piecewise symbolic functions.

the reward is for the largest item that fits, (c) otherwise if
both items can fit then reward x1 + x2 is obtained. Here
we note that the value function is piecewise linear, but it
contains decision boundaries like x1 + x2 + k ≤ 100 that
are clearly non-rectangular; rectangular boundaries are restricted to conjunctions of simple inequalities of a continuous variable and a constant (e.g., x1 ≤ 5∧x2 > 2∧k ≥ 0).

• While the case representation for the optimal K NAP SACK solution shown in (1) is sufficient in theory to
represent the optimal value functions that our DCMDP solution produces, this representation is unreasonable to maintain in practice since the number of
case partitions may grow exponentially on each receding horizon control step. For discrete factored MDPs,
algebraic decision diagrams (ADDs) [1] have been
successfully used in exact algorithms like SPUDD [9]
to maintain compact value representations. Motivated
by this work we introduce extended ADDs (XADDs)
to compactly represent general piecewise functions
and show how to perform efficient operations on them
including symbolic maximization. We also borrow
techniques from [14] for constraint-based pruning of
XADDs that can be applied when XADDs meet certain expressiveness restrictions.

What is interesting to note is that although K NAPSACK is
very simple, no previous algorithm in the DC-MDP literature has been proposed to exactly solve it due to the nature
of its non-rectangular piecewise optimal value function. Of
course our focus in this paper is not just on K NAPSACK
— researchers have spent decades finding improved solutions to this particular combinatorial optimization problem — but rather on general stochastic sequential optimization in DC-MDPs that contain structure similar to K NAP SACK , as well as highly nonlinear structure beyond K NAP SACK . Both types of problem structure are exemplified in
the M ARS ROVER problems we experiment on later.
In proposing a solution to these problems, an important
question arises: if the solution to K NAPSACK is simple and
intuitive, why is it beyond the reach of existing exact DCMDP solutions? In response, it seems that it has not been
clear what value function representation supports closedform computation of the Bellman backup (regression and
maximization operations) for general DC-MDP transition
and reward structures. These questions have been affirmatively addressed for the subset of DC-MDPs with transition functions that are mixtures of delta functions and reward functions that are hyper-rectangular piecewise linear,
which provably lead to value functions of the same structure [8, 11]. However, the literature appears to lack a solution to this problem when, for example, the reward instead
uses piecewise nonlinear functions with linear or nonlinear
boundaries, leading to value functions of similar structure.
In this paper, we propose novel ideas to workaround some
of the expressiveness limitations of previous approaches
and significantly generalize the range of DC-MDPs that can
be solved exactly. To achieve this more general solution,
this paper contributes a number of important advances:
• We propose to represent the transition function of a
DC-MDP using conditional stochastic equations; in
using this formalism, we observe that many aspects
of the proposed symbolic DC-MDP solution become
readily apparent.
• The use of conditional stochastic equations facilitates
symbolic regression of the value function via substitutions. This is precisely the motivation behind symbolic dynamic programming (SDP) [4] used to solve
MDPs with transitions and reward functions defined in
first-order logic, except that in prior SDP work, only
piecewise constant functions have been used; in this

Aided by these algorithmic and data structure advances,
we empirically demonstrate that our SDP approach with
XADDs can exactly solve a variety of DC-MDPs with general piecewise linear and nonlinear value functions for
which no previous analytical solution has been proposed.

2

Discrete and Continuous State MDPs

We first introduce discrete and continuous state Markov decision processes (DC-MDPs) and then review their finitehorizon solution via dynamic programming following [11].
2.1

Factored Representation

In a DC-MDP, states will be represented by vectors of
variables (~b, ~x) = (b1 , . . . , bn , x1 , . . . , xm ). We assume
that each state variable bi (1 ≤ i ≤ n) is boolean s.t.
bi ∈ {0, 1} and each xj (1 ≤ j ≤ m) is continuous s.t.
xj ∈ [Lj , Uj ] for Lj , Uj ∈ R; Lj ≤ Uj . We also assume a
finite set of actions A = {a1 , . . . , ap }.
A DC-MDP is defined by the following: (1) a state transition model P (~b0 , ~x0 | · · · , a), which specifies the probability of the next state (~b0 , ~x0 ) conditioned on a subset of the
previous and next state (defined below) and action a; (2) a
reward function R(~b, ~x, a), which specifies the immediate
reward obtained by taking action a in state (~b, ~x); and (3)
a discount factor γ, 0 ≤ γ ≤ 1.1 A policy π specifies
the action π(~b, ~x) to take in each state (~b, ~x). Our goal is
to find an optimal sequence of horizon-dependent policies
Π∗ = (π ∗,1 , . . . , π ∗,H ) that maximizes the expected sum
1

If time is explicitly included as one of the continuous state
variables, γ = 1 is typically used, unless discounting by horizon
(different from the state variable time) is still intended.

of discounted rewards over a horizon h ∈ H; H ≥ 0:2
"H
#
X
Π∗
h
h~
V (~x) = Eπ∗
γ · r b0 , ~x0 ,
(2)
h=0

Here r is the reward obtained at horizon h following Π∗
where we assume starting state (~b0 , ~x0 ) at h = 0.
h

DC-MDPs as defined above are naturally factored [3] in
terms of state variables (~b, ~x); as such transition structure can be exploited in the form of a dynamic Bayes net
(DBN) [7] where the individual conditional probabilities
P (b0i | · · · , a) and P (x0j | · · · , a) condition on a subset of
the variables in the current and next state. We disallow synchronic arcs (variables that condition on each other in the
same time slice) within the binary ~b and continuous variables ~x, but we allow synchronic arcs from ~b to ~x (note that
these conditions enforce the directed acyclic graph requirements of DBNs). Thus, the joint transition model can be
specified as
P (~b0 ,~x0 | · · · , a) =
n
m
Y
Y
P (b0i |~b, ~x, a)
P (x0j |~b, ~b0 , ~x, a)
i=1

(3)

j=1

where P (b0i |~b, ~x, a) may condition on a subset of ~b and ~x
and likewise P (x0j |~b, ~b0 , ~x, a) may condition on a subset of
~b, ~b0 , and ~x.
As for standard finite discrete factored MDPs, the conditional probabilities P (b0i |~b, ~x, a) for binary variables bi
(1 ≤ i ≤ n) can be represented by conditional probability tables (CPTs). For the continuous variables xj (1 ≤
j ≤ m), we represent the continuous probability functions
(CPFs) P (x0j |~b, b~0 , ~x, a) with conditional stochastic equations (CSEs). For the solution provided here, we only require two properties of these CSEs: (1) they are Markov,
meaning that they can only condition on the previous state,
and (2) they are deterministic meaning that the next state
must be uniquely determined from the previous state (i.e.,
2
x01 = x1 + x22 is deterministic whereas x02
1 = x1 is not be0
3
cause x1 = ±x1 ). Otherwise, we allow for arbitrary functions in these Markovian, conditional deterministic equations as in the following example:
"
P (x01 |~b, ~b0 , ~x, a) = δ x01 −

(
b01 ∧ x22 ≤ 1 :
¬b01 ∨ x22 > 1 :

#
exp(x21 − x22 )
x1 + x2
(4)

Here the use of the Dirac δ[·] function ensures that this is
a conditional probability function that integrates to 1 over

x01 in this case. But in more intuitive terms, one can see
that this δ[·] encodes the deterministic transition equation
x01 = . . . where . . . is the conditional portion of (4). In
this work, we require all CSEs in the transition function for
variable x0i to use the δ[·] as shown in this example.
It will be obvious that CSEs in the form of (4) are conditional equations; they are furthermore stochastic because
they can condition on boolean random variables in the same
time slice that are stochastically sampled, e.g., b01 in (4). Of
course, these CSEs are restricted in that they cannot represent general stochastic noise (e.g., Gaussian noise), but we
note that this representation effectively allows modeling of
continuous variable transitions as a mixture of δ functions,
which has been used heavily in previous exact DC-MDP
solutions [8, 11, 13]. Furthermore, we note that our representation is more general than [8, 11, 13] in that we do
not restrict the equation to be linear, but rather allow it
to specify arbitrary functions (e.g., nonlinear) as demonstrated in (4).
We allow the reward function Ra (~b, ~x) to be any arbitrary
function of the current state for each action a ∈ A, for
example:
(
x21 + x22 ≤ 1 : 1 − x21 − x22
Ra (~b, ~x) =
(5)
x21 + x22 > 1 : 0
or even
√
Ra (~b, ~x) = 10x3 x4 exp(x21 + x2 )

(6)

While our DC-MDP examples throughout the paper will
demonstrate the full expressiveness of our symbolic dynamic programming approach, we note that there are computational advantages to be had when the reward and transition case conditions and functions can be restricted, e.g.,
to polynomials. We will return to this issue later.
2.2

Solution Methods

Now we provide a continuous state generalization of value
iteration [2], which is a dynamic programming algorithm
for constructing optimal policies. It proceeds by constructing a series of h-stage-to-go value functions V h (~b, ~x). Initializing V 0 (~b, ~x) (e.g., to V 0 (~b, ~x) = 0) we define the
quality of taking action a in state (~b, ~x) and acting so as
to obtain V h (~b, ~x) thereafter as the following:
Qh+1
(~b, ~
x) = Ra (~b, ~
x) + γ·
(7)
a
!
Z
n
m
Y
Y
X
P (b0i |~b, ~
x, a)
P (x0j |~b, ~b0 , ~
x, a) V h (~b0 , ~
x0 )d~x0
~
b0

~
x0

i=1

j=1

2

H = ∞ is allowed if an optimal policy has a finitely bounded
value (guaranteed if γ < 1); for H = ∞, the optimal policy is
independent of horizon, i.e., ∀h ≥ 0, π ∗,h = π ∗,h+1 .
3
While the deterministic requirement may seem to conflict
with the label of stochastic, we note that stochasticity enters
through the conditional component, to be discussed in a moment.

Given Qha (~b, ~x) for each a ∈ A, we can proceed to define
the h + 1-stage-to-go value function as follows:
n
o
~b, ~x)
V h+1 (~b, ~x) = max Qh+1
(
(8)
a
a∈A

If the horizon H is finite, then the optimal value function is
obtained by computing V H (~b, ~x) and the optimal horizondependent policy π ∗,h at each stage h can be easily determined via π ∗,h (~b, ~x) = arg maxa Qha (~b, ~x). If the horizon H = ∞ and the optimal policy has finitely bounded
value, then value iteration can terminate at horizon h + 1 if
V h+1 = V h ; then π ∗ (~b, ~x) = arg maxa Qh+1
(~b, ~x).
a
Of course this is simply the mathematical definition. In the
discrete-only case, we can always compute this in tabular
form; however, how to compute this for DC-MDPs with
reward and transition function as previously defined is the
objective of the symbolic dynamic programming algorithm
that we define next.

3

Symbolic Dynamic Programming

As it’s name suggests, symbolic dynamic programming
(SDP) [4] is simply the process of performing dynamic programming (in this case value iteration) via symbolic manipulation. While SDP as defined in [4] was previously only
used with piecewise constant functions, we now generalize
the representation to work with general piecewise functions
needed for DC-MDPs in this paper.
Before we define our solution, however, we must formally
define our case representation and symbolic case operators.
3.1

product of the logical partitions of each case statement
and perform the corresponding operation on the resulting
paired partitions. Letting each φi and ψj denote generic
first-order formulae, we can perform the “cross-sum” ⊕ of
two (unnamed) cases in the following manner:
(
φ1 :
φ2 :

(
f1
ψ1 :
⊕
f2
ψ2 :

k

k

Here the φi are logical formulae defined over the state
(~b, ~x) that can include arbitrary logical (∧, ∨, ¬) combinations of (a) boolean variables in ~b and (b) inequalities
(≥, >, ≤, <), equalities (=), or disequalities (6=) where the
left and right operands can be any function of one or more
variables in ~x. Each φi will be disjoint from the other φj
(j 6= i); however the φi may not exhaustively cover the
state space, hence f may only be a partial function and
may be undefined for some state assignments. The fi can
be any functions of the state variables in ~x.
As concrete examples, consider the transition representation for K NAPSACK in Ex. 1.1, the optimal value function
for K NAPSACK from (1), or any of (4), (5), or (6).
Unary operations such as scalar multiplication c · f (for
some constant c ∈ R) or negation −f on case statements f
are straightforward; the unary operation is simply applied
to each fi (1 ≤ i ≤ k). Intuitively, to perform a binary operation on two case statements, we simply take the cross-

:
:
:
:

f1 + g1
f1 + g2
f2 + g1
f2 + g2

Likewise, we can perform and ⊗ by, respectively, subtracting or multiplying partition values (as opposed to
adding them) to obtain the result. Some partitions resulting from the application of the ⊕, , and ⊗ operators may
be inconsistent (infeasible); we may simply discard such
partitions as they are irrelevant to the function value.
For SDP, we’ll also need to perform maximization, restriction, and substitution on case statements. Symbolic maximization is fairly straightforward to define:

max

(
φ1 :
φ2 :

f1
,
f2

(
ψ1 :
ψ2 :

Case Representation and Operators

Throughout this paper, we will assume that all symbolic
functions can be represented in case form as follows:



φ1 f1
.
..
f = ..
.


φ f

g1
g2

8
>
>φ1 ∧ ψ1
>
<
φ1 ∧ ψ2
=
>
φ
2 ∧ ψ1
>
>
:φ ∧ ψ
2
2

8
φ1 ∧ ψ1 ∧ f1
>
>
>
>
φ1 ∧ ψ1 ∧ f1
>
>
>
>
>
φ
> 1 ∧ ψ2 ∧ f1
! >
<
g1
φ1 ∧ ψ2 ∧ f1
=
>
g2
φ
2 ∧ ψ1 ∧ f2
>
>
>
>
φ
2 ∧ ψ1 ∧ f2
>
>
>
>
φ
∧ ψ2 ∧ f2
>
2
>
:
φ2 ∧ ψ2 ∧ f2

> g1
≤ g1
> g2
≤ g2
> g1
≤ g1
> g2
≤ g2

:
:
:
:
:
:
:
:

f1
g1
f1
g2
f2
g1
f2
g2

One can verify that the resulting case statement is still
within the case language defined previously. At first glance
this may seem like a cheat and little is gained by this symbolic sleight of hand. However, simply having a case partition representation that is closed under maximization will
facilitate the closed-form regression step that we need for
SDP. Furthermore, the XADD that we introduce later will
be able to exploit the internal decision structure of this
maximization to represent it much more compactly.
The next operation of restriction is fairly simple: in this
operation, we want to restrict a function f to apply only
in cases that satisfy some formula φ, which we write as
f |φ . This can be done by simply appending φ to each case
partition as follows:
8
>
φ :
>
< 1
..
f= .
>
>
:
φk :

f1
..
.
fk

8
>
φ ∧φ:
>
< 1
..
f |φ = .
>
>
:
φk ∧ φ :

f1
..
.
fk

Clearly f |φ only applies when φ holds and is undefined
otherwise, hence f |φ is a partial function unless φ ≡ >.
The final operation that we need to define for case statements is substitution. Symbolic substitution simply takes a
set σ of variables and their substitutions, e.g., σ = {x01 =
x1 + x2 , x02 = x21 exp(x2 )} where the LHS of the = represents the substitution variable and the RHS of the = represents the expression that should be substituted in its place.

No variable occurring in any RHS expression of σ can also
occur in any LHS expression of σ. We write the substitution of a non-case function fi with σ as fi σ; as an example,
for the σ defined previously and fi = x01 + x02 then fi σ =
x1 + x2 + x21 exp(x2 ) as would be expected. We can also
substitute into case partitions φj by applying σ to its LHS
and RHS operands; as an example, if φj ≡ x01 ≤ exp(x02 )
then φj σ ≡ x1 + x2 ≤ exp(x21 exp(x2 )). Having now
defined substitution of σ for non-case functions fi and case
partitions φj we can define it for case statements in general:
8
>
φ :
>
< 1
.
f = ..
>
>
:
φk :

f1
..
.
fk

8
>
φ σ:
>
< 1
..
fσ = .
>
>
:
φk σ :

f1 σ
..
.
fk σ

One property of substitution is that if f has mutually exclusive partitions φi (1 ≤ i ≤ k) then f σ must also have
mutually exclusive partitions — this follows from the logical consequence that if φ1 ∧ φ2 |= ⊥ then φ1 σ ∧ φ2 σ |= ⊥.
3.2

Symbolic Dynamic Programming (SDP)

In the SDP solution for DC-MDPs, our objective will be to
take a DC-MDP as defined in Section 2, apply value iteration as defined in Section 2.2, and produce the final value
optimal function V h at horizon h in the form of a case statement.
For the base case of h = 0, we note that setting V 0 (~b, ~x) =
0 (or to the reward case statement, if not action dependent)
is trivially in the form of a case statement.
Next, h > 0 requires the application of SDP. Fortunately,
given our previously defined operations, SDP is straightforward and can be divided into four steps:
1. Prime the Value Function: Since V h will become the
“next state” in value iteration, we setup a substitution
σ = {b1 = b01 , . . . , bn = b0n , x1 = x01 , . . . , xm =
x0m } and obtain V 0h = V h σ.
2. Continuous Integration: Now that we have our primed
value function V 0h in case statement format defined
over next state variables R(~b0 , ~x0 ), we first evaluate the
integral marginalization ~x0 over the continuous variables in (7). Because the lower and upper integration
bounds are respectively −∞ and ∞ and we have disallowed synchronic arcs between variables in ~x0 in the
transition DBN, we can marginalize out each x0j independently, and in any order. Using variable elimination [17], when marginalizing over x0j we can factor
R
out any functions independent of x0j — that is, for x0
j
in (7), one can see that initially, the only functions
that can include x0j are V 0h and P (x0j |~b, ~b0 , ~x, a) =
δ[x0j − g(~x)]; hence, the first marginal over x0j need
only be computed over δ[x0j − g(~x)]V 0h .

What follows is one of the key novel insights of
SDP
in the context of DC-MDPs — the integration
R
0
δ[x
x)]V 0h dx0j simply triggers the substitu0
j − g(~
x
j

tion σ = {x0j = g(~x)} on V 0h , that is
Z
x0j

δ[x0j − g(~x)]V 0h dx0j = V 0h {x0j = g(~x)}. (9)

Thus we can perform the operation in (9) repeatedly
in sequence for each x0j (1 ≤ j ≤ m) for every action
a. The only additional complication is that the form
of P (x0j |~b, ~x, a) is a conditional equation, c.f. (4), and
represented generically as follows:




φ
:
f
1
1


 0
..
.. 

(10)
P (x0j |~b, ~x, a) = δ 
x
=
j
.
. 



φ : f
k

k

Hence to perform (9)R on this more general representation, we obtain that x0 P (x0j |~b, ~x, a)V 0h dx0j
j




φ1 :
.
= ..


φ :
k

V 0h {x0j = f1 }
..
.
V 0h {x0j = fk }

In effect, we can read (10) as a conditional substitution, i.e., in each of the different previous state conditions φi (1 ≤ i ≤ k), we obtain a different substitution
for x0j appearing in V 0h (i.e., σ = {x0j = fi }). Here
we note that because V 0h is already a case statement,
we simply replace the single partition φi with the multiple partitions of V {x0j = fi }|φi . This reduces the
nested case statement back down to a non-nested case
statement as in the following example:

(


ψ1 : f11
φ1 ∧ ψ1 : f11




φ1 :



ψ2 : f12
φ1 ∧ ψ2 : f12
(
=


φ2 ∧ ψ1 : f21
ψ
:
f


1
21




φ2 :
φ2 ∧ ψ2 : f22
ψ2 : f22
To perform the full continuous integration, if we initialize Q̃h+1
:= V 0h for each action a ∈ A, and rea
peat the above integrals for all x0j , updating Q̃h+1
each
a
time, then after elimination of all x0j (1 ≤ j ≤ m), we
will have the partial regression of V 0h for the continuous variables for each action a denoted by Q̃h+1
.
a
3. Discrete Marginalization: Now that we have our partial regression Q̃h+1
for each action a, we proceed to
a
h+1
derive the full backup Qh+1
from
a
P Q̃a by evaluating the discrete marginalization ~b0 in (7). Because
we previously disallowed synchronic arcs between the
variables in ~b0 in the transition DBN, we can sum out

each variable b0i (1 ≤ i ≤ n) independently. Hence,
initializing Qh+1
:= Q̃h+1
we perform the discrete
a
a
regression by applying the following iterative process
for each b0i in any order for each action a:
h
i
h+1
0~
Qh+1
:=
Q
⊗
P
(b
|
b,
~
x
,
a)
|b0i =>
a
a
i
h
i
⊕ Qh+1
⊗ P (b0i |~b, ~x, a) |b0i =⊥ . (11)
a
This requires a variant of the earlier restriction operator |v that actually sets the variable v to the given value
if present. Note that both Qh+1
and P (b0i |~b, ~x, a) can
a
be represented as case statements (discrete CPTs are
case statements), and each operation produces a case
statement. Thus, once this process is complete, we
have marginalized over all ~b0 and Qh+1
is the syma
bolic representation of the intended Q-function.
4. Maximization: Now that we have Qh+1
in case format
a
for each action a ∈ {a1 , . . . , ap }, obtaining V h+1 in
case format as defined in (8) requires sequentially applying symbolic maximization as defined previously:
h+1
h+1
V h+1 = max(Qh+1
a1 , max(. . . , max(Qap−1 , Qap )))

By induction, because V 0 is a case statement and applying SDP to V h in case statement form produces V h+1 in
case statement form, we have achieved our intended objective with SDP. On the issue of correctness, we note that
each operation above simply implements one of the dynamic programming operations in (7) or (8), so correctness
simply follows from verifying (a) that each case operation
produces the correct result and that (b) each case operation
is applied in the correct sequence as defined in (7) or (8).
On a final note, we observe that SDP holds for any symbolic case statements; we have not restricted ourselves
to rectangular piecewise functions, piecewise linear functions, or even piecewise polynomial functions. As the SDP
solution is purely symbolic, SDP applies to any DC-MDPs
using bounded symbolic function that can be written in case
format! Of course, that is the theory, next we meet practice.

4

Extended ADDs (XADDs)

In practice, it can be prohibitively expensive to maintain
a case statement representation of a value function with
explicit partitions. Motivated by the SPUDD [9] algorithm which maintains compact value function representations for finite discrete factored MDPs using algebraic decision diagrams (ADDs) [1], we extend this formalism to
handle continuous variables in a data structure we refer to
as the XADD. An example XADD for the optimal K NAP SACK value function from (1) is provided in Figure 1.
In brief we note that an XADD is like an ADD except
that (a) the decision nodes can have arbitrary inequalities,

x1 + k <= 100

x2 + k <= 100

x2 + k <= 100

0

x2 <= x1

x1 + x2 + k <= 100
x2

x1 + x2

x1 + x2 + k <= 100
x1

Figure 1: The optimal value function for K NAPSACK as a
decision diagram: the true branch is solid, the false branch
is dashed.
equalities, or disequalities (one per node) and (b) the leaf
nodes can represent arbitrary functions. The decision nodes
still have a fixed order from root to leaf and the standard
ADD operations to build a canonical ADD (R EDUCE) and
to perform a binary operation on two ADDs (A PPLY) still
applies in the case of XADDs.
While exact solutions using symbolic dynamic programming are possible in principle for arbitrary symbolic CSE
transition and reward functions, we note that it is much
more difficult to devise a canonical and compact form for
representations such as (6) in comparison to (5). Hence
while we have used general examples throughout the paper to demonstrate the expressiveness of our approach, we
will restrict XADDs to use polynomial functions only. We
note the main advantage of this for the XADD is that we
can put the leaf and decision nodes in a unique, canonical form, which allows us to minimize redundancy in the
XADD representation of a case statement.
It is fairly straightforward for XADDs to support all case
operations required for SDP. Standard operations like unary
multiplication, negation, ⊕, and ⊗ are implemented exactly
as they are for ADDs. The fact that the decision nodes have
internal structure is irrelevant, although this means that certain paths in the XADD may be inconsistent or infeasible
(due to parent decisions). To remedy this, when the XADD
has only linear decision nodes, we can use the feasibility checkers of a linear programming solver (e.g., as also
done in [14]) to prune unreachable nodes in the XADD;
later we show results demonstrating impressive reductions
in XADD size using this style of pruning.
The only two XADD operations that pose difficulty are
substitution and maximization. In principle substitution is
simple, the only caveat is that substitutions modify the decision nodes and hence decision nodes may become unordered. We can use the recursive application of ADD binary operations ⊗ and ⊕ as given in Algorithm 1 to correctly reorder the nodes in an XADD F after substitution.
A related reordering issue occurs during XADD maximization; because XADD maximization can introduce new de-

Algorithm 1: R EORDER(F)
input : F (root node for possibly unordered XADD)
output: Fr (root node for an ordered XADD)
begin
//if terminal node, return canonical terminal node
if F is terminal node then
return canonical terminal node for
polynomial of F ;
//nodes have a true & false branch and var id
if F → Fr is not in Cache then
Ftrue = R EORDER(Ftrue ) ⊗ I[Fvar ] ;
Ffalse = R EORDER(Ffalse ) ⊗ I[¬Fvar ];
Fr = Ftrue ⊕ Ffalse ;
insert F → Fr in Cache;
return Fr ;
end

cision nodes (which occurs at the leaf when two leaf functions are compared) and these decision nodes may be out
of order w.r.t. the diagram, reordering as defined in Algorithm 1 must also be applied after maximization.
On a final note, we mention that an implementation of case
statements without any attempt to merge and simplify cases
often cannot get past the first or second iteration of SDP; as
our results show next, XADDs allow SDP to perform quite
well in practice.

5

Empirical Results

We implemented two versions of our proposed SDP algorithm using XADDs — one that does not prune nodes
of the XADD and another that uses a linear programming
solver to prune unreachable nodes (for problems with linear
XADDs) — and we tested these algorithms on K NAPSACK
and two versions of the Mars Rover domain (adapted from
[6]) that we call M ARS ROVER L INEAR and M ARS ROVER
N ONLINEAR.4
5.1

Domains

In a general M ARS ROVER domain, a rover is supposed
to approach one or more target points and take images of
these points. Actions may consume time and energy. There
are also some domain constraints, e.g., some pictures can
be taken only in a certain time window and can require different levels of energy to be performed. Next we describe
the two domain variants we use.
4

While space limitations prevent a self-contained description of all domains, we note that all Java source code and
a human/machine readable file format for all domains needed
to reproduce the results in this paper can be found online at
http://code.google.com/p/xadd-inference.

M ARS ROVER L INEAR This version has two continuous variables, time t and energy e. For each target point
i (i = 1 . . . k), there is a boolean variable pi indicating
whether the rover is at point i.
There are k(k − 1) actions move i that move the rover from
point i to point j 6= i. There are another k actions take-pic i
that take a picture at point i, which are conditioned on linear expressions over the time and energy variables. The reward is also a function of time and energy, e.g., the reward
for action take-pic i is given by:
8
>
<(e > 3 + 0.0002t) ∧ (t ≥ 3600)
Rtake-pic i (e, t, pi ) =
∧ (t ≤ 50400) ∧ pi :
>
:otherwise :

110
0

which shows that to get a reward of 110, the rover must take
a picture at point i between times 3600 and 50400 with a required energy reserve that increases as the day progresses.
M ARS ROVER N ONLINEAR This version has two different continuous variables — geographic coordinates
(x, y) — and k boolean variables hi for each picture point
i indicating whether the rover has already taken a picture
of point i. There is a single move action in this domain —
it simply reduces the distance from the rover to a specific
point by 31 of the current distance; for all experiments, this
target point was set to (0, 0). The intent of this action is to
represent the fact that a rover may move progressively more
slowly as it approaches a target position in order to reach
the position with high accuracy. take-pic i actions are the
same from M ARS ROVER L INEAR domain but conditioned
by nonlinear expressions over the continuous x and y variables. The reward is also a function of x and y, e.g., the
reward for action take-pic i is given by:
8
2
2
>
<x + y < 4 ∧ hi :
2
2
Rtake-pic i (x, y, hi ) = x + y < 4 ∧ ¬hi :
>
:x2 + y 2 ≥ 4 :

0
4 − x2 − y 2
0
(12)

which indicates that if the rover has not already taken a
picture of point i and the rover is within a radius of 2 from
the picture point (0, 0), then the rover receives a reward
that is quadratically proportional to the distance from the
picture point. Hence for various points, the rover has to
trade-off whether to take each picture at its current position,
or to get a larger reward by first moving and potentially
getting closer before taking the picture.
5.2

Results

For the M ARS ROVER domains, we have run experiments
to evaluate our SDP solution in terms of time and space
cost while varying the horizon and problem size.

Mars Rover Linear

Mars Rover Nonlinear
5000

Horizon 1
Horizon 2
Horizon 3
Horizon 4
Horizon 5
Horizon 6
Horizon 7
Horizon 8

1500

1000

Number of nodes

Number of Nodes

2000

500

0

Horizon 1
Horizon 2
Horizon 3

4000
3000
2000
1000
0

2

3

4

5

1

Number of Pictures
Mars Rover Linear
400000

3

4

3500
Horizon 1
Horizon 2
Horizon 3
Horizon 4
Horizon 5
Horizon 6
Horizon 7
Horizon 8

300000
250000
200000

Horizon 1
Horizon 2
Horizon 3

3000

Time (ms)

350000

Time (ms)

2

Number of Pictures
Mars Rover Nonlinear

150000
100000

2500
2000
1500
1000
500

50000
0

0
2

3

4

5

Number of Pictures

1

2

3

4

Number of Pictures

Figure 2: Space (# XADD nodes in value function) and time to optimally solve different problem sizes of the two M ARS
ROVER domains for varying horizon lengths.
Because the reward and transition functions for M ARS
ROVER L INEAR use piecewise linear case statements, we
note the optimal value function in this domain is also piecewise linear. Hence in this domain, we use a linear constraint feasibility checker to prune unreachable paths in
the XADD — later we will compare solutions for M ARS
ROVER L INEAR with and without this pruning.

SDP vs. horizon for M ARS ROVER L INEAR with three
picture points for SDP with and without XADD pruning.
Here we see an impressive reduction in time and space as
a function of horizon with pruning. Without pruning, both
time and space grow super-linearly with the horizon, while
with pruning time and space appear to grow linearly with
the horizon.

In Figure 2, for both the M ARS ROVER L INEAR and M ARS
ROVER N ONLINEAR domains, we show how the number
of nodes of the value function XADD (proportional to the
space required to represent the value function) varies for
each iteration (horizon) and different problem sizes (given
by the number of pictures). We first note that the nonlinear
variant appears much harder for SDP (much more time required and larger value functions) than for the linear variant
— this is largely due to the fact that the XADD can be optimally pruned in the linear variant. Secondly, we note an
apparent superlinear growth in space and time required to
solve each problem as a function of the number of picture
points — this likely reflects the superlinear growth of combinations of pictures that must be jointly considered as the
number of pictures increases. Finally, from these graphs it
is hard to summarize general algorithm behavior as a function of horizon, but it appears for the linear problem variant
that both the time and space grow linearly as a function of
horizon — this will be confirmed in the next experiments.

In Figure 4, we show the exact optimal value function
on the vertical axis for three domains, K NAPSACK (from
Section 1), M ARS ROVER L INEAR and M ARS ROVER
N ONLINEAR, as a function of two continuous state variables shown on the horizontal axes. We notice here that
the piecewise boundaries for all three plots clearly demonstrate non-rectangular boundaries. In particular, the value
function plot for the M ARS ROVER N ONLINEAR domain
demonstrates nonlinear piecewise boundaries with each
piece being a nonlinear function of the state — it has the
shape of stacked quadratic cones with each lower cone representing the cost of first moving from points farther away
from the picture being receiving the value for taking the
picture within the radius limits from (12).

Figure 3 shows the amount of time for each iteration of

To the best of our knowledge, these results demonstrate the
first exact analytical solutions for DC-MDPs having optimal value functions with general linear and even nonlinear
piecewise boundaries.

Mars Rover Linear 3
3000

Time with Pruning
Time without Pruning

100000

2500

80000

Time (ms)

Number of Nodes

Mars Rover Linear 3

Nodes with Pruning
Nodes without Pruning

2000
1500
1000

60000
40000
20000

500
0

0
1

2

3

4

5

6

7

8

1

2

3

4

Horizon

5

6

7

8

Horizon

Figure 3: Space (# XADD nodes in value function) and time for different iterations (horizons) of SDP on M ARS ROVER
L INEAR with 3 image target points. Results shown for SDP with and without XADD infeasible path pruning.
Mars Rover Linear 2

Mars Rover Nonlinear
Knapsack

300
250

4
200
100

Value

3.5

80

150

3

100

2.5
Value

Value

60

50
40

2
1.5

0

1

20

20

0.5
15

0
120
100

0
15

10

120

80

10

100
60
40
40

20
x2

5
Energy

20
0

0

15

5

80

10
0

60

0
0

1

x1

3

2

4

5
0

−5

5

−5

−10
4

x 10
Time

y

−15

−10
−15
x

Figure 4: Optimal value function (vertical axis) for different domains showing non-rectangular piecewise boundaries.
From left to right, K NAPSACK (with horizontal axes x2 and x1 ), M ARS ROVER L INEAR with two pictures (with horizontal
axes energy and time), and M ARS ROVER N ONLINEAR with one picture (with horizontal axes y and x).

6

Related Work

The most relevant vein of Related work is that of [8]
and [11] which can perform exact dynamic programming
on DC-MDPs with rectangular piecewise linear reward and
transition functions that are delta functions. While SDP
can solve these same problems, it removes both the rectangularity and piecewise restrictions on the reward and value
functions, while retaining exactness. Heuristic search approaches with formal guarantees like HAO* [13] are an attractive future extension of SDP; in fact HAO* currently
uses the method of [8], which could be directly replaced
with SDP. While [14] has considered general piecewise
functions with linear boundaries (and in fact, we borrow
our linear pruning approach from this paper), this work
only applied to fully deterministic settings, not DC-MDPs.
Other work has analyzed limited DC-MDPS having only
one continuous variable. Clearly rectangular restrictions
are meaningless with only one continuous variable, so it is
not surprising that more progress has been made in this restricted setting. One continuous variable can be useful for
optimal solutions to time-dependent MDPs (TMDPs) [5].
Or phase transitions can be used to arbitrarily approxi-

mate one-dimensional continuous distributions leading to a
bounded approximation approach for arbitrary single continuous variable DC-MDPs [12]. While this work cannot
handle arbitrary stochastic noise in its continuous distribution, it does exactly solve DC-MDPs with multiple continuous dimensions.
Finally, there are a number of general DC-MDP approximation approaches that use approximate linear programming [10] or sampling in a reinforcement learning style
approach [15]. In general, while approximation methods
are quite promising in practice for DC-MDPS, the objective of this paper was to push the boundaries of exact solutions; however, in some sense, we believe that more expressive exact solutions may also inform better approximations, e.g., by allowing the use of data structures with nonrectangular piecewise partitions that allow higher fidelity
approximations.

7

Conclusions

In this paper, we introduced a conditional stochastic equation model for the continuous part of the transition function in DC-MDPs. This representation facilitated the use of

symbolic dynamic programming techniques to generate exact solutions to DC-MDPs with arbitrary reward functions
and expressive nonlinear transition functions that far exceeds the exact solutions possible with existing DC-MDP
solvers. In an effort to make SDP practical, we also introduced the novel XADD data structure for representing arbitrary piecewise symbolic value functions and we addressed
the complications that SDP induces for XADDs, such as
the need for reordering the decision nodes after some operations. All of these are substantial contributions that have
contributed to a new level of expressiveness for DC-MDPS
that can be exactly solved.
There are a number of avenues for future research. First off,
it is important examine what generalizations of the transition function used in this work would still permit closedform exact solutions. In terms of better scalability, one avenue would explore the use of initial state focused heuristic search-based value iteration like HAO* [13] that can
be readily adapted to use SDP. Another avenue of research
would be to adapt the lazy approximation approach of [11]
to approximate DC-MDP value functions as piecewise linear XADDs with linear boundaries that may allow for better approximations than current representations that rely
on rectangular piecewise functions. Along the same lines,
ideas from APRICODD [16] for bounded approximation
of discrete ADD value functions by merging leaves could
be generalized to XADDs. Altogether the advances made
by this work open up a number of potential novel research
paths that we believe may help make rapid progress in the
field of decision-theoretic planning with discrete and continuous state.

Acknowledgements
We thank the anonymous reviewers for their comments that have
helped improve the paper. The first author is supported by
NICTA; NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the
Digital Economy and the Australian Research Council through
the ICT Centre of Excellence program. This work has also
been supported by the Brazilian agencies FAPESP (under grant
2008/03995-5) and CAPES.

References
[1] R. Iris Bahar, Erica Frohm, Charles Gaona, Gary
Hachtel, Enrico Macii, Abelardo Pardo, and Fabio
Somenzi. Algebraic Decision Diagrams and their applications. In IEEE /ACM ICCAD, 1993.

[4] Craig Boutilier, Ray Reiter, and Bob Price. Symbolic dynamic programming for first-order MDPs. In
IJCAI-01, pages 690–697, Seattle, 2001.
[5] Justin Boyan and Michael Littman. Exact solutions to
time-dependent MDPs. In Advances in Neural Information Processing Systems NIPS-00, 2001.
[6] John L. Bresina, Richard Dearden, Nicolas Meuleau,
Sailesh Ramkrishnan, David E. Smith, and Richard
Washington. Planning under continuous time and resource uncertainty: A challenge for ai. In Uncertainty
in Artificial Intelligence (UAI-02), 2002.
[7] Thomas Dean and Keiji Kanazawa. A model for reasoning about persistence and causation. Computational Intelligence, 5(3):142–150, 1989.
[8] Zhengzhu Feng, Richard Dearden, Nicolas Meuleau,
and Richard Washington. Dynamic programming for
structured continuous markov decision problems. In
Uncertainty in Artificial Intelligence (UAI-04), 2004.
[9] Jesse Hoey, Robert St-Aubin, Alan Hu, and Craig
Boutilier. SPUDD: Stochastic planning using decision diagrams. In UAI-99, 1999.
[10] Branislav Kveton, Milos Hauskrecht, and Carlos
Guestrin. Solving factored mdps with hybrid state
and action variables. Journal Artificial Intelligence
Research (JAIR), 27:153–201, 2006.
[11] Lihong Li and Michael L. Littman. Lazy approximation for solving continuous finite-horizon mdps. In
National Conference on Artificial Intelligence AAAI05, 2005.
[12] Janusz Marecki, Sven Koenig, and Milind Tambe. A
fast analytical algorithm for solving markov decision
processes with real-valued resources. In International
Conference on Uncertainty in Artificial Intelligence
IJCAI, 2007.
[13] Nicolas Meuleau, Emmanuel Benazera, Ronen I.
Brafman, Eric A. Hansen, and Mausam. A heuristic search approach to planning with continuous resources in stochastic domains. Journal Artificial Intelligence Research (JAIR), 34:27–59, 2009.
[14] J. Scott Penberthy and Daniel S. Weld. Temporal
planning with continuous change. In National Conference on Artificial Intelligence AAAI, pages 1010–
1015, 1994.
[15] Andrew Moore Remi Munos. Variable resolution discretization in optimal control. Machine Learning, 49,
2–3:291–323, 2002.

[2] Richard E. Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, 1957.

[16] Robert St-Aubin, Jesse Hoey, and Craig Boutilier.
APRICODD: Approximate policy construction using
decision diagrams. In NIPS-2000, Denver, 2000.

[3] Craig Boutilier, Thomas Dean, and Steve Hanks.
Decision-theoretic planning: Structural assumptions
and computational leverage. JAIR, 11:1–94, 1999.

[17] Nevin Lianwen Zhang and David Poole. Exploiting
causal independence in bayesian network inference.
J. Artif. Intell. Res. (JAIR), 5:301–328, 1996.

