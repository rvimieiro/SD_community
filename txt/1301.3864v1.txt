282

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

2000

Probabilistic Arc Consistency: A Connection between Constraint Reasoning
and Probabilistic Reasoning

Michael C. Horsch

William S. Havens

mhorsch@cs.sfu.ca

havens®cs.sfu.ca

Intelligent Systems Laboratory,
School of Computing Science,
Simon Fraser University,
Burnaby, B.C., Canada V5A 1S6
Abstract
We document a connection between constraint
reasoning and probabilistic reasoning.
We
present an algorithm, called probabilistic arc
consistency, which is both a generalization of a
well known algorithm for arc consistency used in
constraint reasoning, and a specialization of the
belief updating algorithm for singly-connected
networks. Our algorithm is exact for singly­
connected constraint problems, but can work
well as an approximation for arbitrary problems.
We briefly discuss some empirical results, and re­
lated methods.

1

INTRODUCTION

Constraint reasoning is about finding configurations which
satisfy constraints, possibly optimizing the configurations
according to some objective function. One of the most im­
portant tools in constraint reasoning is the process of arc­
consistency, which reduces the configuration space to those
configurations which meet minimal local consistencies and
their immediate consequences.
In this paper we present an algorithm, called probabilis­
tic arc consistency (pAC), which was developed to com­
pute solution probabilities, i.e., the frequency with which
a variable takes on a particular value in all solutions, in
constraint satisfaction problems. This information can be
used as a heuristic to guide constructive search algorithms:
for a given variable, choose the value which appears in
the most solutions. Similar proposals for counting solu­
tions or estimating solution probabilities have been made
[6, 14, 4, 11, 16, 15] . Solution probabilities are orthogonal
to preference over solutions (e.g., [3, 1]), or probabilistic
constraints (e.g.[1]) in which there is uncertainty regarding
whether a constraint applies.
The main purpose of this paper is to document a connec­
tion between constraint reasoning and probabilistic reason-

ing. We show that pAC algorithm is a generalization of
the basic arc consistency algorithm AC-3 [10], and is also
a specialization of the belief propagation algorithm [8] for
singly-connected Bayesian networks. However, since the
value of our method must be established empirically, we
will briefly describe some of our empirical results in Sec­
tion 6. We feel our results are positive: we can report a dra­
matic decrease in search costs, i.e., number of backtracks,
using pAC as compared to related methods for counting
solutions. A detailed description of our results is found in
[7].
This paper is organized as follows. In Section 2, we will
provide a brief description of the belief propagation al­
gorithm for singly connected Bayesian networks, so that
the comparison between algorithms can be self-contained.
Section 2.1 presents an independence assumption which al­
lows CSPs to be represented compactly, and show how this
assumption changes the belief propagation algorithm. Sec­
tion 3 will give a brief overview of constraint satisfaction
problems, and show that the arc consistency algorithm is
a specialization of belief propagation. In Section 4, we
present the pAC algorithm itself, give a formal statement
of correctness, and show how it generalizes the arc consis­
tency algorithm, and specializes the belief propagation al­
gorithm. In Section 5 we discuss the relationship between
pAC and similar methods in the literature: Section 6 pro­
vides a summary of our empirical evaluation. In Section 7
we close with a discussion of these results.
2

BELIEF PROPAGATION IN BAYESIAN
NETWORKS

Kim and Pearl [8, 13] developed a polynomial-time al­
gorithm for singly-connected Bayesian networks. The
method is based on message passing, and there are two
message types: causal messages, denoted by the symbol
1r, are passed along the direction of the arcs in the DAG;
diagnostic messages, denoted by the symbol A, are passed
against the direction of the arcs in the DAG.
The posterior probability of a variable

X given evidence E

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

is computed by combining the messages it receives from its

2000

2.1

parents and children:

P(X
where

o:

=

xiE) =

( x) ( x)

o: .A

1r

A SIMPLE MODEL OF CAUSAL
INDEPENDENCE

Conditional

independence is a simplifying assumption

based on structure in the factorization of a joint probabil­

is a normalization constant.

The effect of evidence in the descendants of X is summa­
rized by:
.A(x) =

283

ity distribution. This assumption is the basis for modelling
joint distributions using a Bayesian network: a variable is
conditionally independent of its non-descendants, given an

II ,\yi(x)

assignment of values to the variable's parents.

j

Causal independence is a simplifying assumption based on

where the quantity ,\yi(x) summarizes the effect of evi­
dence through child }j.

distribution. The idea behind causal independence is that,

The effect of evidence in the ancestors of X is summarized

for a given configuration of a subset of its parents, the prob­

by:
1r(x) =

L P(x lu1, ..., un) II 7rx(ui)

(1)

ent variables, and the product is the product of all messages

ui.

agnostic model, the proposition that a car starts when the
ignition key is turned may depend directly on factors such

Here, the sum is over the space of assignments to X's par­

rizes the evidence through parent

ability distribution of a variable may be independent of the
remainder of its parents. To take an example from a di­

Ut, ... ,Un

received from any parents. The quantity

structure in the factorization of a conditional probability

x(ui) summa­

1r

Once X has determined its own posterior probability given
the evidence "mentioned" in the messages received by X, it
passes messages to its neighbours, with care taken to avoid
double-counting of evidence. The message X sends to its

Uk reflects all evidence seen by X except for the
evidence already seen by uk:
parent

as the amount of gas in the tank, the state of the battery,

etc.. The conditional probability table allows for arbitrarily
complex interactions among the parent values. However,
the domain expert may judge that a car cannot start if the
battery is dead, no matter how much gas is in the tank. On
the other hand, if the battery voltage is low or high, the
probability of starting may depend on the quantity of gas in
the tank.
There have been many formalizations of causal indepen­
dence

which allow for many interesting

(e.g., [13, 17]),

and important variations.
here is very simple.

The assumption we will use

Suppose a variable X has parents

{U1, ..., Un}. We will assume that the conditional proba­
bility table P(XIU1, ..., Un) can be factored as follows:
where o: is a normalization constant. Notice that in this
n
equation, Ax ( Uk ) omits any information received from uk.
P(XIU1, ... , Un)= II P(XIUi)
The message X sends to its child }j reflects all evidence
i=l
seen by X except for the evidence already seen by }j:
0:

where

o:

is a normalizing constant. We will make this as­

sumption for every variable in a singly-connected Bayesian
network.
where o: is a normalization constant. The first product in
the above expression is over all children except the one to
which X is sending the message; the outgoing 1r-message
includes information received by all parents, as collected in

7r( X ) .

The initial conditions of the algorithm are as follows: if a
node X has no parents, 1r(x) =
has no children, then .A(xt)

=

1

P(X= x) . If a variable X
for all values Xt E

(

S"h. If

X= x is given as evidence, then .A(x) = 1 and .A xt )
for all values Xt E

Ox, Xt # x.

=

0

The correctness of this algorithm is guaranteed by the fact
that the sources of evidence are independent. The complex­
ity of the algorithm, in terms of the number of messages

Under this assumption, the 1r-terms in the polytree algo­
rithm are composed as follows:
1r(x)

= o:

II L P(x lui)7rx(u;)
i ui

The 1r-message sent from X to child }j is unchanged, once
1r(x) is computed.
The .A-message sent to parent

uk is as follows:

.Ax(uk) = o: L .A(x) II L P(x lui)7rx(ui)
k-:j:i U;
X
Note that the .A-message is sent to Ui by summing over X's
values. This is done because the algorithm assumes that the

P(XIU1, .. . , Un) is stored locally
Uk.

sent, is linear in the number of nodes in the network. These

conditional distribution

results are proven in

for X, but is not stored locally for

[13].

(2)

284

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

2000

Given our assumption of causal independence for ev­

straints over tuples of domain values (readers who would

ery

like more background on CSPs than can be presented in

variable

in

the

network,

the

low

dimensional­

P(XIUi), and the fact that P(XIU;)P(U;) =
P(U; IX)P(X), it is not unreasonable to allow U to store
local information P(U;IX), even when X stores P(XIU;).

ity of

here are referred to

[9]). The problem is to find an as­

signment of values to all variables which satisfies all the
constraints. We will denote variables X;,

D;.

0 ::; i

< n, and

We assume that this information is available. In this case,

the domains

X need not sum out its own values before sending the

all the constraints are sets of tuples from pairs of domains,

.A-message; it can let

U;

sum out over X's values, using

P(U; I X) . Thus, we can simplify the .A-messages by send­
ing the following A' -message from X to U;:
.Au, (x) =a.A(x) II L P(xlu;)7rx(u;)
k:j:i U;
This message must be "interpreted" by U; by summing out
the values of X. Under our causal independence assump­
tion, the original .A-message can be reconstructed by U; us­
ing the following:

cording to the causal independence assumption. This im­
plies that under our assumption, the parent-child relation­
ship is symmetric, and we need only pass one kind of mes­
sage (to distinguish this method from the more general case
where our causal independence assumption does not hold,
we will call them 1-messages).

Dj .

As well, we will limit our discussion to

One approach to solving CSPs is to use constructive search.
Values are assigned to variables in some sequence, and af­
ter each assignment, the assignment is checked for consis­
tency with the constraints. If the assignment is consistent,
the search continues recursively. If an assignment is incon­
sistent, a different value is chosen, and if no values for the
current variable in the sequence are consistent, the search
Heuristic information can be used to or­

search.
There is a well-documented problem with simple construc­
tive search, namely that there are values, and combinations
of values, which never appear in any solution. If the con­
structive search method tries one of these assignments, it is
guaranteed to have to backtrack. Furthermore, unless spe­
cial care is taken in the search algorithm, these assignments

V; is simply the product of all

the information received by X except for the information

As above,

x

der the values and the variables in an attempt to speed up

Note that this is exactly how 1r-messages are treated, ac­

V; itself. We define

Bel(x) =a II .Av,(x)
i
.Av,(x) describes the information about X

D;

ues the search.

xEX

received from

C

domains which are finite and discrete.

backtracks to the previously assigned variable, and contin­

.Ax(u;) =a L P(udx).Au,(x)

The message to neighbour

C;j

We will focus on binary CSPs, in which

may be tried often during the search. These problems have
been well-studied

[10], and several pre-processing algo­

rithms have been devised to limit their effects. In partic­
ular, an algorithm called AC-3 reduces the domains of the
variables in a CSP, by removing those values which are not
consistent with some value of its neighbouring variables.

re­

V;, but is not available directly. It is computed
from the 1-message sent from V; to X as follows:

We present AC-3 in more detail in the following section.

ceived from

.Av,(x) =a L P(x lv)lx (v)
vEV;

x E Dx, there is a value y E Dy such
(x, y) E CXY. In this definition, we assume that there
are no unary constraints which may rule out values in Dx
consistent iff for all

that

We have seen how a simple assumption of causal indepen­
dence leads to a simplification of the polytree belief propa­
gation algorithm. We have replaced 1r and .A messages with
used above is derived from

.Av

/V, and it represents the infor­
V.

mation received by X from neighbour

or

Dy.

CONSTRAINT SATISFACTION
PROBLEMS

A constraint satisfaction problem (CSP) is posed as a set
of variables, a domain for each variable, and a set of con-

Clearly, CSPs need not be arc consistent, but if the

domains of the CSP variables were reduced such that each
arc was arc consistent, the cost of search could be reduced.
When all arcs are arc consistent, we say the problem is arc
consistent.
An algorithm for computing arc consistency is given in Fig­
ure

3

[10]. An arc (or

edge) Cxy in a constraint graph for a binary CSP is arc

/U,(x) =a IIL P(xlv;).Av,(x)
k:j:i V;

The determination of the local quantity

ARC CONSISTENCY

Arc consistency can be defined as follows

The outgoing message to all neighbours is:

1 messages.

3.1

1 and is due to [10]. The underlying idea is to cycle

through the variables in some order, and reduce the domain
of each variable such that only values which are arc consis­
tent remain. The effects of reducing a domain are

propa­

gated to neighbouring variables.
There are two key aspects of the AC-3 algorithm. First,

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

in Revise, a domain element is removed if there is some

procedure Revise(i,
begin

neighbour that does not support the element. Equivalently,

delete

a domain element is supported if there is a supporting ele­

F; C D; be the set of
i.
F; ={xED; l'v'jEN;, 3yEDj : (x, y) EC;j 1\ yEFj}
where N; are the neighbours of X;, i.e., variables which
share a constraint with i, and Fj C Dj is the set of sup­
ported values for variable j. We will write F;(x) to rep­
resent the proposition x E F;, and similarly C;j(x, y) to
represent (x, y) EC;j. Thus we have the following equiv­

285

2000

:=

j):

false

for each x E D; do

ment in all neighbours' domains. Let

if there is no y E Dj such that
remove

supported values for variable

delete

x

:=

from D;

E C;j then

true

end if
end for
return delete
end
procedure AC-3
begin
Q :=

{(i,j)l(i,j) E arcs ( G), if j}

while Q not empty do

alence

select and delete any arc
if Revise(k,m) then
Q := Q

The use of'v' and

(x, y)

end if

3 is purely by convention; using x and+
TI to represent

(k, m ) from Q;

U{(i, k)l(i, k) E

arcs

( G), if k, if }
m

end while

to represent 1\ and V, resp. and using L and

end

boolean sums and products, we can rewrite this equivalence
as follows:

F;(x) = IT L (C;j(x, y) x Fj(Y))

Figure
(3)

jEN; yEDj

This is the domain update rule applied to each variable dur­

1: AC-3. An algorithm for achieving arc consis­

tency.

networks. To make the case explicit, we first observe that

ing the propagation of arc consistency.

in a binary CSP, the neighbours of a node are causally

The second key element in AC-3 is the mechanism for cy­

independent, in the sense described in the previous sec­

cling through the variables to ensure soundness of the al­
gorithm. As shown, a queue of pairs of constrained vari­
ables is maintained, and pairs are removed and added to
the queue.
The purpose of the queue is to guarantee that changes to the
domain of one variable are propagated to its neighbours. In
other words, selecting edge

( k, m) from the queue results
Fk, as in Equa­

in a call to Revise(k, m), which updates
tion 3 above, using

F

m.

The queue contains and orders

tion. Second, we replace the conditional probability dis­
tributions
ing

C;j.

P(X;IXj), i # j with boolean arrays represent­

Finally, the operations + and x are replaced by

boolean operations V and /\, respectively.

4

PROBABILISTIC ARC CONSISTENCY

In this section we present the pAC algorithm, which we use
to compute (or approximate, as we will see) the solution

messages between variables, and messages are processed

probabilities for every variable in a CSP. This information

in a sequential manner. The propagation mechanism could

can be used as a heuristic to guide constructive search al­

equally well be expressed as a distributed algorithm: af­

gorithms:

ter each change in the domain of a variable, the variable's

appears in the most solutions.

neighbours are made aware of the changes, and change
their domains accordingly.
Note also that when the processing of edge
in a domain reduction

(i.e.,

( k, m) results

Revise(k, m) returns true), arcs

(i, k) are added to the queue, but the arc ( m, k) is explic­

itly omitted from the queue. That is, when a variable's do­
main is reduced due to information received from a neigh­

bour, it does not cause the neighbour to be checked for re­
vision. This is a matter of efficiency only, because, due
to the symmetric nature of a binary constraint, the neigh­

e.g.,

for a given variable, choose the value which

Definition 1 Let < V, B > be a binary constraint satis­
faction problem with variables V, and binary constraints
B, such that the constraint graph G =< V, B > is singly­
connected. For every X E V, let Dx be the domain of
X, i.e., Dx = {x 1, , x m}. For every pair of variables
(X,Y) such that there is a binary constraint Cxy E B,
define the following:
.

Cxy(i, j)

.

.

{�

if

(x;,Yi)ECxy
otherwise

bour's domain cannot be reduced if the arc were included
in the queue.
Thus we have shown informally that AC-3 can be expressed
as a special case of belief propagation in singly-connected

m

L Cxy(i, j)M��(j)
j =l

286

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

F�)(i)
where

II

a

a

{

{YI(X,Y)EB}

s�l(i)

work, we can observe this phenomenon in about

m

5% of the

large problems we have tried.

0

singly-connected CSPs, the arc consistent domain values

The interpretation of the quantities

=

The pAC equations generalize arc consistency. If boolean
values are used instead of probabilities, and boolean opera­
tors "and" and "or" instead of floating point multiplication
and addition, the algorithm computes arc consistency. In

otherwise

are used in some solution, and therefore have non-zero so­

s�t(i), F�)(i),

The binary constraints in

expressed numerically in Cxy, where
the pair

ing point numbers, at the risk of a non-trivial loss of pre­
cision when the CSPs get very large. In our experimental

L F�)(i) 1
i=l
F�)(i) if s�l(i) > o
s�l(i)

is such that

M�¢1l(i) is as follows.

2000

Cxy(i,j)

(x;, Yi) satisfies the binary constraint.

B are

=

1 iff

The inter­

pretation of the remaining quantities is the subject of the
following theorems (the proofs appear in

[7]).

lution probability.
The pAC equations are also a special case of Pearl &
Kim's belief propagation algorithm for singly-connected
belief networks. To prove this informally, it is sufficient
to observe the following correspondences:
be represented as

P(X

=

x;IY

Yi),

=

CXY(i, j)

can

under the causal

2.1; s�t(i),
M�t(i), and F�)(i) correspond to ..\y(x;), /Y(x;), and

independence assumption given in Section

Lemma 1

Let X, Y be any constrained pair of variables
in a CSP G whose constraint graph is singly-connected.
Let G ' be the subproblem constructed in the following way:
Include in V' only those variables from V which are k steps
away from Y in the constraint graph of G , and which are
separated fromY byX. Likewise, include in B' only those
constraintsCxy E Bfor which bothX E V' andY E V'.
M�t(i) is proportional to the number of times value x; E
Dx is used in all solutions of the sub-problem G '.
Lemma 2 Let X be any variable in a CSP G whose con­
straint graph is singly connected. F�)(i) is the relative
frequency of the use of x; E Dx in all solutions of the
sub-problem including only those variables in G which are
distance k or less fromX. If G is unsatisfiable, F�)(i) = 0
for all i.
Theorem 3 Let G = < V, B > be a CSP such that the
constraint graph for G is singly-connected, with diameter
d. For any variable X E V, the relative frequency that
value x; E Dx is used in all solutions of G is F�) (i).

Bel(x;), resp.,

after

k messages were received by X.

The guarantee of correctness only holds in CSPs with
singly connected constraint graphs.

For more general

CSPs, the equations can be used to approximate solution
probabilities for these problems, by iterating the equations
some number of times. This approach has parallels with
relaxation methods for belief propagation in Bayesian net­
works

[ 12], and decoding turbo-codes [5]. The method is

not guaranteed to converge to a stable set of probability
distributions, and if it does, there is no guarantee that the
approximations are useful. Thus the value of the method
is an empirical question, which we explore elsewhere

[7],

and summarize in Section 6.
To limit computation costs, we use two parameters,

c

and

Maxlter, to detect convergence or non-convergence. Iter­
ation continues while both of the following conditions are
true:

m_;x

The pAC equations can be expressed as a distributed pro­

L(F�+l)(i)- F�)(i))2

>

c

(4)

k

<

Maxlter

(5)

i

cedure. Each variable X; is initialized to have uniform dis­
tributions. At each time step, each variable saves its previ­
ous distribution (F), and prepares to handle incoming mes­
cessed

W hen the change in solution probability is less than a given

(M) from neighbouring variables are pro­

c, the process is declared to have converged; when the

(S), and the results are stored locally, so that mes­

number of iterations exceeds Maxlter, the process is halted

sages. Messages

sages need not be sent to all neighbours if no changes were
made in the distribution. The new distribution is computed
by forming the product of all information stored from the
most recent message received from all neighbours. Finally,
if the variable's distribution has changed significantly, a
new message

(M) is sent to all neighbours, taking care not

to double count.

The approximate solution probabilities computed using
pAC iteratively can be used to guide constructive search.
For example, approximate solution probabilities can be
computed before each assignment, and the probability used
to induce a variable ordering,

i.e.,

choose the variable

whose maximum probability domain value is maximum

The pAC equations require arbitrary precision floating
point numbers. In our implementation, we use

without convergence.

64 bit float-

over all unassigned variables, as well as a value ordering

i.e.,

choose the most likely domain value for the variable.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

5

RELATED WORK

Probabilistic arc consistency is strongly related to tech­
niques proposed in the literature. Dechter and Pearl's [4]
single spanning tree method (SST) determines an exact so­
lution count in singly-connected CSPs. The variables in a
CSP are given an arbitrary ordering, in which
is cho­
sen as the root. The number of solutions in which
takes
value Vt is computed recursively by the following:

Xj
Xj

D'

where

{ ciXc is a child of Xj}
{y E Dc l(x, y) E Cjc}
The leaf variables X1 in the ordering have N (X1 = v) = 1
for all
E D1. This method must be repeated for each
C'
D'

v

variable if solution counts are necessary for all variables.
For more general CSPs, the method is applied to the sub­
problem consisting of the tightest constraints in the original
problem. The method is efficient, but can result in over­
optimistic approximations for the number of solutions in
the original problem.
Meisels et al.'s [11] universal propagation method (UP)
uses a Bayesian network to compute solution counts for
a designated variable, making the same independence as­
sumptions in Section 2.1. The Bayesian network is the
constraint graph whose constraints are directed according
to a pre-established variable ordering. The goal of the
computation is to find the marginal probability distribu­
tion
for the designated sink variable
Under the
same assumption of causal independence as described in
Section 2.1:

P(Xj)

Xj.

P(Xj = x) =a IIL P(Xj = xiXc = y)P(Xc = y)
C' D'

where

C'
D'
P(Xj IXc)
a

{ ciXc is a parent of Xj}
{y E De}
Xj

and where
represents the constraint between
and
and is a normalization constant. W hen applied
to CSPs whose constraint graph is singly-connected, the
method reduces to that of[4], with the addition of a normal­
ization constant, and computes exact solution probabilities
for a designated variable. For more general CSPs, the same
method is applied, producing approximate solution counts,
which can be over-optimistic.

Xc,

Vemooy and Havens [16] method extends [4] by construct­
ing a forest of spanning trees from the original CPS's con­
straint graph. Using either of the previous methods, the

287

solution counts for each tree are determined. An approxi­
mate solution count for the original CSP is determined by
combining the results from each tree, under the assumption
that the solutions to each tree are independent. Using this
assumption, an approximation is made for
that is,
the distribution of solutions for variable
in the original
CSP, by normalizing the product of the distributions from
all trees:

X;

P(X;),

P(X; = x) =a II p(k)(X; = x)
k
is a normalization constant, and p(k) (X; = x)

N(Xj = x) = IIL N(Xc = y)
C'

2000

where a
is the solution probability derived from the kth spanning
tree. Vemooy and Havens explore this multiple spanning
tree method (MST) as both a static and dynamic heuristic.
Peleg [14] develops a probabilistic relaxation method in­
tended to find a satisfying assignment to the variables in
a CSP. Although different in intent, the algorithm itself is
similar to the algorithms discussed above. The difference,
apart from the motivation, is a single factor in the expres­

F�)(

sion for
i) in the pAC algorithm. Peleg's formula uses
the previous iteration's estimate to modify the current esti­
mate:

II

{YI(X,Y)EB}

F�)(

It should be noted that in this interpretation,
i) is no
longer a solution probability, in the sense used in this paper.
Rather, Peleg's method is intended to converge to a solution
to the CSP, i.e., a distribution in which every variable has
only one value with non-zero "probability."
The work of Shazeer et al. [15] on computing probabilistic
preferences over solutions to CSPs develops an algorithm
which is essentially the same as the pAC algorithm. The
main difference is that pAC uses the constraint graph struc­
ture itself, whereas the proposal by Shazeer et al. unrolls
the constraint graph to form a large singly-connected struc­
ture in which the nodes and arcs are repeated some fixed
number of times. A minor difference is that pAC assumes
a uniform prior over all values in a domain, so that the re­
sulting distributions are solution probabilities; the proposal
by Shazeer et al. allows arbitrary probability distributions
over the domain values to express more or less likely val­
ues. Thus these distributions are not strictly solution prob­
abilities, in the sense used in this paper.
6

RESULT S

We have conducted an extensive investigation into the per­
formance of pAC as a method for approximating the solu­
tion frequencies of binary CSPs [7]. We evaluated the ac­
curacy of the approximation computed by pAC, and exam­
ined its effectiveness as a heuristic in constructive search.

288

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

We were able to show that there was a high correlation
between the exact solution probabilities (as computed by
exhaustive search) and the approximations computed by
pAC. We were also able to show that when used as a vari­
able ordering heuristic, the approximate solution probabil­
ities computed by pAC substantially reduce backtracking
in constructive search, by up to two orders of magnitude.
Space constraints prohibit a complete presentation of these
results here.
Our experiments were performed on randomly constructed
CSPs, which can be described using four parameters: the
number of variables, n, the number of values m, the con­
straint density p1 (the probability that any two variables
share a constraint), and the constraint tightness, P2 (the
probability that any pair of values is disallowed in a given
constraint). It is well known that random CSPs vary greatly
in difficulty, across the parameter space for these problems.
We evaluated the accuracy of the approximation by direct
comparison to exact solution counts, computed by exhaus­
tive search. We applied pAC to 3 sets of random CSPs of
varying topology and constrainedness, with n = 20, m =
10. The difficulty of these problems varied from very sim­
ple (having very many solutions, or being obviously over­
constrained), to very difficult (having only only a few solu­
tions). The convergence criteria were set fairly high, with
5 and Maxlter= 1000.
f = 10Of the problems with fewer than 1 million solutions and at
least one solution, the average correlation between the ex­
act solution probability and the approximation determined
by pAC ranged between 0.83 for for Pl = 0.2 (sparse
graphs), through 0.78 for Pl = 1.0 (complete graphs).
As well, pAC was able to identify the majority of over­
constrained problems very quickly; this is what should be
expected of a method which generalizes arc consistency.
For about 10% of the problems, pAC failed to converge.
In some cases, the convergence was just very slow, and
in other cases, the values computed by pAC oscillated be­
tween two distinct points in the distribution space. W hen
pAC does not converge, any degree of accuracy is possible.
We also evaluated the effectiveness of the heuristic as used
in constructive search. We compared pAC to the methods
of Dechter and Pearl (SST) [4], Meisels et al. (UP) [11],
Vemooy and Havens (SMST) [16] and Peleg [ 14]. Each of
these methods was used as a static value ordering heuris­
tic (i.e., the heuristics were computed as a preprocessing
step, but not updated as assignments were made). The MST
method of Vemooy and Havens was also used dynamically
(DMST), i.e., the solution probabilities were recomputed
after every assignment. A random value ordering strategy
was included in this comparison to provide a baseline.
We applied simple constructive search to 2 18 random prob­
lems, using the various heuristics as a value ordering

2000

heuristic. These problems were selected from a larger set
of random problems, from which we discarded the over­
constrained problems. They were constructed with 20 vari­
ables and 10 values each, and p1 = 1.0 (complete graphs),
and P2 E [0.1, 0.23]. Some of these problems were very
easy, but many were much more difficult.
Figure 2 shows the results. The horizontal axis shows
search costs, in terms of the number of backtracks; the ver­
tical axis indicates the cumulative fraction of the problems
solved. Each curve shows the cumulative fraction of prob­
lems solved using a given number of backtracks. For ex­
ample, the median number of backtracks for each method
can be read from the graph on the horizontal line through
the 0.5 mark.
The graph clearly shows that pAC is superior to all the
methods, except for Peleg's method, which was able to
converge on a solution for roughly 66% of the problems,
requiring no backtracking whatsoever. However, Peleg's
method is not much of an improvement over random value
orders for some problems. We repeated this experiment for
random problems with different constraint densities, with
very similar results.
We also evaluated the approximate solution probabilities
computed by pAC when used as a dynamic variable and
value ordering heuristic, i.e., after each assignment, the so­
lution probabilities were recomputed. We found that the
reduction in search costs is dramatic, up to two orders of
magnitude smaller than First Fail [6] or Least-Constrained
[2].
The price for this success is the cost of computing the
heuristic values: even for relatively large f, say f = 0.1,
and few iterations, e.g., Maxiter= 50, the cost of perform­
ing all the floating point operations is such that search re­
quires almost an order of magnitude more time using ap­
proximate solution probabilities than the First Fail or Least­
Constrained heuristics.
7

DISCUSSION AND FUTURE WORK

In this paper we have presented three algorithms, belief
propagation (BP) in singly-connected Bayesian networks,
arc consistency (AC-3) for binary constraint satisfaction
problems, and probabilistic arc consistency (pAC) for bi­
nary CSPs. We have shown that probabilistic arc consis­
tency is a special case of the belief propagation algorithm,
under an assumption of causal independence. We have also
shown that the arc consistency algorithm is a special case
of probabilistic arc consistency, specialized to perform of
boolean arithmetic.
In the case of singly-connected topologies, all three algo­
rithms are exact, and run in polynomial time. The BP al­
gorithm assumes conditional independence of a variable's
parents for correctness. Likewise, when a CSP's constraint

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

289

2000

Run Time Distribution for p1

=

1.0

0.9
0.8
"0
Q)
.2
0
"'
"'

0.7

E
Q)
:i5
e

0.6

0

0.5

a.

Random ­
SST ----·
UP ·····
SMST ···········
DMST -·-·­
Peleg - · - · pAC ······

c

0

u
nl
u:

0.4
0.3
0.2
0.1

1

10

100

1000

Number of backtracks

10000

100000

Figure 2: A plot of the number of backtracks required to solve a set of random CSPs with constraints between every pair of
variables, and between 10 and 23 percent of the pairs disallowed by the constraints. The plot shows the cumulative fraction
of the number of problems solved using a given number of backtracks.

graph is singly-connected, the pAC algorithm is also exact,
correctly computing solution probabilities for all variables.
Finally, AC-3 is exact for all topologies and runs in poly­
nomial time. However, when the constraint graph is singly­
connected, constructive search over the reduced domains is
backtrack-free. This property is not true for AC-3 on binary
CSPs of arbitrary topology.
The BP algorithm has been suggested and studied as an
approximation method for more general topologies (e.g.,
[13, 12]). It is almost startling that the propagation method,
designed for singly connected graphs, often converges on
arbitrary graphs to a distribution which is a reasonable
approximation of the exact distribution. Obviously, the
method will work well in graphs in which the conditional
dependence is weak.
Since approximation in Bayesian networks of arbitrary
topology is NP-hard, it is expected that it might take a long
time for BP to converge in at least a few instances. This has
been observed in empirical studies, and in some cases, the
algorithm has been observed to oscillate indefinitely [12].
The pAC algorithm can also be used to approximate solu­
tion probabilities for constraint problems of arbitrary topol­
ogy. Although pAC has stopping conditions, it is not guar­
anteed that the process will converge. Again, as solving
binary CSPs is an NP-hard problem, it was expected that
convergence of PAC might be slow in some cases. In some

cases, as with the BP algorithm, the iterative process oscil­
lates.
If the process converges, it usually converges to a good ap­
proximation to the solution probabilities. However, when
pAC is oscillating, any arbitrary cutoff point is likely to
provide no better than random choice heuristic information.
Fortunately, the oscillation problem can disappear when
values are assigned; this means that oscillation will incur
some extra search costs, but these seem to be small. Un­
fortunately, the reverse is true as well: making assignments
can also induce oscillation on the remaining variables.
We have found that if pAC oscillates, it does so between
two "poles," that is, two distributions which favour dramat­
ically different configurations. We were able to construct a
problem with 5 variables and two "loops," on which pAC
demonstrates oscillatory behaviour. The problem in this ex­
ample is that messages circulate around the loop, and due
to loops having different lengths, arrive at loop junctures
"out of phase."
The relationship between belief propagation, arc consis­
tency and probabilistic arc consistency has demonstrated to
be useful for constraint reasoning. In hindsight, it should
have been obvious that the relationship would lead to fairly
accurate approximations to solution probabilities, based on
the experience of applying belief propagation to arbitrary
Bayesian networks. The relationship should also prove use-

290

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS

ful for probabilistic reasoning in ways that may not yet be
obvious.
We are currently investigating a number of interesting is­
sues. Computing approximate solution probabilities using
pAC as described in this paper is an expensive operation.
There are many variations of the scheme which could be
explored. The goal would be to reduce computational costs,
and maintain the effectiveness of the heuristic. Obviously,
the convergence criteria can be modified to reduce the num­
ber of iterations. Propagation could be limited to some sub­
set of the constraints (the work of [4, 16] limit propagation
to tree structures, but other subsets are possible). We are
also investigating techniques to reason explicitly about the
trade-offs between propagating solution probabilities, and
search costs.
Recently, a theory of so-called "Semiring CSPs" [1] has
unified several variations of constraint problems, including
satisfaction of classical CSPs, and optimization in proba­
bilistic, fuzzy and valued CSPs. As described in [14, 15],
the propagation of solution probabilities can perform some
kinds of optimization by giving domain elements non­
uniform a priori weights. We are looking at using prop­
agation techniques to provide heuristic information for op­
timization of different kinds of objective functions, extend­
ing both [3, 1].

2000

[6] Richard M. Haralick and Gordon L. Elliot. Increasing

tree search efficiency for constraint satisfaction prob­
lems. Artificial Intelligence, 14(3):263-313, 1980.
[7] Michael C. Horsch and William S. Havens. How to

count Solutions to CSPs. Technical report, School of
Computing Science, Simon Fraser University, 2000.
[8] Jin H. Kim and Judea Pearl. A computational model

for causal and diagnostic reasoning in inference sys­
tems. In Proceedings of the Eighth Intemationalloint
Conference on Artificial Intelligence, pages 190-193,
1983.
[9] Vipin Kumar. Algorithms for constraint-satisfaction
problems: A survey. AI Magazine, 13(1):32-44,
1992.
[10] Alan K. Mackworth. Consistency in networks of re­
lations. Artificiallntelligence, 8(1):99-118, 1977.
[11] Arnno n Meisels, Solomon Ehal Shimonoy, and Gadi

Solotorevsky. Bayes networks for estimating the
number of solutions to a csp. In Proceedings of the

Fourteenth National Conference on Artificial Intelli­
gence, 1997.
[12] Kevin P. Murphy, Yair Weiss, and Michael I. Jordan.

Loopy belief propagation for approximate inference:
An empirical study. In Proceedings of the Fifteenth

Acknowledgements

Conference on Uncertainty in Artificial Intelligence,
Thanks to Matt Vemooy for providing the data from his
thesis, and to the anonymous referees for their helpful com­
ments.

pages 467-475, 1999.

Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Reasoning. Morgan

[13] Judea Pearl.

Kaufmann Publishers, Los Altos, 1988.

References

[14] Shmuel Peleg. A new probabilistic relaxation method.
[1] S. Bistarelli, U. Montanari, F. Rossi, T. Schieux,

G. Verfaille, and H. Fargier. Semiring-Based CSPs
and Valued CSPs: Frameworks, Properties and Com­
parison. Constraints, 4(3):199-240, 1999.
[2] D. Brelaz. New methods to color the verticies of a
graph. JACM, 22(4):251-256, 1979.
[3] Rina Dechter, Avi Dechter, and Judea Pearl.

Opti­

mization in constraint networks. In Influence Dia­
grams, Belief Nets and Decision Analysis, pages 411425. John Wiley and Sons Ltd, 1990.
[4] Rina Dechter and Judea Pearl. Network-based heuris­

tics for constraint-satisfaction problems. Artificial In­

telligence, 34:1-34, 1988.
[5] Brendan J. Frey and David J. C. MacKay. A revo­

lution: Belief propagation in graphs with cycles. In

Advances in Neural Information Processing Systems,
pages 479-485, 1998.

IEEE Transactions on Pattern Matching and Machine
Intelligence, 2(4):362-369, 1980.
[15] Noam M. Shazeer, Michael L. Littman, and Greg A.

Keirn. Constraint satisfaction with probabilistic vari­
able values. Technical Report CS-99-03, Duke Uni­
versity, Department of Computer Science, 1999.
[16] Matt Vemooy and William S. Havens. An examina­

tion of probabilistic value-ordering heuristics. In Pro­

ceedings of the 12th Australian Joint Conference on
Artificial Intelligence, 1999.
[17] Nevin Lianwen Zhang and David Poole.

Exploit­
ing Causal Independence in Bayesian Network In­
ference. Journal of Artificial Intelligence Research,
5:301-328, 1996.

