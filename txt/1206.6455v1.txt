Regularizers versus Losses for Nonlinear Dimensionality Reduction

Yaoliang Yu, James Neufeld, Ryan Kiros, Xinhua Zhang, Dale Schuurmans
Department of Computing Science, University of Alberta, Edmonton, AB T6G 2E8 Canada

Abstract
We demonstrate that almost all nonparametric dimensionality reduction methods can be expressed by a simple procedure:
regularized loss minimization plus singular
value truncation. By distinguishing the role
of the loss and regularizer in such a process, we recover a factored perspective that
reveals some gaps in the current literature.
Beyond identifying a useful new loss for manifold unfolding, a key contribution is to derive new convex regularizers that combine
distance maximization with rank reduction.
These regularizers can be applied to any loss.

1. Introduction
Dimensionality reduction is an ubiquitous and important form of data analysis. Recovering the inherent
manifold structure of dataâ€”i.e. the local directions
of large versus minimal variationâ€”enables useful representations based on encoding highly varying directions. Not only can this reveal important structure
in data, and hence support visualization, it also provides an automated form of noise removal and data
normalization that can aide subsequent data analysis.
Although linear dimensionality reduction is a well
studied topic, recent progress continues to be made
with the investigation of convex regularizers, such as
the trace norm or 2,1-norm, that enable application
to general losses beyond squared error (Candes et al.,
2011; Xu et al., 2010; Srebro & Shraibman, 2005). The
literature on nonlinear dimensionality reduction, by
comparison, has grown more rapidly yet devoted relatively less attention to developing appropriate regularizers. This provides one of our main motivations.
The focus of this paper is on unsupervised dimensionality reduction; that is, we will not directly address suAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012.
Copyright 2012 by the author(s)/owner(s).

pervised variants, e.g. (Weinberger & Saul, 2009). We
will also focus on non-parametric formulations that
do not require an explicit map connecting the low
and high dimensional representations; e.g. as in restricted Boltzmann machines (Larochelle & Bengio,
2008), auto-encoders (Rifai et al., 2011), or parameterized kernel reductions (Wang et al., 2010).
The primary benefit of focusing on unsupervised, nonparametric formulations is that it allows a simple yet
comprehensive overview of current methods. In particular, we observe that nearly all current non-parametric
methods can be expressed as regularized loss minimization of a reconstruction matrix followed by singular value truncation. This perspective allows us to
distinguish the role of the loss from that of the regularizer : the loss relates the learned reconstruction
to the data, whereas the regularizer relates the reconstruction to the desired topology independent of the
data. Such a separation allows a simpler organization
of current methods than current overviews (Burges,
2010; Lee & Verleysen, 2010a). More importantly,
it reveals new research directions. A brief overview
of current losses, for example, reveals a useful alternative that remains uninvestigated. Similarly, an assessment of current regularizers reveals that very few
have been explored: in fact, only one family of convex regularizers has been widely used in the nonlinear
case (distance maximization), which has known weaknesses. Although non-convex regularizers have been
proposed to mitigate these weaknesses, these introduce
intractability. Our main contribution is to derive efficient new convex regularizers that are able to combine
distance maximization with rank reduction.

2. Preliminaries
Below we will need to manipulate and relate data, kernel, and Euclidean distance matrices respectively. Assume one is given t observations, either expressed as
a t Ã— n data matrix X; a t Ã— t kernel matrix K where
K = K 0 and K < 0; or a t Ã— t squared Euclidean distance matrix D where D = D0 , D â‰¥ 0, Î´(D) = 0
and HDH 4 0, such that Î´ denotes diagonal and

Regularizers versus Losses for Nonlinear Dimensionality Reduction

H = I âˆ’ 1t 110 denotes the centering matrix. Then
we can map between these various matrices via
K(X)

=

XX 0

(1)

D(X)

=

Î´(XX 0 )10 + 1Î´(XX 0 )0 âˆ’ 2XX 0

(2)

D(K)

=

Î´(K)10 + 1Î´(K)0 âˆ’ 2K

(3)

=

âˆ’ 12 HDH

(4)

K(D)

(where the function intended is determined by the argument). Note that it is easy to map a data matrix to
its corresponding kernel or Euclidean distance matrix,
but such a map is neither linear nor invertible: kernel
matrices drop orientation, while Euclidean distance
matrices drop orientation and translation. However,
by centering the data or kernel matrix, thus removing
translation information, the relationship between kernel and Euclidean distance matrices becomes simple.
Proposition 1 A linear bijection exists between centered kernel and squared Euclidean distance matrices.
It is easy to verify that D(K(D)) = D and K(D(K)) =
HKH for a valid Euclidean distance and kernel matrix respectively. Therefore if f (D) is convex in D then
f (D(K)) must be convex in K. Proposition 1 thus allows one to equivalently re-express problems in terms
of centered kernel matrices or Euclidean distance matrices without affecting expressiveness or convexity.
In this paper we assume the target dimensionality d is
fixed beforehand. That is, we are not addressing the
problem of estimating the intrinsic dimensionality; for
a survey see (Lee & Verleysen, 2010a, Ch.3). We will
also need to make use of the indicator function

0
if the predicate Ï† is true
[[Ï†]] =
. (5)
âˆž if the predicate Ï† is false

3. Background: Linear Case
First briefly consider linear dimensionality reduction,
which illustrates some basic points. Here one is given a
data matrix X and seeks a reduced rank representation
XÌ‚. It turns out that a simple, generic strategy covers
almost all methods that have been proposed: First,
solve the regularized loss minimization problem
min L(XÌƒ; X) + R(XÌƒ)

(6)

XÌƒ

for a given loss L and regularizer R, obtaining the
reconstruction XÌƒ. Then recover the low rank representation XÌ‚ by truncating all but the top d singular
0
values of XÌƒ; that is, XÌ‚ = UÌƒ:,1:d Î£Ìƒ1:d,1:d VÌƒ:,1:d
such that
0
UÌƒ Î£ÌƒVÌƒ = XÌƒ is the singular value decomposition. Note
that the loss relates XÌƒ to the data, X, whereas the regularizer enforces assumptions on the reconstruction XÌƒ

that are independent of the data. Interestingly, it is
the regularizer, not the loss, that typically determines
the computational difficulty of this problem.
Regularizers: The role of the regularizer is to encourage the desired topology. To illustrate, consider
the common regularizers proposed for the linear case
[[rank(XÌƒ) â‰¤ d]]

R1 (XÌƒ)

=

R2 (XÌƒ)

= Î±kXÌƒktr

R3 (XÌƒ)

(7)
(8)

0

= Î±kXÌƒ k2,1

(9)

where Î± â‰¥ 0 is a regularization parameter. All of these
clearly encode a desire for reduced rank.
In particular, the rank indicator (7) is the standard regularizer for spectral dimensionality reduction,
which eliminates the need for truncation. Unfortunately rank is not convex, and enforcing (7) is only
known to be tractable for squared loss (10) (Jolliffe,
1986). For other losses, such as absolute error (11) or
Bregman divergence (12), rank is normally enforced by
means of alternating descent in a factored representation: minAB L(AB; X) where A and B are t Ã— d and
d Ã— n respectively (Collins et al., 2001; Gordon, 2002).
Unfortunately, this cannot guarantee optimality.1
The difficulty of working with rank explains the emergence of convex, rank-reducing regularizers such as the
trace norm (8) (Candes et al., 2011; Srebro & Shraibman, 2005) and block norm (9) (Xu et al., 2010). In
fact, the trace norm is known to be the tightest convex approximation to rank.2 These regularizers allow
a tractable formulation for general convex losses, and
also allow a desired rank to be enforced by appropriately choosing Î± (Cai et al., 2008).
Loss Functions: Despite the importance of regularization, it is interesting to observe that until recently
much of the research effort in linear dimensionality reduction investigated alternative losses, including
L1 (XÌƒ; X) = kX âˆ’ XÌƒk2F

(10)

L2 (XÌƒ; X) = kX âˆ’ XÌƒk1,1

(11)

L3 (XÌƒ; X) = BF (XÌƒkX), such that

(12)
0

BF (XÌƒkX) = F (XÌƒ)âˆ’F (X) âˆ’ tr(âˆ‡F (X) (XÌƒ âˆ’X)). (13)
Here k Â· kF denotes Frobenius norm, k Â· k1,1 denotes
1,1 block norm, and (13) is a Bregman divergence associated with strictly convex potential F . Beyond the
squared error (10) used for PCA (Jolliffe, 1986), the
1

Random projection has also been considered, but
it is difficult to establish guarantees for general losses
(Brinkman & Charikar, 2003).
2
Specifically, it is the bi-conjugate of the rank function
over the unit sphere in spectral-norm (Recht et al., 2010).

Regularizers versus Losses for Nonlinear Dimensionality Reduction

absolute loss (11) has been recently proposed for robust PCA (Candes et al., 2011; Xu et al., 2010), and
Bregman divergences (12) have been implicitly proposed in exponential family PCA (Collins et al., 2001;
Gordon, 2002).3 Interestingly, all these standard loss
functions are convex in XÌƒ.

mensional curved submanifold that is to be â€œunfoldedâ€
into a linear subspace. Unfortunately, current proposals conflate the role of the loss and regularizer in such a
process, and propose only specific combinations of the
two. We find it revealing to consider them separately.

The conclusion we draw from the linear case that the
loss functions considered are standard, convex, and not
the source of computational difficulty. Instead, it is
regularization that has posed the greatest difficulty,
particularly the desire to reduce rank. Interestingly,
for the non-parametric case we find the same holds:
almost every method follows regularized loss minimization plus truncation, almost every loss adopted is standard (if expressed between Euclidean distance matrices), and the main difficulty lies in devising regularizers that encourage desired topology. Such a simple
perspective is surprisingly not widely appreciated.

4.1. Regularizers

4. Nonlinear Case
A general non-parametric approach to dimensionality
reduction can be obtained by expressing the problem
in terms of kernel matrices. (Recall that data or Euclidean distance matrices can always be converted to
kernel matrices.) Here we assume one is given a t Ã— t
kernel matrix K determined by the data and seeks a
reconstruction KÌƒ that allows a reduced rank representation. Here too it turns out that a simple, generic
strategy covers almost all methods that have been proposed: First, solve the regularized loss minimization
min

L(D(KÌƒ); D(K)) + R(KÌƒ)

(14)

KÌƒ=KÌƒ 0 ,KÌƒ<0,KÌƒ1=1

for a given loss L and regularizer R, obtaining the reconstruction KÌƒ. Then recover the low rank representation XÌ‚ by truncating all but the top d eigenvalues and
1/2
factoring; that is, XÌ‚ = QÌƒ:,1:d Î›Ìƒ1:d,1:d , where QÌƒÎ›ÌƒQÌƒ0 = KÌƒ
is the eigenvalue decomposition of KÌƒ.
(The reason for stating the loss in (14) in terms of
D(KÌƒ) will be explained below. Note that if the loss
function L is convex in its first argument it must also
be convex in KÌƒ by Proposition 1.)
Although the difference between the linear and nonparametric formulations does not appear large, (14)
offers far greater flexibility in recovering alternative
topological structures, such as nonlinear manifolds in
the original data. In fact, a key intuition behind most
non-parametric dimensionality reduction methods is
unfolding, where one supposes the data lay on a low di3
Noting the equivalence between regular exponential
families and Bregman divergences (Banerjee et al., 2005).

The natural role for regularization in dimensionality
reduction is to relate the reconstruction KÌƒ to a desired
topology, expressing prior assumptions about the nature of the target representation independent of the
data. For example, one might seek a coordinate representation in a linear subspace, in which case it is desirable to encourage â€œflatteningâ€ by spreading distances.
However, regularization can express other topologies
(see below). As before, the computational challenges
appear to be primarily dictated by the regularizer.
The most common target topology is a linear subspace,
for which the most common regularizers considered are
[[rank(KÌƒ) â‰¤ d]]

R1 (KÌƒ)

=

(15)

R2 (KÌƒ)

= Î²tr(KÌƒ)

(16)

R3 (KÌƒ)

= âˆ’Î±tr(KÌƒ)

(17)

where Î± â‰¥ 0 and Î² â‰¥ 0 are regularization parameters.
For example, the rank indicator (15) is the most commonly used regularizer, often associated with classical
spectral dimensionality reduction (kernel PCA) using
squared error (22) (Schoelkopf et al., 1999; Ham et al.,
2004). Unfortunately, rank is not convex, and efficient
training procedures are not known for other losses. Instead, for other losses, rank is typically enforced by
resorting to local minimization in a factored representation: minXÌ‚ L(D(XÌ‚ XÌ‚ 0 ); D) where XÌ‚ is a t Ã— d representation matrix. Unfortunately, no current loss provides a convex formulation in XÌ‚, and optimal solutions
usually cannot be guaranteed.
Consequently, convex regularizers have also played a
prominent role in non-parametric dimensionality reduction. For example, applying the trace norm here
yields (16). Unfortunately, tr(KÌƒ) = 10 D(KÌƒ)1/(2t)
for centered KÌƒ, thus minimizing trace is equivalent to
shrinking distances; in opposition to the desire to unfold manifolds. Consequently, the negated regularizer
(17) has proved more effective, forming one of the key
components of maximum variance unfolding (MVU)
(28) (Weinberger et al., 2004; 2007). Neither of these
regularizers is completely satisfactory however.
Partitioned regularizer: It is obvious what one
would desire: to spread distances in the top d dimensions and shrink distances in the remaining dimensions, for the target dimensionality d. Such a regu-

Regularizers versus Losses for Nonlinear Dimensionality Reduction

larizer has been proposed by (Shaw & Jebara, 2007):
R4 (KÌƒ) = âˆ’Î± max tr(P KÌƒ) + Î² min tr((I âˆ’ P )KÌƒ) (18)
P âˆˆP

P âˆˆP

= min Î²tr(KÌƒ) âˆ’ (Î± + Î²)tr(P KÌƒ), where (19)
P âˆˆP

P

= {P : P 0 = P, I < P < 0, tr(P ) = d}.

(20)

Unfortunately, (19) is not convex; (Shaw & Jebara,
2007) resort to alternating minimization between KÌƒ
and P . Below we provide new convex regularizers that
approximate (19), providing our main contribution.
Topographic Methods: As an aside, it is interesting to note that alternative topologies can be encouraged via regularization. For example, given a graph
expressed as an adjacency matrix G âˆˆ {0, 1}tÃ—t , a regularizer can encourage KÌƒ to adopt Gâ€™s structure
R5 (KÌƒ) = âˆ’

M âˆˆ{0,1}

max
tÃ—t

tr(M KÌƒM G), (21)
0

which provides a generalized approach to topographic
embedding (Quadrianto et al., 2010; Bishop et al.,
1998). Unfortunately, (21) is concave in KÌƒ and the
inner optimization over M is an NP-hard quadratic
assignment problem, so we do not pursue this further.
4.2. Loss Functions
The role of the loss function is to relate the reconstruction to the data. In the formulation (14) we have
chosen to express losses as between Euclidean distance
matrices. We will see that such a viewpoint, although
nonstandard, provides clarity. For example, unfolding
is naturally enabled by loss locality: errors in reconstructing small target distances should be punished
more harshly than errors in larger distances. Loss
locality allows larger distances in the reconstruction
more leeway to adapt to the desired target topology.
Expressing current losses in terms of distances reveals
that they are almost all standard, convex, and obvious.
We briefly survey standard losses to demonstrate how
broadly the perspective applies to current methods,
and to highlight useful alternatives.
Classical Losses: The oldest losses used for dimensionality reduction do not express locality, and therefore tend to recover linear subspace representations:

âˆš

=

L2 (DÌ‚; D)

=

Local Losses: Unlike the classical losses, however,
local losses encourage unfolding by de-emphasizing errors on large distances. Let N (D) âˆˆ {0, 1}tÃ—t denote
an adjacency function, such that N (D)ij = 1 indicates
that i and j are neighbors in D (i.e. either within a
distance of  or among the k nearest neighbors). The
best known examples of local losses are
L5 (DÌ‚; D)

=

P

âˆ’ DÌ‚ij )2 /Dij

(26)

=

P

âˆ’ DÌ‚ij )2 w(DÌ‚ij )

(27)

=

P

[[N (D)ij = 1 and DÌ‚ij 6= Dij ]] (28)

=

P

N (D)ij (Dij âˆ’ DÌ‚ij )2 .

0

, M 1=1, 10 M =1

L1 (DÌ‚; D)

(Schoelkopf et al., 1999) (recall HDH = âˆ’2K(D)).
Although (23) is frequently mentioned in the multidimensional scaling (MDS) literature (Cox & Cox, 2001),
its use is rare since it cannot be tractably combined
with rank (15) (Dattorro, 2012). The absolute loss
(25) in an important alternative that has been used in
robust MDS (Cayton & Dasgupta, 2006).

kH(D âˆ’ DÌ‚)Hk2F

L6 (DÌ‚; D)
L7 (DÌ‚; D)
L8 (DÌ‚; D)

(23)

L3 (DÌ‚; D)
L4 (DÌ‚; D)

= kD âˆ’ DÌ‚k1,1

(25)

(24)

Here Â· is applied component-wise. These losses are
all convex in DÌ‚ (Dattorro, 2012, Ch.7). The doubly centered squared loss (22) is used in kernel PCA

ij (Dij
ij

ij

(29)

These are the Sammon loss (26) (Sun et al., 2011;
Lee & Verleysen, 2010a); the curvilinear components
loss (27) (Sun et al., 2010); the neighborhood indicator used in MVU and Isomap (28) (Weinberger et al.,
2004; Tenenbaum et al., 2000); and the relaxed loss introduced in regularized MVU (29) (Weinberger et al.,
2007), respectively. All such losses emphasize errors
on small target distances (or predictions) over errors
on large target distances. Moreover, they are all convex in DÌ‚.4 Although the convexity of these losses with
respect to DÌ‚ has not always received significant notice
in the literature, this is an important fact for (14).
Bregman Divergences: Bregman divergences provide another loss specification that emphasizes locality. Recall that a Bregman divergence is defined by
BF (DÌ‚kD) = F (DÌ‚) âˆ’ F (D) âˆ’ tr(âˆ‡F (D)0 (DÌ‚ âˆ’ D)) for
a strictly convex differentiable potential F , which by
construction must be convex in DÌ‚. A number of such
divergences have proved to be important in the dimensionality reduction literature, including
BF9 (DÌ‚ij kDij ) = DÌ‚ij (log DÌ‚ij âˆ’log Dij )+Dij âˆ’ DÌ‚ij (30)
BF10 (DÌ‚ij kDij ) = exp(âˆ’DÌ‚ij /Ïƒ) âˆ’ exp(âˆ’Dij /Ïƒ)

(22)

kD âˆ’ DÌ‚k2F
p
âˆš
= k D âˆ’ DÌ‚k2F

ij (Dij

+ exp(âˆ’Dij /Ïƒ)(DÌ‚ij âˆ’ Dij )/Ïƒ

(31)

0

BF11 (DÌ‚i: kDi: ) = p(Di: )(log p(Di: ) âˆ’ log p(DÌ‚i: )) (32)
exp(âˆ’Di: )
(33)
such that p(Di: ) =
exp(âˆ’Di: 1)
4

The convexity of curvilinear components loss (27) deË† for example exp(âˆ’d/Ïƒ),
Ë†
pends on w(d);
1(dâ‰¤)
, or 1/dË† (Lee
Ë†
& Verleysen, 2010a). The latter makes (27) convex.

Regularizers versus Losses for Nonlinear Dimensionality Reduction

BF12 (DÌ‚kD) = tr(p(D)0 (log p(D) âˆ’ log p(DÌ‚))) (34)
exp(âˆ’D)
such that p(D) = 0
, (35)
1 exp(âˆ’D)1
where Ïƒ > 0 is a scale parameter, log and exp are
applied component-wise, and it is assumed the divergences are summed over all ij or all i where necessary.
The unnormalized entropy (30) was proposed in (Sun
et al., 2011) to approximate the Sammon loss (26),
whereas the reciprocal exponential Bregman divergence (31) was proposed in (Sun et al., 2010) to approximate the curvilinear components loss (27) under
Ë† = exp(d/Ïƒ).
Ë†
w(d)
However, the latter approximation
was achieve by placing DÌ‚ in the second position, using
`(DÌ‚ij ; Dij ) = BF (Dij kDÌ‚ij ), which is no longer convex
(see below). The Bregman divergence (32) matches
the loss used in SNE (van der Maaten & Hinton, 2008)
up to a minor variation: the transfer p in SNE does
not normalize over the entire vector, but only over the
vector minus the current entry. The later, symmetric SNE error can also be recovered (almost) from the
matrix-wise Bregman divergence (34) up to the same
minor variation, plus a second exception: even though
p(DÌ‚) is computed as in (35), p(D) is computed by averaging the column and row probabilities through ij
using (33).
Surprisingly, the exponential divergence (31) has not
previously been used with DÌ‚ in the first, convex position. This yields a highly localized loss that is
well suited to manifold unfolding, demonstrating even
stronger locality than the other divergences. Therefore,
we investigate its behavior further below.
Large Margin Losses: Large margin losses for nonlinear dimensionality reduction have also been proposed in (Shaw & Jebara, 2009):
L13 (DÌ‚;D) =

X

max NÌ„ (D)ij (max N (D)ik DÌ‚ik âˆ’ DÌ‚ij )

i

j

to make sure that the sum of estimated distances on
N (D) are less than on any alternative adjacency matrix in N, plus a margin dictated by how far N âˆˆ N is
from N (D). Both losses are convex.
Non-convex Losses: Finally, even though nonconvex losses are computationally problematic, a few
important methods are expressible in this manner:
P
L15 (DÌ‚; D) = ij p(D)ij (log p(D)ij âˆ’log q(DÌ‚)ij ) (38)
where q(DÌ‚)ij = P

(1 + DÌ‚ij )âˆ’1

k6=l (1

+ DÌ‚kl )âˆ’1

(39)

L16 (DÌ‚; D) = âˆ’
max
W :Î´(W )=0,WNÌ„ =0,W 1=1


tr (I âˆ’ W )((1 âˆ’ Ï)D + ÏDÌ‚)(I âˆ’ W )0 (40)
where Ï â‰¥ 0 is a weighting parameter. The first loss
corresponds to tSNE (38), which is a modification of
the symmetric SNE loss (34), using the same transfer
p(D) on the target distances D, but using a â€œheavier
tailedâ€ transfer function on DÌ‚ (van der Maaten & Hinton, 2008). The second loss (40) can be shown to be
equivalent to local linear embedding (LLE) (Roweis &
Saul, 2000; Saul & Roweis, 2003; Ham et al., 2004) if
one tracks the solution in the limit as Ï & 0. Clearly,
this formulation shows that the LLE loss is highly nonconvex in DÌ‚.
Note that other non-parametric dimensionality reduction methods can also be expressed in terms of regularized minimization of a loss between distance matrices,
but the above suffices to illustrate how comprehensively current methods can be covered. Interestingly,
this loss-based framework generalizes probabilistic formulations (Lawrence, 2011), since e.g. large margin
losses cannot be naturally expressed as log-likelihoods.

5. New Convex Regularizers

k

where NÌ„ (D)ij = 1 âˆ’ N (D)ij

(36)

L14 (DÌ‚;D) = max `(N, N (D)) + tr(DÌ‚(N (D)âˆ’N )) (37)
N âˆˆN

where ` is a local margin loss function. The intuition
behind the loss (36) is simple: for each node i, one
would like the distances to all disconnected nodes j
(such that N (D)ij = 0) to be greater than the distance to the furthest connected node k (i.e. such that
N (D)ik = 1). The second loss (37) is defined with
respect to a class of alternative adjacency matrices N
producible by running an efficient dynamic program
on candidate distance matrices DÌ‚. This loss, termed
â€œstructure preservingâ€ in (Shaw & Jebara, 2009), behaves like a structured output SVM loss that tries

Our main contribution is to propose two new convex
regularizers for non-parametric dimensionality reduction. In particular, we formulate convex relaxations of
the partitioned regularizer (19), which simultenously
seeks to spread distances on the top d dimensions while
shrinking distances in the remaining directions. The
consequences for both rank reduction and manifold unfolding are clear. We first introduce a slight modification of (19) by adding a small quadratic smoother
R5 (KÌƒ) = min
P âˆˆP

Î³
tr(KÌƒ 2 )+Î²tr(KÌƒ)âˆ’(Î±+Î²)tr(P KÌƒ), (41)
2

where Î³ > 0, and P is the same as defined previously
in (20). Although this modified regularizer is still not
convex in KÌƒ, it enables two useful relaxations.

Regularizers versus Losses for Nonlinear Dimensionality Reduction

5.1. Completed Square
The first relaxation we propose is extremely simple:
(41) can be made jointly convex in KÌƒ and P simply
by completing the square, yielding
R6 (KÌƒ)

=

min Î²tr(KÌƒ) +

P âˆˆP

Î³
KÌƒ âˆ’
2

Î±+Î²
Î³ P

2

. (42)

The main benefit of this relaxation is that it is extremely simple and computationally attractive: a simple modification of the alternating minimization strategy of (Shaw & Jebara, 2007) now yields a global solution. This does not, however, yield the tightest convex
approximation of (41), as we now demonstrate.
5.2. Bi-conjugation
Recall that the conjugate of a function f is defined by
f âˆ— (y) = supxâˆˆdom(f ) hx, yi âˆ’ f (x) (Borwein & Lewis,
2006). Importantly, any function is lower bounded
by its bi-conjugate; i.e. f â‰¥ f âˆ—âˆ— (Borwein & Lewis,
2006, Â§4.2). Therefore, a general strategy for deriving
maximal convex lower bounds on objective functions
can be based on Fenchel bi-conjugation (Jojic et al.,
2011). Here, we obtain a new regularizer by formulating the tightest convex relaxation of (41) based on its
bi-conjugate and define
=

R5âˆ—âˆ— (KÌƒ).

g âˆ—âˆ— (U ) = maxt tr(U Z) âˆ’ g âˆ— (Z)

(43)

By construction, this must be a convex function in KÌƒ.
Theorem 1
R5âˆ—âˆ— (U ) = max0 tr(U Z)

= maxt min min tr(U Z) âˆ’ tr(ZK)
ZâˆˆS P âˆˆP K<0

Î³
+ tr(K 2 ) + Î²tr(K) âˆ’ (Î± + Î²)tr(KP )
2
= maxt min tr(U Z) âˆ’
ZâˆˆS P âˆˆP

max tr[K(Z âˆ’ Î²I + (Î± + Î²)P )] âˆ’
K<0

= maxt tr(U Z) âˆ’
ZâˆˆS

(44)

where [Â·]+ = max(0, Â·) is applied to the eigenvalues.
Proof: We compute the Fenchel bi-conjugate of
(
R5 (K) if K < 0
g(K) =
.
(45)
âˆž
otherwise
First, the conjugate of g(K) is easily derived as
g âˆ— (Z) = max tr(Z 0 K) âˆ’ R5 (K)
K<0

Î³
tr(K 2 ) âˆ’ Î²tr(K)
2
+ (Î± + Î²)tr(KP ).
(47)

= max max tr(Z 0 K) âˆ’
K<0 P âˆˆP

(46)

Î³
kKk2F (49)
2

1
max k[Z âˆ’Î²I +(Î±+Î²)P ]+ k2F .
2Î³ P âˆˆP

The last equality is due to von Neumannâ€™s trace inequality (Borwein & Lewis, 2006) and the elementary
1
(y)2+ .
equality maxxâ‰¥0 xy âˆ’ Î³2 x2 = 2Î³
Putting Î³ = 0, Theorem 1 implies that the Fenchel
biconjugate of (19) is exactly (17) (for Z = âˆ’Î±I) if
d > 0 and is (16) (for Z = Î²I) if d = 0, which might
explain the success of MVU (Weinberger et al., 2004).
Although (44) appears to be a complex regularizer, it
is not computationally much harder to optimize than
(42). To evaluate R5âˆ—âˆ— (U ), an optimal Z in (44) must
be solved, but this is a convex problem and admits
efficient optimization. First note that the inner maximization in (44) necessarily attains its maximum at
some extreme point of the set P (for we are maximizing
a convex function). Next inspecting (49) and invoking
von Neumannâ€™s trace inequality one more time we reduce (44) to its vector counterpart. Let {zi } and {ui }
(both arranged in decreasing order) be the eigenvalues
of Z and U , respectively, then we just need to solve:

Z=Z

1
âˆ’ max k[Z âˆ’Î²I +(Î±+Î²)P ]+ k2F ,
2Î³ P âˆˆP

(48)

ZâˆˆS

F

This is guaranteed to be an upper bound on (41) since
2
we are merely adding a nonnegative term Î³2 k Î±+Î²
Î³ P kF .
A lower bound on (41) can be recovered by subtracting
d(Î±+Î²)2
from (42) since kP k2F â‰¤ d for all P âˆˆ P.
2Î³

R7 (KÌƒ)

Note that the domain of g âˆ— is actually all tÃ—t matrices,
but (47) implies that g âˆ— (Z) = g âˆ— (Z 0 ) = g âˆ— ((Z+Z 0 )/2),
hence for the purpose of computing g âˆ—âˆ— no generality
is lost if we restrict the domain of g âˆ— to symmetric
matrices. Now for any U < 0 we obtain

min âˆ’(2Î³)u0 z +

zi â‰¥zi+1

d
t
X
X
[zi + Î±]2+ +
[zi âˆ’ Î²]2+ . (50)
i=1

i=d+1

Temporarily ignoring the constraints, the optimal z is
obvious since all elements of z are separated in (50):
(
Î³ui âˆ’ Î± if i â‰¤ d
zÌ‚i =
.
(51)
Î³ui + Î² if i â‰¥ d + 1
Now observe that zÌ‚i automatically satisfies the order
constraints in the two blocks, thus the only possibility
of violating the order constraint is between the blocks,
i.e. zÌ‚d < zÌ‚d+1 . But if we knew zÌ‚d = Î», we would be
able to fix the order by setting
(
max(zÌ‚i , Î») if i â‰¤ d
zi (Î») =
.
(52)
min(zÌ‚i , Î») if i â‰¥ d + 1

Regularizers versus Losses for Nonlinear Dimensionality Reduction

Data set

Loss
rMVU
Swiss
SNE
expBreg
rMVU
Face
SNE
expBreg
rMVU
3d-sin
SNE
expBreg
rMVU
Brush
SNE
expBreg
mean

tr(K)
246M (33)
7.82 (42)
0.00 (1)
36K (15)
1.31 (31)
0.27 (0.6)
112K (25)
1.22 (35)
0.39 (1)
252
(2)
0.99 (2)
0.06 (0.2)
204M

âˆ’tr(K)
1.00 (0.2)
1.00 (14)
1.00 (1)
1.00 (0.2)
1.00 (12)
1.00 (0.3)
1.00 (0.1)
1.00 (4)
1.00 (0.5)
1.00 (0.3)
1.00 (0.4)
1.00 (0.1)
1.00

partition
1.00 (5)
1.69 (4)
0.47 (5)
1.00 (3)
1.69 (6)
1.00 (3)
1.00 (6)
2.00 (2)
1.01 (2)
1.61 (0.4)
2.43 (0.7)
0.55 (0.6)
1.29

partition(Î³)
1.00 [0.0] (5)
1.69 [1K] (4)
0.47 [1.3] (5)
1.00 [0.01] (3)
1.69 [974] (6)
1.00 [0.03] (2)
1.00 [0.01] (4)
2.00 [757] (2)
1.01 [2.4] (2)
1.61 [0.3] (0.4)
2.43 [38] (0.6)
0.55 [1.1] (0.6)
1.29

bi-conjugate
0.33 [-1.7] (6)
0.61 [143] (100)
0.00 [-1.7] (6)
0.64 [-0.6] (47)
1.03 [486] (35)
0.27 [-0.1] (1)
0.25 [-2.0] (6)
0.76 [116] (29)
0.41 [-1.6] (3)
0.41 [-1.2] (1)
0.98 [-14] (7)
0.07 [-1.3] (1)
0.48

compl. sq.
0.95 [15] (6)
0.64 [442] (74)
0.00 [15] (7)
0.82 [17] (2)
0.83 [503] (32)
0.34 [17] (2)
0.80 [19] (7)
0.70 [296] (23)
0.42 [20] (2)
0.42 [59] (0.3)
0.79 [70] (2)
0.07 [59] (0.4)
0.57

Table 1. Relative reconstruction losses using different losses (SNE (34), regularized MVU loss (29), exponential divergence
(31)) and six different regularizers (from left to right: (16), (17), (19), (41), (44), (42)). The trace maximizing regularizer
(17) is being used as the reference. In each case d = 2, Î± = 1, Î² = 0.1, Î³ = 10âˆ’3 , and losses were scaled by 10. In the
three leftmost columns, the number (s) in parentheses gives the run time in seconds. In the three rightmost columns, the
numbers [o](s) in the parentheses give the objective function value [o] and run time in seconds (s) respectively.

Therefore we only need to find Î» that minimizes (50),
which is a univariate convex piecewise quadratic function in Î», hence can be done very quickly.
To summarize, for evaluating R5âˆ—âˆ— (U ) (and obtaining a
subgradient), we need to perform SVD on U and then
solve (50). The former costs O(t3 ) while the latter
costs O(t), where t is the number of points.

6. Experimental Evaluation
Most evaluations of dimensionality reduction methods
resort to subjective assessments of specific case studies.
However, many proposals exist for quantitatively evaluating the quality of different methods in a somewhat
objective manner (Sun et al., 2011; Lee & Verleysen,
2010b; van der Maaten, 2009). Evaluation is clearer
when the regularizer and loss components are considered separately. In particular, to compare regularizers, an objective assessment can be based on the loss
values achieved by the low-rank reconstruction. For
a given loss function L, we measure the reconstruction loss L(D(XÌ‚ XÌ‚ 0 ); D) achieved by the recovered low
rank representation XÌ‚. Another objective assessment
can be based on the run time of the corresponding
methods.5 Finally, we can assess the quality of the
convex relaxations by measuring the gap between the
final objective values achieved using the relaxed regularizers versus their source regularizer R5 (41).
5

Here we measure run time using a common implementation based on accelerated projected subgradient descent
(Tseng, 2008). Accelerated projected subgradient descent
proves to be a far more scalable and generally applicable
method for these problems than a generic SDP solver.

In our experiments, we compare different regularizers
on a representative set of loss functions. The losses
we considered are: regularized MVU loss (29), SNE
(34), and the reciprocal exponential Bregman divergence (31). The results are given in Table 1 for six regularizers and three losses on four different data sets.6
The regularizers used are R2 (16), R3 (17), R4 (19),
R5 (41), R6 (42), and R7 (44). Additionally, we report
the objective values for R5 and its relaxations R6 and
R7 , to assess the approximation gap.
In terms of reconstruction error, the new convex regularizers obtain the best reconstruction errors overall,
almost always outperforming the other competitors.
A particular disadvantage of the partitioned regularizers is that they are not convex, hence sensitive to initialization: the objective values (shown in the square
brackets in the rightmost three columns) demonstrate
that inferior local minima do exist, since the completed
square regularizer (R6 ) provides an upper bound on
the optimal partition(Î³) objective. The run times for
the new regularizers are somewhat slower.

7. Conclusion
We have presented simple view of non-parametric dimensionality reduction by separating the roles of the
regularizer and loss function. The result is a compact survey of a large fraction of the literature, which
reveals a natural loss function that has not been thoroughly explored. More importantly, we developed two
new convex relaxations of a useful, but non-convex reg6

Data set details are given in the supplement.

Regularizers versus Losses for Nonlinear Dimensionality Reduction

ularizer. We investigated the behavior of the new regularizers across a representative sample of loss functions. Important directions for future research include
extending the convex regularization framework to consider other target topologies, sparsity, and mixtures.

References
Banerjee, A., Merugu, S., Dhillon, I., and Ghosh, J. Clustering with Bregman divergences. Journal of Machine
Learning Research, 6:1705â€“1749, 2005.
Bishop, C., Svensen, M., and Williams, C. GTM: the generative topographic mapping. Neural Computation, 10
(1):215â€“234, 1998.
Borwein, J. and Lewis, A. Convex Analysis and Nonlinear
Optimization. Springer, 2nd edition, 2006.
Brinkman, B. and Charikar, M. On the impossibility of
dimension reduction in l1 . In Proc. STOC, 2003.
Burges, C. Dimension reduction: a guided tour. Foundations and Trends in Machine Learning, 2:275â€“365, 2010.
Cai, J., Candes, E., and Shen, Z. A singular value thresholding algorithm for matrix completion. SIAM Journal
on Optimzation, 20:1956â€“1982, 2008.

Quadrianto, N., Smola, A., Song, L., and Tuytelaars, T.
Kernelized sorting. IEEE PAMI, 32:1809â€“1821, 2010.
Recht, B., Fazel, M., and Parrilo, P. Guaranteed minimumrank solutions of linear matrix equations via nuclear
norm minimization. SIAM Review, 52:471â€“501, 2010.
Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio,
Y. Contractive auto-encoders: explicit invariance during
feature extraction. In ICML, 2011.
Roweis, S. and Saul, L. Nonlinear dimensionality reduction
by locally linear embedding. Science, 290(5500):2323â€“
2326, 2000.
Saul, L. and Roweis, S. Think globally, fit locally: unsupervised learning of low dimensional manifolds. Journal
of Machine Learning Research, 4:119â€“155, 2003.
Schoelkopf, B., Smola, A., and Mueller, K. Kernel principal
component analysis. In NIPS 12, 1999.
Shaw, B. and Jebara, T. Minimum volume embedding. In
AISTATS, 2007.
Shaw, B. and Jebara, T. Structure preserving embedding.
In ICML, 2009.
Srebro, N. and Shraibman, A. Rank, trace-norm and maxnorm. In COLT, 2005.

Candes, E., Li, X., Ma, Y., and Wright, J. Robust principal
component analysis? JACM, 58:1â€“37, 2011.

Sun, J., Crowe, M., and Fyfe, C. Curvilinear components
analysis and Bregman divergences. In ESANN, 2010.

Cayton, L. and Dasgupta, S. Robust Euclidean embedding.
In ICML, 2006.

Sun, J., Crowe, M., and Fyfe, C. Extending metric multidimensional scaling with Bregman divergences. Pattern
Recognition, 44:1137â€“1154, 2011.

Collins, M., Dasgupta, S., and Schapire, R. A generalization of principal component analysis to the exponential
family. In NIPS 14, 2001.
Cox, T. and Cox, M. Multidimensional Scaling. Chapman
and Hall, 2nd edition, 2001.
Dattorro, J. Convex Optimization and Euclidean Distance
Geometry. Meboo Publishing, 2012.

Tenenbaum, J., de Silva, V., and Langford, J. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319â€“2323, 2000.
Tseng, P. On accelerated proximal gradient methods for
convex-concave optimization. Submitted to SIAM Journal on Optimization, 2008.

Gordon, G. Generalized2 linear2 models. In NIPS 15, 2002.

van der Maaten, L. Learning a parametric embedding by
preserving local structure. In AISTATS, 2009.

Ham, J., Lee, D., Mika, S., and Schoelkopf, B. A kernel
view of dimensionality reduction of manifolds. In ICML,
2004.

van der Maaten, L. and Hinton, G. Visualizing data using
t-SNE. J. Mach. Learn. Research, 9:2579â€“2605, 2008.

Jojic, V., Saria, S., and Koller, D. Convex envelopes of
complexity controlling penalties. In AISTATS, 2011.
Jolliffe, I. Principal Components Analysis. Springer, 1986.
Larochelle, H. and Bengio, Y. Classification using discriminative restricted Boltzmann machines. In ICML, 2008.
Lawrence, N. Spectral dimensionality reduction via maximum entropy. In AISTATS, 2011.

Wang, M., Sha, F., and Jordan, M. Unsupervised kernel
dimension reduction. In NIPS 23, 2010.
Weinberger, K. and Saul, L. Distance metric learning for
large margin nearest neighbor classification. Journal of
Machine Learning Research, 10:207â€“244, 2009.
Weinberger, K., Sha, F., and Saul, L. Learning a kernel
matrix for nonlinear dimensionality reduction. In ICML,
2004.

Lee, J. and Verleysen, M. Nonlinear Dimensionality Reduction. Springer, 2010a.

Weinberger, K., Sha, F., Zhu, Q., and Saul, L. Graph
Laplacian regularization for large-scale semidefinite programming. In NIPS 19, 2007.

Lee, J. and Verleysen, M. Scale-independent quality criteria for dimensionality reduction. Pattern Recognition
Letters, 31:2248â€“2257, 2010b.

Xu, H., Caramanis, C., and Sanghavi, S. Robust PCA via
outlier pursuit. In NIPS 23, 2010.

