119

Efficient Estimation of the Value of
Information in Monte Carlo Models

Tom Chavez l,2 and Max Henrion l,3

1 RockweJI International Science Lab,

444 High St., Palo Alto, CA 94301

2 Department of Engineering-Economic Systems, Stanford University
3 The Institute for Decision Systems Research, Inc.,

Abstract The expected value of information

(EVI) is

the most powerful measure of sensitivity to uncer­
tainty in a decision model:

it measures the potential

of information to improve the decision, and hence
measures the expected value of the outcome. Stan­

350 Cambridge Ave., #380, Palo Alto, CA 94306

model affect its outputs most significantly. In this way, it
helps a decision maker focus attention on what assump­
tions really matter. It also helps a decision modeler to
assign priorities to his efforts to improve, refine, or extend

dard methods for computing EVI use discrete vari­

his model by identifying those variables for which it will

that contain more than a few variables. Monte Carlo

more knowledgeable experts, or to build more elaborate

ables and are computationally intractable for models

simulation provides the basis for more tractable

evaluation of large predictive models with continu­
ous and discrete variables, but so far computation of

be most valuable to find more complete data, to interview
submodels.

EVI in a Monte Carlo setting also has appeared

The expected value of information (EVI) on a variable xi

based on preposterior analysis for estimating EVI in

information about xi and make a decision with higher­

impractical. We introduce an approximate approach

Monte Carlo models. Our method uses a linear

approximation to the value function and multiple
linear regression to estimate the linear model from
the samples. The approach is efficient and practical
for extremely large models.

It allows easy estima­

tion of EVI for perfect or panial information on

individual variables or on combinations of variables.
We illustrate its implementation within Demos (a
decision modeling system) , and its application to a
large model for crisis transponation planning.

measures the expected increase in value y if we learn new

expected value in light of that information. It is the most
powerful method of sensitivity analysis because it ana­
lyzes a variable's importance in terms of the overall pre­
scription for action, and it expresses that importance in the
utility or value units of the problem. Other methods, such
as rank-order correlation, express importance in terms of
the correlation between an uncertain variable and the out­
put of the decision model. There are many cases where a
variable can show high sensitivity in this way, yet still

1.0

EVI: What's so, and What's New

A ny model is inevitably a simplification of reality, and
most of its input quantities are

invariably uncertain. Sensi­

tivity analysis identifies which sources of uncertainty in a

have no effect on the selection of an optimal decision.
Deterministic perturbation measures importance in utility
or value units, but it ignores nonlinearities and interactions
among variables, and also fails to measure a variable's

importance in te rms of that variable's ability to change the
recommended decision.

120

Chavez and Henrion

One calculates EVPI (Expected Value of Perfect Infor·
mation) in discrete models by rolling back the decision

tree. The computation itself is straightforward in the sense

that, to compute EVI, one simply places at the front of the
tree the chance variables to be observed. The EVPI is
computed as the difference between the expected value
computed for this scenario and the expected value for the
regular tree, without observations.
Computing EVI with continuous variables is less intuitive,
because we have no tidy way of reversing the uncertainty,

2.0

Framework

A decision model consists of a set of n state variables

x1, ,xn, which we will denote by X. The decision maker
••

has control of a decision variable D, which can assume

one of m possible values d1 ,.. ,dm. The value or utility func­
tion v(X,di) expresses the payoff to the decision maker

when X obtains and decision di is chosen.

In a typical decision model, the state variables are uncer­

tain. We express prior knowledge about X in the form of a

unlike the discrete case. Yet continuous models are

probability distribution, denoted {XI �},where �denotes

increasingly the norm for risk and decision analysis, first

a prior state of information. The optimal Bayes' decision
1
maximizing the expected value is given by

because discretizing inherently continuous variables intro­

l = Arg

duces unnecessary approximation, and second because
Monte Carlo methods and their variants (e.g., Latin hyper­
cube) generate tractable, highly efficient solutions to pre­
dictive models that contain thousands of variables. An
especially useful feature of the Monte Carlo method is
that, for a specified error,the computational complexity

variable x , denoted d

d• x

[Morgan and Henrion, 1990]. Exact methods require com­
putation time that is exponential in the number of vari­

for computing EVI on continuous variables in a Monte
pute EVI on single variables or on any combination of

__

*

_.,

is

Arg max
(v(X,d)Jx,�)
d

EVPI (x) = (v (X, l x) I �)- {v (X, d*) I �).

In a similar fashion, we define the optimal expected­
value decision given the revelation of evidence e,

d*e• as

l =Arg max (v(X,d )Je.�)
e
d

variables, and (2) the ability to compute both perfect and
partial values of information. Perfect information
removes uncertainty entirely. Partial information reduces
uncertainty.

'

We define EVPI on x as

There is thus a need to develop flexible, efficient methods
Carlo setting. A ftexible method has (I) the ability to com­

d

l' )
(v ( X d) ..,
I .

The optimal decision given perfect information on state

increases linearly in the number of uncertain variables

ables.

max

Then the EVI for evidence e is

EVI(e) = (v(X,d.e)l�)-(v(X,l)l�)

We present a general framework for calculating EVI based
on preposterior analysis. Using that framework,we
develop a technique for computing EVI that depends on a
linear approximation to the value function and on multiple
linear regression to estimate the constants for the linear
function. We also discuss a heuristic method for measuring
the value of partial information in terms of what we call

2.1

Binary decisions and Function z

Let us consider a simplified decision problem with two
decision alternatives: one of them is the optimal Bayes'

d*; the other we denote tr".

the relative information multiple (RIM). We have

decision

implemented these methods in detachable computational

In view of the uncertainty in the state variables, there must

modules using Demos, a decision modeling system from

exist uncertainty in the outputs as well. Thus, for each

Lumina, Inc., Palo Alto, CA. We demonstrate their use on
a large model to aid in military transportation crisis plan­
ning.

1. We use Howard's inferential notation (see, for example,
Howard, 1970). {XIS) denotes the probability densit y of X condi­
tional on S; (XIS) denotes the expectation of X conditio nal on
S.

121

Efficient Estimation of the Value of Information in Monte Carlo Models

decision

d;, there exists a unique probability distribution

on value { v(X,d;)l �} (see Figure 1). For notational conve­

ing cY instead of

d'; its probability is just its correspond­

ing value on the density curve. Therefore, we have that

nience we let

v(d)

=

(v(X, d) I�).

EVPI

0

=

f lzl {zl �} dz.

(EQ 1)

We now define
z

=

v(X,l) -v(X,tf).

2.2

Preposterior analysis helps us to calculate the effect on X

Function z is the pivotal element in our framework for
computing EVI because it describes the difference in value
between the best and second-best decisions. In Figure
we have graphed the probability distribution of z. The

2,

shaded area represents the total probability of making a
bad decision, i.e., doing

d' when cY would yield higher

value. Exploiting information encoded in the shaded, neg­

ative portion of the z distribution's curve will provide the
necessary clues to compute EVPI and EVI.

FIGURE 1.

Pro

density

Preposterior Analysis

of our seeing evidencee, given a prior state of information

�. At the heart of preposterior analysis is the specification
of a preposterior distribution, which is a a prior proba­
bility distribution on a posterior mean. Probability theory
provides a principled basis for calculating a preposterior
distribution, given a prior and an adequate means of speci­
fying the effects of learning new information.
How do we represent perfect information on a continuous

Probability �istributions on value for the two
decisions d and cr-.

random variable

X? If X were known with certainty, then

its variance would be equal to 0. Thus, we can think of evi­

dence

e

as an information-gathering activity that somehow

reduces the variance of X to

{v(X,tf)[�} {v(X,l)[s}

0. Evidencee that provides

partial information reduces the variance on the prior of X,

without shrinking that prior to

0.

The following lemma, taken from basic probability theory,
is known as the conditional expectation formula.
Lemma 1:

(XIs>

=

((X] e)l �) .

A further useful result is the following lemma, which

FIGURE 2.

Function z: the difference in value between
the best and second-best decisions.

gives the formula for conditional variance.
Lemma 2: Var(
Let J.l'

*
d best

They

(X] e, �))

=

Var(Xls) -(V a r (XI e) I 1;).

and u' 2 z denote the prior mean and variance of z.

�e computed from our prior uncertainties X and our

value function

v.

If we observe e, then we might ask how

influences z; in particular, we would like to know howe

e

affects J.l' . We will denote the posterior mean of z given
evidence

� by J.l"

z

.The distribution { J.l"

sity on the posterior mean J.l"

density. Substituting J.l"
z

In fact, we can use the intuition behind Figure 2 to write an
expression for the general EVPI, which is EVPI on all
state variables. The absolute value of z in the negative

shaded portion is the utility that we could gain by chaos-

z

z

I�}

is a prior den-

; that is, it is a preposterior

h

for t e inner

(XI e) on the right­

hand side of the equation in Lemma 1 reveals that

(EQ 2)

122

Chavez and Henrion

Eq. 2shows that the mean of the preposterior distribution
{ ll" I � } is the same as the prior mean ll'
z

If cr"

z

.

2 denotes the posterior variance of z after

z

The type of integral given in Eq. 4 is known as a

linear
loss integral. In general, such an integral is impossible to
evaluate analytically, so we must rely on statistical tables

e,

then

or numerical approximatation methods to evaluate it.

application of Lemma 2 shows that

Var {!l" z1

�}

==

cr'2z - cr" 2.

(EO 3)

z

If the prior and posterior on z are normal, then, as proved
in [Raiffa and Schlaifer,

1961], the preposterior on z is

normal also. That is, the normal distribution is conjugate

to the normal sampling process. We thus require z to be
distributed normally.

3.0

Complexity and Non-Additivity of

EVI
Inference in probabilistic models with discrete variables is

exponential in the number of variables so we would
,

expect the exact calculation ofEVI to be exponential in
the number of variables also. For simplicity, we assume a
single decision variable with

In Figure

3, we show a prior on z. a possible posterior on z

m

alternatives.

Let k; be the

number of states for the ith state variable x;. To evaluate a

given evidence e, and a preposterior density { ll" I� } .

decision tree with n state variables, we require a number of

Note that the preposterior has the same mean as the prior,

value computations at the leaves equal to

z

and that its variance is the difference between the prior

m

variance and the posterior variance.

FIGURE 3.

IJk;.

i= 1

Prior, posterior, and preposterior densities.

Computing EVI on some subset of variables requires at
least the same number of value computations at the leaves,
and thus we see that

exact calculation ofEVI is exponen

­

tial inn. Also note that EVI calculations in discrete models
are possible for perfect information only.
If c; represents the value of information on state variable

X;, and C represents the value of information on all the

state variables simultaneously, then

C'#L,c;·
The above relation makes it difficult to devise separable,
or incremental, procedures for computingEVI, because
EVI will often demonstrate nonlinearities for varying
combinations of variables and for varying cases of perfect
and partial information.
The preposterior density encodes a state of knowledge

about z in light of what evidence e might reveal. Its inter­
pretation is the same as in Figure 2. Because it is a proba­
bility density on value, we can integrate over its negative
area to calculate the EVI of evidence

EVI (e)

e.

Thus, we have

Approximation of EVI

We are ready to apply the preceding analysis to develop an
efficient algorithm

for estimating EVI. We introduce a lin­

ear approximation to the value function, which in tum

0

J I ll"

4.0

z

l {j!" zl S} dj!" z'

(EO 4)

allows us to derive an expression for z, the net difference
in value between two decision alternatives. Preposterior

analysis on z provides a flexible mechanism for estimating

EVI.

123

Efficient Estimation ot the Value of Information in Monte Carlo Models

4.1

rior variance on xk. Since

e is perfect information on xk,
cr" �=0. Eq. 10 gives an approximation to the prior vari­

The Linear Value Model

We require a key approximating assumption: The value

function v(X,d;) can be approximated by a first-order
(linear) equation for each decision d;. that is, we can

write v; as a linear function of the x;.

v (X, d) L�ijxi+ a1

ance on z,

cr';. Given e, we know that the kth term in the

expression in Eq. 10 must be equal to zero. We can thus

write the posterior variance for z given

e,

cr"

2:

z

i=l...n,j=l,2.

=

j

(EQ 11)

We assume, for now, that the x; are independent. (The
assumption is not necessary; we use it to simplify our pre­

In view of Eq.

3, the preposterior variance on z is

sentation.) We denote the prior mean of x; by �·; and

denote its prior variance by cr'2;. Our approximating

Var [J.l"

assumption allows us to perform simple but useful proba­
bilistic analysis. First, by linearity of expectation, we can

write the mean ii (d;) as
v

4.2

(d;) = L,l\p'; + ar

(EQ 5)

i

�)�cr·;.

=

;

and one for

=

{XI�}. A
Xs is ann-tuple of state- variable assignments
to X. v(Xs,di) is equal to the value or utility generated by
scenario

(EQ 6)

v(X,d\

v(X,l)

Monte Carlo methods: Estimation of the
Coefficients

narios by sampling from the prior distriubtions

By our approximating assumption, we can write a linear
approximation for

the sth scenario for the ith decision alternative.

L,13:X;+a*;
i

(EO 7)

the average of the values

The optimal Bayes' decision is the maximum of those

averages. (Naturally, higher sample sizes give answers of
greater precision.) We represent this process for our binary
decision problem inTable 1:

TABLE 1.
(EQ 8)

Combining Eqs. 5-8 with the definition of z, we can write

expressions for the prior mean and variance of z:

J.l' z

v(l)-v(cf)

"
�

(A�� +) f.i.'. +(a* -a+).
1-'f
I

l

sth
Scenario

e

Value with

d1

XI

(EQ 9)

Xwo

expresses perfect information on xk and no

i denote the poste-

Value

with d2

v(X1,d2)

v(X1rod2)

v(X100,d1)

100v(X d)
L 100
s'

Average
information about the other x;. Let cr"

Determining the optimal decision in Monte Carlo decision
sample size= 100 and two decision

ar�alysis with
alternatives

(EQ 10)

Suppose that

d1 as
v(Xs,di) over the scenario in dex

We can estimate the expected value of each decision
s.

v(X,d+),

=

(EQ 12)

In Monte Carlo simulation, we generate a sample ofn sce­

Second, the variance of { v(X,d;)} can be written as

Var{v(X,d;)i �}

zl �]

s

=I

I

too

2:

s=1

v (Xs,

d1)

100

124

Chavez and Henrion

l ,

arg max

)=1,2

source could tell me roughly twice as much as I know

( � v(Xs, d) )
s"' I

now, then the equivalent RIM is 2.

100

(pk-�k)

The only outstanding task is to estimate the constants for
the linear-approximation model. To this end, we apply
multiple linear regression analysis to estimate the con­
sion alternatives, and letj be an index into the

n

2 '2
o-" 2 = -1 (rtf'k• _A+)
�-'k o k'

variables. From [Shavelson, 1988], we can use multiple

as follows:

n

z

state

linear regression to write constants for v;- value for the

z

by

stants in Eqs. 7 and 8. Let i be an index into set of m deci­

ith decision alternative in terms of the

o-' 2

is
A varia ble xk's contribution to the prior variance
z
+ 2 '2
. nm
. Eq . 10 as rt*
o k.For aRIM=rofevig1ve
dence e on variable xk, the posterior variance o" 2 is given

r

(EQ 14)

The preposterior variance is estimated as

state variables­

Var [)l" zl S)

r=

I

-r-

2
(� -�+) o-' 2k·
•

(EQ 15)

The preposterior mean for partial information stays the

(EQ 13)

same, as in Eq.

4.4

9.

Z Is Normal

where Rij = correlation(vi,x). Ru= correlation(xi,x) ,S; =

We will assume that the x;, are normally distributed. In

estimate these quantities directly from our Monte Carlo

our linear-approximating assumption requires z to be

standard deviation(v;), and o ;=standard deviation(xj). We

light of the following proposition from probability theory,

samples.

normally distributed also.

Recall from Section 2. 1 that the v; generate probability

Proposition: Let X; be a collection of n normal random

to think of them as random variables with corresponding

the random vari able Y as

distributions in a Monte Carlo model. Thus, it makes sense

variables with means given by J..li and variances

sample correlations and standard deviations. The a. are

i

n

aj

4.3

=

Then Y is normally distributed also, with mean given by

(v)- L �;}i'r
;,. I

a+

Relative Information Multiple

Suppose now that e expresses partial, rather than perfec t

information on xk. It is not immediately obvious how to

,

specify partial information on an uncertain variable. We

suggest the following method, based on our concept of a

RIM. ARIM of evidence e on variable xk is defined to be
the ratio between the prior variance o' and the posterior
variance o " on xk after e has been seen. In intuitive

i

�

terms, theRIM measures how much we could know rela­
tive to what we know

Y = a+'�.x
L,. I I..

1

estimated as follows:

now.

It is a

m ultiple

o'2• Define

on missing but

knowable information. For example, if an information

ri �iJ..li'

and variance

0

Observe that our approximating assumption allows us to
write the mean and variance of z using standard probabil­
ity formulae. There is nothing about our framework, how­

ever, that forces the actual distribution of z to belong to the
same family as do z's component distributions. For exam-

Efficient Estimation of the Value of Information in Monte Carlo Models

ple, if the x; are Poisson, normal, and exponential, then z is
a hard-to-assess, mongrel distribution. Assuming that the
xi are normal forces z to be normal also. If the x; are non­
normal, then we must make an extra approximating
assumption that z is normal also, although we must
emphasize that this assumption would not be analytically
true.
A limiting aspect of the technique presented here is that it
measures EVI relative to only two decisions. In [Chavez,
1994], we show how to extend it to accommodate multiple
(�3) decision alternatives.

4.5

Var {�"

2. Define variable z as the difference between v(X,d*) and
v(X,d+).

3. Calculate regression constants ��,
and
�+,a·,
I
I

a+.

4. Using Eqs. 2 and 9, calculate the mean (!l" zl s> of the
preposterior distribution of z:
·

(!l" z Js> = 11' z

5. For

=

+ (a·-a+).
"c�·-�+)!l'
£..,
J
J
j

j

perfect informati on on Xi, define

Var {!l" z1 S}

=

•
2
(� -�+) a'2;·

For partial information on Xi with RIM=k, define

Var{!l"zJS}

k;- 1

k�l( �·-� +)2a,2i·

=

For perfect information on variables with indices in S,
define

i

k;- I

=

l: -k.l

l

•

2

(� -�+) a'\

;

Normal ( (!l" zl

2

a'

2

;+I,

je S

•

(� -�+)

2

a'

2

s>

=

ll'z· Var {!l" zl S})

11. Express EVI as
0

=

f ill"zl

{!l"zJS}d!"l z

12. Perform the integration in (11) numerically.

5.0

Application

We now describe an application of our method to a large
decision model developed at Rockwell's Palo Alto Science
Laboratory to support Course of Action (COA) analysis
for Noncombatant Evacuation Operations (NEO).
Implemented in Demos, NEO-COA allows a user to
instantiate a generic NEO plan with specific parameter
values for locations, forces, and destinations of troops and
civilians. The model provides insights into the relative
strengths of alternative plans by scoring them using differ­
ent evaluation metrics, such as time to complete the opera­
tion. Because many of the elements of a real-world
military planning scenario are not known with certainty,
several of the model's inputs are specified as continuous
probability distributions.
In the current version of NEO-COA, there are three deci­
sion variables, or factors over which a military planner
exercises control:
•

F or partial information on variables with indices i and
a corresponding ordered set of RIM's k;, define

Var {!l" zl S}

•

=I,-(�
-�+)
k

10.Define the preposterior density on z, { !l" z S}, as
l

•

8.

z1;}

Algorithm for EV I

1. Select the two decision alternatives generatin¥ the
highest and second-highest expected value, d and cr.

7.

For perfect information on variables with indices in S
mixed with partial information on variables with indi­
ces i and a corresponding ordered set of RIM's k;.

EVI

We now summarize, in algorithmic form, our general tech­
nique for estimating EVI in a Monte Carlo decision
model:

6.

9.

125

Security forces: Security forces vary in their starting
locations, dates of availability, and capabilities in pro­
viding security.
Safe havens: The places where civilians gather to take

safe havens differ in terms of distances from
the assembly areas and port capacities.

shelter,
•

Transportation assets: A configuration of transporta­
tion assets is a sequencing of transportation capability
over a fixed period of time. "Three C-141 's available

126

Chavez and Henrion

on day 2 and 5 C-14l's on days 3 through 10" is an
example of a particular transportation configuration.
Because each of these decision variables currently possess
three alternatives, there are a total of 27 available courses
of action. In addition, the NEO-COA model possesses
over 100 different input variables; of those, currently nine
are specified as probabilistic quantities.
Once the decision variables and inputs have been speci­
fied, the model performs a dynamic simulation of the flow
of U.S. citizens (the non-combatants) from their starting
locations within a country to a set of selected assembly
areas, and then on to the safe havens. It also includes risk
factors associated with both U.S. citizens and U.S. mili­
tary personnel as functions of time. For example, risk to
U.S. citizens at the assembly areas can rise and fall over
the course of an entire operation in response to uncertain
events, such as the arrival of security forces. The func­
tional representations of the risk factors are then used to
compute expected casualties- civilian and military for varying alternatives.
A top level view of the NEO-COA model as implemented
in Demos is shown in Figure 4 . There are three uncertain­
ties for the NEO-COA model: Initial USCITS, proba bil ­
ity distributions on the number of U.S. citizens in each of
the three regions of the country (capital, north, and south)
at the start of a crisis planning operation; Country
Regions Attrition Risk, which is the risk posed to non­
combatants over the course of an operation; and Transfer
Rate, which is the speed at which civilians move from
their starting locations to the assembly areas. Thus a total
of nine continuous probability distributions must be
assigned; typically, these are subjective assessments pro­
vided by military planners using the model.
In Figure 5, we show the results of applying the EVI
approximation technique to the NEO model for perfect
information. We see, for example, that the uncertainty
about the number of American citizens in the capital has
EVI equal to about six lives, and the uncertainty about the
transfer rates in the capital has perfect information value
equal to more than seven lives. In all cases, the value of
information is highest for uncertainties relating to the cap­
ital region, reflecting that the highest number of citizens
are concentrated there. The integral in Eq. 4 is evaluated
numerically. Perfect information calculations on nine

uncertainties took Demos 1 minute, 53 seconds running on
a Macintosh Ilfx computer.

6.0 Conclusions and Future Directions
We have described a general analytic framework for esti­
mating EVI in a decision model using preposterior analy­
sis. It employs a linear-approximating assumption that
allows us to write the value function as a first-order equa­
tion in the inputs. We define variable z to be the difference
in value for the two decision alternatives. Multiple linear
regression on the inputs provides the necessary constants
for the linear value equation; we estimate the regression
constants from Monte Carlo sample information. Applying
preposterior analysis to z allows us to write an approxima­
tion to the value of perfect and partial information for any
combination of state variables.
There are several areas in which we plan to extend the
work presented here. First, we would like to develop a sis­
ter technique for approximating EVI on continuous deci­
sion variables. Second, we would like to examine how
well our technique performs relative to an exact, more
costly approach. To this end we will apply our method to
several large models, run it several times, and compare its
results to the corresponding exact answers. Third, we wilk
apply statistical proof techniques to analyze formally the
algorithm's convergence and error characteristics.

7.0

References

Chavez, Tom. (1994). "Recovering Value oflnformation from Pairwise Peeks,"
Rockwell Palo Alto Science Lab. Technical Memorandum.
Hennon. Max and Morgan, Granger.

(1990).

Uncertainty: A Guilk ro

Dealing with

Uncertainry in Quantitative Risk and Policy Analysis. Cambridge University Press.

Cambridge.

Howard. R. A. (1970). "Proximal Decision Analysis." Management Science, 17,
No. 9:507-541.
Raiffa, Howard and Schlaifer, Raben.

MIT Press, Boston.

(1961 ). Applied Statistical Decision Th�ory,

(1988). Statistical Reasoning for IM Behavioral Sciences. 2nd
ed. Allyn and Bacon, Inc, Boston.
Shavelson, Richard.

Efficient Estimation of the Value of Information in Monte Carlo Models

FIGURE 4.

Top-level view of the NEO-COA model.

FIGURE 5.

Approximation of perfect information values.

Mid Value of Value of Perfect Information
Key:

Uncertai a input vars

X Axis:

Tl

Country Regions

,..I

7.5

5

2.5

0

C�p"ihl

North

South

CountriJ Regions
Ke11

Uncertain input v�rs

-

Number of UC eits

-

Attrition r�te

lllll5il

Tr�nsfeor rat�.>

•

127

