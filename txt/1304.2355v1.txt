I
ON THE LOGIC OF CAUSAL MODELS

*

Dan Geiger & Judea Pearl
Cognitive Systems Laboratory, Computer Science Department
University of California Los Angeles, CA 90024
Net address: geiger@cs.ucla.edu
Net address: judea@cs.ucla.edu

ABSTRACT
This paper explores the role of Directed Acyclic Graphs (DAGs) as a representation of condition­
al independence relationships. We show that DAGs offer polynomially sound and complete inference
mechanisms for inferring conditional independence relationships from a given causal set of such relation­
ships. As a consequence, d-separation, a graphical criterion for identifying independencies in a DAG, is
shown to uncover more valid independencies then any other criterion. In addition, we employ the
Armstrong property of conditional independence to show that the dependence relationships displayed by a
DAG are inherently consistent, i.e. for every DAG D there exists some probability distribution P that em­
bodies all the conditional independencies displayed in D and none other.

INTRODUCTION AND SUMMARY OF RESULTS
Networks employing Directed Acyclic Graphs (DAGs) have a long and rich tradition, starting
with the geneticist Wright (1921). He developed a method called path analysis [Wright, 1934] which
later on, became an established representation of causal models in economics [Wold, 1964], sociology
[Blalock, 1971] and psychology [Duncan, 1975]. Influence diagrams represent another application of
DAG representation [Howard and Matheson, 1981], [Shachter, 1988] and [Smith, 1987]. These were
developed for decision analysis and contain both chance nodes and decision nodes (our definition of
causal models excludes decision nodes). Recursive models is the name given to such networks by statisti­
cians seeking meaningful and effective decompositions of contingency tables (Lauritzen, 1982), (Wer­
muth & Lauritzen, 1983], [Kiiveri et al, 1984]. Bayesian Belief Networks (or Causal Networks) is the
name adopted for describing networks that perfonn evidential reasoning ((Pearl, 1986a, 1988]). This pa­
per establishes a clear semantics for these networks that might explain their wide usage as models for
forecasting, decision analysis and evidential reasoning.
DAGs are viewed as an economical scheme for representing conditional independence relation­
ships. The nodes of a DAG represent variables in some domain and its topology is specified by a list of
conditional independence judgements elicited from an expert in this domain. The specification list desig­
nates parents to each viuiable v by asserting that v is conditionally independent of all its predecessors,
given its parents (in some total order of the variables). This input list implies many additional conditional
independencies that can be read off the DAG. For example, the DAG asserts that, given its parents, v is
also conditionally independent of all its non-descendants [Howard and Matheson, 1981]. Additionally, if
S is a set of nodes containing v 's parents, v 's children and the parents of those children, then v is in*This work was partially supported by the National Science Foundation Grant #IRI-8610155. "Graphoids: A
Computer Representation for Dependencies and Relevance in Automated Reasoning (Computer Information
Science)".

136

I
I
I
I
,,
I
I
I
I
I
I
I
I

1·.

I
I
I
I

I
I
I
I
I

dependent of all other variables in the system, given those inS [Pearl, 1986a]. These assertions are exam­
ples of valid consequences of the input list i.e., assertions that hold for every probability distribution that
satisfies �e conditional independencies specified by the input. If one ventures to perfonn topological
transfonnations (e.g., arc reversal or node removal [Shachter, 1988]) on the DAG, caution must be exer­
cised to ensure that each transfonnation does not introduce extraneous, invalid independencies, and/or
that the number of valid independencies which become obscured by the transfonnation is kept at a
minimum. Thus, in order to decide which transfonnations are admissible, one should have a simple
graphical criterion for deciding which conditional independence statement is valid and which is not.

I

1.

What are the valid consequences of the input list ?

2.

What are the valid consequences of the input list that can be read off the DAG ?

3.

Are the two sets iden tic al?

I
I
I·

This paper deals with the following questions:

The answers obtained are as follows
1.

A statement is a valid consequence of the input set if and only if it can be derived from it using

the axioms of semi-graphoids [Dawid, 1979; Pearl & Paz ,1985]. Letting X, Y, and Z stand for
three disjoint subsets of variables, and denoting by I(X, Z , Y) the statement: the variables in X
are conditionally independent of those in Y. given those in Z we may express these axioms as
follows:
"

I
I

",

Symmetry

I(X, Z, Y) ==) /(Y, Z, X)
Decomposition

I
I
I
I

(l. c)

Weak Union

I(X, Z, Y u W)� I(X, Z uW, Y)
(l.d)

Contraction

I(X, ZuY, W) &l(X, Z, Y)=>I(X,Z, YuW)

2.

Every statement that can be read off the DAG using the d-separation criterion is a valid conse­
quence of the input list [Venna, 1986].
The d -separation condition is defined as follows [Pearl, 1985]: For any three disjoint subsets
X, Y, Z of nodes in a DAG D, Z is said to d -separate X from Y, denoted I(X, Z, Y)o , if there
is no path from a nod e in X to a node in Y along which: 1. every node that delivers an arrow is
outside Z, and 2. every node with converging arrows either is in Z or has a descendant in Z (the
definition is elaborated in the next section).

I
I

(l.b)

l(X, Z, YuW)�l(X, Z, Y) & I(X, Z, W)

I
I

(La)

3.

The two sets are identical, namely, a statement is valid IF AND ONLY IF it is graphically­
validated under d-separation in the DAG.

137

I
The frs
i t result establishes the decidability of verifying whether an arbitrary statement is a valid conse­
quence of the input set. The second result renders the d-separation criterion a polynomially sound infer­
ence rule, i.e., it runs in polynomial time and certifies only valid statements. The third renders the d­
·separation criterion a polynomially complete inference rule, i.e., the DAG constitutes a sound and com­
plete inference mechanism that identife
i s, in polynomial time, each and every valid consequence in the
system.
The results above are true only for causal input sets i.e., those that recursively specify the relation
of each variable to its predecessors in some (chronological) order. The general problem of verifying
whether a given conditional independence statement logically follows from an arbitrary set of such state­
ments, may be undecidable. Its decidability would be resolved upon finding a complete set of axioms for
conditional independence i.e., axioms that are powerful enough to derive aU valid consequences of an ar­
bitrary input list. The completeness problem is treated in [Geiger & Pearl, 1988] and completeness
results for specialized subsets of probabilistic dependencies have been obtained. All axioms encountered
so far are derivable from Dawid's axioms, which suggests that they are indeed complete, as conjectured
in [Pearl & Paz, 1985]. Result-1 can be viewed as yet �other completeness result for the special case in
which the input statements form a causal set. This means that applying axioms (l.a) through (l.d) on a
causal input list is guaranteed to generate all valid consequences and none other. Interestingly, result-2
above holds for any statement s that obey Dawid's axioms, not necessarily probabilistic conditional in­
dependencies. Thus, DAGs can serve as polynomially sound inference mechanisms for a variety of
dependence relationships, e.g., partial correlations and qualitative database dependencies. In fact, the
results of this paper prove that d -separation is complete for partial correlation as well as for conditional
independence statements, whereas completeness for qualitative database dependencies has not been ex­
amined.

SOUNDNESS AND COMPLETENESS

The definition of d-separation is best motivated by regarding DAGs as a representation of causal
relationships. Designating a node for every variable and assigning a link between every cause to each of
its direct consequences defines a graphical representation of a causal hierarchy. For example, the proposi­
tions "It is raining" (a), "the pavement is wet" (�) and "John slipped on the pavement" (y) are well
represented by a three node chain, from a through � to y ; it indicates that either rain or wet pavement
could cause slipping, yet wet pavement is designated as the direct cause; rain could cause someone to slip
if it wets the pavement, but not if the pavement is covered. Moreover, knowing the condition of the pave­
ment renders "slipping" and "raining" independent, and this is represented graphically by a d -separation
condition, I (a. y, �)0, showing node a and � separated from each other by node y. Assume that "broken
pipe" (o) is considered another direct cause for wet pavement, as in figure 1. An induced dependency ex­
ists between the two events that may cause the pavement to get wet: "rain" and "broken pipe". Although
they appear connected in Figure l, these propositions are marginally independent and become dependent
once we learn that the pavement is wet or that someone broke his leg. An increase in our belief in either
cause would decrease our belief in the other as it would "explain away" the observat ion. The following
definition of d -separation pennits us to graphically identify such induced dependencies from the DAG (d
connoted "directional").
Definition: If X, Y, and Z are three disjoint subsets of nodes in a DAG D, then Z is said to d-separate X

138

I
a·

I
I
-1
I
I
I
I
I
I
I
I
I
I
I
I
I

I

I
I
I
I
I

from

Y, denoted I (X,Z, Y)0, iff the're is no path* from a node in X to a node in Y along which every

node that delivers an arrow is outside Z and every node with converging arrows either is or has a descen­

Z. A path satisfying the conditions above is said to be active, otherwise it is said to be blocked
(by Z). Whenever a statement I (X,Z, Y)o holds in a DAG D, the predicate I (X, Z, Y) is said to be

dant in

graphically-verified (or an independency}, otherwise it is graphically-unverified by D (or a dependency).
In figure 2, for example, X= {2}and Y ={31 are d-separated by Z = [1J; the path 2 t- 1 � 3 is
blocked by 1 e Z while the path 2 � 4 t- 3 is blocked because 4 and all its descendents are outside Z.
Thus I (2, 1, 3) is graphically-verified by D. However, X and Y are not d- separated by Z' = { 1, 5} be­
cause the path 2 � 4 t- 3 is rendered active. Consequently, I (2, { 1,5} ,3) is graphically-unverified by D ;
by virtue of 5, a descendent of 4, being in Z. Learning the value of the consequence 5, renders its causes
2 and 3 dependent, like opening a pathway along the converging arrows at 4.

I
I
I
I

I
I
I
I
I
I
I
I
I

Figure 1

Figure

2

P, then X and Y are
iff
Z,
denoted
I (X,Z, Y)p
P (X,Y I Z) = P (X I Z)·P (Y I Z) for all possible values of X, Y and Z for which P (Z) > 0.
I (X,Z,Y)p is called a (conditional independence) statement. A conditional independence statement cr
Definition: If X, Y, and Z
said

to

be

are three disjoint subsets of variables of a distribution

conditionally

independent

logically follows from a set 1: of such statements if

given

cr holds in every distribution that obeys

1:. In such case

we also say that cr is a valid consequence of :E.
Ideally, to employ a DAG D as a graphical representation for dependencies of some distribution
P we would like to require that for every three disjoint sets of variables in P (and nodes in D) the follow­
ing equivalence would hold

I(X,Z,Y)0

iff

/(X,Z,Y)p

(2)

This would provide a clear graphical representation of all variables that are conditionally independent.

When equation (2) holds, D is said to be a perfect map of P. Unfortunately, this requirement is often too
strong because there are many distributions that have no perfect map in DAGs. The spectrum of proba­
bilistic dependencies is in fact so rich that it cannot be cast into any representation scheme that uses poly­
nomial amount of storage ([Verma, 1987]). Geiger [1987] provides a graphical representation based on a
collection of graphs (Multi-DAGs) that is powerful enough to perfectly represent an arbitrary distribution,

*

By

path we mean a sequence of edges in the underlying undirected gmph, i.e ignoring the directionality of the

links.

139

I
however, as shown by Vetrna, it requires, on the average, an exponential number of DAGs. Being unable
to provide perfect maps at a reasonable cost, we compromise the requirement that the graphs represent
each and every dependency of P , and allow some independencies to escape representation.

Definition: A DAG D is said to be an !-map of P if for every three disjoint subsets X, Y and z of vari­
ables the following holds:

l(X, Z, Y)0

�

l(X .z, Y)p

The natural requirement for these 1-maps is that the number ofundisplayed independencies be minimized.
The task of finding a DAG which is a minimal I-map of a given distribution

P

was solved in

[Venna 1986; Pearl & Venna, 1987]. The algorithm consists of the following steps: assign a total order­

d to the variables of P. For each variable i of P, identify a minimal set of predecessors S; that
renders i independent of all its other predecessors (in the ordering of the first step). Assign a direct link
from every variable inS; to i. The resulting DAG is anI-map of P, and is minimal in the sense that no
ing

edge can be deleted without destroying its 1-mapness. The input list L for this construction consists of n

conditional independence statements, one for each variable, all of the fonn I (i, S;, U <n-S;) where U (i) is

i and S; is a subset of U (i) that renders i conditionally independent of all its
other predecessors. This set of conditional independence statements is called a causal input list and is said
to define the DAG D. The term "causal" input list stems from the following analogy: Suppose we order

the set of predecessors of

the variables chronologically, such that a cause always precedes its effect. Then, from all potential causes
of

an effect i ,

a causal input list selects a minimal subset that is sufficient to explain

i , thus rendering all
direct causes of i

other preceding events superfluous. This selected subset of variables are considered the
and therefore each is connected to it by a direct link.

Clearly, the constructed DAG represents more independencies than those listed in the input,
namely, all those that are graphically verified by the d-separation criterion. The analysis of [Venna, 1986]
guarantees that all graphically-verified statements are indeed valid in P i.e., the DAG is anI-map of P.
However, this paper shows that the constructed DAG has an additional property; it graphically-verifies

every conditional independence statement that logically follows from L (i.e. holds in every distribution
that obeys L ). Hence, we canno t hope to improve the d-separation criterion to display more indepen­
dencies, because all valid consequences of L (which defines D) are already captured by d -separation.
The three theorems below fonnalize the above discussion.

Theorem 1 (soundness) [Verma, 1986]: Let D be a DAG defined by a causal input list L. Then, every
graphically-verified statement is a valid consequence of L.

Theorem 2 (closure) [Verma, 1986]: Let D be a DAG defined by a causal input list L. Then, the set of
graphically-verified statements is exactly the closure of L under axioms (La) through ( l .d).

Theorem 3 (complet eness): Let D be a DAG defined by a causal input list L. Then, every valid conse­
quence of L is graphically-verifie d by
a valid consequence of L ).

D

(equivalently, every graphically-unverified statement in

140

D

is not

I
I
I

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

I
I
I
I
I
I

Theorem 1 guarantees that the DAG displays only valid statements. Theorem 2 guarantees that
the DAG displays all statements that are derivable from L via axioms

(1).

The third theorem, which is

the main contribution of this paper, assures that the DAG displays all statements that logically follow

from L i.e., the axioms in ( 1) are compl ete, cap able of de riving all valid consequences of a causal input

list. Moreover, since a statement in a DAG can be verified in polynomial time, theorem

3 provides a com­

plete polynomial inference mechanism for deriving all independency statements that are implied by a
causal input set.
Theorem

3

is proven in the appendix by actually constructing a distribution

Pcr that satisfies all

conditional independencies in L and violates any statement cr graphically-unverified in D. This distribu­
tion precludes cr f rom being a valid consequence of L and therefor e, since the construction can be repeat­
ed for every graphically-unverified statement. none of these statemep.ts is a valid consequence of L .
The first two theorems are more general than the third i n the sense that they hold for every depen­

I

dence relati on ship that obeys axioms (La) through (l.d), not necessarily those based on probabilistic con­
ditional_ independence (proofs can be found in [Verma,I986J). Among these dependence relationships are

partial correlations ([Pearl & Paz, 1985]) and qualitative dependencies ([Fagin, 1982], [Shafer at al,

I
I

I
I
I
I
I
I
I
I
I
I

1987]) which can re ad il y be shown to obey axioms (1). Thus, for example, the trans fo rm at ion of arc­

reversal and node removal ([Howard & Matheson, 1981]) can be shown valid by purely graphical con­
sideration, simply showing that every statement verified in the transformed graph is also graphically­
verified in the original graph.

The proof of theorem

3

assumes that L contains only statements of the form I (i,

Si, U Ci)-Si ).

Occasionly, however, we are in possession of stronger forms of independence relationships, in which case

additional statements should be read of the DAG. A common example is the case of a variable that is

deterministic variable, [Shachter,
i could be encoded in L by a statement of global indepen­
conditione d on Si, i is independent of all o ther variables, not merely

functionally dependent on its corresponding parents in the DAG (
1988]). The existence of each such variable
dence I (i,

Si ,U -Si-i) asserting that

of its predecessors. The independencies that are implied by the modified input list can be read from the
DAG using an enhanced version of d -separation, named ID -separation.

Definition: If X, Y, and Z are three disjoint subsets of nodes in a DAG D, then Z is said to ID-separate

X to a node in Y along which 1. every node which delivers
Z, 2. every node with converging arrows either is or has a descendant in Z and 3. no
node is functionally determined by Z.
X

from Y, iff there is no path from a node in

an arrow is outside

d -separation plus additional
ones due to the enhancement of the input list. It has been shown that theorem 1 through 3 hold for ID­
The new criterion certifies all independencies that are revealed by

separation whenever L contains global independence statements (Geiger & Verma. 1988].

These graphical criteria provide easy means of recognizing conditional indep end ence in influence

diagrams as well as identifying the set of parameters needed for any given computation. Shachter [1985,

19881 has devised

an algorithm for finding a set of nodes

M guaranteed to contain sufficient infonnation

for computing P (xI y ), for two arbitrary sets of variables x an d y . The outcome of Shachter' s algorithm
can now be stated declaratively; M contains every ancestor of

x u y tha t is not ID·separated from

x

given y and none other. The completeness of ID-separation implies that M is m ini m al ; no node in M can
be excluded on purely topological grounds (i.e., without considering the numerical values of the probabil­
ities inv olved).

141

I
We conclude by showing how these theorems can be employed as an inference mechanism. As­
sume an expert has identified the following conditional independencies between variables denoted 1
through5:
L

·

=

{ !(2, 1, 0), /(3, 1, 2), !(4, 23, 1), /(5, 4, 123)}

(the first statement in L is trivial). We address two questions. First, what is the set of all valid conse­
quences of L ? Second, in particular, is I (3, 124, 5) a valid consequence of L ? For general input lists the
answer for such questions may be undecidable but, since L is a causal list, it defines a DAG that graphi­
cally verifies each and every valid consequences ofL. The DAG D is the one shown in figure 2, which
constitutes a dense representation of all valid consequences of L. To answer the second question, we sim­
ply observe that I (3, 124, 5) is graphically-verified in D . A graph-based algorithm for another subclass
of statements, called fixed context statements, is given i_n [Geiger & Pearl, 1988]. In that paper, results
analogous to theorem 1 through 3 are proven for Markov-fields; a representation scheme based on un­
directed graphs ((Isham, 1981], [Lauritzen, 1982]).

EXTENSIONS AND ELABORATIONS
Theorem 3 can be restated to assert that for every DAG D and any dependency a there exist a
probability distribution P 0 that satisfies D's input set L and the dependency a. By theorem 2, P 0 must
satisfy all graphically-verified statements as well because they are all derivable from L by Dawid 's ax­
ioms. Thus, theorems 2 and 3 guarantee the existence of a distribution P a that satisfies all graphically
verified statements and a single arbitrary-chosen dependency. The question answered in this section is the
existence of a distribution P that satisfies all independencies of D and all its dependencies (not merely a
single dependency). We show that such a distribution exists, which legitimizes the use of DAGs as a
representation scheme for probabilistic dependencies; a model builder who uses the language of DAGs to .
express dependencies is guarded from inconsistencies.
The construction of P is based on the Armstrong property of conditional independence.
Definition: Conditional independence is an Armstrong relation in a class of distributions P if there exists
operation® that maps finite sequences of distributions in P into a distribution of P, such that if a is a
conditional independence statement and if Pi i=l..n are distributions in P, then a holds for
® { P i I i=l..n} iff cr holds foreachPi.
an

The notion of Annstrong relation is borrowed from database theory ([Fagin 1982]). We concen­
trate on two families of distributions P: All distributions, denoted PD and strictly positive distributions,
denoted PD+. Conditional independence can be shown to be an Armstrong relation in both families. The
construction of the operation® is given below, however the proof is omitted and can be found in ([Geiger
& Pearl, 1988]).

Theorem 4 ([Geiger & Pearl, 1988]): Conditional independence is an Armstrong relation in PD and in

PD+.

We shall construct the operation® for conditional independence using a binary operation®' such
that if P = P 1®' P 2 then for every conditiona! independency statement a we get

142

I
I
I
I
I
I
I
I
'
I
I
I
I
I
I
I
I

I
I
I
I
I
I

®'Pi obeys

I
I

I
I
I
I
I
I
I
I
I
I

iff P 1 obeys

cr

and

P 2 obeys

cr.

(5)

The operation® is recursively defined in terms of®' as follows:
® { P; I i=l..n} =((P1®' Pz)®'P3)®'· · ·P" ) .
Clearly, if®' satisfies equation (5), then® satisfies the the requirement of an Armstrong relation, i.e.
P obeys

cr

iff 'i P; obeys cr.
j

Therefore, it suffices to show that®' satisfies (5).
, An be the
Let P 1 and P 2 be two distributions sharing the variables x 1 , · , x" . Let A 1,
domains of x 1,
, x" in P 1 and let ar1 instantiation of these variables be <X1, · · · , <X11• Similarly, let
B 1o
, B" be the domains of x 1,
, X11 in P 2 and �1• · · · , ��� an instantiation of these variables. Let
the domain of P = P 1®' P 2 be the product domain A 1B 1 ,· · , A11 B11 and denote an instantiation of the
variables of P by <X1 �1 , · · · , <X"�". Define P 1®' P 2 by the following equation:
·

·

•

I

cr

•

·

·

·

·

·

·

•

·

•

·

·

P(<Xl�l • <Xz�2 • ·

·

·

• <X"�") = P1C<X1, <Xz,

··

·, <Xn) P 2C�1, �2,
·

· · ·,

���).

The proof that P satisfies the condition of theorem 4 uses only the definition of conditional independence
and can be found in [Geiger & Pearl 1988]. The adequacy of this construction for PD+ is due to the fact
that® produces a strictly positive distribution whenever the input distributions are strictly positive.
Theorem 5: For every DAG D there exists a distribution P such that for every three disjoint sets of vari­
ables X, Y and Z, the following holds;
I (X ,Z Y )D
,

iff I (X ,z,Y)p

Proof: Let P =® { P cr I cr is a dependency in a DAG D } where P cr is a distribution obeying all in­
dependencies of D and a dependency cr. By theorem 3, a distribution P cr always exists. P satisfies the re­
quirement of theorem 5 because it obeys only statements that hold in every P cr and these are exactly the
ones verified by D . 0

The construction presented in the proof of theorem 5 leads to a rather complex distribution, where
the domain of each variable is unrestricted. It still does not guarantee that a set of dependencies and in­
dependencies represented by DAGs is realizable in a more limited class of distributions such as normal or
those defined on binary variables. We conjecture that these two classes of distributions are sufficiently
rich to permit the consistency of DAG representation.
ACKNOWLEDGMENT

We thank Ron Fagin for pointing out the usefulness of the notion of Armstrong relation and to
Ross Shachter for his insightful comments. Thomas Verma and Azaria Paz provided many useful discus­
sions on the properties of dependency models.

143

APPENDIX

Theorem J (completeness): LetD

be a DAG defined by a causal input list L. Then, every valid consequence of L is

graphically-verified byD .

Proof: Let a =I (X.

Z, Y) be an arbitrary graphically-unverified statement in D . We construct a distribution P., that satisfies all
'This distribution precludes a from being a valid consequence of L

conditional independencies in the input list L and violates a.

and therefore, every valid consequence of L must be graphically-verified inD .
>From the definition of d -separation, there must exist an active path between an element a.

in X

and an element

� in Y

that is not d-separated by Z. Ensuring that P 0 violates the conditional independency /(a., Z, /3), denoted a', guarantees that a is

also violated, because any distribution that renders X andY conditionally independent must render each of their individual vari­

ables independent as well (axiom (l.b)).

P 0 is defined in terms of

a

simplified DAGD 0• This DAG is constructed by removing

as

many links

as

possible from

D such that a remains unverified inD0• Tills process clearly preserves all previously verified statements but caution is exercised
not to remove links that would render a graphically-verified inD "" We will conclude the proof by constructing a distribution P 0
which satisfies all graphically-verified statements ofD" (hence also those ofD)
Let q be an active path (by Z) between
verging arrows) denoted, left to right, h1
directed path from h; to

Z;

(if h; e

Z

,h2.

then

z,.

,

and violates a'.

a. and� with the minimum number of head-to-head nodes (i.e. nodes with con­

... ,hk. Let z; be the closest (wrt path length) descendent of h; in Z and letp; be the

= h1) . We defineD" to be a sub graph ofD containing only the links that form the

paths p; 's and the path q. We make two claims about the topology of the resulting DAG. First, the paths p; are all distinct.
Second, for any i, h; is the only node shared by p; and q. The resulting DAG is depicted in figure 3 (note that some nodes, in­

cluding nodes of Z, might become isolated inD 0).

�
@
00

00

Figure3
Proof of claim

1:

(i < j) with a common node"(. Under
a and J3 that has less head-to-head nodes then q, contradicting the minimality of
then the pa th (a, h;, y, hi• �)is an active path (by Z); Each of its head-to-head nodes is or has a

Assume, by contradiction, that there are two paths p; and pi

this assumption, we find an active path between
the Iauer. If"( is neither h; nor hi
descendent

is outside

in Z because it is either"( or a head-to-head node of q. Every other node lies either on the active path q and therefore
Z or lies on p1 (pi) in which case, since it has a descendent y, it must also be outside Z. The resulting path contradicts

the minimality of q since both h; and hi are no longer head-to-head nodes while"( is the only newly introduced head-to-head
node. If y =hi then, using similar arguments, the path

(a., h;, y,

�) (see figure

readily be shown active (the casey= h; is similar).

144

4),

which has less head-to-head nodes then q, can

·I
·I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

I
I
I
I

o .o

I
I
I

00

Figure4
Proof of claim 2: Assume p; and q have in common a node rocher chen h; and assume w.l.o.g that it lies between h;

Z be­
(a., h1• y, �)must be an active path which contradicts the minimali­

and �·This node is not a head-to-head node on q because Pi is distinct from all other Pi's. The node y cannot belong to

cause otherwise q would not have been active. Thus, the path

ty of q (figure

5).

I
I

00

I
I
I
I
I

Figure5

In the following discussion we call a path containing no head-to-head node a regular paJh. Let P" be a nonnal distri­
bution with the following covariance matrix r

r = (p;i)
Since

Pii

{0

= p1

There exists

no

There exists

a

regular path from node i to node j
regular palh of length l between node i and node

D" is singly connected there exist at most one path between any two nodes. Any value of p satisfying

I

I
I
I

0

(3)

j .
<n

·

p2 < 1 would

render r positive definite and therefore a valid covariance matrix. We claim that this distribution satisfies all independencies of

D a and violates I (a., Z, �). To evaluate I (ex, Z, �) we first form the projection of P" on the variables a., � and Z. Since P" is

0

normal, this projection is also normal and its covariance matrix is a submatrix f"' of r that corresponds to the variables ex, � and
Z. The statement I (a., Z, �) holds in Po iff det (f"'CJll) = where f"'all is a submatrix off"' obtained by removing the ex-th line anc,i
the �-th column ([Miller, 1964]). Both f' and ral} ate given below. The matrix f"' is a tri-diagonal matrix whose off main­
diagonal elements ate integer powers of p or zeros. The columns and lines off' correspond to the following order of variables: a,
;
z 11
,Zt, P and then all other variables of Z (see fig 3), thus, for example, the term p , located at (1,2) in r' is the correlation factor
between ex and z l because these variables are the first two in the above order. The integer i 1 is the length of the path be tween a
and z 1o i2 is the length of the path between z 1 and z2 and so on. The construction off'all is based on the observation that the loca­
tion (a, �)in r is (1, k+2) where k is the number of head-to-head nodes of q .
...

I

00

0 00 00
f'= 00 0 0
000
1

pi·

i·
p

1

pi· 1

00 0 000
000
·
pi

pi·

pj• 0

f"'al}==

pi• 1

pi•

pi·
1

p i·

1

0

145

I
(These matrices are given for the case of two head-to-head nodes and a single additional isolated node of Z. their general form is
obvious). Clearly, det (r'ajl) = p i (k <: 0) and therefore chasing p >t 0 guarantees that I (ex, Z, �)does not hold in P 0•
It remains to show that every graphically-verified statement in D" is satisfied by P 0• We assign a total order d on the
nodes of D 0 consistent with the partial order determined by D.,. We show that the n statements that fonn the causal input list
that defines D 0 are satisfied inP 0• Theorem 1 ensures that all other gra�hically verified statements are valid consequences of this

would all be satisfied inP 0• In what follows we use the tag of a node as its name. Let I (i, S1, U <o-S;) be a
statement of L where S1 are the parents of i and U<I> are all the variables preceding i in the order d . By the topology of D "' S;
contains no more then two nodes.

input list and therefore

Assume S1

is empty. This implies that i i s

not connected via a regular path to either of its predecessors. Hence, by the

construction of r. p11 = 0 for every j e U(I)• and therefore the statements I (i, 0, j) hold in P 0• However, in normal distributions,
the correlation between single variables determines the dependency between the sets containing these variables because the fol­
lowing axiom holds.

I

I
I
I
I
I

(Composition-Decomposition)

l"(X,Z, Y u W)

<=>

/(X, Z, Y) & /(X, Z, W)

(4)

Accordingly, I (i, 0, U<i>) holds inP a and this statement is exactly equal to I(i, S1, U<1>-S1) since S1 is empty.

Assume S1 consists of a single node b. In light of axiom (4) it is enough to show that for every j e U<t>-h the state­
ment I (i, h J) holds in P 0• If there exists a path from j to i, it must pass through h. Therefore, by definition (3) of r. since h is
the only parent of i, the equality p11 = PIA PltJ must be satisfied. This equality is a necessary and sufficient condition under which
I (i, h, j) holds in any normal distribution in which p1i are the correlation factors (in particular, P 0).
Assume S1 ={g, h} (see figure 6). Again. it is· enough to show that I (i, {g ,h } ,j) holds in P 0 for every j e U<o-S1•
Construct the covariance matrix for the variables g, h, i and j (the columns of the matrix correspond to this ord er). By equation
(3), p1; = p, Pili = p and p811 = 0. The resulting matrix is given below,
·

I"'=

[ 01 0
1

p

p

1 1l

p

p

p

p

:

Pti

Pv P"i Pti

0

Figure 6

The statement I (i. {g ,h } ,j) holds in this distribution iff the submatrix rij I is singular, i.e.

I
I
I
I

I
I
I
I
I
I
I

146

I

I
I

[ � �]
0

det

1

=

0

Pv P�i P ;i

I
I
I
I

I
I
I
I
I
I
I
I

([Miller, 1964]) or equivalently,

Pii

=

(Pni + P1i ) · p .

The latter equality, however, holds for all possible selections of a node j ; If j is not connected to j via a regular path,
i.e. Pii

=

0, then it is not connected through a regular path to either of i 's parents and therefore both Pli and

Pni

are zero. If j is

connected through a regular path of length I to g (similarly when connected to h ) then it is connected to its son i with a path of
length

/+1

and is not connected to i 's other parent. in which case

Pii

=

p1

holds. D.

+1,

p1i

=

p1 , Pni

=

0 and therefore the above equality

REFERENCES
Blalock, H.M. 1971 . Causal Models in the Social Sciences, London: Macmillan.
Dawid A.P. 1979. Conditional Independence in Statistical Theory, JR. Statist. Soc. B, vol. 4 1 , no. 1, pp. 1 -3 1 .
Duncan O.D. 1975. Introduction to Structural Equation Models, New York: Academic Press.

Fagin R. 1982. Hom Clauses and Database Dependencies, JACM. vol 29, no 4, pp. 952-985.
Geiger D. 1987. Towards the Formalization of Informational Dependencies, (M.S. thesis). UCLA Cognitive Systems Laborato­
ry, Technical Report R-102.
Geiger D., Pearl J. 1988. Logical and Algorithmic Properties of Conditional Independence, UCLA Cognitive Systems Laborato­
ry, Technical Report 870056 (R-97).
Geiger D., Verma T. S. 1988. Recognizing Independence in Influence Diagrams with Deterministic Nodes, UCLA Cognitive
Systems Laboratory, Technical Report R -116, in preparation.
Howard R. A., Matheson J. E. 1 98 1 . Influence Diagrams, In Principles and ApplicaJions of Decision Analysis, Menlo Park, Ca.:
Strategic Decisions Group.
Isham V. 198 1 . An Introduction to Spatial Point Processes and Markov Random Fields, inlernational Statistical Review, vol. 49,
pp. 21-43.
Kiiveri H., Speed T. P., Carlin J. B. 1984. Recursive Causal Models, Journal ofAustralian Math Society, val. 36, pp. 30-52.
Lauritzen S. L. 1 982. Lectures on Contingency Tables, 2nd Ed., University of Aalborg Press, Aalborg, Denmark.
Miller K. S. 1964, Multidimensional Gausian distributions, Wiley Siam series in applied math.

Pearl I. 1 985 . A Constraint-Propagation Approach to Probabilistic Reasoning. Proc. First Workshop on Uncertainty in AI, Los
Angeles, pp.

3 1 -42.

Pearl J. 1986a. Fusion, Propagation and Structuring in Belief Networks, Artiftcial lnlelligence, val. 29, no. 3, pp. 241 -288.
Pearl J. 1988. Probabilistic Reasoning in Intelligent Systems, San Mateo:Morgan-Kaufmann.

Pearl J., Paz A. 1985. GRAPHOIDS: A Graph-based Logic for Reasoning about Relevance Relations, UCLA Computer Science
Deparunent Technical Report 850038 (R-53), October 1985; also in Advances in Artiftcial lntelligence-l/, Edited by

B. Du

Boulay et al. Amsterdam: North-Holland Publishing Co. 1987.
Pearl I., Verma T. S. 1987. The Logic of Representing Dependencies by Directed Acyclic Graphs, Proc. 6th Nat. Conf. on AI

(AAAI-87), Seaule, vol. 1, pp. 374-379.

pp.

374-379.

Shachter R. 1985. Intelligent Probabilistic Inference, Proc. First Workshop on Uncertainty in AI, Los Angeles, pp. 237-244.
Shachter R. 1988. Probabilistic Inference and Influence Diagrams, to appear in Operations Research.
Shafer G., Shenoy P.P., Mellouli K. 1987. Propagating Belief Functions in Qualitative Markov Trees, The University of Kansas,
Lawrence School of Business, working paper no. 190. To appear in lnterfUllionaJ Jownal ofApproximate Reasoning.
Smith J.Q. 1987. Influence Diagrams for B ayesian Decision Analysis, Technical report # 100, Department of S tatistics, Universi­

I
I
I
I

ty of Warwick, Coventry, England.
Verma T. S. 1986. Causal Networks: Semantics and Expressiveness, UCLA Cognitive Systems t.aboratory Technical Report R65, Los Angeles, California Also in (Verma and Pearl, 1988], this volume.
Verma T. S. 1987. Some Mathematical Properties of Dependency Models, UCLA Cognitive Systems Laboratory, Los Angeles,
California, Technical Report R-103.
Wermuth N., Lauritzen S. L. 1983. Graphical and Recursive Models for Contingency Tables, Biometrika, vol. 70, pp. 537-552.

Wold H. 1964. Econometric Model Building, Amsterdam: North-Holland Publishing Co..
Wright S. 1934. The Method of Path Coefficients, Ann. Math. Statist., vol. 5, pp. 1 6 1-215.

147

