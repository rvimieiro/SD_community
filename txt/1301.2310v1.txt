SHELTON

496

UAI2001

Policy Improvement for POMDPs
using Normalized Importance Sampling

Christian R. Shelton

Artificial Intelligence Lab
Massachusetts Institute of Technology
Cambridge, MA 02139
cshelton�ai.mit.edu

Abstract

gathering data are unrestricted.

Either we did not

have control over the method for data collection, or
We present a new method for estimating the
expected return of a POMDP from experi­
ence.

The estimator does not assume any

knowledge of the POMDP, can estimate the
returns for finite state controllers, allows ex­
perience to be gathered from arbitrary se­
quences of policies, and estimates the return
for any new policy. We motivate the estima­
tor from function-approximation and impor­
tance sampling points-of-view and derive its
bias and variance. Although the estimator is
biased, it has low variance and the bias is of­
ten irrelevant when the estimator is used for
pair-wise comparisons.

We conclude by ex­

tending the estimator to policies with mem­
ory and compare its performance in a greedy
search algorithm to the REINFORCE algo­
rithm showing an order of magnitude reduc­
tion in the number of trials required.

we would like to allow the learning algorithm the free­
dom to pick any policy for any trial and still be able
to use the data.
Importance sampling has been studied before in con­
junction with reinforcement learning.
lar, [Precup et al.,

2000,

Precup et al.,

In particu­

2001]

use im­

portance sampling to estimate Q-values for MDPs
with
all
icy.

function

approximation for

the

case

where

data have been collected using a single pol­
[Meuleau et al.,

uses

2001]

importance sam­

pling for POMDPs, but to modify the REINFORCE
algorithm [W illiams,
als.

which ignores past tri­

1992]

[Peshkin and Mukherjee,

2001]

considers estima­

tors very similar to the ones developed here and prove
theoretical PAC bounds for them. This paper differs
from previous work in that it allows multiple sampling
policies, uses normalized estimators for POMDP prob­
lems, derives exact bias and variance formulas for nor­
malized and unnormalized estimators, and extends im­
portance sampling from reactive policies to finite state

1

controllers.

Introduction

In the next section we develop two estimators ( unnor­

We assume a standard reinforcement learning setup:

malized and normalized). Section 3 shows that while

an agent interacts with an environment modeled as

the normalized estimator is biased, its variance is much

a partially-observable Markov decision process. Con­

lower than the unnormalized (unbiased) estimator re­

sider the situation after a sequence of interactions.

sulting in a better estimator for comparisons.

The agent has now accumulated data and would like

tion 4 demonstrates some results on simulated envi­

Sec­

to use that data to select how it will act next. In par­

ronments. We conclude with a discussion of how to

ticular, it has accumulated a sequence of observations,

improve the estimator further.

actions, and rewards and it would like to select a pol­
icy, a mapping from observations to actions, for future
interaction with the world. Ultimately, the goal of the
agent is to find a policy mapping that maximizes the
agent's return, the sum of rewards experienced.
[Kearns et al.,

1999]

presents a method for estimating

2
2.1

Estimators
Notation

In this paper we will use

s

to represent the hidden

the return for every policy simultaneously using data

state of the world, x for the observation,

gathered while executing a fixed policy.

tion, and

In this pa­

per we consider the case where the policies used for

r

a

for the ac­

for the reward. Subscripts denote the time

index and superscripts the trial number. We will be

497

SHELTON

UAI2001

studying episodic tasks of fixed-length, T. The start­

with low variance.

ing distribution over states is fixed (and unknown).
Let
tion

1r(x, a )
a

be a policy (the probability of picking ac­

x).

upon observing

For the moment we will

consider only reactive policies of this form. h repre­
sents a history1 (of T time steps) and therefore is a

tuple of four sequences: states ( s1 through sT ) , obser­

vations

(x1

through

XT),

actions

(

a1

through aT , and

)

rewards (r1 through rr ). The state sequence is not
available to the algorithm and is for theoretical con­

sideration only. Lastly, we let R be the return (or sum

of r1 through rr ) .

1r1

hn

through

1rn

are the

are the associated

n

n

policies tried.

h

1

through

histories with R1 through Rn

2.3

Sampling Ratios

We have accumulated a set of histories

A key observation is that we can calculate one fac­
tor in the probability of a history given a policy. In
particular, that probability has the form
T

p(h\1r) p( s! ) IJ p(xt I st)7r(Xt, at)P( St+list, at)
=

resulting in the history

1ri

=

hi. Ri is used as a shorthand notation for R ( hi ) , the
return of

2.2

h'.

=

t=l

[v(si) D,p(xtlst)p(st+Iist, at)] [D,1r(xt, at)]
W(h)A(h, 1r) .

Importance Sampling

Importance sampli ng is typically presented as a
method for reducing the variance of the estimate of

an expectation by carefully choosing a sampling dis­
tribution [Rubinstein,

1981].

direct method for evaluating
ple i.i.d.
mate.

through

to find a guess at the best policy.

being the returns of those histories. Thus during trial

i the agent executed policy

(h1

hn ) each recorded by executing a (possibly different)
policy (1r1 through 1rn ) . We would like to use this data

X;

""'

p(x)

and use

For example, the most

J f(x)p(x) dx is to sam­
* 2:::; f(x;) as the esti­

However, by choosing a different distribution

q(x) which has higher density in the places where
lf(x)l is larger, we can get a new estimate which is
still unbiased and has lower variance. In particular,

*

we now draw
and use
2:::;
as our
_
estimate.
This can be viewed as estimating the expec­

x; q(x)
rv

f(x;)��::l

f(x)� with respect to q(x) which is like
approximating J f(x)�q(x) dx with samples drawn
from q(x). If q(x) is chosen properly, our new estimate
tation of

has lower variance. It is always unbiased provided that
the support of

p(x) and q(x) are the same.

In this pa­

per we only consider stochastic policies that have a
non-zero probability of taking any action at any time.
Therefore, our sampling and target distributions will
always have the same support.

q(x) to reduce variance, we
q(x) because of how our data

W ( h) , the effect of the world, is not because it depends

on knowledge of the underlying state sequence. How­
ever,

W(h)

does not depend on

Instead of choosing

will
was

collected. Unlike the traditional setting where an es­
timator is chosen and then a distribution is derived
w hich will achieve minimal variance, we have a distri­
bution chosen and we are trying to find an estimator
1It might be better to refer to this as a trajectory since
we will not limit h to represent only sequences that have
been observed; it can also stand for sequences that might
be observed. However, the symbol t is over used already.

Therefore, we have chosen to use h to represent state­

1r.

This implies that

the ratios necessary for importance sampling are ex­
actly the ratios that are computable without knowing
the state sequence. In particular, if a history h was

drawn according to the distribution induced by

1r

and

we would like an unbiased estimate of the return of

1f1,

then we can use R(h)

��W',;? and although neither

the numerator nor the denominator of the importance
sampling ratio can be computed, the

W(h)

term in

each cancels leaving a ratio of A(h, 1r1 ) to A( h, 1r ) which
can be calculated. A different statement of the same
fact has been shown before in [Meuleau et aL,

2001].

This fact will be exploited in each of the estimators in
this paper.
2.4

Importance Sampling

as

Function

Approximation
Because each

be forced to use

observation-action-reward sequences.

A (h, 1r), the effect of the agent, is computable whereas

1ri

is potentially different, each hi is

drawn according to a different distribution and so
while the data are drawn independently, they are not
identically distributed. This makes it difficult to apply
importance sampling directly. The most obvious thing
to do is to construct

n

estimators (one from each data

point) and then average them. This estimator has the
problem that its variance can be quite high. In par­

ticular, if only one of the sampled policies is close to
the target policy, then only one of the elements in the

sum will have a low variance. The other variances will
be very high and overwhelm the total estimate. We
might then only use the estimate from the policy that

498

SHELTON

the t arget p oli cy. Yet, we
do better by using all of the data.

is most similar to
hope to
To

motivate

the

we d emonstrate

estimator

of

how importance

viewed in terms

of function

sampling

in

the next section,
sampling can be

approximation.

}(x)
p(x)

general

=

=

Im­

seeks

f(xi(x))

;
a

=

• • •

Figure 1: Dep enden c y graph for agent-world interac­
tion with memory model

p(xi(x)) .

We now must define the size of the "basin" near sample
xi. In particular we let oJ be the size of the region of
the sampling space closest to xi. In the case where the
sampling space is discrete, this is the number of points
which are closer to sampled point xi than any other
sampled point. For continuous sampling spaces, a; is
the volume of space which is closest to xi. W ith this
definition,

J }(x)p(x) dx

t ime

would

to estimate
J f(x)p(x) dx. Consider estimating this integral by
e valuating J }(x)p(x) dx where j and pare approxi­
mations of f and p derived from data. In particular,
with a bit of foresight we will choose j and p to be
neare st-n eighb or estimates. Let i(x) b e the index of
the data point nearest to x. Then,
portance

UAI 2001

L tif(xi)p(xi) .
'

we will need to ap­
q(x) be the distribution from which
xi was sampled. On average, we expect the density
of points to be in versely proportional to the volume
nearest each point. For inst an c e , if we have sample d
uniformly from a uni t volume and the average den­
sity of points is d, then the average volume nearest
any given point is �- Extending this princi ple , we will
i
take the estimate of a to be inversely proportional to
the s amplin g density at xi. This yields the standard
importance s ampli ng estimator

which, when

translated to the

POMDP estimation

problem, becomes

(1)
is unbiased ( shown in the full version
of this paper [Shelton, 2001]) and has a lower variance
than the sum of n sing le sample estimators because
if one of the sampling distributions is near the target
distribution, then all elements in the sum share the
benefit.
This estimator

cannot be computed and thus

proximate it. Let

2.5

Normalized Estimates

We can normalize the importance sampling esti­
mate to obtain a lower variance estimate at the
cost of adding bias. Previous work has used a
variety of names for this including weighted uni­
form sampling [Rubinstein, 1981], weighted impor­
t an ce samplin g [Prec up et al ., 2000], and ratio esti­
mate [He st er berg , 1995]. In general, such an estimator
has the form

�
�i

x')
J(X;)p(
<i(X1)

� p(x')
�i q(x')

More impo rt antl y,

this derivation gives insight into
how to merge s amples from different distributions,
q1 (x) through qn(x). Not until the estim ati on of ai did
we require knowledge about the sampling densit y. We
can us e the same approximations for j and p. W hen
e stimating ai we need only an estimate of the den­
sity of points at ai to estimate the volume near xi.
We therefore take the mixture density, * 2:::; q;(x) (the
average of all of the sa mpling densities) as the distri­
bution of points in sample space. Applying this change
results in the estimator

problem with the previous estimator can be seen
by noting that the function approximator p(h) does
not int egrate ( or sum) to 1. Inste ad of p = p(xi(x) ) , we
make sure p int egrates ( or su ms ) to 1: p p(xi(xl)jZ
where Z
2:::; aip(xi). W hen recast in terms of our
POMDP problem the estimator i s
T he

=

=

�n
�i=l

Ri

�n
�i=l

2.6
So

p h'lrr)

j'�1 p(h'lrrJ)

p(h'lrr)

Lj'�1 p(h'ln1)

(2)

Adding Memory

far

we have on ly

policies

discussed estimators for reactive
(p oli cies that map the immediate observation

499

SHELTON

UA12001

unnormalized

to an action). We would like to be able to also estimate

normalized

the return for policies with memory. Consider adding
memory in the style of a finite-state controller.

At

each time step, the agent reads the value of the mem­
ory along with the observation and makes a choice

about which action to take and the new setting for

the memory.

The policy now expands to the form

K(x,m, a,m1)
ing action
vation

x

a

=

p(a, m'Jx, m), the

probability of pick­

and new memory state m' given obser­

and old memory state

m.

Now let us factor

this distribution, thereby slightly limiting the class of
policies realizable by a given memory size but mak­
ing the model simpler. In particular we consider an
agent model where the agent's policy has two parts:

11'a(x, m, a) and 7rm(x, m,m'). The former is the proba­
bility of choosing action a given that the observation is

x

and the internal memory ism. The latter is the prob­

ability of changing the internal memory to m' given the
observation is

p(a, m'Jx, m)

x

=

and the internal memory ism. Thus

7ra(x, m, a)7rm(x, m,m').

By this fac­

toring of the probability distribution of action-memory
choices, we induce the dependency graph shown in fig­
ure 1.

If we let

M

be the sequence mt, m2, ..

can be written as

I)(h,Mj11')
M
=

. , mr, p(hj11')

estimates of the return differences as a function of the
number of data points. The data were collected by ex­
ecuting the policy corresponding to the point (0.4, 0.6)
in figure 3. The estimators were evaluated at the poli­

cies (0.3, 0 . 9) and (0.4, 0.5).

T

t=l

1Tm (xt, mt, mt+l )p(st+llst, at)

IT

[Lp(ml)
11'a(Xt, mt, at)1Tm(Xt, ffit, ffit+l)l
M
t=l
W(h)A(h, n ) ,

once again splitting the probability into two factors:
one for the world dynamics and one for the agent dy­

A(h, n)

term involves a sum over all pos­

sible memory sequences. This can easily be computed
by noting that

A(h, 1r)

is the probability of the ac­

tion sequence given the observation sequence where
the memory sequence is unobserved. This is a (slight)

variation of a hidden Markov model: an input-output

eraged over 600 experiments. The horizontal line on
the plots of the mean represent the true difference in
returns. The normalized plots fit the theoretical values
well. The unnormalized plots demonstrate that even

deviation should decay as

observation probabilities for a time step (the memory
policy and the action policy respectively) depend on
at that time step. Yet, the x's are visible

Jn·

However, because the

unnormalized estimator is much more asymmetric (it
relies on a few very heavily weighted unlikely events to
offset the more common events), the graph does not
correspond well to the theoretical values. This is in­
dicative of the general problem with the high variance
unnormalized estimates.

As such, we can now use the same estimators and al­
low for policies with memory. In particular, the esti­
mator has explicit knowledge of the working of the
memory.

This is in direct contrast to the method

of adding the memory to the action and observation
spaces and running a standard reinforcement learning
algorithm where the agent must learn dynamics of its
own memory. With our explicit memory model, the
learning algorithm understands that the goal is to pro­
duce the correct action sequence and uses the memory
state to do so by coordinating the actions in different
time steps.

HMM. The difference is that the HMM transition and

x

Plotted above are the

means and standard deviations of these estimates av­

bias or variance. The unnormalized mean should be

M

the value of

Empirical estimates of the means and stan­

constant at the true return difference and the standard

Lp(s1)p(m1 ) ITp(xt\st)11'a(Xt,mt,at)

namics. The

2:

600 trials are not enough to get a good estimate of the

[p(sl) gp(xtJst)p(st+llst,a t)l

=

Figure

dard deviations for the unnormalized and normalized

3
It

Estimator Properties
is

well

making it possible to compute the probability and its

timates

derivative by using the forward-backward algorithm.

are

known

(both

consistent

that

importance sampling es­

normalized

and

[Hesterberg, 1995,

unnormalized)
Geweke, 1989,

500

SHELTON

Kloek and van Dijk, 1978].

Additionally, normalized

UAI2001

si- y

and

estimators have smaller asymptotic variance if the

and

TJk y

are measures of (approximately)

bA,B

centrali�ed second moments.

sampling distribution does not exactly match the

is the bias of the

normalized estimate of the return difference.

distribution to estimate [Hesterberg, 1995]. However,

The means and variances are2

we are more interested in the case of finite sample
sizes.

The estimator of equation 1 is unbiased. That is, for a

{ n1, n2, ..

} , the expectation
the estimator evaluated at n is the

set of chosen policies,

over experiences of

.

, nn

true expected return for executing policy

n.

Similarly,

E[Du]

= RA- RB

E[D N]

= RA- R s- -bA B
n '

[

varDu

the estimator of section 2.5 (equation 2) is biased. In

specific, it is biased t owards the expected returns of

{n 1 ,n2 , .

.

.

,n

1
2
2
l =-(
2
-2sA8+s88)
sAA
,
n
'

The goal of constructing these estimators is to use

var[DN]

them to choose a good policy. This involves compar­
1r.

Therefore

and

ns.

In other words, we will

us

2

1

'

-

'

-

­

= -(s� ' A- 2s� B+sB
2 B)
n

,

,

1----­
1

the difference of the estimator evaluated at two differ­

1l"A

z

- ;;:(TJ�,A-2TJ�,B+ 7/�,B)

instead of considering a single point we will consider

ent points,

2

1

'

-21JA B + TJB B)
- -(TJA
,A
n

n} .

ing the estimates for different values of

1

1

(RA- R s)bA B + 0(-) .
- 3n2
n
'

e the

estimators to calculate an estimate of the difference in

expected returns between two policies. The appendix

The bias of the normalized return difference estimator

of the full version of this paper [Shelton, 2001] details

the derivation of the biases and variances.

and the variance of both return differences estimators

We only

decrease as l.
.

quote the results here. These results are for using the
same data for both the estimates at

1rA

and

nB.

for small

1

p( h)p(g) and thus
n�I (RA-Rs).
=

n,

the same variance would cause greater

relative fluctuations).

E[RJnx]):

In general we expect bA,B to be of the same sign as

.

L._, P(hJ 7T')
p(h) = - ""
n

=

is just as good as the true return difference (of course,

ized estimator as D N. First, a few useful definitions
=

p( h, g)

In this case ED
[ N]

If the estimator is only used for comparisons, this value

ized estimator asDu and the difference for the normal­

Rx

It is useful to note that if all of the

bA,B::::: RA-Rs.

We denote the difference in returns for the unnormal­

(allowing

n

1r"s are the same, then

.

� LP(hJ7Ti)p(gJni)
B)
p(
bA,B = Jj !R(h)- R(g)] h���=�:�1l' p(h,g)dhdg
2 Y = j R2(h p(hJ7Tx )p(hJ7Ty) dh
sx,
)
p(h)
h
h� J7ry)
si-,Y = j(R(h)- Rx)(R(h)- Ry)p( J 7T�l�
p(hl1rx)p(gJny) (h,g dhdg
p
= jj R (h)R(g)
)
p(h)p(g)
TJ� Y = JJ(R(h)- Rx )(R(g)- Ry)
,

p(h,g) =

RA - Rs.
than

si y

si,Y

and

We would also expect

(and similarly

TJi y

'IJk,Y

s\ y
,

and

p

'IJ.k,v)·

depend on the difference of the returns

fro n the ex ected return under 7TX and

�

to be less

to be less than

1ry. si- y
'

TJk y depend on the difference of the returns from

zero. Without any other knowledge of the underlying

POMDP, we expect that the return from an arbitrary
history be closer to

RA or RB than the arbitrarily cho­

dh sen value 0. If bA,B is the same sign as the true differ­

ry2
X,Y

p(hl1rx )p(gJ1ry) -(h ) dhd
p ,g
9
p(h)p(g)

Note that all of these quantities are invariant to the

number of samples provided that the relative frequen­

cies of the sampling policies remains fixed.

p

and

p

are measures of the average distribution over histo­

ries.

sky

and

r!i,v

are measures of second moments

ence in returns and the overlined values are less than

their counterparts, then the variance of the normalized
estimator is less than the variance of the unnormalized

estimator. These results are demonstrated empirically

in figure 2 where we compared the estimates for the
problem described in section 4.2.

2
For the normalized difference estimator, the expecta­
tions shown are for the numerator of the difference. The
denominator is a positive quantity and can be scaled to be
approximately 1. Because the difference is only used for
comparisons, this scaling makes no difference in its perfor­
mance. See the full version [Shelton, 2001] for more details.

SHELTON

UAI2001

4
4.1

501

Experiments
Reinforcement Learning Algorithm

We can turn either of these estimators into a greedy
learning

algorithm. To

find a

policy

by which to act,

the ag en t maximizes the value of the estimator

by hill­
climbing in the space of policies ( usin g the previous
policy as a starting point) until it reaches a maximum.
The agent uses this new policy for the next trial. After
the trial, it adds the new policy-history-return trip le
to its data and repeats.
The hill-climbing algorithm must be carefully

chosen.

varies greatly in magnitude (as shown below ) . There­

fore, we have found it best to use the direction
gradient, but not its magnitude

to

of the

determine the di­

rection in which to climb. In particular, we employ a

algorithm
1992].

ratio line search [Press et al.,

4.2

the "load-unload" world. This

to the positioning of a cart. The vertical axis

indicates

using a golden­

the position of the cart (five observations denoted by

boxes ) .

The cart is loaded when it reaches the left­

if it reaches the right-most position
it is unloaded and the agent receives a

most state and
while loaded,
single unit

of

reward.

The agent has two actions at

each point: move left or move right. Moving left or
right off the end leaves the cart unmoved. Each trial
begins in the

left-most

state and lasts 100 time st ep s

.

Two-dimensional Problem

Figure 3 shows a simple world for which the policy

12

two numbers (the probability of
going left when in the left half and the probability
of going left when in the right half ) and the true ex­
pected return as a function of the policy. F igu re 4
compares the normalized (equ ation 1) and unnormal­
ized ( e q uation 2) es timato r s both with the greedy pol­
icy improvement algorithm and under random policy
can be described by

10

choices. We feel this example is illustrative of the rea­
the problems we have tried.

is

not

.

60

100

12

The

10

estimator is wil ling to extrapolate to unseen regions
where the unnormalized estimator

6()

normalized greedy algorith m

Its bias to observed

returns works well to smooth out the space.

40

20

sons that the normalized estimate works much better

on

of

whether the cart is loaded. The agent only observes

For many estimates, the derivative of the estimate

conjugate gradient descent

Figure 5: Diagram

world has nine states. The horizontal axis corresponds

This also

causes the greedy algorithm to explore new areas of

the policy space whereas the unnormalized estimator
gets trapped in the visited area under greedy explo­
ration

and

does not successfully maximize the return

function.
4.3

500

Twenty-dimensional Problem

Although the left- r ight problem was nice because the
estimators could be plotted, it is very simple. The
load-unload problem of figure 5 i s more challenging.

To achieve reasonable p erformance
depend

on

the

actions must

the history. We give the agent one memory

bit as in sectio n

2.6;

policy parameters.

this results in twenty independent

et

[Williams, 1992]

REINFORCE

has also been used to attack a

[Peshkin

,

al. , 1999]. We

very

similar problem

compare the results of the

normalized estimator with greedy

1000

1500

2000

REINFORCE

search

to REIN-

Figure 6:

Comparison of

the greedy algorithm with

a normalized estimator to standard REINFORCE

on

the load-unload problem. Plotted are the returns as a
function of trial number

for ten runs of the

algori thm.

In the case of REINFORCE, the returns have been
smoothed over ten trials. The center line is the median
of the returns.

The lines on either side are the first

and third quartiles.

The top

and bottom lines are the

minimum and maximum values.

SHELTON

502

I

UAI2001

I

Figure 3: Left: Diagram of the "left-right" world. This world has eight states. The agent receives no reward in
the outlined states and one unit of reward each time it enters one of the solid states. The agent only observes

whether it is in the left or right set of boxed states (a single bit of information) . Each trial begins in the fourth

state from the left and lasts 100 time steps. Right: The true expected return as a function of policy for this
world.

normalized

unnormalized
random

greedy

greedy

random

.,

--.,

'
.

'

..

·
· .

I'

t.
.

.

'

. ·

'

·; .

..
p

i.

.

.

.•
p;wtllo'll

'

02

F(lolllloll�

c.�

u

0.1

0.2

1

'

,.,., ....

M

0.111

-- :

.-·

• ..

.

[3.
.

•

.

'

�Ioii i Ioft!

:

I

, .. ··

.

•··.

'

· . ....

· ·· , , .
··..,:

.

'

p(lotl.....

ll- O

'

0
......

0
LQ

��:±1.L11
�
�1....0

lloO

Figure 4: A comparison of the normalized and unnormalized estimators for single set of observations. For each
estimator, the return estimates are shown plotted after 5, 10, and 50 iterations

( samples) .

The left column is

for the greedy policy improvement algorithm and the right column is for uniformly sampled policies. The first
row shows the returns as a function of trial number.

The second shows the path taken in policy space

(or,

for right columns, the random samples taken ) . Both estimators were given the same sequence of data for the

random case. The random sampling of policies produces a better return surface in general, whereas the greedy

algorithm quickly maximizes the return ( within 10 episodes ) and provides a better estimate of the surface near
the maximum.

UAI2001

SHELTON

FORCE in figure 6. The REINFORCE algorithm fre­

quently gets stuck in local minima. The graph shown
is for the best settings for the step size schedule and
bias term of REINFORCE. In the best case, REIN­
FORCE converges to a near optimal policy in around

503

AT&T, Central Research Institute of Electric Power In­
dustry, Eastman Kodak Company, Daimler-Chrysler, Dig­
ital Equipment Corporation, Honda R&D Co., Ltd., NEC
Fund, Nippon Telegraph & Telephone, and Siemens Cor­
porate Research, Inc.

500 trials The greedy algorithm run with the norm al­

References

only does it reuse old experience, it has an explicit

[Geweke, 1989] Geweke, J. (1989).
Bayesian inference
in econometric models using monte carlo integration.
Econome trica, 57(6):1317-1339.

ized estimate makes much better use of the data. Not

model of the memory bit and therefore does not need
to learn the "dynamics" of the memory.

Most runs

converge to the optimal policy in about 50 trials. One
trial took about twice as long to converge to a slightly

suboptimal policy. Although not shown here, if we re­
move REINFORCE's memory model disadvantage by
requiring both algorithms to use "external" memory,
the importance sampling algorithm's performance de­
grades by approximately a factor of

5

2.

We think this normalized estimator shows promise. It
makes good use of the data and when combined with a

greedy algorithm produces quick learning. We would
like to extend it in two immediate ways. The first is

to provide error estimates or bounds on the return es­
timate. Although we have a formula for the varian ce
of the estimator, we still need a good estimate of this

( the

formula requires full

knowledge of the POMDP ) . Such an estimate would

allow for exploration to be incorporated into the al­
gorithm.
fied."

Second, the estimate needs to be "sparsi­

After

n

[Kearns et al., 1999] Kearns, M., Mansour, Y., and Ng, A.
(1999). Approximate planning in large POMDPs via
reusable trajectories. In Advances in Neural Information
Processing Systems, pages 1001-1007.
[Kloek and van Dijk, 1978] Kloek, T. and van Dijk, H. K.
(1978). Bayesian estimates of equation system param­
eters: An application of integration by monte carlo.
Econometrica, 46(1):1-19.

Conclusion

variance from the samples

[Hesterberg, 1995] Hesterberg, T. (1995). Weighted aver­
age importance sampling and defensive mixture distri­
butions. Technometrics, 37(2):185-194.

trials, computing the estimate

derivative) for a given policy takes

O(n)

(or

its

work. This

makes the entire algorithm quadratic in the number
of trails. However, a similar estimate could probably
be achieved with fewer points. Remembering only the

important trials would produce a simpler estimate.
Finally, it may seem disturbing that we must remem­
ber which policies were used on each trail.

The re­

turn doesn't really depend on the policy that the agent
wants to execute; it only depends on how the agent ac­
tually does act. In theory we should be able to forget
which policies were tried; doing so would allow us to
use data which was not gathered with a specified pol­
icy. The policies are necessary in this paper as proxies
for the unobserved state sequences. We hope in future
work to remove this dependence.

Acknowledgements
T his report describes research done within CBCL in
the Department of Brain and Cognitive Sciences and
in the AI Lab at MIT. This research is sponsored by
a grants from ONR contracts Nos. N00014-93-l-3085 &
N00014-95-l-0600, and NSF contracts Nos. IIS-9800032
& DMS-9872936. Additional support was provided by:

[Meuleau et al., 2001] Meuleau, N., Peshkin, L., and Kim,
K.-E. (2001). Exploration in gradient-based reinforce­
ment learning. Technical Report AI-MEMO 2001-003,
MIT, AI Lab.
[Peshkin et a!., 1999] Peshkin, L., Meuleau, N., and Kael­
bling, L. P. (1999). Learning policies with external mem­
ory. In Proceedings of the Sixteenth International Con­
ference on Machine Learning.
[Peshkin and Mukherjee, 2001] Peshkin, L. and Mukher­
jee, S. (2001). Bounds on sample size for policy eval­
uation in markov environments. In Fourteenth Annual
Conference on Computational Learning Theory.
[Precup et al., 2001] Precup, D., Sutton, R. S., and Das­
gupta, S. (2001). Off-policy temporal-difference learning
with function approximation. In Proceedings of the Eigh­
teenth International Conference on Machine Learning.
[Precup et a!., 2000] Precup, D., Sutton, R. S., and Singh,
S. (2000). Eligibility traces for off-polcy policy evalua­
tion. In Proceedings of the Seventeenth International
Conference on Machine Learning.
[Press et al., 1992] Press, W. H., Teukolsky, S. A., Vet­
terling, W. T ., and Flannery, B. P. (1992). Numerical
Recipes in C. Cambridge University Press, second edi­
tion.
[Rubinstein, 1981] Rubinstein, R. Y. (1981). Simulation
and the Monte Carlo Method. John Wiley & Sons.
[Shelton, 2001] Shelton, C. R. (2001). Policy improvement
for POMDPs using normalized importance sampling.
Technical Report AI-MEMO 2001-002, MIT, AI Lab.
[W illiams, 1992] Williams, R. J. (1992). Simple statisti­
cal gradient-following algorithms for connectionist rein­
forcement learning. Machine Learning, 8:229-256.

