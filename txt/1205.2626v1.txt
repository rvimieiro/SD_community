UAI 2009

MARLIN ET AL.

383

Group Sparse Priors for Covariance Estimation

Benjamin M. Marlin, Mark Schmidt, and Kevin P. Murphy
Department of Computer Science
University of British Columbia
Vancouver, Canada

Abstract
Recently it has become popular to learn
sparse Gaussian graphical models (GGMs)
by imposing ℓ1 or group ℓ1,2 penalties on
the elements of the precision matrix. This
penalized likelihood approach results in a
tractable convex optimization problem. In
this paper, we reinterpret these results as performing MAP estimation under a novel prior
which we call the group ℓ1 and ℓ1,2 positivedefinite matrix distributions. This enables
us to build a hierarchical model in which
the ℓ1 regularization terms vary depending
on which group the entries are assigned to,
which in turn allows us to learn block structured sparse GGMs with unknown group assignments. Exact inference in this hierarchical model is intractable, due to the need to
compute the normalization constant of these
matrix distributions. However, we derive upper bounds on the partition functions, which
lets us use fast variational inference (optimizing a lower bound on the joint posterior). We
show that on two real world data sets (motion capture and financial data), our method
which infers the block structure outperforms
a method that uses a fixed block structure,
which in turn outperforms baseline methods
that ignore block structure.

1

Introduction

Reliably estimating a covariance matrix Σ is a fundamental problem in statistics and machine learning that
arises in many application domains. Covariance estimation is well known to be a statistically challenging
problem when the dimensionality of the data D is high
relative to the sample size N . In the D > N regime,
the standard maximum likelihood estimate (the sam-

ple covariance matrix S) is not positive-definite. Even
when N > D the eigenstructure of the sample covariance matrix can be significantly distorted unless D/N
is very small (Dempster, 1972).
One particularly promising regularization approach to
covariance estimation is to penalize the ℓ1 -norm of the
precision matrix, Ω = Σ−1 , to encourage sparsity in
the precision matrix (Banerjee et al., 2008; Friedman
et al., 2007; Yuan and Lin, 2007; Duchi et al., 2008;
Schmidt et al., 2009). Zeros in the precision matrix
result in absent edges in the corresponding Gaussian
graphical model (GGM), so the ℓ1 -norm can be interpreted as preferring graphs that have few edges.
The resulting penalized negative log-likelihood objective function is convex and can be optimized by a variety of methods.
For some kinds of data, it is reasonable to assume
that the variables can be clustered (or grouped) into
types, which share similar connectivity or correlation
patterns. For example, genes can be grouped into
pathways, and connections within a pathway might be
more likely than connections between pathways. Recent work has extended the above ℓ1 penalized likelihood framework to the case of block sparsity by penalizing the ℓ∞ -norm (Duchi et al., 2008) or the ℓ2 -norm
(Schmidt et al., 2009) of each block separately; the
analogous results for linear regression are known as the
simultaneous lasso (Turlach et al., 2005) and the group
lasso (Yuan and Lin, 2006) respectively. The resulting
objective function is still convex, and encourages block
sparsity in the underlying graphs.
For many problems the group structure may not be
known a priori, so methods that can simultaneously
infer the group structure and estimate the covariance
matrix are of great interest. In this paper we convert
the ℓ1 and group ℓ1,2 regularization functions into distributions on the space of positive-definite matrices.
This enables us to use them as components in larger
hierarchical probabilistic models. The key contribution is the derivation of a novel upper bound on the

384

MARLIN ET AL.

intractable normalizing term that arises in each distribution, which involves an integral over the positivedefinite cone. This allows us to lower bound the loglikelihood and derive iterative model fitting procedures
that simultaneously estimate the structure and the covariance matrix by optimizing the lower bound. We
present analysis and simulation studies investigating
properties of the two distributions and their bounds.
We apply the bounding framework to the problem of
covariance estimation with unknown block structure.

2

Related work

The prior distributions we derive and the subsequent covariance estimation algorithms we propose are
closely related to the work of Yuan and Lin (2007);
Banerjee et al. (2008), which impose sparsity on the
elements of the precision matrix Ω using an ℓ1 -norm
subject to the constraint that Ω be symmetric and
positive-definite. The penalized log-likelihood objective function used to fit the precision matrix is given
by
log det(Ω) − trace(ΩS) −

D
D X
X
i=1 j=1

λij |Ωij |

(2.1)

where S is the sample covariance matrix and λij ≥ 0
are the penalty parameters. This objective function
is concave for fixed penalty parameters, and various
efficient algorithms (typically O(D3 ) or O(D4 ) time
complexity per iteration) have been proposed to solve
it (Friedman et al., 2007; Duchi et al., 2008; Schmidt
et al., 2009).
In the case where we have known groups of variables,
denoted by Gk , we can use the following alternative
penalization function:
X
−
λkl · ||{Ωij : i ∈ Gk , j ∈ Gl }||pkl
(2.2)

UAI 2009

(Duchi et al., 2008). Tikhonov regularization provides
a useful baseline estimator for covariance estimation
when N/D is small.
The case of group penalization of GGMs with unknown
groups has only been studied very recently. In our
previous paper (Marlin and Murphy, 2009), we used a
technique that is somewhat similar to the one to be
presented in this paper, in that each variable is assigned to a latent cluster, and variables in the same
cluster are “allowed” to connect to each other more
easily than to variables in other clusters. However,
the mechanism by which we inferred the grouping was
quite different. Due to the intractability of evaluating the global normalization constant (discussed below), we used directed graphical models (specifically,
dependency networks), which allowed us to infer the
cluster assignment of each variable independently (using EM). Having inferred a (hard) clustering, we then
used the penalty in Equation 2.2. In this paper, we
instead bound the global normalization constant, and
jointly optimize for the group assignments and for the
precision matrix parameters at the same time.
We recently came across some independent work (Ambroise et al., 2009) which presents a technique that
is very similar to our group ℓ1 method (Section 3.2).
However, their method has two flaws: First, they ignore the fact that the normalization constant changes
when the clustering of the variables changes; and second, they only use local updates to the clustering,
which tends to get stuck in local optima very easily (as
they themselves remarked). In our paper, we present
a mathematically sound derivation of the method, an
extension to the group ℓ1,2 case, and a much better
optimization algorithm.

3

The Group ℓ1 and Group ℓ1,2
Distributions

kl

where pkl specifies which norm to apply to the elements from each pair of groups. (Duchi et al., 2008)
consider the case pkk = 1 within groups and pkl = ∞
between groups. (Schmidt et al., 2009) consider the
case pkk = 1 within groups and pkl = 2 between
groups, which we refer to as group ℓ1,2 penalization.
In the limit where each edge is its own group, both
penalization functions reduce to the independent ℓ1
penalization function. If all diagonal penalty parameters are all equal to some λ > 0 and the off-diagonal
penalty parameters are zero, the optimal precision matrix can be found in closed form by differentiating the
penalized log-likelihood objective function. We obtain
Ω̂ = (S + λI)−1 , or equivalently Σ̂ = S + λI. We refer
to this method as Tikhonov regularization, following

It is well known that ℓ1 regularized linear regression
(i.e., the lasso problem) is equivalent to MAP estimation using a Laplace prior. In this section, we derive
the priors that correspond to various ℓ1 regularized
GGM likelihoods, assuming a known assignment of
variables into groups. In later sections, we will use
these results to jointly optimize over the precision matrix and the groupings.
3.1

The Independent ℓ1 Distribution

The penalized log-likelihood in Equation 2.1 (or rather
a slight variant in which we only penalize the upper
triangle of X, since the matrix is constrained to be
symmetric) is equivalent to MAP estimation with the
following prior, which we call the ℓ1 positive-definite

UAI 2009

MARLIN ET AL.

matrix distribution:
PL1 (X|λ) =

1
ZI1 pd(X)

D
D Y
Y

i=1 j>i

exp(−λij |Xij |) (3.3)

We represent the positive-definiteness constraint using
the indicator function pd(X), which takes the value
one if X is positive-definite, and zero otherwise. The
normalization term ZI1 is obtained by integrating the
unnormalized density over the positive-definite cone.
This integral is intractable, but as long as the λij terms
are held fixed, the term is not needed for MAP estimation.
3.2

D

PGL1 (X|λ, z) =
D
D Y
Y

i=1 j>i

Y
1
exp(−λD |Xii |)
pd(X)
Z1
i

exp(−(λ1 δzi ,zj + λ0 (1 − δzi ,zj ))|Xij |) (3.4)

We refer to this as the group ℓ1 positive-definite matrix
distribution. Note that, despite the name, this distribution does not enforce group sparsity as such, since
each element is independently penalized. However, elements in the same group share the same regularizer,
which will encourage them to “behave” similarly in
term of their sparsity pattern. (This is equivalent to
the model considered in Ambroise et al. (2009).)
3.3

the precision entry between a pair of variables {i, j}
within the same group is penalized using an ℓ1 -norm
with penalty parameter λ1 , except for diagonal entries
which are penalized with λii = λD . For each pair of
distinct groups k, l, the between group precision entries are penalized jointly under an ℓ2 -norm with a
penalty parameter equal to λkl = Ckl λ0 where Ckl is
the product of the size of group k and the size of group
l (Schmidt et al., 2009). Scaling by the group size (or
any power of the group size greater than 1/2) ensures
that the between-group penalties are always greater
than the within-group penalties when λ0 > λ1 . The
corresponding prior is shown below; we refer to it as
the group ℓ1,2 positive-definite matrix distribution.

The Group ℓ1 Distribution

Our focus in this paper is developing algorithms that
infer Ω under a block structured prior while simultaneously estimating the blocks. The ℓ1 distribution does
not have a block structure by default, so we augment
it with an additional layer of discrete group indicator variables. We assume that the data variables are
partitioned into K groups, and we let zi indicate the
group membership of variable i. We encourage group
sparsity by constraining the λij such that λij = λ0 if
i and j are in different blocks, and λij = λ1 if i and
j are in the same block, where λ0 > λ1 . In addition,
we introduce a separate parameter for the diagonal of
the precision matrix λii = λD . The full distribution is
given below (where we use the Kronecker delta notation δi,j = 1 if i = j and δij = 0 otherwise).

×

385

The Group ℓ1,2 Distribution

We now derive a prior which yields behavior equivalent
to the penalty term in Equation 2.2 for the case pkl = 2
(again, we only penalize the upper triangle of the matrix). Unlike the previous group ℓ1 prior, this group
ℓ1,2 prior has the property that elements within the
same group are penalized together as a group. More
precisely, under the group ℓ1,2 regularization function

PGL1L2 (X|λ, z) =
×
×

D
D Y
Y

i=1 j>i

exp(−λ1 δzi ,zj |Xij |)

K
K Y
Y

k=1 l>k

3.4

D
Y
1
exp(−λD |Xii |)
pd(X)
Z12
i=1






exp −λ0 Ckl 

D
D X
X
i=1 j=1

1/2 

δzi ,k δzj ,l (Xij )2  
(3.5)

Group Sparsity Property

The main property of the group ℓ1 and group ℓ1,2 distributions that we are interested in is the supression
of matrix entries between groups of variables. To investigate this property we develop Gibbs samplers for
both the group ℓ1 and group ℓ1,2 distributions (see Appendix A for details of the samplers). We consider the
illustrative case D = 4 yielding five distinct partitions
(groupings) of the variables. We run each Gibbs sampler on each grouping with the settings λD = λ1 = 0.1
and λ0 = 1. We record a sample after each complete
update of the matrix X. We collect a total of 1000
samples for each grouping.
In Figure 1 we show estimates of the expected absolute
values of Xij (E[|Xij |]) for the group ℓ1 distribution
(the figure is nearly identical in the group ℓ1,2 case).
Studying the expected absolute value of Xij is necessary to reveal the structure in X that results from
the underlying partition of the data variables, since
off-diagonal terms can be positive or negative. As can
clearly be seen in Figure 1, the off-diagonal terms between groups are suppressed while off diagonal terms
within groups are not. This is exactly what we would
expect based on the group structure of the distribution.
An interesting and unanticipated result is that larger
groups appear to have larger diagonal entries under

386

MARLIN ET AL.
Z=[ 1 2 3 4 ]

Z=[ 1 2 3 3 ]

UAI 2009

Z=[ 1 1 2 2 ]

Z=[ 1 2 2 2 ]

Z=[ 1 1 1 1 ]

1

1

1

1

1

2

2

2

2

2

3

3

3

3

3

4

4

4

4

4

15
10
5

1

2

3

4

1

2

3

4

1

2

3

4

1

2

3

4

1

2

3

4

Figure 1: This figure shows an estimate of the mean of the absolute value of X (E[|X|]) under the group ℓ1
distribution in four dimensions. The parameters used are λD = λ1 = 0.1 and λ0 = 1. Estimates under the group
ℓ1,2 distribution are nearly identical to the group ℓ1 case shown here.
both distributions. Based on the mean of an independent exponential distribution, a reasonable hypothesis
for the current parameters would be average diagonal entries of 1/0.1 = 10 units, which is the case for
the partition where every variable is in its own group.
However, the partition with all variables in the same
group [1, 1, 1, 1] shows a significantly higher diagonal of
approximately 15 units. This result clearly illustrates
coupling between entries in the matrix X induced by
the positive-definite constraint.

4

Lower Bounds

We recall that for fixed structure and penalty parameters, the estimation of the precision matrix Ω under
either the group ℓ1 or group ℓ1,2 prior distribution is
easy since the normalizing term in each distribution
is independent of Ω. The difficulty lies in updating
the group structure since the intractable normalization term is not constant with respect to changes in the
assignment of variables to groups. In this section we
derive upper bounds on the intractable normalization
terms that allow us to lower bound the log posterior.

4.1

Group ℓ1 Bound

We first note that the unnormalized densities are always non-negative. As a result, increasing the volume of the domain of integration when computing the
normalization term will provide an upper bound on
the intractable integral. Instead of integrating over
the positive-definite cone SD
++ , we integrate over the
strictly larger space of symmetric matrices with a positive diagonal. We denote this space of matrices by SD
P.
In the case of the group ℓ1 distribution, the integrand
completely decouples into independent parts corresponding to standard univariate Laplace and exponential integrals, yielding the following analytic soution
for the bound.

Z1 ≤

D
D Y
Y

Z

SD
P i=1 j≥i

exp(−λij |Xij |)dX

(4.6)

D D
D
Y
2
1 YY
·
=
λ
+
(1
− δzi ,zj )λ0
λ
δ
i=1 ii i=1 j>i zi ,zj 1

(4.7)

Group ℓ1,2 Bound

4.2

We now derive an upper bound on the normalization
term for the group ℓ1,2 distribution. We again apply
the strategy of increasing the volume of the domain of
integration from the set of positive-definite matrices to
the set of symmetric matrices with positive diagonal.
Z Y
D
D Y
D
Y
Z12 ≤
exp(−λ1 δzi ,zj |Xij |)
exp(−λD |Xii |)
SD
P i=1

·

K
K Y
Y

k=1 l>k

i=1 j>i






exp −λ0 Ckl 

D
D X
X
i=1 j=1

1/2 

δzi ,k δzj ,l (Xij )2   dX
(4.8)

Unlike the group ℓ1 case, the between-block precision
entries are coupled together by the ℓ2 -norm so the integrand does not completely decouple across the upper
triangular portion of the matrix X. However, a convenient expression for the bound can still be obtained
by breaking up the integral into a product of diagonal,
within-group, and between-group
terms. We introduce
P P
the auxiliary variables CT = i j>i δzi ,zj to represent the total number of within-group entries across all
blocks, and Ckl to represent the number of precision
entries between variables in group k and group l.

Z12 ≤
·

D Z
Y

i=1

∞

0

K Z
Y

k,l6=k

exp(−λD |x|)dx ·

RCkl



exp −λ0 Ckl

CT Z
Y

i=1

Ckl
X
i=1

∞

−∞

exp(−λ1 |x|)dx

!1/2 
 dx
x2i

(4.9)

UAI 2009

MARLIN ET AL.

The solution to the first two integrals in the bound
are standard univariate exponential and Laplace normalization constants. The integral resulting from the
between block entries is the normalization constant
for the multivariate Laplace distribution in Ckl dimensions (Gómez et al., 1998). This allows us to complete
the bound as follows.


Ckl −1
Ckl + 1
2
D  CT Y

2Ckl
Γ
K π
2
1
2
Z12 ≤
λD
λ1
(λ0 Ckl )Ckl
k,l6=k

(4.10)

4.3

Evaluation of the Bounds

In the case where D = 2 it is possible to obtain the
exact normalizing constant for the group ℓ1 distribution for arbitrary values of λD = λ1 and λ0 and the
two possible clusterings z1 = z2 and z1 6= z2 . We
note that in two dimensions the group ℓ1 and group
ℓ1,2 distributions are equivalent, Z12 = Z1 = Z. To
obtain the normalizing term we evaluate the following
integral where λ12 = λ1 if z1 = z2 and λ12 = λ0 is
z1 6= z2 .
Z
Z=
exp(−λ1 (|X11 | + |X22 |)) exp(−λ12 |X12 |)dX
SD
++

=

Z

0

∞

Z

0

∞

Z

√

ab

√

− ab

exp(−λ1 (a + b) − λ12 |c|)dadbdc
(4.11)

√
In the case z1 = z2 the integral evaluates to (8π 3 −
18)/(27λ31 ). We note that the value of the bound
in this case is 2/λ31 . The bound thus overestimates
the true normalizing term by a constant multiplicative factor approximately equal to 2.115, corresponding to an overestimation of the log normalization term
by a constant additive factor equal to 0.7491. In the
case z1 6= z2 we have obtained an explicit formula for
the normalizing term that is defined everywhere except
2λ1 = λ0 . The solution is significantly more complex
as seen in Equation 4.12. We have verified empirically
that the function is positive and real valued except at
the noted singularity.
 √

λ21 − 14 λ20
arctan 2
λ0
λ0

Z=−
+

1 2
2
2
−3/2
2 λ1 − 4 λ0 λ1
λ21 − 14 λ20
(4.12)
In Figure 2(a) we show a plot of the error under the
bound and under a Monte-Carlo approximation as a

387

function of λ1 and λ0 in the two dimensional case. We
show results for the two unique partitions [1, 1] and
[1, 2]. We use a simple importance sampling method
for the Monte-Carlo estimate where the proposal distribution is Wishart with a fixed scale matrix equal
to the identity matrix and 2 degrees of freedom. We
draw 100, 000 samples. The primary trend in the error of the bound is revealed by plotting the error as
a function of λ0 /λ1 . The error rapidly and smoothly
decreases as a function of λ0 /λ1 . The reason for this
is that as λ0 /λ1 increases, the support of the group
ℓ1 distribution collapses onto the sub-space of diagonal matrices where the bound is exact. Finally, the
Monte-Carlo estimate of the log normalization term
has approximately zero error over the whole range of
λ0 /λ1 values for both partitions.
We extend our analysis to the four dimensional case
in Figure 3 where we plot the estimated error between
the log bound and log normalizing term for each of
the five partitions as a function of λ1 and λ0 . We use
the Wishart importance sampler to estimate the normalization terms with scale matrix equal to the identity matrix and 4 degrees of freedom. We draw 107
samples. Similar to the exact analysis in the two dimensional case, we see that the minimum discrepancy
between the bound and the true normalizing term occurs for the case where all data dimensions are in their
own groups and λ0 /λ1 is large. The largest discrepancies occur in the case where all data dimensions are in
the same group and λ0 = λ1 .
In Figure 2(b) we show a plot of the bound and the
Monte-Carlo estimate of the normalizing term as a
function of the matrix dimension D. We use λ1 = 0.1
and λ0 = 0.5. We consider the partition where every
dimension is in the same group (1 grp), and the partition where every dimension is in its own group (D
grps). Initial investigations suggested that the bound
is tightest for the D groups case and weakest for the
one group case, so we only consider these two partitions. We also note again that the group ℓ1 and
group ℓ1,2 distributions are equivalent for these two
partitions. The results show that bound in the one
group case diverges more rapidly from the corresponding Monte-Carlo estimate of the true log normalizing
term compared to the bound on the D groups case.
The fact that the discrepancy in the bound changes as
a function of the grouping is somewhat troubling as it
may bias model selection towards models with more
groups.

388

MARLIN ET AL.

0.6

Normalization Approximation: Bound vs Sample
Log Normalization Estimate

Error in Log Normalization

Normalization Approximation Error: Bound vs Sample
0.8

Bound [1 2]
Bound [1 1]

0.4

Sample [1 2]
Sample [1 1]

0.2
0
0

5

10
λ0/λ1

15

UAI 2009

Bound D Grps
Bound 1 Grp
Sample D Grps
Sample 1 Grp

4

10

3

10

2

10

1

10

20

2

(a) Bound in 2D as a function of λ0 /λ1

4

8
16
32
Matrix Dimenstion D

64

(b) Bound as a function of D

Figure 2: Figure (a) shows the approximation error for the log normalization term under the bound and the
Monte-Carlo estimate as a function of λ0 /λ1 in two dimensions. Figure (b) shows both the bound and MonteCarlo estimate of the log normalization term as the number of dimensions D is varied for λ0 = 0.5 and λ1 = 0.1.
Z=[1 2 3 3]

Z=[1 1 2 2]

Z=[1 2 2 2]

Z=[1 1 1 1]
2

1.5

1.5

1.5

1.5

1.5

1
0.5

1
0.5

1
λ0

2

1
0.5

1
λ0

2

λ1

2

λ1

2

λ1

2

λ1

λ1

Z=[1 2 3 4]
2

1
0.5

1
λ0

2

6
4

1
0.5

1
λ0

2

2
1
λ0

2

0

Figure 3: This figure shows the approximation error for the log normalization term under the group ℓ1 bound
as a function of λ1 and λ0 for each of the five partitions in four dimensions. This figure is best viewed in color.

5

Block Sparse Precision Estimation
With Unknown Blocks

In this section we describe a hierarchical blockstructured model for precision estimation using the
group ℓ1 and group ℓ1,2 prior distributions, and discuss strategies for fitting the models. The hierarchical model includes a Gaussian likelihood term, a discrete distribution over the group indicators zi with
parameter θ, and a symmetric Dirichlet prior distribution on θ with parameter α0 /K where K is the
number of groups. The prior distribution on the precision matrix PG (Ω|λ, z) can be either the group ℓ1
distribution PGL1 (Ω|λ, z) or the group ℓ1,2 distribution PGL12 (Ω|λ, z).
P (θ|α0 ) = D(θ; α0 )
P (zi = k|θ) = θk
P (Ω|λ, z) = PG (Ω|λ, z)

(5.14)
(5.15)
(5.16)

P (xn |µ, Ω) = N (xn ; µ, Ω−1 )

(5.17)

Plugging the upper bound for the normalization term
of the group ℓ1 or group ℓ1,2 distribution into the com-

plete data log posterior yields an initial lower bound.
In addition we employ a variational Bayes approximation q(θ|α) for the posterior on the mixing proportions θ, where q(θ|α) is a Dirichlet distribution. This
is necessary since the size of θ varies according to the
number of groups, so such parameters need to be integrated out to perform proper model comparison. In
the group ℓ1 case we employ a fully factorized variational distribution on the group indicator variables
q(zi = k|φi ), where φi is a discrete distribution over
the K groups. The variational Bayes approximations
further lower bound the log posterior. In the group ℓ1,2
case, the mixture indicators are coupled through the
bound on the normalization term, so we work directly
with the discrete group indicator variables. We show
the bound on the log posterior for the group ℓ1 case,
using the variational Bayes approximation, in Equation 5.14. (The notation p̃(Ω|z) refers to the unnormalized distribution, and Ẑ1 is our approximate bound
from Equation 4.7. The last line corresponds to the entropy of the variational distribution.) The derivation
of the bound for the group ℓ1,2 case is very similar.

UAI 2009

MARLIN ET AL.

log p(Ω|X) = log

Z X
z

=

X
z

389

p(X|Ω)p(Ω|z)p(z|θ)p(θ|α0 )dθ ≥ log

Z X

− log Ẑ1 (z) + [log p(X|Ω) + log p̃(Ω|z)] + log

p(X|Ω)

z

Z

1
Ẑ1 (z)

p̃(Ω|z)p(z|θ)p(θ|α0 )dθ

(5.13)

[p(z|θ)p(θ|α0 )dθ]

n
o
≥ max Eq − log Ẑ1 (z) + [log p(X|Ω) + log p̃(Ω|z)] + log p(z|θ) + log p(θ|α0 ) − log q(z, θ|α, φ)
α,φ

D
X
N
N
(−λD |Ωii | + log(λD ))
(− log(2π) + log det(Ω)) − trace(ΩS) + log(pd(Ω)) +
α,φ,Ω 2
2
i=1

≥ max
+

D
D X
X
i=1 j>i

+

(− log(2) + Eq [δzi ,zj ](−λ1 |Ωij | + log(λ1 )) + (1 − Eq [δzi ,zj ])(−λ0 |Ωij | + log(λ0 ))

K
D X
X

i=1 k=1

−

K
D X
X

i=1 k=1

Eq [δzi ,k ]Eq [log(θk )] + log(Γ(α0 )) − K log(Γ(α0 /K)) +
Eq [log(φik )] − log(Γ(

K
X

αk )) +

k=1

In the group ℓ1 case, model estimation consists of optimizing Ω, and the variational parameters α and φ.
In the group ℓ1,2 case, model estimation consists of
optimizing Ω, the variational parameters α, and the
partition z. The strategy we use for both prior distributions is to start with all the data dimensions in the
same group. On each iteration we propose splitting
one group into two sub-groups. Given the updated
partition z (or updated variational parameters φ) we
employ the convex optimization procedure developed
by (Duchi et al., 2008) to update the precision matrix
under the group ℓ1 prior, and the convex optimization procedure developed by (Schmidt et al., 2009)
to update the precision matrix under the group ℓ1,2
prior. The computational cost of model estimation
is dominated by the precision matrix updates, which
are themselves iterative with an O(D3 ) cost per iteration. Finally, we evaluate the lower bound on the
log likelihood for the new grouping and parameter settings. We accept the split if it results in an increase
in the lower bound. We then update the variational
α parameters, which
PDhave the simple closed-form update αk = α0 + i=1 φik in the group ℓ1 case and
PD
αk = α0 + i=1 δzi ,k in the group ℓ1,2 case. In the
group ℓ1 case we update the variational φik parameters, which also have a simple closed-form solution. In
the group ℓ1,2 case we perform a local update for each
group indicator zi by reassigning it to the group that
gives the maximum value of the bound on the posterior. Each of these steps is guaranteed to increase the
value of the bound, and we continue splitting clusters
until no split is found that increases the bound.
The key to making the algorithm efficient is the choice

K
X

k=1

log(Γ(αk )) −

K
X

K
X

(α0 /K − 1)Eq [log θk ]

k=1

(αk − 1)Eq [log θk ]

k=1

of split proposals. We propose a split for a given group
by running a graph cut algorithm on a weighted graph
derived from the current precision matrix. More precisely, let U = {i : zi = k} be the set of variables
belonging to group k, and U be the other variables. In
the group ℓ1 case we use the MAP assignments under
the variational posterior zi = maxk φik . We propose a
split by computing a normalized cut of the weighted
graph W = |Ω(U, U )| + 0.5|Ω(U, U )||Ω(U, U )T |, which
measures the similarity of variables within group k to
each other, as well as the similarity in their relationships to other variables.
We consider two different methods for choosing which
groups to split. In the first method, we compute the
optimal split for each group. We sort the groups in
ascending order according to the weight of the cut divided by the number of variables in the group. We
evaluate the split for each group by updating all the
model parameters given the new group structure. We
accept the first split that results in an increase in the
bound on the log posterior. If none of the splits are
accepted, we terminate the model estimation procedure. We refer to this as the greedy method. In the
second method, we exhaustively evaluate the split for
all groups. To save on computation time we perform
an approximate update for the precision matrix where
we only update precision entries between each variable
in the group we are splitting and all of the other variables. This is a substantial savings when the groups
become small. We select the split giving the highest value of the bound, perform a full update on the
precision matrix, and re-compute the bound on the
log posterior. If the selected split fails to increase the

390

MARLIN ET AL.

bound on the log posterior, we terminate the model estimation procedure. We refer to this as the exhaustive
method.

6

Covariance Estimation Experiments

In this section we apply the group ℓ1 and group ℓ1,2
distributions to the regularized covariance estimation
problem. We consider the group ℓ1 greedy method
for unknown groups (GL1-ug), the group ℓ1 exhaustive method for unknown groups (GL1-ue), the group
ℓ1,2 greedy method for unknown groups (L12-ug), and
the group ℓ1,2 exhaustive method for unknown groups
(GL12-ue). We compare against three other methods:
Tikhonov regularization (T), independent ℓ1 regularization (IL1), and group ℓ1,2 regularization with known
groups (GL12-k). We compute test set log likelihood
estimates using five-fold cross validation. We hold out
an additional one fifth of the training set to use as a
validation set for selecting the penalization parameters
λD ,λ1 ,λ0 . For each penalty parameter we consider 10
values from 104 to 1 equally spaced on a log scale. We
consider all combinations of values subject to the constraint that λ0 > λ1 > 0.5λD . We set α0 = 1. We
center and scale all of the data before estimating the
models. We report test set log likelihood results using the parameters that achieve the maximum average
validation log likelihood.
6.1

CMU Motion Capture Data Set

In this section, we consider the motion-capture data
set used in our previous work (Marlin and Murphy,
2009). This consists of 100 data cases, each of which is
60 dimensional, corresponding to the (x, y, z) locations
of 20 body markers. These were manually partitioned
into five parts (head and neck, left arm, right arm,
left leg, and right leg), which we refer to as the known
structure.
We give test log likelihood results for the CMU data
set in Figure 6a. All of the methods that estimate
the group structure from the data (GL1-ug, GL1-ue,
GL12-ug, GL12-ue) significantly out perform the unstructured Tikhonov (T) and independent ℓ1 methods (IL1), and give an improvement over the group
ℓ1,2 method with known groups based on body parts
(GL12-k). The best method on the CMU data set
is the group ℓ1 exhaustive search method (GL1-ue).
Figure 6b gives the total training time (based on a
Matlab implementation) over all crossvalidation folds
and parameter settings. Note that the vertical axis is
on a log scale. We can see that all of the methods
that estimate the group structure require significantly
more computation time relative to the unstructured
Tikhonov and independent ℓ1 methods, as well as the

UAI 2009

group ℓ1,2 method with known groups (GL12-k). However, the computation times among the methods that
estimate the group structure are all quite similar.
We show the known or inferred group structure for
each of the group methods in Figures 5a-e. We
show results for the fold with the highest test log
likelihood. All of the methods that estimate group
structure appear to over-partition the data variables relative to the known structure. However,
the over-partitioning is quite systematic and mostly
corresponds to breaking up the given groups into
sub-groups corresponding to their x-coordinates, ycoordinates, and z-coordinates (note that the ordering
of the variables is x1 , y1 , z1 , x2 , y2 , z2 , ...). The apparent over-partitioning also results in improved test set
log likelihood relative to the known groups, indicating
that it is well supported by the data, and not simply
an artifact of the bounds.
6.2

Mutual-Fund Data Set

The second data set we consider consists of monthly
returns for 59 mutual-funds in four different sectors
including 13 US bond funds, 30 US stock funds, 7 balanced funds investing in both U S stocks and bonds
and 9 international stock funds. There are 86 data
cases each corresponding to the returns of all 59 funds
for a different month. While the funds are naturally
split into groups based on their content, the groups
are clearly not independent since the balanced funds
group contains both stocks and bonds. This data set
has been used previously by Scott and Carvalho (2008)
in the context of local search for decomposable GGM
graph structure.
We give test log likelihood results for the mutual-funds
data set in Figure 6c. We first note that there is much
less variation in median test log likelihood across the
methods compared to the CMU data set, which likely
results from the mutual fund data set having a less obvious block structure. Indeed, the group ℓ1,2 method
(GL12-k) based on the fund-type grouping yields a median test set log likelihood that is only slightly better
than the independent ℓ1 method. All of the methods that estimate the group structure from the data
(GL1-ug, GL1-ue, GL12-ug, GL12-ue) result in median test log likelihood performance that is no worse
than the independent ℓ1 method. The best method
overall is again the group L1 method with exhaustive
search (GL1-ue). This method in fact yields better
performance than the independent ℓ1 method across
all test folds. The trend in the computation time results is very similar to the CMU data set and is not
shown.
We present the known or inferred structure for each

UAI 2009

MARLIN ET AL.

391

Total Training Time (s)

Test Log−Likelihood

5

10
4
10
3
10
2
10
1
10

−30

GL12−k

GL12−ue

GL1−ue

T

(b) CMU Training Time

GL12−ug

−40

GL12−k

GL12−ue

GL12−ug

GL1−ue

GL1−ug

IL1

T

GL12−k

GL12−ue

GL12−ug

GL1−ue

GL1−ug

IL1

(a) CMU Test Log Likelihood

−20

GL1−ug

−1

10
−2
10

−20

−10

IL1

−10

0
Log Likelihood

0

T

Log Likelihood

Test Log−Likelihood

(c) Mutual-fund Test Log Likelihood

10

10

20

20

20

30
40
50

30
40
50

60
2

3
Blocks

4

5

30
40
50

60
1

2

3

4

5 6
Blocks

7

(b) GL1-ug

(c) GL1-ue

8

9

40
50

60

1 2 3 4 5 6 7 8 9 10 11 12 13 14
Blocks

30

60
1

2

3

4

5 6
Blocks

7

8

9

1

(d) GL12-ug
5

5

10

10

15

15

15

25
30
35
40

20
25
30
35
40

20
25
30
35
40

20
25
30
35
40

45

45

45

45

50

50

50

50

55

55

55

55

1

2

3
Blocks

(f) Known

4

123 4567 8910111213141516171819202122232425262728
Blocks

(g) GL1-ug

1

2 3 4

5 6 7 8 9 10 11 12
Blocks

(h) GL1-ue

Data Dimensions

5
10

15

Data Dimensions

5
10

Data Dimensions

5
15
20

2

3

4
5
Blocks

6

7

8

(e) GL12-ue

10
Data Dimensions

Data Dimensions

(a) Known

40
50

60
1

30

Data Dimensions

10

20

Data Dimensions

10

20

Data Dimensions

10
Data Dimensions

Data Dimensions

Figure 4: Test set log likelihood and training time for CMU, and test log likelihood for and mutual-funds.

20
25
30
35
40
45
50
55

1

2

3

4

Blocks

(i) GL12-ug

1

2
Blocks

(j) GL12-ue

Figure 5: Inferred clusterings on the CMU motion capture data (top row) and mutual funds data (bottom row).
of the group methods in Figures 5f-j. We show results for the fold with the highest test log-likelihood.
Unlike the CMU case, some of the unknown group
methods select many more groups than the known
grouping, while others select less. The group ℓ1 exhaustive search method (GL1-ue), which obtains the
best test log likelihood, selects 12 groups. Interestingly, it recovers the bonds group with only one error,
splits the international stock funds into two groups,
but mixes the balanced funds with the US stock funds
in several small groups. The group ℓ1,2 exhaustive
search method (GL12-ue) apparently terminates after
correctly splitting the variables into one group of bond
funds and one group of all other funds. The group ℓ1,2
greedy search method (GL12-ug) recovers the bonds
and international stock groups with only one error
while mixing the US stocks and balanced funds into
two groups.

A

Gibbs Sampler Outline

Suppose that P (X) is an arbitrary density function
defined over the space of positive-definite matrices.
To implement a Gibbs sampler we require the conditional distribution P (Xij |X−ij ) where X−ij denotes
all the entries in X except for Xij and Xji . Independent of the form of the density, the positivedefiniteness constraint implies that the conditional distribution for Xij will only have support on an interval
b0 (X−ij ) < Xij < b1 (X−ij ). Due to the fact that
the positive-definite cone is a convex set, this interval,
which is the intersection of a line with the positivedefinite cone, will also be convex.
The exact end points b0 and b1 can be obtained in
closed form for any i, j and matrix X. We omit the full
derivation due to space limitations, but sketch a brief
outline. First, we note that b0 and b1 are the maximum
and minimum values for Xij that render X indefinite.
Finding them reduces to the problem of solving the
equation det(X) = 0 in terms of Xij . Assuming X is

392

MARLIN ET AL.

otherwise positive-definite, the determinant is a linear
function of a diagonal entry Xii , leading to a finite positive lower bound b0 and an upper bound b1 = ∞. The
determinant is a non-degenerate quadratic function of
an off diagonal entry Xij leading to finite values for
b0 and b1 . These results are intuitive since increasing
a diagonal entry of a matrix that is already positivedefinite will keep it positive-definite, while sufficiently
increasing or decreasing an off diagonal entry will make
it violate positive-definiteness.
Assuming we have derived the allowable range for Xij
given X−ij we consider particular cases for the density function P (X). In the case of the group matrix ℓ1
distribution we obtain the following conditional distribution, which is easily seen to be a truncated Laplace
distribution for off diagonal entries, and a truncated
exponential distribution for on diagonal entries given
the results for b0 and b1 that we have just derived.
Sampling from these truncated distribution is simple
using inversion of the corresponding cumulative distribution functions.
P (Xij |X−ij ) =

(Xij ∈ [b0 , b1]) exp(−λij |Xij |)
Z b1
exp(−λij |Xij |)dXij
b0

The group matrix ℓ1,2 distribution has identical conditional distributions for diagonal and off-diagonalwithin-block entries. The off diagonal between block
entries have the form of a truncated hyperbolic distribution due to the application of the ℓ2 -norm. We give
the result below assuming that dimension i belongs to
group k and dimension j belongs to group l. Sampling
from the truncated hyperbolic distribution can also be
done efficiently by exploiting the fact that this form
of the hyperbolic distribution is a generalized inverse
gaussian (GIG) scale mixture with zero mean. We can
sample the scale parameter from the correct GIG distribution, and then use inversion of the CDF to sample
from a truncated univariate normal distribution with
the sampled scale parameter.
q


2 + X2
(Xij ∈ [b0 , b1]) exp −λkl γij
ij
P (Xij |X−ij ) =
Z b1
q


2 + X 2 dX
exp −λkl γij
ij
ij
b0

γij =

P

s6=i

PD

2
t6=j,t>s δzs ,k δzt ,l Xst

1/2

The complexity of the sampler is dominated by the
calculation of the the truncation range [b0 , b1 ]. Solving
for b0 and b1 requires inverting a matrix of size D−1, at
a cost of O((D − 1)3 ). The complete cost of T updates
to all the unique matrix parameters is O( 12 T D(D +
1)(D−1)3 ), or approximately O(T D5 ), which is clearly
intractable unless D is small.

UAI 2009

References
C. Ambroise, J. Chiquet, and C. Matias. Inferring
sparse Gaussian graphical models with latent structure. Eletron. J. of Statistics, 3:205–238, 2009.
O. Banerjee, L. E. Ghaoui, and A. d’Aspremont.
Model selection through sparse maximum likelihood
estimation for multivariate gaussian or binary data.
J. of Machine Learning Research, 9:485–516, 2008.
A. Dempster. Covariance selection. Biometrics, 28(1),
1972.
J. Duchi, S. Gould, and D. Koller. Projected subgradient methods for learning sparse gaussians. In UAI,
2008.
J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation the graphical lasso. Biostatistics, 2007.
E. Gómez, M. A. Gomez-Viilegas, and J. M. Marı́n. A
multivariate generalization of the power exponential
family of distributions. Communications in Statistics - Theory and Methods, 27:589–600, 1998.
B. Marlin and K. Murphy. Sparse Gaussian Graphical Models with Unknown Block Structure. In Intl.
Conf. on Machine Learning, 2009.
M. Schmidt, E. van den Berg, M. Friedlander, and
K. Murphy. Optimizing Costly Functions with
Simple Constraints: A Limited-Memory Projected
Quasi-Newton Algorithm. In AI & Statistics, 2009.
J. Scott and M. Carvalho. Feature-inclusion stochastic
search for gaussian graphical models. Journal of
Computational and Graphical Statistics, 17(4):790–
808, 2008.
B. Turlach, W. Venables, and S. Wright. Simultaneous variable selection. Technometrics, 47(3):349–
363, 2005.
M. Yuan and Y. Lin. Model selection and estimation
in regression with grouped variables. J. Royal Statistical Society, Series B, 68(1):49–67, 2006.
M. Yuan and Y. Lin. Model selection and estimation
in the gaussian graphical model. Biometrika, 94(1):
19–35, 2007.

