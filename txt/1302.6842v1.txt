506

Belief Updating by Enumerating High-Probability
Independence-Based Assignments

Solomon Eyal Shimony

Eugene Santos Jr.

Dept. of Electrical and Computer Engineering
Air Force Institute of Technology
Wright-Patterson AFB, OH 45433-7765
esantos@afit.af.mil
Abstract

Independence-based (IB) assignments to
Bayesian belief networks were originally pro­
posed as abductive explanations. IB as­
signments assign fewer variables in abduc­
tive explanations than do schemes assign­
ing values to all evidentially supported vari­
ables. We use IB assignments to approxi­
mate marginal probabilities in Bayesian be­
lief networks. Recent work in belief up­
dating for Bayes networks attempts to ap­
proximate posterior probabilities by finding a
small number of the highest probability com­
plete (or perhaps evidentially supported) as­
signments. Under certain assumptions, the
probability mass in the union of these assign­
ments is sufficient to obtain a good approx­
imation. Such methods are especially use­
ful for highly-connected networks, where the
maximum clique size or the cutset size make
the standard algorithms intractable.
Since IB assignments contain fewer assigned
variables, the probability mass in each as­
signment is greater than in the respective
complete assignment. Thus, fewer IB assign­
ments are sufficient, and a good approxima­
tion can be obtained more efficiently. IB as­
signments can be used for efficiently approxi­
mating posterior node probabilities even in
cases which do not obey the rather strict
skewness assumptions used in previous re­
search. Two algorithms for finding the high
probability IB assignments are suggested:
one by doing a best-first heuristic search, and
another by special-purpose integer linear pro­
gramming. Experimental results show that
this approach is feasible for highly connected
belief networks.
Keywords: Probabilistic Reasoning, Bayesian Belief
Networks, Relevance, Belief Updating, Belief Revision
Constraint Satisfaction.

Dept. of Math. and Computer Science
Ben Gurion University of the Negev
P.O. Box 653, 84105 Beer-Sheva, Israel
shimony@ bengus. bitnet
1

INTRODUCTION

Finding the posterior distribution of variables in a
Bayesian belief network is a problem of particular re­
search interest for the probabilistic reasoning commu­
nity. Although a polynomial-time algorithm for com­
puting the probabilities exists for polytrees [17], the
problem was proved to be NP-hard in the general case
in [5]. Several exact algorithms exist for computing
posterior probabilities: clustering and junction-trees
[18, 16], conditioning [6], and term evaluation [19].
These are all exponential-time algorithms in the worst
case. Newer algorithms attempt various refinements
of these schemes [9].
Several approximation algorithms also exist. In [14]
and similar papers, approximation is achieved by
stochastically sampling through instantiations to the
network variables. In [10], the idea was to use the con­
ditioning method, but to condition only on a small,
high probability, subset of the (exponential size) set of
possible assignments to the cutset variable.
Recently, approximation algorithms have emerged
based on deterministic enumeration of high probability
terms or assignments to variables in the network. The
probability of each such assignment can be computed
quickly: in O(n), or sometimes even (incrementally)
in 0(1). The probability of a particular instantiation
to a variable v (say v = vl) is approximated by simply
dividing the probability mass of all assignments which
contain v = v1 by the total mass of enumerated as­
signments. If only assignments compatible with the
evidence are enumerated, this approximated the pos­
terior probability of v = v1. The approximation im­
proves incrementally as successively more probability
mass accumulates.
In [8] incremental operations for probabilistic reason­
ing were investigated, among them a suggestion for
approximating marginal probabilities by enumerating
high-probability terms. One interesting point is the
skewness result: if a network has a distribution such
that every row in the distribution arrays has one en­
try greater than n;;-l, then collecting only n + 1 assignments, we also have at least � of the probability

507

Belief Updating and IB Assignments

mass. Taking the topology of the network into ac­
count, and using term computations, this can presum­

ably be achieved effi ci ently. However, the skewness
assumption as is seems somewhat restrictive. It may
hold in some domains, such as circuit fault diagnosing ,
but certainly not in the typical case, e . g . in randomly

generated networks.

Slightly relaxing the constraint,

than ( n�1 )2, already
assignments to get similar

In [30], IB assignments were the candidates for rel­
evant explanation. Here, we suggest that comput­
ing ma r ginal probabilities (whether prior or pos terior) ,
can be done by enu mera ting high-probability IB as­
signments , rather than c omp l ete assignments. Since
IB assignments usually have fewer variables assigned,
each IB assignment is exp ec ted to hold more probabil­

say to probabili ty entries greater

ity mass than a respective complete (or even a partial,

res u lts.

ability of an

requires on the order of

In

n2

query and evidence supported) assignment. The p rob­
P (A)

[21] p artial assignments to no d es in the network are
from the root nodes down. The probability

created

of each such assignment is easily computable.

Much

savi n g in computational effort is achieved by not both­

e rin g about irrelevant nodes (barren nodes), i.e. node s

set no d e , or nodes that

that are not above some query

are d-separated from the evidence nodes. Later in that

paper, an assumption of extreme probabilities is made.
This is similar to the skewness assumption above. In

fact , in the circuit fault diagnosis expe ri ment in

[21],

assignment is

IB

vE s

IT

pan( A )

easily computed [30):

P(A{v}IAparents(v) )

where As is the assignment A restricted to the set of

nodes 5. The product terms can each be retrieved in
constant time.

One might argue that searching for high-probability
assignments for approximating
is

a

bad

idea,

since

marginal distributions

coming up

probab ility assi gnment is

with the

NP-ha rd [31].

high­

We might

the numbers actually used are well within the bounds

have expected that a polynomial time algorithm be

used later on in that paper in order to narrow the

show s that even approximating marginal prob abili­

12) that belief networks
structure that is not rep­

P =NP. Th er efore , using this kind of approximation al­

of the skewness assumption. The conflict scheme was
search.

was already suggested [30,
f re q uently have independence

It

r esented by the topology .

Sometimes independence

holds given a partlcu/ar assignment to a set of variables
V, rather than to all possible assignments to V
In

such cases, the topology is no help in determining in­

dependence (e.g. d-separation, which is defined based
strictly on topology

distrib utions

not hold), t he ac t ual

[20), might

might have to be examined. In

idea of independence-based

(IB)

[30] the

assignments was in­

troduced. An assignment is a set of (node, value) pairs,
which can also be written as a set of node=value in­

stanti ati ons. An assignment is consistent if each node
is assigned at most one value.

Two

assignments

compatible if their union is consistent.

are

Each assign­

ment denotes a (sample space) event, and we thus use
the assignment and the event it denotes as synony­

An assignment

mous terms whenever unambiguous.

A is subsumed by assig n ment
The

B if A� B.

IE condition ho l ds at node v w.r.t. assignment

A if the value assigned to

v

by A is independen t of

all possible assignments to the ancestors of v given
Aparents(v)• the assignment made by A to the imme­

diate predecessors (parents) of v. 1 An assignment is
IB if the IB condition h olds at every v E span(A),
where

span(A) is

the set of nodes assig ned by

hypercube 1i is an assignment to a node

its parent s . In this case, we say that

v

A.

However, [7]

ties in belief networks is NP-hard, and thus there is
no polynomial- time approximation algorithm unless

gorithm is a reasonable proposition, p rov ided that for
some sub-classes of the problem that are bad for exist­
ing algorithms, our approxim ation algorithm behaves
well.
The rest of the paper is o r ganiz ed as

follows: section
2 discusses the details of how to approximate poste­
rior probabilities from a set of h igh- probabi lity 1B as­

signments , and how to modify the IB MAP algorithm

of [30] for
3 reviews

computing posterior probabilities.

Section

the reduction of IB MAP co mputation to

linear systems

of equations [30],

and presents a few

i mprovements that reduce the number of equations.

Searching for next-best assignments using linear pro­

gramming is discussed.

Section 4 presents

experimen­

tal timing results for approximation of p osterior prob­

abilities on random networks. We conclude with other

related work and an evaluation of

the IB

MAP meth­

ods.
2

COMPUTING MARGINALS

The probabili ty of a certain node instantiat ion ,

v = v1,

is approximated by the probability mass in the

IB

as­

signments containing v = v1 divided by the total mass.

v.

node.

1l is an IB hypercube if the IB condition holds at v
w.r.t. 1l.

If we need to find the probability of

v,

then vis a query

Nodes where evidence is introduced are called

evidence no des. We also assume that the evidence is

conjunctive in nature, i.e. it is an assignment of values
to the evidence nodes. We assume tha t each enumer­
ated

1This means that the conditional probability P(v == d I
assignment to parents of v ), where
E A, does not
change if we condition on any other ancestors of v.

( d, v)

approximations.

A

and some of

1l is based on

sufficient to compute

IB ass ignment A contains some

assignment

to

query node v, and enforce this condition in the algo­
rithm. Let I be a set of IB enumerated assignments .
To approximate the p robability of v

= Vi,

we compute:

508

Santos and Shirnony

a v
P(

_
-

v,

·

)

_
-

P({AIAEII\{v=v;}EA})
P({AlAE I})

where the probability of a set of assignments is the
probability of the event that is the union of all the
events standing for all the assignments (not the prob­
ability of the union of the assignments ). If we are
computing the prior probability of v = v1, we can ei­
ther assume that the denominator is 1 (and not bother
about assignments assigning v a value other than v1),
or use 1 P({AlA E I}) as an error bound. If all
IB assignments are disjoint, this is easily computable:
simply add the probabilities of the IB assignments in
each set to get the probability of the set.
-

However, since IB assignments are partial, it is possible
for the events denoted by two different IB assignments
to overlap. For example, let { u, v, w} be nodes, each
with a domain {1,2,3}. Then A= {u= 1,v = 2}
has an overlap with B = {u = 1, w= 1}. The overlap
C= AU B is also an assignment: C = {u = 1, v =
2, w = 3P. Thus, computing the probability of the
union of the IB assignments is non-trivial. We can use
the following property of IB assignments:
Theorem 1 Let A, B be compatible IB assignments.
Then

AUB

is also an IB assignment.

Evaluating the probability of a set of IB assignments
may require the evaluation of an exponential number
of terms. That is due to the equation for implementing
the inclusion-exclusion principle of compound proba­
bilities:
m

I,:(-l)k+t

k=l

where

I,: nf=1Ea;

t::;a,<.<ak::;m

E; is the ith event.

Fortunately, as we go to higher-order terms, their
probability diminishes, and we can ignore them in the
computation. That is because low-probability assign­
ments are going to be ignored in the approximation
algorithm anyway. How many of the highest probabil­
ity IB assignments are needed in order to get a good
approximation? Obviously, in the worst case the num­
ber is exponential in n. However, under the skewness
assumption [8) (also section 1) the number is small. In
fact, it follows directly from the skewness theorem [8]
that if the highest (or second highest) probability com­
plete assignment is compatible with Aopt the highest
probability IB assignment, and Aopt has at least log2 n
unassigned nodes, then the 2 highest IB assignments
contain most (� �) of the probability mass. It is pos­
sible to extend the skewness theorem to include O( nk )
terms, in which case the mass will be at least Tk(l),
where Tk (x) is the polynomial consisting of the Arst
2 Note that for two assignments A, B, the union of A and
B denotes the event that is the intersection of the events
denoted by A and B.

k terms of Taylor expansion of ex. Thus, under the
above conditions, if Aopt has (k + 1) log2 n unassigned
nodes, the highest probability IB assignment will con­
tain at least rkp) of the probability mass.

Additionally, all non-supported (redundant) nodes can
be dropped from the diagram. A node v is supported
by a set of nodes V if it is in V or if v is an ancestor
of some node in V. A node supported by the evidence
nodes is called evidentially supported, and a node sup­
ported by a query node is called query supported. We
are usually only interested in IB assignments properly
evidentially supported by some set of evidence nodes.
An assignment is properly evidentially supported if all
the nodes in the assignment have a directed path of as­
signed nodes to an evidence node. Likewise, an IB as­
signment is properly query supported if every node in
the assignment obeys the above condition w.r.t. query
nodes.
Before we start searching for IB assignments, we can
drop all evidence nodes that are d-separated from the
query nodes, as well as all the nodes that are not either
query supported or supported by one of the remaining
evidence nodes.
We now present the anytime best-first search algo­
rithm, which is essentially the same as in [30), but with
provisions for collecting the probability mass in sets of
IB assignments. It keeps a sorted agenda of states,
where a state is an assignment, a node last expanded,
and a probability estimate:
•

•

Input: a Bayesian belief network B, evidence £ (a
consistent assignment), a query node q.
Output: successively improved approximations
for P(q = q;), for each value q; in the domain
of node q.

1. Preprocessing

Initialize IB hypercubes for each node v EB.
Sort the nodes of B such that no node ap­
pears after any of its ancestors.
2. Initializing: remove redundant nodes, and for
each q; in the domain of q do:
(a) Set up a result set for q;.
(b) Push the assignment £ U {q = q;} onto the
agenda, with a probability estimate of 1.
3. Repeat until empty agenda:
(a) Pop assignment with highest estimate A
from the agenda, and remove duplicate states
(they will all be at the top of the agenda).
(b) If the assignment is IB, add it to the result
set of q;, where {q= q;}EA, and update the
posterior probability approximation.
(c) Otherwise, expand A at v, the next node,
into a set of assignments S, and for each as­
signment Ai ES do:
i. Estimate the probability of Aj.
•

•

509

Belief Updating and IB Assignments

11.

Push Ai with its probability estimate and
last-expanded node v into the agenda.

Expanding a state and the probability estimate is ex­
actly as in
based on

v

[30]: Ai =AU 1ij

is the jth IB hypercube

that is maximal w.r.t. subsumption and

consistent with A. The probability estimate is the
product of hypercube probabilities for all nodes where

the IB condition holds. The posterior probability ap­

proximation for q

=

Pa(q = q; \ £)

q; given the evidence is:
=

set

arbitrary distribu tion. For each node v and value in
Dv ( the domain of v ) , there is a set of ku d maximal
IB hypercubes based on

that set by

The preprocessing is independent of the query and ev­
idence sets, and can thus be done once per network.

It is also possible to do preprocessing incrementally by
moving it into the loop, initializing the hypercubes for

[3])

a b ductive

conclusions.

It is easy to generalize this algorithm to handle

m

>

1

query nodes, or to compute the probability of a par­
ticular joint state of

m

nodes, or even their joint pos­

terior distribution. This can be done by a somewhat
different initialization and estimation steps, which is
beyond the scope of this paper.
Experimental results from

[30] suggest that at least the

highest probability IB assignment (the IB-MAP ) can
be found in reasonable time for medium-size networks
(up to 100 nodes ) , but that problems start occurring

for many instan ces of larger networks.

Du).

We denote

1ivd, and assume some indexing on the set.
1ivd is denoted 1i'/, with kv• 2 j 2 1.

Definition 1 From the belief network B and the ev­
idence £, we construct a system of inequalities L =
LIB(B, £) as follows:

The idea of

d

is a set of variables hf , indexed by the set of
all evidentially supported maximal hypercubes He
(the set of hypercubes H such that if H is based
on w, then w is evidentially supported). Thus,

1. V

V

WIMP

might benefit from not having to generate parts of

the network unless needed for

dE

A system of inequalities Lis a triple (V, I, c), where V
is a set of variables, I is a set of inequalities, and c is

itly represented in entirety. Applications which con­
as

(where

Member j of

a node only when expanded. By using this scheme, it is
not even necessary that the belief network be explic­

struct belief networks incrementally (such

v

an assignment cost function.

q;)
for q;)

P(result set for

2::; ?(result

the distribution. We usually omit reference to V and
assume that all discussion is with respect to the same

•

={hid /Hi

E

H£}.3,

2.

c(hid, 1) -log(P(Ht)),

3.

I

=

•

and c(hi , 0)

=

0.

is the following collection of znequalities:

(a) For each tnple of nodes (v, x, y) s. t. x =f y
and v E parents(x) nparents(y), and for each
dE

Dv:

2: hf +

2: h( -s: 1

( 1)

(u,d)EH�· ,eEDr (v,d')EHJ1 .JEDy,#d'

(b) For each evidentially su pport e d node v that is
not a query node and is not in span ( £ ) :

using IB assignments to approximate posterior prob­
abilities is independent of the

search method.

Any

(2)

algorithm providing the IB assignments in the correct
order will do. In the next section, we discuss how the
linear programming techniques used in [25, 27, 24, 30]
can be used to deliver IB assignments in decreasing

(c) For each pair of nodes w, v such that v
parents(w), and for each valued E Dv:

E

order of probability, for posterior probability approxi­
mation.

3
In

REDUCTION TO ILP

[25], [27], [26],

and

[24],

a method of converting the

(d) For each (v, d) E e:

compl ete MAP problem to an integer linear program
was shown. In [30] a similar method that con­

(ILP)

ve r ts the problem of finding the IB MAP to a linear
inequality system was shown. We begin by reviewing

the reduction, which is modified somewhat from [30]
in order to decrease the nu mb er of equations, and dis­

(4)

i=l

(e) For each query node q:
(5)

cuss the further changes necessary to make the system

find the next-best IB assignments.

The linear system of inequalities has a variable for
each maximal IB hypercube. The inequality gener­
ation is reviewed below. A belief network is denoted
by

B

=

(G, D),

where G is the underlying graph and D

3The superscript
d by the hypercube

d
v

st ates that node v is assigned value
is based on v), and the sub­

(which

script i states that this is the ith hypercube among the
hypercubes based on

v

that assign the value d to

v.

510

Santos and Shimony

The intuition behind these inequalities is as follows:
inequalities of type a enforce consistency of the solu­
tion. Type b inequalities enforce selection of at most
a single hypercube based on each node. Type c in ­
equalities enforce the IB constraint, i.e. at least one
hypercube based on v must be selected if v is assigned.
Type d inequalities introduce the evidence, and type
e introduces the query nodes. Modifications from [30)
include imploding several type a equations into one,
reducing the number of such equations by roughly a
factor quadratic in the number of hypercubes per node.
Other modifications are making type b and d into
equalities to make a simpler system, and adding the
equations for the previously unsupported query nodes.
Following [25], we define an assignment s for the vari­
ables of L as a function from V to n. Furthermore:
I. If the

range of

The objective function to optimize is:
=

-

� s(h 'j
�

d

d

)/og(P(Hi ))

(6)

Min
20736

Max
186624

Avg
84096

Med
73728

II
II

Figure 1: 10 node networks summary. States indicate
total number of possible complete assignments in this
network.
Once the optimal 0-1 solution is found, we need to add
equation prohibiting that solution, and then to find
an optimal solution to the resulting set of equations.

an

Let S
duced
tem,

be the set of nodes in the IB assignment A in­
by the optimal 0-1 solution. To update the sys­
add the following equation :

L

s is in {0, 1} then s is a 0-1 assign­

ment.
2. If s satisfies all the inequalities of types a-d then
s is a solution for L.
3. If solution s for L is a 0-1 assignment, then it is a
0-1 solution for L.

8L1a(s)

II
II States

L

vES {H,'"i(v,d)E.A}

hid< lSI

This equation prevents any solution which induces an
assignment B s.t. the variables in S are assigned the
same val ues as in A. Thus, it is not just a recurrence
of A that is prohibited, but of any assignment B sub­
sumed by A, in which case we would also like to ignore
B.
4

EXPERIMENTAL RESULTS

we mentioned earlier, because they are partial as­
signments, each IB MAP gathers more mass per as­
signment than the complete MAPs. We studied this
mass accumulation for IB MAPs by taking each assign­
ment one at a time in order of probability. By plotting
the percentage of mass accumulated versus the num­
ber of assignments used, we can get a fair idea of the
IB MAP approach's growth rate. In particular, we
extracted the top 25 IB assignments per problem in­
stance from 50 randomly generated networks (see [30)
for generation method) each having 10 nodes. (We
chose 10 nodes since it was still feasible to compute
each and every possible assignment in order to get the
exact mass.) Figure 1 gives a brief summary of our
networks.
As

In [30) it was shown that a optimal 0-1 solution to
the system of inequalities induces an IB MAP on the
original belief network. The minor modifications in­
troduced here, while having a favorable effect on the
complexity, encode the same constraint and this do
not affect the problem equivalence results of [30).
If the optimal solution of the system happens to be
0-1, we have found the IB MAP. Otherwise, we need
to branch: select a variable h which is assigned a non
0,1 value, and create two sets of inequalities (subprob­
lems), one with = 1 and the other with h = 0. Each
of these now needs to be solved for an optimal 0-1 solu­
tion, as in [27]. This branch and bound algorithm may
have to solve an exponential number of systems, but
in practice that is not the case. Additionally, the sub­
problems are always smaller in number of equations or
number of variables.

h

h

To create a subproblem,
is clamped to either 0 or
1. The equations can now be further simplified: a
variable clamped to 0 can be removed from the system.
For a variable clamped to 1, the following reductions
take place: Find the type b inequal ity, the type d
equat ion (if any) and all the type a inequalities, in
which h appears. In each such inequality clamp all the
other variables to 0 (removing them from the system),
and delete the inequality. After doing the above, check
to see if any inequality contains only one variable, and
if so clamp it accordingly. For example, if a type d
equation has only one variable, clamp it to 1. Repeat
these operations until no more reductions can be made.
,

Looking at our plot in Figure 2, we can see that mass is
accumulated fairly quickly and is contained in a small
set of assignments as we expected. After 10 IB MAPs,
we have already obtained on average roughly 80% of
the total mass (and 60% for the worst diagram in­
stance in the experiment). Note that this result is for
unskewed distributions, we expect a far higher accu­
mulation rate for skewed distributions.
W ith the favorable results for the 10 node cases, we
should proceed to the larger network instances. Un­
fortunately, as we well know, trying a brute force
technique of generating all the IB assignments for
larger networks is still infeasible. Furthermore, as
we mentioned earlier, even the heuristic method for
just finding the best IB assignment begins to d eteri­
orate rapidly starting at 100 nodes. Hence, we turn

Belief Updating and IB Assignments

Mass Accumulation

% orTot.al Mass
1.00
090

8

0. 0
0.70
0.60
O.lO
0.40
O.lO
0.20
010

. =•···��
ll!·l l!···!!!· 1!1!! !!1111111
ii#J
i:
�:
::
J
I=���
• �.:I. ;:n !n!•IJ.:
i
. :
···· I!:!!::::.!
+�· ..=
:

-m�·

�. r:;�. ...
'
....

1'· 1 · •••
:::1::
....
:: •: :
I'
·'

T
000

I

I

!

I
!
!

I
10.00

20.00

i
.

i
!
i

II
3000

40.00

•

Mass

2000 Nodes (Timing)

CPU SecOnds x 1o3

j..---'r'-..-'-'--....
,..---' -tl�
. .. l
-t=----''-'-'---'-'-.+-'----'l �-t-t- ----;14.00 +---1-----+--1 -. --·--------t---+13.00 +--->---1
12.00- ----t------t---1--ll.OO +---1-----'--+---t10.00 +---1-----+---'9.00 +---+----�---+---;-8.00 +---'----:---+--r1 ---+--�- --f'
7.00 +---'-----+
I
i
.
!
+--------- .-_._--_ ;__
4.00 - - --;··�----;--------·---·-�-..... ------------- .. --t- �
' -,,---3.00 +----+-----�1-,
.
2.00
.
i
!00
15.00

'

I

;
50.00

o of Ia MAPs

to our linear programming approach. Preliminary re­
sults show that our constraints approach can solve for
the IB MAP in networks of up to 2000 node. Fig­
ure 3 shows the results of our approach on 50 net­
works each consisting of 2000 nodes. For the most
part, we found our solutions relatively quickly. We
would like to note though, that our package for solv­
ing integer linear programs was crudely constructed
by the authors without the additional optimizations
such as sparse systems, etc. Furthermore, much of our
computational process is naturally parallelizable and
should benefit immensely from techniques such as par­
allel simplex [13] and parallel ILP [1, 2].
5

RELATED WORK

The work on term computation [8] and related papers
are extremely relevant to this paper. The skewness
assumption made there, or a weaker version of it, also
make our method applicable. In a sense, these meth­
ods complement each other, and it should be interest­
ing to see whether IB assignments (or at least maximal
IB hypercubes) can be incorporated into a term com­
putation scheme.
This paper enumerates high probability IB assign­
ments using a backward search from the evidence. [21]
also enumerates high probability assignments, but us­
ing a top down (forward) search. Backward constraints
are introduced through conflicts. It is clear that the
method is efficient for the example domain (circuit
fault analysis), but it is less than certain whether other
domains would obey the extreme probability assump­
tion that makes this work. If that assumption does not
hold, it may turn out that backward search is still bet­
ter. On the other hand, it may still be possible to take
advantage of IB hypercubes even in the forward search
approach. It should also be possible to improve perfor­
mance of the backward search considerably by using
a different heuristic than we did. In [ 4] our heuristic

I

000
000

o!LP
•
•

Rest-First

Best-Fir.st (railed)

���-1-------L

:: -_t

----------

Figure 2: Mass accumulation for 10 node networks.

511

10.00

20.00

..

!·

lO.OO

"'

•• i

'I'

40.00

i

�.00

Problem lnstanct

Figure 3: 2000 node networks.
is called "cost so far", and a "cost sharing" heuristic
defined there greatly outperforms "cost so far" when
applied to proof graphs generated by WIMP [3]. Pre­
liminary attempts to apply cost sharing to finding IB
MAPs show a great performance improvement.
The above cited papers [8, 21] as well as this one, are
essentially deterministic approximation algorithms.
Comparison with stochastic approximation algorithms
should also be interesting. Stochastic simulation to
approximate marginal probabilities [15] is one such
stochastic algorithm. We do not have a ready per­
formance comparison, and the method does not seem
immediately applicable to this work.
Other stochastic approximation algorithms find the
MAP. For example, in [ 11] simulated annealing is used.
It is not clear, however, how one might use it either to
enumerate a number of high-probability assignments
or make it search for the IB MAP. A genetic algo­
rithm for finding the MAP [22] makes a more interest­
ing case. The authors in [22] note that the probability
mass of the population rises during the search and con­
verges on some value. They do not say whether assign­
ments in the population include duplicates, however,
and make no mention of the possibility of approxi­
mating marginal probabilities with that population.
It seems likely that if the search can be modified to
search among IB assignments, then the fact that a
whole population is used, rather than a single candi­
date, may provide a ready source of near-optimal IB
assignments. Of course, we are not guaranteed to get
IB assignments in decreasing order of probability, so
slightly different methods would have to be used to
approximate the marginal probabilities.

512

Santos and Shimony

Finally, it should be possible to modify the algorithms
presented in this paper to work on G IB assignments
and 6-IB assignments, where an even greater probabil­
ity mass is packed into an assignment [30, 29]. Some
theoretical issues will have to be dealt with before we
can do that, however.
6

linear programming problems.

[2]

SUMMARY

Computing marginal ( prior or posterior ) probabilities
in belief networks is NP-hard. Approximation schemes
are thus of interest. Several deterministic approxima­
tion schemes enumerate terms, or assignments to sets
of variables, of high probability, such that a relatively
small number of them contain most of the probabil­
ity mass. This allows for an anytime approximation
algorithm, whereby the approximation improves as a
larger number of terms is collected. IB assignments
are partial assignments that take advantage of local
independencies n ot represented by the topology of the
network, to reduce the number of assigned variables,
and hence the probability mass in each assignment.
What remains to be done is to come up with these
IB assignments in a decreasing order of probability.
This is also a hard problem in general, unfortunately.
The factors contributing to complexity, however, are
not maximum clique size or loop cutset, but rather
the number of hypercubes. Under probability skew­
ness assumptions, the search for high probability IB
assignments is typically more efficient, and the result­
ing approximation (collecting a small number of as­
signments) is better.
Two algorithms for approximating marginal algo­
rithms are presented: a modification of a best-first
search algorithm for finding the IB MAP, and an al­
gorithm based on linear programming. The latter, as
expected, proves to be more efficient. We have also
experimented on highly connected diagrams where the
conditional probabilities are represented as sets of hy­
percubes ( distribution arrays are precluded, since they
are exponential in size ) , and got favorable results in
cases where the standard (j o in tree or conditioning )
algorithms cannot handle in practice.
-

Future work will attempt to apply the approximation
algorithms to cases where the IB condition holds ap­
proximately, called 6-IB assignments [28]. This should
enable representation of noisy OR nodes in a linear
number of IB hypercubes, where currently this is only
possible for perfect or "dirty" OR nodes [30]. An­
other approach would be to reduce the dimensionality
of the conditional tables by using approximation func­
tions [23]. This will directly impact the size of the ILP
problem.
References

[1] Paul D. Bailor and Walter D. Seward. A dis­
tributed computer algorithm for solving integer

In Proceedings of

the Fourth Conference on Hypercubes, Concurrent

[3]

[4]

[5]

[6]

Computers and Applications, pages 1083-1088,
1989.
Rochelle L. Boehning Ralp h M. Butler, and
Billy E. Gillett. A parallel integer li ne ar program­
ming algorithm. European Journal of Operational
Research, 34:393-398, 1988.
Eugene Charniak and Robert Goldman. A logic
for semantic interpretation. In Proceedings of the
26th Conference of the ACL, 1988.
Eugene Charniak and Saadia Husain. A new
admissible heuristic for minimal-cost proofs. In
Proceedings of AAAI Conference, pages 446-451,
1991.
Gregory F. Cooper. The computational complex­
ity of probabilistic inference using Bayesian belief
networks. Ar tificial Intelligence, 42 (2-3):393-405,
1990.
Gregory Floyd Cooper. NESTOR: A Computer­
,

Based

Medical

Diagnosis

Aid

that

Causal and Probabzlistic Knowledge.

Integrates

PhD thesis,

Stanford University, 1984.
[7] Paul Dagum and Michael Luby. Approximating
probabilistic inference in Bayesian belief networks
is NP-hard. Artificiallntelligence, 60 (1):141-153,
1993.
[8] Bruce D'Ambrosio. Incremental probabilistic in­
ference. In Uncertainty in AI, P roceedings of the

July 1993.
F. J. Diez. Local conditioning in Bayesian net­
works. Technical Report R-181, Cognitive Sys­
tems Lab. , Dept. of Computer Science, UCLA,
July 1992.
Gregory
F.
Cooper
Eric
J.
Horvitz,
H. Jacques Suermondt. Bounded conditioning:
Flexible inference for decisions under scarce re­
sources. In 5th Workshop on Uncertaznty in AI,
August 1989.
Stuart Geeman and Donald Geeman. Stochastic
relaxation, Gibbs distributions and the Bayesian
restoration of images. IEEE Transactions on Pat­
tern A nalyszs and Machine Intelligence, 6:721741, 1984.
Dan Geiger and David Heckerman. Advances in
probabilistic reasoning. In Proceedings of the 7th
Conference on Uncertainty in AI, 1991.
B. E. Gillett. Introduction to Operations Re­
9th Conference,

[9]

[10]

[11]

[12]

[13]

search:

A Computer-Oriented Algorithmic Ap­

McGraw Hill, 1976.
[14] Max Henrion. Propagating uncertainty by logic
sampling in Bayes' networks. Technical report,
Department of Engineering and Public Policy,
Carnegie Mellon Uni versity, 1986.
[15] Max Henrion. Towards efficient probabilistic di­
agnosis in multiply connected belief networks. In
proach.

Belief Updating and IB Assignments

J. Q. Smith and R. Oliver, editors,

Influence Di­
agrams for Decision Analysis, Inference and Pre­
diction. John Wiley and Sons, in preparation.

[16] F. V. Jensen, K. G. Olsen, and S. K. Ander­
sen. An algebra of Bayesian belief universes for
knowledge-based systems. Networks, 20:637-660,
1990.
[17] Jin H. Kim and Judea Pearl. A computation
mo del for causal and diagnostic reasoning in in­
ference systems. In Proceedings of the 6th Inter­
national Joint Conference on AI, 1983.
[18] S.L. Lauritzen and David J. Speigelhalter. Lo­
cal computations with probabilities on graphical
structures and their applications to expert sys­
tems. Journal of the Royal Statistical Society,
50:157-224, 1988.
[19)

Z.

Li and Bruce D'Ambrosio. An efficient ap­

p roach to probabilistic inference in beli ef nets. In

Proceedings of the Annual Canadian AI Confer­
ence, May 1992.

[20) Judea Pearl.

Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Mor­

gan Kaufmann, San Mateo, CA, 1988.
[21) David Poole. The use of conflicts in searching
Bayesian networks. In Uncertainty in AI, Pro­
ceedings of the 9th Conference, July 1993.
[22] Carlos Rojas-Guzman and Mark A. Kramer.
GALGO: A Genetic ALGOrithm decision support
tool for complex uncertain systems modeled with
Bayesian belief networks. In Uncertainty in AI:
Proceedings of the 9th Conference. Morgan Kauf­
mann, 1993.
[23) Eugene Santos, Jr. On spline approximations for
bayesian computations. Technical Report CS-9247, Department of Computer Science, Brown Uni­
versity, 1992.
[24) Eugene Santos, Jr. A fast hill-climbing approach
without an energy function for finding mpe. In

Proceedings of the 5th IEEE International Con­
ference on Tools with Artificial Intelligence, 1993.

[25] Eugene Santos, Jr. A linear constraint satisfac­
tion approach to cost-based abduction. Artificial
Intelligence, 65(1):1-28, 1994.
[26) Eugene Santos Jr. Cost-based abduction, linear
constraint satisfaction, and alternative explana­
tions. In Proceedings of the AAAI Workshop on
Abduction, 1991.
[27) Eugene Santos Jr. On the generation of alterna­
tive explanations with implications for belief re­
vision. In Proceedings of the 7th Conference on

Uncertainty in AI,

pages 339-347, 1991.

[28] Solomon E. Shimony. Explanation, irrelevance
and statistical independence. In AAAI Proceed­
ings, pages 482-487, 1991.

513

[29] Solomon E. Shimony. Relevant explanations: Al­
lowing disjunctive assignments. In Proceedings of
the 9th Conference on Uncertainty in AI, 1993.
[30) Solomon E. Shimony. The role of relevance in
explanation 1: Irrelevance as statistical indepen­
dence. International Journal of Approximate Rea­
soning, June 1993.
[31) Solomon E. Shimony. Finding MAPs for belief
networks is NP-hard. to appear in Artificial In­
tel ligence Journal, 1994.

