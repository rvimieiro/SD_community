On the Robustness of Most Probable Explanations

Hei Chan∗
Adnan Darwiche
School of Electrical Engineering and Computer Science
Computer Science Department
Oregon State University
University of California, Los Angeles
Corvallis, OR 97330
Los Angeles, CA 90095
chanhe@eecs.oregonstate.edu
darwiche@cs.ucla.edu

Abstract
In Bayesian networks, a Most Probable Explanation (MPE) is a complete variable instantiation with the highest probability given
the current evidence. In this paper, we discuss the problem of finding robustness conditions of the MPE under single parameter
changes. Specifically, we ask the question:
How much change in a single network parameter can we afford to apply while keeping the MPE unchanged? We will describe a
procedure, which is the first of its kind, that
computes this answer for all parameters in
the Bayesian network in time O(n exp(w)),
where n is the number of network variables
and w is its treewidth.

1

Introduction

A Most Probable Explanation (MPE) in a Bayesian
network is a complete variable instantiation which
has the highest probability given current evidence [1].
Given an MPE solution for some piece of evidence,
we concern ourselves in this paper with the following
question: What is the amount of change one can apply to some network parameter without changing this
current MPE solution? Our goal is then to deduce robustness conditions for MPE under single parameter
changes. This problem falls into the realm of sensitivity analysis. Here, we treat the Bayesian network as
a system which accepts network parameters as inputs,
and produces the MPE as an output. Our goal is then
to characterize conditions under which the output is
guaranteed to be the same (or different) given a change
in some input value.
This question is very useful in a number of application
areas, including what-if analysis, in addition to the
∗
This work was completed while Hei Chan was at
UCLA.

Figure 1: An example Bayesian network where we are
interested in the MPE and its robustness.

design and debugging of Bayesian networks. For an
example, consider Figure 1 which depicts a Bayesian
network for diagnosing potential problems in a car.
Suppose now that we have the following evidence: the
dashboard test and the lights test came out positive,
while the engine test came out negative. When we
compute the MPE in this case, we get a scenario in
which all car components are working normally. This
seems to be counterintuitive as we expect the most
likely scenario to indicate at least that the engine is not
working. The methods developed in this paper can be
used to debug this scenario. In particular, we will be
able to identify the amount of change in each network
parameter which is necessary to produce a different
MPE solution. We will revisit this example later in
the paper and discuss the specific recommendations
computed by our proposed algorithm.
Previous results on sensitivity analysis have focused
mostly on the robustness of probability values, such
as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9]. Because
probability values are continuous, while MPE solutions
are discrete instantiations, abrupt changes in MPE so-

lutions may occur when we change a parameter value.
This makes the sensitivity analysis of MPE quite different from previous work on the subject.

highest probability [1]:

This paper is structured as follows. We first provide
the formal definition of Bayesian networks and MPE
in Section 2. Then in Section 3, we explore the relationship between the MPE and a single network parameter, and also look into the case where we change
co-varying parameters in Section 4. We deduce that
the relationship can be captured by two constants that
are independent of the given parameter. Next in Section 5, we show how we can compute these constants
for all network parameters, allowing us to automatically identify robustness conditions for MPE, and provide a complexity analysis of our proposed approach.
Finally, we show some concrete examples in Section 6,
and then extend our analysis to evidence change in
Section 7.

=

2

MPE (e)

A Bayesian network is specified by its structure, a directed acyclic graph (DAG), and a set of conditional
probability tables (CPTs), with one CPT for each network variable [1]. In the CPT for variable X with parents U, we define a network parameter θx|u for every
family instantiation xu such that θx|u = Pr (x | u).
Given the network parameters, we can compute the
probability of a complete variable instantiation x as
follows:
Y
Pr (x) =
θx|u ,
(1)
xu∼x

where ∼ is the compatibility relation between instantiations, i.e., xu ∼ x means that xu is compatible with
x). Now assume that we are given evidence e. A most
probable explanation (MPE) given e is a complete variable instantiation that is consistent with e and has the

=

arg max Pr (x)
x∼e
Y
arg max
θx|u .
x∼e

(2)

xu∼x

We note that the MPE may not be a unique instantiation as there can be multiple instantiations with
the same highest probability. Therefore, we will define MPE (e) as a set of instantiations instead of just
one instantiation. Moreover, we will sometimes use
MPE (e, ¬x) to denote the MPE instantiations that
are consistent with e but inconsistent with x.
In the following discussion, we will find it necessary to
distinguish between the MPE identity and the MPE
probability. By the MPE identity, we mean the set of
instantiations having the highest probability. By the
MPE probability, we mean the probability assumed by
a most likely instantiation, which is denoted by:

Most Probable Explanations

We will formally define most probable explanations in
this section, but we specify some of our notational conventions first. We will denote variables by uppercase
letters (X) and their values by lowercase letters (x).
Sets of variables will be denoted by bold-face uppercase letters (X) and their instantiations by bold-face
lowercase letters (x). For variable X and value x, we
will often write x instead of X = x, and hence, Pr (x)
instead of Pr (X = x). For a binary variable X with
values true and false, we will use x to denote X = true
and x̄ to denote X = false. Therefore, Pr (X = true)
and Pr (x) represent the same probability in this case.
Similarly, Pr (X = false) and Pr (x̄) represent the same
probability. Finally, for instantiation x of variables X,
we will write ¬x to mean the set of all instantiations
x? 6= x of variables X. For example, we will write
Pr (x) + Pr (¬x) = 1.

def

MPEp (e)

def

=

max Pr (x).
x∼e

(3)

This distinction is important when discussing robustness conditions for MPE since a change in some network parameter may change the MPE probability, but
not the MPE identity.

3

Relation Between MPE and
Network Parameters

Assume that we are given evidence e and are able to
find its MPE, MPE (e). We now address the following
question: How much change can we apply to a network
parameter θx|u without changing the MPE identity of
evidence e? To simplify the discussion, we will first
assume that we can change this parameter without
changing any co-varying parameters, such as θx̄|u , but
we will relax this assumption later.
Our solution to this problem is based on some basic
observations which we discuss next. In particular, we
observe that complete variable instantiations x which
are consistent with e can be divided into two categories:
• Those that are consistent with xu. From Equation 1, the probability of each such instantiation
x is a linear function of the parameter θx|u .
• Those that are inconsistent with xu. From Equation 1, the probability of each such instantiation
x is a constant which is independent of the parameter θx|u .
Let us denote the first set of instantiations by Σe,xu
and the second set by Σe,¬(xu) . We can then conclude
that:

• The set of most likely instantiations in Σe,xu remains unchanged regardless of the value of parameter θx|u , even though the probability of such
instantiations may change according to the value
of θx|u . This is because the probability of each
instantiation x ∈ Σe,xu is a linear function of the
value of θx|u : Pr (x) = r · θx|u , where r is a coefficient independent of the value of θx|u . Therefore,
the relative probabilities among instantiations in
Σe,xu remain unchanged as we change the value
of θx|u . Note also that the most likely instantiations in this set Σe,xu are just MPE (e, xu) and
their probability is MPEp (e, xu). Therefore, if we
define:
r(e, xu)

def

=

∂MPEp (e, xu)
,
∂θx|u

for any x ∈ MPE (e, xu).
• Both the identity and probability of the most
likely instantiations in Σe,¬(xu) are independent
of the value of parameter θx|u . This is because
the probability of each instantiation x ∈ Σe,¬(xu)
is independent of the value of θx|u . Note that the
most likely instantiation in this set Σe,¬(xu) is just
MPE (e, ¬(xu)). We will define the probability of
such an instantiation as:
=

MPEp (e, ¬(xu)).

Region A

k(e,xu)

0

k(e,xu) / r(e,xu)

θx|u

Figure 2: A plot of the relation between the MPE
probability MPEp (e) and the value of parameter θx|u .
On the other hand, if θx|u < k(e, xu)/r(e, xu), i.e.,
region B of the plot, then we have MPE (e) =
MPE (e, ¬(xu)), and thus the MPE solutions are inconsistent with xu. Moreover, the MPE identity and
probability will remain unchanged as long as the value
of θx|u remains less than k(e, xu)/r(e, xu).

Pr (x) = r(e, xu) · θx|u ,

def

Region B

(4)

we will then have:

k(e, xu)

MPEPr(e)

(5)

Given the above observations, MPE (e) will either be
MPE (e, xu), MPE (e, ¬(xu)), or their union, depending on the value of parameter θx|u :
MPE (e)

if r(e, xu) · θx|u > k(e, xu);
 MPE (e, xu),
MPE (e, ¬(xu)), if r(e, xu) · θx|u < k(e, xu);
=

MPE (e, xu) ∪ MPE (e, ¬(xu)),
otherwise.
Moreover, the MPE probability can always be expressed as:
MPEp (e) = max(r(e, xu) · θx|u , k(e, xu)).
Figure 2 plots the relation between the MPE probability MPEp (e) and the value of parameter θx|u .
According to the figure, if θx|u > k(e, xu)/r(e, xu),
i.e., region A of the plot, then we have MPE (e) =
MPE (e, xu), and thus the MPE solutions are consistent with xu. Moreover, the MPE identity will remain
unchanged as long as the value of θx|u remains greater
than k(e, xu)/r(e, xu).

Therefore, θx|u = k(e, xu)/r(e, xu) is the point where
there is a change in the MPE identity if we were to
change the value of parameter θx|u . At this point,
MPE (e) = MPE (e, xu)∪MPE (e, ¬(xu)) and we have
both MPE solutions consistent with xu and MPE solutions inconsistent with xu. There are no other points
where there is a change in the MPE identity. If we
are able to find the constants r(e, xu) and k(e, xu) for
the network parameter θx|u , we can then compute robustness conditions for MPE with respect to changes
in this parameter.

4

Dealing with Co-Varying
Parameters

The above analysis assumed that we can change a parameter θx|u without needing to change any other parameters in the network. This is not realistic though
in the context of Bayesian networks, where co-varying
parameters need to add up to 1 for the network to induce a valid probability distribution. For example, if
variable X has two values, x and x̄, we must always
have:
θx|u + θx̄|u = 1.
We will therefore extend the analysis conducted in
the previous section to account for the simultaneously
changes in the co-varying parameters. We will restrict
our attention to binary variables to simplify the discussion, but our results can be easily extended to multivalued variables as we will show later.
In particular, assuming that we are changing parame-

ters θx|u and θx̄|u simultaneously for a binary variable
X, we can now categorize all network instantiations
which are consistent with evidence e into three groups,
depending on whether they are consistent with xu,
x̄u, or ¬u. Moreover, the most likely instantiations
in each group are just MPE (e, xu), MPE (e, x̄u), and
MPE (e, ¬u) respectively. Therefore, if x ∈ MPE (e),
then:

 r(e, xu) · θx|u , if x ∈ MPE (e, xu);
r(e, x̄u) · θx̄|u , if x ∈ MPE (e, x̄u);
Pr (x) =

k(e, u),
if x ∈ MPE (e, ¬u);

Therefore, all we need are the constants r(e, xu) and
k(e, u) for each network parameter θx|u in order to
define robustness conditions for MPE. The constants
k(e, u) can be easily computed from the constants
r(e, xu) by observing the following:

where:

As the algorithm we will describe later computes the
r(e, xu) constants for all family instantiations xu, the
algorithm will then allow us to compute all the k(e, u)
constants as well.

r(e, xu)

=

∂MPEp (e, xu)
;
∂θx|u

r(e, x̄u)

=

∂MPEp (e, x̄u)
;
∂θx̄|u

k(e, u)

= MPEp (e, ¬u);

and the MPE probability is:
MPEp (e) = max(r(e, xu) · θx|u , r(e, x̄u) · θx̄|u , k(e, u)).
Therefore, changing the co-varying parameters θx|u
and θx̄|u will not affect the identity of either
MPE (e, xu) or MPE (e, x̄u), nor will it affect the identity or probability of MPE (e, ¬u).
The robustness condition of an MPE solution can now
be summarized as follows:
• If an MPE solution is consistent with xu, it remains a solution as long as the following inequalities are true:
r(e, xu) · θx|u
r(e, xu) · θx|u

≥ r(e, x̄u) · θx̄|u ;
≥ k(e, u).

• If an MPE solution is consistent with x̄u, it remains a solution as long as the following inequalities are true:
r(e, x̄u) · θx̄|u
r(e, x̄u) · θx̄|u

≥ r(e, xu) · θx|u ;
≥ k(e, u).

• If an MPE solution is consistent with ¬u, it remains a solution as long as the following inequalities are true:
k(e, u) ≥ r(e, xu) · θx|u ;
k(e, u) ≥ r(e, x̄u) · θx̄|u .
We note here that one can easily deduce whether an
MPE solution is consistent with xu, x̄u, or ¬u since
it is a complete variable instantiation.

k(e, u)

= MPEp (e, ¬u)
=
max
MPEp (e, u? )
?
?
u :u 6=u

=
=

max

MPEp (e, xu? )

max

r(e, xu? ) · θx|u? .

xu? :u? 6=u
xu? :u? 6=u

(6)

As a simple example, for the Bayesian network whose
CPTs are shown in Figure 3, the current MPE solution without any evidence is A = a, B = b̄, and
has probability .4. For the parameters in the CPT of
B, we can compute the corresponding r(e, xu) constants. In particular, we have r(e, ba) = r(e, b̄a) =
r(e, bā) = r(e, b̄ā) = .5 in this case. The k(e, u)
constants can also be computed as k(e, a) = .3 and
k(e, ā) = .4. Given these constants, we can easily
compute the amount of change we can apply to covarying parameters, say θb|a and θb̄|a , such that the
MPE solution remains the same. The conditions we
must satisfy are:
r(e, b̄a) · θb̄|a

≥ r(e, ba) · θb|a ;

r(e, b̄a) · θb̄|a

≥ k(e, a).

This leads to θb̄|a ≥ θb|a and θb̄|a ≥ .6. Therefore,
the current MPE solution will remain so as long as
θb̄|a ≥ .6, which has a current value of .8.
We close this section by pointing out that our robustness equations can be extended to multi-valued variables as follows. If variable X has values x1 , . . . , xj ,
with j > 2, then each of the conditions we showed earlier will consist of j inequalities instead of just two. For
example, if an MPE solution is consistent with x1 u, it
remains a solution as long as the following inequalities
are true:
r(e, x1 u) · θx1 |u
r(e, x1 u) · θx1 |u

5

≥ r(e, x? u) · θx? |u for all x? 6= x1 ;
≥ k(e, u).

Computing Robustness Conditions

In this section, we will develop an algorithm for computing the constants r(e, xu) for all network parameters θx|u . In particular, we will show that they can
be computed in time and space which is O(n exp(w)),
where n is the number of network variables and w is
its treewidth.

A
a
a
ā
ā

A ΘA
a .5
ā .5

ΘB|A
.2
.8
.6
.4

B
b
b̄
b
b̄

Figure 3: The CPTs for Bayesian network A −→ B.

+

5.2

*

*

+

*

0.5

θa

0.2

λa

θ b|a

λb

+

*

*

0.8

0.6

θ b |a

θ b|a

*

0.4

λb

θ b |a

0.5

λa

θa

Figure 4: An arithmetic circuit for the above Bayesian
network. The bold lines depict a complete sub-circuit,
corresponding to the term λa λb̄ θa θb̄|a .
5.1

with x. Moreover, the term for x evaluates to the probability value Pr (e, x) when the evidence indicators are
set according to e. Note that this function is multilinear. Therefore, a corresponding arithmetic circuit
will have the property that two sub-circuits that feed
into the same multiplication node will never contain a
common variable. This property is important for some
of the following developments.

Arithmetic Circuits

Our algorithm for computing the r(e, xu) constants is
based on an arithmetic circuit representation of the
Bayesian network [10]. Figure 4 depicts an arithmetic
circuit for a small network consisting of two binary
nodes, A and B, shown in Figure 3. An arithmetic
circuit is a rooted DAG, where each internal node corresponds to multiplication (∗) or addition (+), and
each leaf node corresponds either to a network parameter θx|u or an evidence indicator λx ; see Figure 4.
Operationally, the circuit can be used to compute the
probability of any evidence e by evaluating the circuit
while setting the evidence indicator λx to 0 if x contradicts e and setting it to 1 otherwise. Semantically
though, the arithmetic circuit is simply a factored representation of an exponential-size function that captures the network distribution. For example, the circuit in Figure 4 is simply a factored representation of
the following function:
λa λb θa θb|a + λa λb̄ θa θb̄|a + λā λb θā θb|ā + λā λb̄ θā θb̄|ā .
This function, called the network polynomial, includes
a term for each instantiation x of network variables,
where the term is simply a product of the network parameters and evidence indicators which are consistent

Complete Sub-Circuits and Their
Coefficients

Each term in the network polynomial corresponds to a
complete sub-circuit in the arithmetic circuit. A complete sub-circuit can be constructed recursively from
the root, by including all children of each multiplication node, and exactly one child of each addition
node. The bold lines in Figure 4 depict a complete
sub-circuit, corresponding to the term λa λb̄ θa θb̄|a . In
fact, it is easy to check that the circuit in Figure 4 has
four complete sub-circuits, corresponding to the four
terms in the network polynomial.
A key observation about complete sub-circuits is that
if a network parameter is included in a complete subcircuit, there is a unique path from the root to this
parameter in this sub-circuit, even though there may
be multiple paths from the root to this parameter in
the original arithmetic circuit. This path is important
as one can relate the value of the term corresponding
to the sub-circuit and the parameter value by simply
traversing the path as we show next.
Consider now a complete sub-circuit which includes a
network parameter θx|u and let α be the unique path
in this sub-circuit connecting the root to parameter
θx|u . We will now define the sub-circuit coefficient
w.r.t. θx|u , denoted as r, in terms of the path α such
that r · θx|u is just the value of the term corresponding
to the sub-circuit.
Let Σ be the set of all multiplication nodes on this path
α. The sub-circuit coefficient w.r.t. θx|u is defined as
the product of all children of nodes in Σ which are
themselves not on the path α. Consider for example
the complete sub-circuit highlighted in Figure 4 and
the path from the root to the network parameter θa .
The coefficient w.r.t. θa is r = λa λb̄ θb̄|a . Moreover,
r · θa = λa λb̄ θa θb̄|a , which is the term corresponding to
the sub-circuit.
5.3

Maximizer Circuits

An arithmetic circuit can be easily modified into a
maximizer circuit to compute the MPE solutions, by
simply replacing each addition node with a maximization node; see Figure 5. This corresponds to a circuit

Algorithm 1 D-MAXC(M: a maximizer circuit, e:
evidence)

0.4
max

0.4

0

*

*

0.8

0.6

max

max

0.2

0.8

0.6

0.4

*

*

*

*

1: evaluate the circuit M under evidence e; afterwards
the value of each node v is p[v]
2: r[v] ← 1 for root v of circuit M
3: r[v] ← 0 for all non-root nodes v in circuit M
4: for non-leaf nodes v (parents before children) do
5:
if node v is a maximization node then
6:
r[c] ← max(r[c], r[v]) for each child c of node v
7:
if node v is a multiplication
Q node then
8:
r[c] ← max (r[c], r[v] c? p[c? ]) for each child c of
node v, where c? are the other children of node v

0.5

1

0.2

1

0.8

0.6

1

0.4

0

0.5

θa

λa

θ b|a

λb

θ b |a

θ b|a

λb

θ b |a

λa

θa

Figure 5: A maximizer circuit for a Bayesian network,
evaluated under evidence A = a. Given this evidence,
the evidence indicators are set to λa = 1, λā = 0,
λb = 1, λb̄ = 1. The bold lines depict the MPE subcircuit.
that computes the value of the maximum term in a
network polynomial, instead of adding up the values
of these terms. The value of the root will thus be the
MPE probability MPEp (e). The maximizer circuit in
Figure 5 is evaluated under evidence A = a, leading to
an MPE probability of .4.
To recover an MPE solution from a maximizer circuit,
all we need to do is construct the MPE sub-circuit
recursively from the root, by including all children of
each multiplication node, and one child c for each maximization node v, such that v and c have the same
value; see Figure 5. The MPE sub-circuit will then
correspond to an MPE solution. Moreover, if a parameter θx|u is in the MPE sub-circuit, and the sub-circuit
coefficient w.r.t θx|u is r, then we have r · θx|u as the
probability of MPE, MPEp (e).
Consider Figure 5 and the highlighted MPE subcircuit, evaluated under evidence A = a. The term
corresponding to this sub-circuit is A = a, B = b̄,
which is therefore an MPE solution. Moreover, we
have two parameters in this sub-circuit, θa and θb̄|a ,
with coefficients .8 = (1)(.8) and .5 = (.5)(1)(1) respectively. Therefore, the MPE probability can be obtained by multiplying any of these coefficients with the
corresponding parameter value, as (.8)θa = (.8)(.5) =
.4 and (.5)θb̄|a = (.5)(.8) = .4.
5.4

Computing r(e, xu)

Suppose now that our goal is to compute MPE (e, xu)
for some network parameter θx|u . Suppose further
that α1 , . . . , αm are all the complete sub-circuits that

include θx|u . Moreover, let x1 , . . . , xm be the instantiations corresponding to these sub-circuits and
let r1 , . . . , rm be their corresponding coefficients w.r.t.
θx|u . It then follows that the probabilities of these
instantiations are r1 · θx|u , . . . , rm · θx|u respectively.
Moreover, it follows that:
MPEp (e, xu) = max r1 · θx|u , . . . , rm · θx|u ,
i

and hence, from Equation 4:
∂MPEp (e, xu)
= r(e, xu) = max r1 , . . . , rm .
i
∂θx|u
Therefore, if we can compute the maximum of these
coefficients, then we have computed the constant
r(e, xu).
Algorithm 1 provides a procedure which evaluates the
maximizer circuit and then traverses it top-down, parents before children, computing simultaneously the
constants r(e, xu) for all network parameters. The
procedure maintains an additional register value r[.]
for each node in the circuit, and updates these registers as it visits nodes. When the procedure terminates,
it is guaranteed that the register value r[θx|u ] will be
the constant r(e, xu). We will also see later that the
register value r[λx ] is also a constant which provides
valuable information for the MPE solutions. Figure 6
depicts an example of this procedure.
Algorithm 1 can be modelled as the all-pairs shortest path procedure, with edge v −→ c having weight
0 = − ln 1 if v is a maximization node, and weight
− ln π if v is a multiplication node, where π is the
product of the values of the other children c? 6= c of
node v. The length of the shortest path from the root
to the network parameter θx|u is then − ln r(e, xu). It
should be clear that the time and space complexity
of the above algorithm is linear in the number of circuit nodes.1 It is well known that we can compile a
1

More precisely, this algorithm is linear in the number
of circuit nodes only if the number of children per multiplication node is bounded. If not, one can use a technique
which gives a linear complexity by simply storing two additional bits with each multiplication node [10].

1

0.4
max

1

1

0.4

0

*

*

.5

0

0.8

0.6

max

max

.5

.8

.4

.5

.5

0

0

0.2

0.8

0.6

0.4

*

*

*

*

.1

.5

0

.4

0

.3

0

0.5

1

0.2

1

0.8

0.6

1

0.4

0

0.5

θa

λa

θ b|a

λb

θ b |a

θ b|a

λb

θ b |a

λa

θa

Figure 7: A list of parameter changes that would produce a different MPE solution.
Figure 6: A maximizer circuit for a Bayesian network,
evaluated under evidence A = a. Next to each node is
the value r[.] computed by Algorithm 1.
circuit for any Bayesian network in O(n exp(w)) time
and space, where n is the number of network variables
and w is its treewidth [10]. Therefore, all constants
r(e, xu) can be computed with the same complexity.
We close this section by pointing out that one can
in principle use the jointree algorithm to compute
MPEp (e, xu) = r(e, xu) · θx|u for all family instantiations xu with the above complexity. In particular,
by replacing summation with maximization in the jointree algorithm, one obtains MPEp (e, c) for each cluster
instantiation c. Projecting on the families XU in cluster C, one can then obtain MPEp (e, xu) for all family instantiations xu, which is all we need to compute
robustness conditions for MPE.2 Our method above,
however, is more general for two reasons:
• The arithmetic circuit for a Bayesian network can
be much smaller than the corresponding jointree
by exploiting the local structures of the Bayesian
network [12, 13].
• The constants computed by the algorithm for the
evidence indicators can be used to answer additional MPE queries, which results after variations
on the current evidence. This will be discussed in
Section 7.

6

Example

We now go back to the example network in Figure 1,
and compute robustness conditions for the current
2
However, in case some of the parameters are equal to
0, one needs to use a special jointree [11].

MPE solution using the inequalities we obtain in Section 4, and an implementation of Algorithm 1. After
going through the CPT of each variable, our procedure
found nine possible parameter changes that would produce a different MPE solution, as shown in Figure 7.
From these nine suggested changes, only three changes
make sense from a qualitative point of view:
• Decreasing the probability that the ignition is
working from .9925 to at most .9133. (6th row)
• Decreasing the probability that the engine is
working given both the battery and the ignition
are working from .97 to at most .9108. (1st row)
• Decreasing the false-negative rate of the engine
test from .09 to at most .0285. (9th row)
If we apply the first parameter change, we get a new
MPE solution in which both the ignition and the engine are not working. If we apply either the second or
third parameter change, we get a new MPE solution
in which the engine is not working.

7

MPE under Evidence Change

We have discussed in Section 5.2 the notion of a complete sub-circuit and its coefficient with respect to a
network parameter θx|u which is included in the subcircuit. In particular, we have shown how each subcircuit corresponds to a term in the network polynomial, and that if a complete sub-circuit has coefficient
r with respect to parameter θx|u , then r · θx|u will be
the value of the term corresponding to this sub-circuit.
The notion of a sub-circuit coefficient can be extended
to evidence indicators as well. In particular, if a complete sub-circuit has coefficient r with respect to an evidence indicator λx which is included in the sub-circuit,

then r · λx will be the value of the term corresponding
to this sub-circuit.
Suppose now that α1 , . . . , αm are all the complete subcircuits that include λx . Moreover, let x1 , . . . , xm be
the terms corresponding to these sub-circuits and let
r1 , . . . , rm be their corresponding coefficients with respect to λx . It then follows that the values of these
terms are r1 · λx , . . . , rm · λx respectively. Moreover, it
follows that:

The result above also has implications on the identification of multiple MPE solutions given evidence e.
In particular, suppose that variable X is not set in
evidence e, then:
• If the coefficients for the evidence indicators λx
and λx̄ are equal, we must have both MPE solutions with X = x and MPE solutions with X = x̄.
In fact, the coefficients must both equal the MPE
probability MPEp (e) in this case.

MPEp (e − X, x) = max r1 , . . . , rm ,

• If the coefficient for the evidence indicator λx is
larger than the coefficient for the evidence indicator λx̄ , then every MPE solution must have
X = x.

i

where e − X denotes the new evidence after having
retracted the value of variable X from e (if X ∈ E,
otherwise e−X = e). Therefore, if we can compute the
maximum of these coefficients, then we have computed
MPEp (e − X, x). Note, however, that Algorithm 1
already computes the maximum of these coefficients
for each λx as the evidence indicators are nodes in the
maximizer circuit as well, and therefore the register
value r[λx ] gives us MPEp (e − X, x) for every variable
X and value x.
Consider for example the circuit in Figure 6, and the
coefficients computed by Algorithm 1 for the four evidence indicators. According to the above analysis,
these coefficients have the following meanings:
λx
λa
λā
λb
λb̄

e − X, x r[λx ] = MPEp (e − X, x)
a
.4
ā
.3
a, b
.1
a, b̄
.4

For example, the second row above tells us that the
MPE probability would be .3 if the evidence was A = ā
instead of A = a. In general, if we have n variables,
we then have O(n) variations on the current evidence
of the form e − X, x. The MPE probability of all of
these variations are immediately available from the coefficients with respect to the evidence indicators.
The computation of these coefficients allows us to deduce the MPE identity after evidence retraction. In
particular, suppose that variable X is set as x in evidence e, and MPEp (e) ≥ MPEp (e−X, x? ) for all other
values x? 6= x. We can then conclude that MPEp (e) =
MPEp (e − X). Moreover, MPE (e) = MPE (e − X)
if MPEp (e) > MPEp (e − X, x? ) for all other values
x? 6= x, or MPE (e) ⊂ MPE (e − X) if there exists
some x? 6= x such that MPEp (e) = MPEp (e − X, x? ).
Therefore, the current MPE solutions will remain so
even after we retract X = x from the evidence. This
means that X = x is not integral in the determination
of the current MPE solutions given the other evidence,
i.e., e − X.

In the example above, we have r[λb̄ ] > r[λb ], suggesting that every MPE solution must have b̄ in this case.

8

Conclusion

We considered in this paper the problem of finding robustness conditions for MPE solutions of a Bayesian
network under single parameter changes. We were able
to solve this problem by identifying some interesting
relationships between an MPE solution and the network parameters. In particular, we found that the robustness condition of an MPE solution under a single
parameter change depends on two constants that are
independent of the parameter value. We also proposed
a method for computing such constants and, therefore,
the robustness conditions of MPE in O(n exp(w)) time
and space, where n is the number of network variables
and w is the network treewidth. Our algorithm is the
first of its kind for ensuring the robustness of MPE
solutions under parameter changes in a Bayesian network.

Acknowledgments
This work has been partially supported by Air Force
grant #FA9550-05-1-0075-P00002 and JPL/NASA
grant #1272258. We would also like to thank James
Park for reviewing this paper and making the observation on how to compute k(e, u) in Equation 6.

References
[1] Judea Pearl. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers, San Francisco, California, 1988.
[2] Hei Chan and Adnan Darwiche. When do numbers really matter? Journal of Artificial Intelligence Research, 17:265–287, 2002.

[3] Hei Chan and Adnan Darwiche. Sensitivity analysis in Bayesian networks: From single to multiple parameters. In Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence (UAI), pages 67–75, Arlington, Virginia,
2004. AUAI Press.
[4] Enrique Castillo, José Manuel Gutiérrez, and
Ali S. Hadi. Sensitivity analysis in discrete
Bayesian networks. IEEE Transactions on Systems, Man, and Cybernetics, Part A (Systems
and Humans), 27:412–423, 1997.
[5] Veerle M. H. Coupé, Niels Peek, Jaap Ottenkamp,
and J. Dik F. Habbema. Using sensitivity analysis
for efficient quantification of a belief network. Artificial Intelligence in Medicine, 17:223–247, 1999.
[6] Uffe Kjærulff and Linda C. van der Gaag. Making
sensitivity analysis computationally efficient. In
Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI), pages
317–325, San Francisco, California, 2000. Morgan
Kaufmann Publishers.
[7] Kathryn B. Laskey. Sensitivity analysis for probability assessments in Bayesian networks. IEEE
Transactions on Systems, Man, and Cybernetics,
25:901–909, 1995.
[8] Malcolm Pradhan, Max Henrion, Gregory
Provan, Brendan Del Favero, and Kurt Huang.
The sensitivity of belief networks to imprecise
probabilities: An experimental investigation. Artificial Intelligence, 85:363–397, 1996.
[9] Linda C. van der Gaag and Silja Renooij.
Analysing sensitivity data from probabilistic networks. In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence
(UAI), pages 530–537, San Francisco, California,
2001. Morgan Kaufmann Publishers.
[10] Adnan Darwiche. A differential approach to inference in Bayesian networks. Journal of the ACM,
50:280–305, 2003.
[11] James D. Park and Adnan Darwiche. A differential semantics for jointree algorithms. Artificial
Intelligence, 156:197–216, 2004.
[12] Mark Chavira and Adnan Darwiche. Compiling Bayesian networks with local structure. In
Proceedings of the Nineteenth International Joint
Conference on Artificial Intelligence (IJCAI),
pages 1306–1312, Denver, Colorado, 2005. Professional Book Center.

[13] Mark Chavira, Adnan Darwiche, and Manfred
Jaeger. Compiling relational Bayesian networks
for exact inference. International Journal of Approximate Reasoning, 42:4–20, 2006.

