GLOBERSON & JAAKKOLA

133

Convergent Propagation Algorithms via Oriented Trees

Amir Globerson
CSAIL
Massachusetts Institute of Technology
Cambridge, MA 02139

Abstract
Inference problems in graphical models are
often approximated by casting them as constrained optimization problems. Message
passing algorithms, such as belief propagation, have previously been suggested as methods for solving these optimization problems.
However, there are few convergence guarantees for such algorithms, and the algorithms
are therefore not guaranteed to solve the corresponding optimization problem. Here we
present an oriented tree decomposition algorithm that is guaranteed to converge to
the global optimum of the Tree-Reweighted
(TRW) variational problem. Our algorithm
performs local updates in the convex dual
of the TRW problem – an unconstrained
generalized geometric program. Primal updates, also local, correspond to oriented
reparametrization operations that leave the
distribution intact.

1

Introduction

The problem of probabilistic inference in graphical
models refers to the task of calculating marginal distributions or the most likely assignment variables. Both
these problems are generally NP hard, requiring approximate methods.
Many approximate inference methods, including message passing algorithms, can be viewed as trying to
solve a variational formulation of the inference problem. The idea in variational approaches is to cast approximate inference as a constrained minimization of
a free energy function (see [14] for a recent review).
Two key questions arise in this context. The first is
how to choose the free energy, and the second is how to
design efficient algorithms that minimize it. When the
Bethe free energy is used, it has been shown [16] that

Tommi Jaakkola
CSAIL
Massachusetts Institute of Technology
Cambridge, MA 02139

fixed-points of the belief propagation (BP) algorithm
correspond to local minima of the free energy. However, BP is not generally guaranteed to converge to a
fixed-point. Although there do exist algorithms that
are guaranteed to converge to a local minimum of the
Bethe free energy [15, 17], its global minimization is
still a hard non-convex problem for which no efficient
algorithms are known.
The difficulties with the Bethe free energy derive
from its non-convexity and corresponding local minima problem. To avoid this difficulty, several authors
have recently studied convex free energies [6, 7, 13].
The associated convex optimization problems can in
principle be solved using generic convex optimization
procedures [1] with guarantees of finding the global optimum in polynomial time. Although this presents a
significant improvement over the non-convex case, the
generic optimization route may be very costly in large
practical problems. For example, when using a generic
convex solver, every update of the variables has complexity O(n), where n is the number of variables. In
contrast, the optimization using message passing algorithms can be reduced to local updates with O(1)
operations. Interestingly, even in the convex setting,
the convergence of these message passing algorithms
is typically not guaranteed, and damping heuristics
are required to ensure convergence in practice [13]. A
prominent exception is [7] where the author provides
a provably convergent message passing algorithm for
free energies where the entropy term is a non-negative
combination of joint entropies.
Here we provide a provably convergent message passing algorithm for a specific variational setup, namely
the Tree-Reweighted (TRW) optimization problem of
Wainwright et al. [13]. The algorithm we propose is
guaranteed to converge to the global optimum of the
free energy, and does not require additional parameters such as the damping ratio. A key step in obtaining the updates is deriving the convex-dual of TRW,
which we show to be an unconstrained instance of a

134

GLOBERSON & JAAKKOLA
generalized geometric program (GP) [3]. We derive a
message passing algorithm, which we call TRW Geometric Programming (TRW-GP), that yields monotone improvement of the dual GP. We demonstrate the
utility of our TRW-GP algorithm by providing an example where the TRW message passing algorithm in
[13] does not converge, but TRW-GP does.

2

where H(Xi ) is the entropy of µi (xi ) and I(Xi ; Xj )
is the mutual information calculated from µij (xi , xj ).
Note that this expression is independent of the direction of the edges in the tree. We will make use of the
directed edges in the next section.
Define the following variational free energy function
F (µ; ρ, θ)

The Tree-Reweighting Formulation

We consider pairwise Markov random fields (MRF)
over a set of variables x = x1 , . . . , xn . Given a graph
G with n vertices V and a set of edges E, an MRF is
a distribution over x defined by
p(x; θ) =

1 Pij∈E θij (xi ,xj )+Pi∈V
e
Z(θ)

θi (xi )

Our focus here is on approximating singleton
marginals of p(x; θ), namely p(xi ; θ). This problem is
closely related to that of evaluating the partition function Z(θ). We focus on the TRW variational problem
which yields an upper bound on Z(θ) as well as a set of
approximate marginals obtained from the minimizing
solution.
We begin by briefly reviewing the TRW formalism.
Consider a set of k spanning trees on G denoted by
T 1 , . . . , Tk , P
and a distribution ρi over these trees where
ρi ≥ 0 and ρi = 1. To avoid overloading notation in
subsequent analysis, we assume here that the trees are
directed, so that the same tree structure may appear
multiple times with different edge orientations. This
differs from the presentation in [13] though the distinction is immaterial in the remainder of this section.
We also introduce the notion of pseudomarginals
defined as the singleton and pairwise marginals
µi (xi ), µij (xi , xj ) associated with the edges and nodes
of G. We use µ to denote the set of all these marginals
and C(G) the set of µ’s that are pairwise consistent
P
P
µij (xi , xj ) = µi (xi ) ,
µij (xi , xj ) = µj (xj )
xj
xi
P
µi (xi ) = 1 , µij (xi , xj ) ≥ 0 .
xi

For a given tree T and µ ∈ C(G), define the entropy
H(µ; T ) to be the entropy of an MRF on the tree T
with marginals given by µ. Note that only a subset
of the pairwise distributions in µ will be used for each
tree, namely µij (xi , xj ) such that ij is an edge in T .
The tree entropy may be written in closed form as (cf.
[13])
X
X
H(µ; T ) =
H(Xi ) −
I(Xi ; Xj )
(2)
ij∈T

k
X

ρi H(µ; Ti ) .

(3)

i=1

In [13] it is shown that minimizing F (µ; ρ, θ) results
in an upper bound on the log-partition function
log Z(θ) ≤ − min F (µ; ρ, θ) .
µ∈C(G)

(1)

where θij (xi , xj ) and θi (xi ) are parameters, θ denotes
all the parameters, and Z(θ) is the partition function.

i

F (µ; ρ, θ) = −µ · θ −

(4)

The minimization also results in an optimal (minimizing) µ, which is used to approximate the marginals
of p(x; θ). Empirical results in [13] show that TRW
usually performs as well as, and often better than the
standard Bethe free energy approximations, especially
in regimes where BP fails to converge.

3

Conditional Entropies and Directed
Edge Probabilities

Our goal is to use convex duality to obtain the dual
problem of Eq. (4). To achieve this, we first seek a
representation of F (µ; ρ, θ) that is a convex function of
µ for all values of µ, and not just within the consistent
set µ ∈ C(G). For example, the entropy term in Eq. (2)
is concave only for µ ∈ C(G) but not for a general µ.
We therefore seek an alternative expression for the tree
entropy.
Let r(T ) be the root node of T (recall that the trees
are directed). We write the entropy associated with
the tree as
X
H(µ; T ) = H(Xr(T ) ) +
H(Xi |Xj )
(5)
j→i∈T

where j → i ∈ T implies that there is a directed edge
from vertex j to vertex i in the directed tree T . The
conditional entropy H(Xi |Xj ) is assumed to be calculated only on the basis of the joint marginal µij (xi , xj ),
and does not involve µi (xi ). The entropy H(Xr(T ) )
is calculated via the singleton marginal µr(T ) (xr(T ) ).
The expressions in Eq. (5) and Eq. (2) will agree whenever µ ∈ C(G). However, they will yield different results when µ ∈
/ C(G).
The advantage of Eq. (5) is that H(µ; T ) is now a concave function of the set of marginals µ. The concavity
follows immediately from the concavity of H(Xi ) as a
function of µi (xi ) and the concavity of the conditional
entropy H(Xi |Xj ) as a function of µij (xi , xj ) [6].

GLOBERSON & JAAKKOLA
The function F (µ; ρ, θ) involves a summation over a
potentially large number of tree entropies. To express
this compactly while maintaining directionality, we define ρi|j as the probability that the directed edge j → i
is present in a tree drawn according to the distribution
ρ over trees. Similarly, we define ρ◦i as the probability that node i appears as a root. We note that it
is possible to find such edge probabilities for distributions (e.g. uniform) over the set of all spanning trees
by employing a variant of the matrix tree theorem for
directed trees (see [12] p. 141 and [11]).
The function F (µ; ρ, θ) can now be written as
−µ · θ −

X

i∈V

ρ◦i H(Xi ) −

X

ij∈Ē

ρi|j H(Xi |Xj )

now on) and refer to the consistency constraints by
~
C(G).
The TRW primal problem is then
P T RW :

min F (µ; ρ, θ) .

(6)

The TRW Convex Dual

(8)

~
µ∈C(G)

The convex dual of P T RW is derived in App. A. and
is in fact a convex unconstrained minimization problem. In what follows we describe this dual. The dual
variables will be denoted by βij (xi , xj ) for ij ∈ E, and
are not constrained.2 The dual objective is given by
X
X ρ−1 (θ (x )−P
k∈N (i) λk|i (xi ;β))
FD (β; ρ, θ) =
ρ◦i log
e ◦i i i
xi

i

where the edge set Ē contains edges in both directions.
In other words, if ij ∈ Ē then ji is also in Ē. The new
function F (µ; ρ, θ) is convex in µ without assuming
consistency of the marginals.

4

135

where λj|i (xi ; β) is a function of the β variables:
λj|i (xi ; β) = −ρj|i log

X

e

ρ−1
(θij (xi ,xj )+δj|i βij (xi ,xj ))
j|i

xj

and δj|i is defined as

1 ji ∈ E
δj|i =
−1 ij ∈ E .
The dual TRW optimization problem is then

The TRW primal problem is given by
min F (µ; ρ, θ) .

µ∈C(G)

DT RW :
(7)

Since the function F (µ; ρ, θ) is now convex for all µ
and the set of constraints is linear, this optimization
problem is convex and thus has an equivalent convex
dual [1].1 However, it is not immediately clear how
to derive this dual in closed form. The main difficulty is that two terms in the objective F (µ; ρ, θ) depend on µij (xi , xj ), namely H(Xi |Xj ) and H(Xj |Xi ).
To get around this problem we introduce additional
variables to the primal problem. Specifically, we replace µij (xi , xj ) by two copies which we denote by
µi|j (xi , xj ) and µj|i (xi , xj ), and require that these
two copies are identical. The entropy H(Xi |Xj ) is
then evaluated via the variables µi|j (xi , xj ). We shall
also find it convenient to replace the consistency constraints in C(G) by the following equivalent directed
consistency constraints
µi|j (xi , xj ) = µj|i (xi , xj )
P
P
µj|i (xi , xj ) = µi (xi ) ,
µi|j (xi , xj ) = µj (xj )
xj
xi
P
µi (xi ) = 1 , µi|j (xi , xj ) ≥ 0 , µj|i (xi , xj ) ≥ 0 .
xi

For simplicity we will continue to denote the new extended variable set by µ (as we will be using it from
1
Strict duality follows from Slater’s conditions, which
are satisfied in this case.

min FD (β; ρ, θ) .
β

(9)

We re-emphasize the fact the DTRW is an unconstrained minimization of a function of β. The variables λj|i (xi ; β) are introduced merely for the purpose
of notational convenience. The mapping between dual
and primal variables can be shown to be
−1

µi (xi ) ∝ eρ◦i (θi (xi )−
µj|i (xj |xi ) ∝ e

P

k∈N (i)

λk|i (xi ;β))

ρ−1
(θij (xi ,xj )+δj|i βij (xi ,xj ))
j|i

.

(10)

This relation maps the optimal β to the optimal µ,
but we shall also use it for non-optimal values.
The dual objective FD (β; ρ, θ) is a convex function
(see App. B) and therefore has no local minima.

5

Dual Gradient and Optimum

The DTRW problem presented above is unconstrained
and can thus be solved using a variety of gradient
based algorithms, such as conjugate gradient or BFGS
[10]. The gradient of FD (β; ρ, θ) w.r.t. β is
∂FD (β; ρ, θ)
= µi|j (xi |xj )µj (xj ) − µj|i (xj |xi )µi (xi )
∂βij (xi , xj )
where the distributions are given by the dual to primal
mapping in Eq. (10). The gradient is thus a measure
2
Note that β variables are not directed, i.e., there is one
variable βij per edge.

136

GLOBERSON & JAAKKOLA
of the discrepancy between two ways of calculating
the joint pairwise marginal, based on the two different
orientations of the edge ij.
To characterize the optimum of DTRW we set the gradient to zero, yielding the following simple dual optimality criterion
µi|j (xi |xj )µj (xj ) = µj|i (xj |xi )µi (xi ) .

(11)

Thus at the optimum the two alternative ways of estimating µij (xi , xj ) will yield the same result.
Calculating the gradient w.r.t a given βij (xi , xj ) has
complexity O(1), and relies only on βij (xi , xj ) for
edges containing i or j. Thus the gradient can be calculated locally, and gradient descent algorithms can
be implemented efficiently. One drawback of gradient based algorithms is their reliance on line-search
modules for finding a step size that decreases the objective. In the next section we consider updates that
are parameter-free.

6

Local Marginal Updates

The gradient updates described in the previous section
use the difference between two joint distributions. We
will now focus on updates relying on the ratio between
these distributions. Consider
t+1
t
βij
(xi , xj ) = βij
(xi , xj )+ǫ log

µtj|i (xj |xi )µti (xi )

µti|j (xi |xj )µtj (xj )

(12)

where µtj|i (xj |xi ) and µti (xi ) are functions of β as in
Eq. (10) and ǫ is a step size whose value will be discussed in the next section. As a ratio of two expected
values, the update is reminiscent of Generalized Iterative Scaling [5]. We shall assume for simplicity that
only one edge is updated at each time step t.

Lemma 6.1 : For 0 < ǫ < min(ρ◦i , ρ◦j , ρi|j , ρj|i ) the
dual objective is decreased at every iteration so that
∆D (µt ) ≥ 0 for all t. Furthermore, ∆D (µt ) = 0 holds
if and only if the optimum condition of Eq. (11) is
satisfied.
Any choice of ǫ that is smaller than
min(ρ◦i , ρ◦j , ρi|j , ρj|i ) will result in monotone improvement of the objective. In the current implementation we use ǫ = 12 min(ρ◦i , ρ◦j , ρi|j , ρj|i ). This value
turns out to minimize a first order approximation of
the improvement in the objective, and was found to
work well in practice. The convergence to the global
optimum now follows from Lemma 6.1.
Lemma 6.2 : The updates in Eq. (12) with ǫ as in
Lemma 6.1 converge to the joint optimum of PTRW
and DTRW.
Proof: Denote the mapping from µt to µt+1 by
R(µt ) = µt+1 . The mapping is clearly continuous.
By Lemma 6.1 the sequence FD (β t ; ρ, θ) is monotonically decreasing. It is also bounded since FD (β; ρ, θ)
is bounded and thus the difference series ∆D (µt ) converges to zero. Taking t to infinity then implies that
µt has a convergent subsequence that converges to
some µ∗ . This µ∗ will then satisfy FD (µ∗ ; ρ, θ) =
FD (R(µ∗ ); ρ, θ). We know from the Lemma 6.1 that
such a point necessarily satisfies the zero gradient condition in Eq. (11), and thus µ∗ (or more precisely, the
corresponding β) minimizes the dual objective.3

7

Tree Re-parametrization View

The TRW problem can be interpreted in terms of iterating through different re-parametrizations of the distribution p(x; θ) [13]. Here we present a related view
of our algorithm.

The update in Eq. (12) is performed on the β variables.
An equivalent, and somewhat simpler update may be
derived in terms of the variables µti|j (xi |xj ) and µtj (xj ).
The resulting updates and algorithm are described in
Figure 1. We call the resulting algorithm TRW-GP
(TRW Geometric Programming).

We wish to show that the marginal variables obtained
by the algorithm can always be used to obtain the
original distribution via
Y
Y
µtj|i (xj |xi )ρj|i . (13)
p(x; θ) = ct
µti (xi )ρ◦i

6.1

For t = 0 this is clearly true. We proceed by induction.
Assume that at iteration t we have a reparametrization with constant ct . Substituting the update rule in
Figure 1 and using simple algebra shows that we again
have a reparametrization, only with

Convergence Proof

To analyze the convergence of the update in the previous section, we need to consider the resulting change
in the objective FD (β; ρ, θ), namely FD (β t ; ρ, θ) −
FD (β t+1 ; ρ, θ). It can be shown (see App. D) that
this difference only depends on the µ variables in the
TRW-GP algorithm, and thus we denote it by ∆D (µt ).
Since FD (β t ; ρ, θ) should be minimized, this difference
needs to be non-negative. This is indeed guaranteed
by the following lemma (see App. D):

i

ct+1 = ct eFD (β

ij∈Ē

t+1

;ρ,θ)−FD (β t ;ρ,θ)

= ct e−∆D (µ

t

)

.

3
To carefully account for the possibility that some of
the converging marginals would involve zero probabilities,
the updates in the primal form, along with the objective,
can be written in a form without any ratios.

GLOBERSON & JAAKKOLA

137

Inputs: A graph G = (E, V ), parameter vector θ on G, root probabilities ρ◦i and directed edge probabilities
ρi|j for (ij), (ji) ∈ E.
−1

Initialization: Set µ0i (xi ) ∝ eρ◦i θi (xi ) and µ0i|j (xi |xj ) ∝ eρi|j θij (xi ,xj )
−1

Algorithm: Iterate until small enough change in marginals:
• Set ǫ =

1
2

min(ρ◦i , ρ◦j , ρi|j , ρj|i ), and update

µt+1
(xi ) ∝ µti (xi )
i
µt+1
i|j (xi |xj )

∝

P
xj



µtj|i (xj |xi )

−1
µti|j (xi |xj )1−ǫρi|j



!ρj|i ρ◦i
ǫρ−1
j|i

−1

µti|j (xi |xj )µtj (xj )
µtj|i (xj |xi )µti (xi )

µtj|i (xj |xi )µti (xi )
µtj (xj )

ǫρ−1
i|j

Output: Final values of marginals.
Figure 1: The TRW-GP algorithm expressed in terms of conditional and singleton marginals.
In other words the multiplicative constant turns
out to be related to the improvement in the dual
function. This creates an interesting link between
reparametrization and minimization, and may be used
to study message passing algorithms where a dual is
more difficult to characterize.

8

Relation to Previous Work

Heskes [7] recently presented a detailed study of convex free energies. When the entropy term is a positive
combination of joint and singleton entropies (and is
therefore concave), he provides a local update algorithm that is monotone in the convex dual, and converges to the global optimum. He then discusses the
application of the same algorithm to the case where the
singleton entropies all have negative weight, and the
overall entropy is convex over the set of constraints.4
In this case, the dual is generally not given in closed
form and it is not known if the algorithm decreases it
at every step. However, Heskes argues that with sufficient damping the algorithm can be shown to converge,
although the exact form of damping is not given.
Since the TRW entropy can be shown to decompose
into positively weighted pairwise entropies and negatively weighted singleton entropies, it satisfies the
above condition in Heskes’ work. Our analysis provides several advantages over the algorithm in [7].
First, we derive a closed form solution of dual. Second, the dual is unconstrained, and thus allows unconstrained minimization methods to be applied. Third,
unlike most belief propagation variants, our algorithm
4

The discussion in [7] is in terms of general regions,
not just pairs. We present his argument for the simpler
pairwise case.

is shown to provide a monotone improvement of an
objective function5 , and thus diverges from the standard fixed point analysis used in message passing algorithms.
Finally, another algorithm which is guaranteed to converge to a global minimum of convexified free energies
is the double loop CCCP algorithm of Yuille [17]. The
main disadvantage of CCCP is that each iteration requires solving an optimization problem. This usually
results in slower convergence, and furthermore it is
not clear what precision is required for the inner loop
optimization, and how this affects convergence guarantees. The algorithm we present here is essentially a
single loop method, and is thus easier to analyze.

9

Empirical Demonstration

The original TRW message passing (TRW-MP) algorithm presented in [13] is not generally guaranteed to
converge. However, we observed empirically that when
damping of α = 0.5 is applied to the log-messages,
convergence is always achieved.6 To compare TRWMP to TRW-GP, we use the pseudomarginals generated by TRW-MP7 as marginals in the primal objective F (µ; ρ, θ) in Eq. (3). This value is not expected
to be an upper or lower bound on the optimum of
F (µ; ρ, θ), since the TRW-MP pseudomarginals are
5

As mentioned above, Heskes presents such an algorithm for positively weighted singleton and pairwise entropies. It is however not clear that such entropies are
useful in practice
6
This observation is in line with Heskes’ argument that
sufficiently damped messages will converge for the case of
the TRW free energy.
7
See Equations (58) and (59) in [13].

138

GLOBERSON & JAAKKOLA
TRW−GP
TRW−MP
TRW−MP(damped)

880

870

860

850
0

100

Iteration

200

300

140

Primal−Dual Value

Primal−Dual Value

890

TRW−GP
TRW−MP
TRW−MP(damped)

135

130

125

120
0

100

Iteration

200

300

Figure 2:

Illustration of the dual message passing algorithm for a 10 × 10 Ising model. The TRW-GP curve
shows the dual objective value FD (β; ρ, θ) obtained by the
TRW-GP algorithm. The TRW-MP curves show the primal objective values F(µ; ρ, θ) obtained by TRW message
passing algorithms. The damped TRW-MP used a damping of 0.5 in the log domain. The MRF parameters were
set as follows: αF = 1, αI = 9 for the left figure, and
αF = 1, αI = 1 for the right figure.

not guaranteed to be pairwise consistent, except at
the optimum. However, since the TRW-MP pseudomarginals converge to the optimal primal marginals,
the value F (µ; ρ, θ) will converge to the primal optimum. The progress of TRW-GP may be monitored by
evaluating FD (β; ρ, θ) at every iteration. This value is
guaranteed to decrease and converge to the optimum
of FD (β; ρ, θ) which is identical to the optimum of
F (µ; ρ, θ). We can thus observe the rate at which the
different algorithms converge to their joint optimum.
To study the convergence rate of the two algorithms, we used an Ising model on a 10 × 10 grid
with interaction parameters θij drawn uniformly from
[−αI , αI ] and field parameters θi drawn uniformly
from
[−αF , αFP]. The MRF is given by p(x; θ) ∝
P
θij xi xj + i∈V θi xi
ij∈E
e
where xi ∈ {+1, −1}. We
used a uniform distribution over directed spanning
trees calculated as in [11].
Figure 2 (left) shows an example run where the undamped TRW-MP algorithm does not converge, but
the TRW-GP and the damped TRW-MP do converge,
and do so roughly at the same rate. Figure 2 (right)
shows an example where both TRW-MP algorithms
converge and do so at a faster rate than TRW-GP.
We experimented with various values of αF and αI
and have observed that at lower interaction levels (e.g.,
αI ≤ 4 for αF = 1) the TRW-MP algorithms outperform TRW-GP, whereas for higher interaction levels
the undamped TRW-MP does not converge, but the
damped version converges at roughly the same rate as
TRW-GP. We also experimented with conjugate gradient minimization of FD (β; ρ, θ), but these did not
yield better rates than TRW-GP.

10

Conclusions

We presented a novel message passing algorithm whose
updates yield a monotone improvement on the dual of
the TRW free energy minimization problem. In order
to obtain a closed form dual we used two tricks. The
first was to decouple different entropies that depend on
the same marginals by introducing multiple copies of
these marginals. The second was to use uni-directional
consistency constraints, so that every copy of a joint
marginal appears in a single consistency constraint.
Although we presented the method in the context of
tree decompositions, the algorithm itself still applies
as long as ρi|j and ρ◦i are non-negative (although the
upper bound on the log partition function may not be
guaranteed in this case).
The TRW-GP algorithm resolves the convergence
problems with the undamped TRW-MP algorithm.
However, we observed empirically that the damped
TRW-MP algorithm always converges, and typically at
a better rate than TRW-GP. Thus, the main contribution of the current paper is in introducing a dual framework for message passing algorithms, which could be
used to analyze existing algorithms, and possibly develop faster variants in the future.
Free energies may be defined using marginals of more
than two variables [13, 16]. In a recent paper [6]
we study the relation between such free energies and
GP. It will be worthwhile to study generalizations of
TRW-GP to this case. Another interesting extension is
to the MAP problem, where the corresponding variational problem is a linear program. Global convergence
results for MAP message passing algorithms such as
max-product are also hard to obtain in the general
case. It turns out that an approach similar to the one
presented here may be used to obtained convergent
algorithms to solve the MAP linear program. These
algorithms will be presented elsewhere.

References
[1] D. P. Bertsekas. Nonlinear Programming. Athena Scientific, Belmont, MA, 1995.
[2] S. Boyd and L. Vandenberghe. Convex Optimization.
Cambridge Univ. Press, 2004.
[3] M. Chiang. Geometric programming for communication systems. Foundations and Trends in Communications and Information Theory, 2(1):1–154, 2005.
[4] M. Chiang and S. Boyd. Geometric programming duals of channel capacity and rate distortion. IEEE
Trans. on Information Theory, 50(2):245–258, 2004.
[5] J.N. Darroch and D. Ratcliff. Generalized iterative
scaling for log-linear models. Ann. Math. Statist.,
43(5):1470–1480, 1972.

GLOBERSON & JAAKKOLA
[6] A. Globerson and T. Jaakkola. Approximate inference using conditional entropy decompositions. In
AISTATS, 2007.
[7] T. Heskes. Convexity arguments for efficient minimization of the Bethe and Kikuchi free energies.
Journal of Artificial Intelligence Research, 26:153–
190, 2006.
[8] E.L. Peterson R.J. Duffin and C. Zener. Geometric
programming. Wiley, 1967.
[9] L. Vandenberghe S. Boyd, S.J. Kim and A. Hassibi.
A tutorial on geometric programming. Optimization
and Engineering, 2007.
[10] F. Sha and F. Pereira. Shallow parsing with conditional random fields. In Proc. HLT–NAACL, 2003.
[11] X. Carreras T. Koo, A. Globerson and M. Collins.
Structured prediction models via the matrix-tree theorem. In EMNLP, 2007.
[12] W. Tutte. Graph Theory. Addison-Wesley, 1984.
[13] M. J. Wainwright, T. Jaakkola, and A. S. Willsky. A new class of upper bounds on the log partition function. IEEE Trans. on Information Theory,
51(7):2313–2335, 2005.
[14] M.J. Wainwright and M.I. Jordan. Graphical models,
exponential families, and variational inference. Technical report, UC Berkeley Dept. of Statistics, 2003.
[15] M. Welling and Y.W. Teh. Belief optimization for
binary networks: A stable alternative to loopy belief
propagation. In Uncertainty in Artificial Intelligence,
2001.
[16] J.S. Yedidia, W.T. Freeman, and Y. Weiss. Constructing free-energy approximations and generalized belief
propagation algorithms. IEEE Trans. on Information
Theory, 51(7):2282–2312, 2005.
[17] A. L. Yuille. CCCP algorithms to minimize the Bethe
and Kikuchi free energies: Convergent alternatives to
belief propagation. Neural Computation, 14(7):1691–
1722, 2002.

A

Deriving the TRW Dual

Our goal is to show that the problems in Eq. (8) and
Eq. (9) are convex duals of each other.
First, we claim that the convex dual of the PTRW
problem in Eq. (8) is given by
DT RW C
P
P
P −1
min
ρ◦i log eρ◦i (θi (xi )− k∈N (i) λk|i (xi ))
xi
i
P ρ−1 (θij (xi ,xj )+δj|i βij (xi ,xj )+λj|i (xi ))
j|i
s.t.
e
≤1.
xj

The variables in the above problem are λj|i (xi ),
λi|j (xj ) and βij (xi , xj ) for every edge ij ∈ E.

139

The duality between PTRW and DTRWC results from
the duality between conditional entropy maximization
and geometric programs, and appears in several works
in slightly different forms [3, 8]. A derivation of the
duality result can be found in [2] (page 256) and [4].
It is important to note that the dual can be found in
this case because the objective is a sum of conditional
entropies (and singleton entropies) as in Eq. (6). It
is not clear how to derive a dual if tree entropies are
expressed via mutual information as in Eq. (2).
Due to complementary slackness conditions, the inequality in the constraints of DTRWC will hold with
equality at the optimum iff the optimal primal variables satisfy µi (xi ) > 0. In App. C we show that
for the current objective, this will always happen, i.e.,
µi (xi ) > 0 for all i and xi . We thus conclude that all
the inequality constraints in DTRWC are always satisfied as equalities at the optimum. We therefore lose
nothing by replacing them with equality constraints
X ρ−1 (θ (x ,x )+δ β (x ,x )+λ (x ))
e j|i ij i j j|i ij i j j|i i = 1 . (14)
xj

Since each variable λj|i (xi ) appears in only one constraint, we can eliminate it by expressing it as a function of the β variables
X ρ−1 (θ (x ,x )+δ β (x ,x ))
λj|i (xi ; β) = −ρj|i log
e j|i ij i j j|i ij i j .
xj

Since the λj|i (xi ) variables have been eliminated and
the equality constraints are satisfied, optimization is
now only over the β variables, yielding the DTRW
problem in Eq. (9)
min
β

B

P
i

ρ◦i log

P

eρ◦i (θi (xi )−
−1

P

k∈N (i)

λk|i (xi ;β))

.

xi

Convexity of the Dual

Here we argue that the function FD (β; ρ, θ) is a convex
function of β. We first define the class of posynomial
functions as functions of the form [9]
f (x1 , . . . , xn ) =

K
X

ck xa1 1k xa2 2k , . . . , xan1n

(15)

k=1

where ck > 0. A function f (x1 , . . . , xn ) is said to be
a generalized posynomial if it is either a posynomial
or it can be formed from generalized posynomials using the operations of addition, multiplication, positive
power, maximum and composition. A key property
of generalized posynomials is that they can be turned
into a convex function by a simple change of variables.
Specifically, if f (x) is a generalized posynomial, then
F (y) = log f (ey ) is a convex function of y [9].

140

GLOBERSON & JAAKKOLA
It is easy to see that fj|i (xi ; eβ ) = e−λj|i (xi ;β) is a generalized posynomial in eβ (since it is a positive power
of a posynomial). The function
X ρ−1 θ (x )−P
k∈N (i) λk|i (xi ;β))
gi (eβ ) =
e ◦i ( i i
xi

=

X
xi


eρ◦i θi (xi ) 
−1

Y

ρ−1
◦i
fk|i (xi ; eβ )

k∈N (i)

is then also a generalized posynomial.
Therefore log gi (eβ ) is P
a convex function of β. Since
β
FD (β; ρ, θ) =
i ρ◦i log gi (e ), it follows that
FD (β; ρ, θ) is a convex function of β.

C

Strict Positivity of TRW Marginals

Here we want to show that the solution of the primal
problem must satisfy µi (xi ) > 0. To do so, we employ
an alternative formulation of TRW [13]. Assign a parameter vector θ T to every tree in the set of trees, and
denote by Z(θ T ) the partition function of an MRF on
tree T with parameters Z(θT ). TRW can then be cast
as
P
min PT ρT log Z(θT )
(16)
s.t.
T ρT θ T = θ .

µi|j (xi , xj ), µj|i (xi , xj ), µi (xi ) and µj (xj )) and that
ǫ < min(ρ◦i , ρ◦i , ρi|j , ρj|i ). The resulting difference in
objective value can be written as ∆D (µt ) = fi + fj
where
X
ρ−1 (λt (x )−λt+1 (x ))
fi = −ρ◦i log
µti (xi )e ◦i j|i i j|i i . (18)
xi

The λtj|i (xi ) − λt+1
j|i (xi ) difference can be written in
terms of µt as
 t
ǫρ−1
j|i
1−ǫρ−1
µ (xi |xj )µt (xj )
t
j|i
µ
(x
|x
)
=
j i
xj
µt (xi )
t (x |x )µt (x )
µ
i j
j
P
(1−ǫρ−1
) log µt (xj |xi )+ǫρ−1
log
j|i
j|i
µt (xi )
ρj|i log xj e
ρj|i log

P

Since
convexity of the
P 0 < ǫ < ρj|i we can use theP
log exp function and the fact that xj µtj|i (xj |xi ) =
1 to obtain
λtj|i (xi )

−

λt+1
j|i (xi )

≤

ǫ log

X µti|j (xi |xj )µtj (xj )
xj

µti (xi )

.

P
Note that since log exp is strictly convex, equality here is achieved if and only if µt (xj |xi ) =
µt (xi |xj )µt (xj )
,
µt (xi )

which implies the optimum condition
in Eq. (11) is satisfied.

At the optimum, all tree distributions p(x; θT ) can be
shown to have the same singleton marginals µi (xi ),
and these correspond to the marginals that solve
PTRW. The optimization above can be rewritten as
P
min PT ρT DKL [p(x; θ)|p(x; θT )]
(17)
s.t.
T ρT θ T = θ .

Substituting this in the expression for fi in Eq. (18)
and rearranging we have

The objective in Eq. (16) can be obtained from that in
Eq. (17) by expanding the DKL and using the fact that
the constraints hold. The two objectives then differ by
a constant log Z(θ)

expression
is
of
the
form
TheP above
log xi p(xi )1−η q(xi )η
where
p(xi ), q(xi )
are
distributions over xi .
Define a distribution
r(xi ) ∝ p(xi )1−η q(xi )η .
Simple algebra then
yields
X
− log
p(xi )1−η q(xi )η = (1 − η)DKL [r|p] + ηDKL [r|q]

Assume that at the optimum of PTRW, there exist i
and xi such that µi (xi ) = 0. Then the above argument implies that all trees distributions will have this
zero marginal. However, in that case there will be an
assignment x such that p(x; θ T ) = 0. On the other
hand, for any finite θ the true distribution p(x; θ) will
be strictly greater than zero. The DKL will then be
infinite, implying the parameters are not optimal and
resulting in a contradiction.8

D

Monotonicity of Updates

Assume we perform an update on the µ variables
corresponding to an edge ij ∈ E (i.e., update
8

We note that the same argument can be applied to
show that the optimal pairwise marginals µij (xi , xj ) are
never zero.

fi ≥ −ρ◦i log

X
xi


1− ρǫ
◦i

(µti (xi )



X
xj

 ρǫ

◦i

µti|j (xi |xj )µtj (xj )

xi

where DKL is the KL divergence, and is non-negative.
Here η = ρǫ◦i and thus 0 < η < 1 and the above
weighted sum of the two DKL divergences is always
non-negative.
It follows that fi ≥ 0 with equality if and only if the
condition in Eq. (11) is satisfied . A similar argument
shows that fj ≥ 0 if with equality iff Eq. (11) is satisfied. The result for the non-negativity of ∆D (µt ) then
follows immediately.

