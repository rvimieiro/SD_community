112

DECHTER & LARKIN

UAI2001

Hybrid Processing of Beliefs and Constraints*

Rina Dechter and David Larkin
Department of Information and Computer Science
University of California, Irvine, CA 92697-3425
{ dechter,dlarkin}@ics.uci.edu
Abstract

This paper explores algorithms for process­
ing probabilistic and deterministic informa­
tion when the former is represented as a be­
lief network and the latter as a set of boolean
clauses. The motivating tasks are 1. evalu­
ating belief networks having a large number
of deterministic relationships and 2. evaluat­
ing probabilities of complex boolean queries
or complex evidence information over a be­
lief network. We present and analyze a vari­
able elimination algorithm that exploits both
types of information, and provide empirical
evaluation demonstrating its computational
benefits.
1

Introduction and motivation

The paper addresses the question of processing deter­
ministic relationships that interact with probabilistic
information expressed as belief networks. Two pri­
mary sources of determinism are considered: network­
based and query-based. Network determinism means
that a portion of the probabilistic network contains de­
terministic relationships, such as OR, AND and Par­
ity functions. A second source of determinism can be
generated outside the knowledge-base, when evaluat­
ing the posterior belief of complex constraint-based
queries, or when given complex evidence structure
(e.g., disjunctive information).
We will show that both sources of determinism can
be reduced to evaluating the probability of Boolean
queries. While we will assume that the deterministic
information is expressed as boolean formulas in con­
junctive normal form (CNF), the framework is exten­
sible, in principle, to relational constraint expressions
over multi-valued domains.
'This work was supported in part by NSF grant IIS0086529 and by MURI ONR award N00014-00-1-0617

The paper presents a variable-elimination algorithm
for computing the probability of a CNF query over
a belief network. It is known that such queries can
be handled by modeling the formula as part of the
belief network ([Pearl, 1988]). However, as we demon­
strate, it is computationally beneficial to distinguish
between the deterministic and probabilistic informa­
tion. It facilitates constraint processing, especially
search and constraint propagation ( e.g. unit resolu­
tion), which has proven essential for efficient process­
ing of Boolean and constraint expressions. We analyze
the algorithm's complexity based on its dependency
graph. Preliminary experiments show that exploit­
ing deterministic information can lead to significant
speedup of up to a factor of 2 on the average.
2

Preliminaries and background

Let X = {X1, ..., Xn} be a set of random variables
over multi-valued domains, D1, ... , D,., respectively. A
belief network is a pair (G, P) where G = (X, E)
is a directed acyclic graph over the variables, and
P = {Pi}, where Pi denotes conditional probability
tables (CPTs) Pi = {P(XiiPai)}, and pa; is the set
of parent nodes pointing to X; in the graph. When
the CPTs entries are "0" or "1" only, they are called
deterministic or functional CPTs. When some of the
CPT's entries are "0" or "1" they are called mixed
CPTs. The family of X;, F;, includes Xi and its
parent variables. The belief network represents a
probability distribution over X having the product
form P(x1, . . .. ,x,.) = ITf=1P(xiiXpa.) where an as­
signment (X 1 = Xt. ... , Xn = Xn) is abbreviated to
x = (x1, ... , xn) and where xs denotes the restriction
of a tuple x over a subset of variables S. An evidence
set e is an instantiated subset of variables. We use
upper case letters for variables and nodes in a graph
and lower case letters for values in a variable's domain.
The scope of an arbitrary function is its set of argu­
ments. The moral graph of a directed graph is the
undirected graph obtained by connecting the parent
nodes of each variable and eliminating direction.

UAI2001

DECHTER &

LARKIN

3

?
l

®
�@

Belief network
P(g, f, d, c, b, a)
Figure 1:
P(gJf, d)P(flc, b)P(dJb, a)P(bJa)P(cla)P(a)

=

the ori e s .
Propositional variables
which take only two values {true, false} or { 1, 0}, are
denoted by uppercase letters P , Q , R. Propositional
literals (i.e., P, •P) stand for P =true or P = false,
and disjunctions of literals, or clauses, are denoted by
a, ;3, . . .. For instance, a= (P V Q V R) is a clause. A
unit clause is a clause of size 1. The resolution op­
eration over two clauses (a V Q) and (j3 V •Q) results
in a clause (a V ;3), thus eliminating Q. A formula r.p
in conjunctive normal form ( CNF) is a set of clauses
r.p = { a1, . . . , Or} that denotes their conjunction. The
set of models or solutions of a formula <p, denoted m(<p)
is the set of all truth assignments to all its symbols that
do not violate any clause. r e sol v e(<p , a) is the set of
resolvents of each clause in r.p with a.

Example 2.1

Figure la gives an example of a belief
network over 6 variables. Assume that the CPTs as­
sociated with C is mixed given by P(C = lJA = 0) =
1, P(C = 1, A = 1) = 0.5 and that G is associated
with a deterministic function: G = D V F. The rest of
the CPTs are positive. The moral graph is given in
Figure lb.
elimination.
Bucket elimination is a
unifying algorithmic framework for variable elim­
ination algorithms applicable to probabilistic and
deterministic reasoning [Bertele and Brioschi, 1972,
N. L. Zhang and Poole, 1994, Dechter, 1996]. The in­
put to a bucket-elimination algorithm is a set of func­
tions or relations. Given a variable ordering, the algo­
rithm partitions the functions (e.g., CPTs) into buck­
ets, where a function is placed in the bucket of its lat­
est argument in the ordering. The algorithm processes
each bucket, from last to first, by a variable elimina­
tion procedure that computes a new function that is
placed in an earlier (lower) bucket.
The time and
space complexity of such algorithms is exponential in
a graph parameter called induced width w•. For more
information see [Dechter, 1999].

Bucket

Tasks

The primary basic query over belief networks is belief
updating, namely evaluating the posterior probability
of each singleton proposition given some evidence. In
this paper we address complex queries and complex ev­
idence that are expressible as Boolean formulas on sub­
sets of the variables. In addition we will discuss the
processing of hybrid networks containing deterministic
and mixed CPTs, and show that both explicit and im­
plicit deterministic information in such net. works can
be exploited computationally by appropriate transfor­
mation to CNF query evaluation.
3.1

Propositional

113

Complex queries, given complex evidence

CNF Probability Evaluation (CPE). The prob­
lem of evaluating the probability of CNF queries over
belief networks has application to query answering in
massive databases. In particular, for massive data
archives, it is possible to construct an approximate
model of the data offiine using a belief network and
then to answer real-time queries using the approxi­
mate model (without recourse to the original data)
[Pavlov et al., 2000].

Another application is to network reliability. Given a
communication graph with a source and destination,
one seeks to diagnose failure of communication. Since
several paths may be available, the reason for failure
can be described by a CNF formula. Failure means
that for all paths (conjunctions) there is a link on that
path (disjunction) that fails. Given a probabilistic
fault model of the network, the task is to assess the
probability of a failure [Portinale and Bobbio, 1999].
DEFINITION 3.1 (CPE)

Given a belief network ( G, P), defined over proposi­
X= {X1, ... ,Xn} and given a CNF
query tp over a subset Q= { Q1, ... Qr}, where Q � X,
the CNF Probability Evaluation (CPE) is to find the
probability P(<p).
tional variables

Complex evidence.

We can envtswn situations
when one wants to assess belief of a proposition given
partial, disjunctive information. For example, given
that a customer purchased a coat or a shirt, but did
not buy a tie, what is the probability that they will also
purchase shoes? This type of query is very valuable
for predictive modeling, e.g., "cross-sell" applications
where we determine which other products a customer
is likely to purchase.
Belief assessment conditioned on a CNF evidence is
the task of assessing P(XJr.p) for every variable X.
Since P(XJr.p) = aP(X A r.p) when o: is a normalizing
constant relative to X, computing P(XJtp) reduces to a
CPE task for the query ((X= x) 1\r.p). More generally,

DECHTER

114

P(¥?1¢') can be derived from P(¥?1¢') = a'P P(<p 1\ ¢')
when a'P is a normalization constant relative to all the
·

models of <p.
A

can

also be defined over multi-valued

X1, .. .Xn. Its propositions are (X;, a ) , where
ED;. The proposition is true if X; is assigned value
E D; and is false otherwise. The CNF is augmented

variables
a
a

with a collection of 2-CNFs for each variable, that for­
bids assignments of more than one value to a variable.

Namely, for every i
3.2

(X;, a)

--t

-.(X;, b)

if

a

=J. b.

belief

and

networks

have

deterministic

a

hybrid

relationships.

probabilis­
Such

net­

works appear in medical applications in coding net­

[R.J.

works

McEliece and Cheng,

1989].

1997]

Recent work in dynamic decision

networks reveals the need to express large portion of
the knowledge using deterministic constraints. We ar­
gue that treating such information in

a

special manner,

using constraint processing methods is likely to yi eld

significant computational benefit.
Hybrid networks

a triplet <

G, P, F

A hybrid belief network (HBN) is

>,

G

=

(X, E), where X is a set
X = R U D. Variables

of variables partitioned into
in

R

=

1\

x)

e) ::::: P((X

1\

x)

=

e 1\ c l(F) A cl(P))
X :: x we can eval­
formula <p
((X =

uate the probability of the CNF

x)

1\

e

1\

cl(F) 1\ cl(P))

=

over the original HBN. While

some of the information is expressed redundantly, both
in the network and in the query, it is semantically cor­
rect.

Consider the HEN in Figure 1. We can
extract the clauses r.p = {(-,DvG), ( -.FVG), ( -,Qv DV
F)} from the only deterministic function G D V F.
From the mixed CPT of C we can extract the clause
(A V C). Answering the query P(X 1\ -.G) when
X is any variable is equivalent to evaluating P(X 1\
-,Q, A(-.DV G) 1\ (...,F VG) 1\ (-.G v D VF) A (A V C)}.
Example 3.1

and in net­

causally independent

works having CPTs that are
[Beckerman,

we can write:

P((X

=

Evaluating beliefs in hybrid networks

Often
tic

UAI2001

Therefore, to evaluate the belief of

query

CNF

& LARKIN

are probabilistic and have regular CPTs while

variables in D are deterministic having a function de­
fined from their parents to the variable. The CPTs of
probabilistic variables can be positive or mixed. In the

latter case some probability ent r ies in the CPTs are 0

4

Bucket-elimination for CPE

The following paragraphs derive a bucket-elimination
algorithm for CPE. This is a straightforward exten­
sion of the variable elimination algorithm Elim-bel for
belief updating [Dechter,

1996]. Given a belief net­
X = {X1, ... , Xn} and a
X, where the size of Q

work defined over variables
CNF query <p over1 Q �
is

the

r,

CP E

task is to compute a sum of prob­
r.p, namely:

abilities of all the models of

LxqEm('P) P(xQ)

where x

=

(x1, ... , Xn).

the belief-network product form we get:

P(<p)

=

Using

P(<p)

=

L{xli'qEm('P)} rr=l P(x;IXpa,). For derivation pur­
pose, we next assume that Xn is one of the query vari­

ables, and we separate the summation over Xn and

or 1.

X- {Xn}. We denote by In the set of all clauses con­

Belief assessment in an HEN translates to a CPE

taining

task. The idea is to collect together all the determin­
istic information appearing in the functions of

F

and

to extract the deterministic information in the mixed
CPTs, and then transform it all to one CNF expres­

Xn and by f3n all the rest of the clauses. The
rn is denoted Qn, Sn =X- {Xn} and Un is

scope of

the set of all variables in the scopes of the CPTs and
clauses that mention

We get:

Xn. We

define x;

n

sion. This expression can then be treated as a CNF
query over the original network. Clearly, every func­
tion can be expressed
try in a mixed CPT

(x

as

a CNF formula. Also, each en­

P(x;lxpa.) ::::: 1,
the family of X;) can be
--t x;, and all such entries

P(X;Ipa;) ,

is a tuple of variables in

translated to the clause Xpa;

having

constitute a conjunction of clauses.
Let

H BN

evidence

=

e,

< C, P, F

=

P(r.p)

=

IT P(x; lxpa.)

{x,._1jx5,. Em(/3,. )} {x,. lxqn Em(-y,.)} i=l

Denoting by tn the indices of functions in the product
that

do not

mention

get:

Xn

and by ln

=

{1, .. . n}- in

we

)..Xn

(1)

> be a hybrid network. Given

assessing the posterior probability of a

single variable

P(XIe)

( x1, . . . ,x;).

=

X

aP(X

1\

given evidence

e).

Let

ci(P)

e

is to compute

be the clauses ex­

tracted from the mixed CPTs, and

Therefore:

let cl(F) be the

P(<p)

F.
cl(P),

clauses expressing the conjunction of functions in
The network's deterministic portion is

cl(F)

1\

and because this conjunction is redundant relative to
the given network, namely since

P(cl(F)

1\

cl(P) :: 1

1

=

2::: m

{f,._ dxs,. E

(,B,.)}

(II Pj)
j Etn

•

It is easy to extend this to propositions over multi­

valued variables

UAI2001

where

DECHTER

>.X,

over

Un- {Xn},

is defined by

Therefore, if we place all CPTs and clauses mentioning

Xn into the bucket of Xn we can compute the func­

( 2).

The computation of the rest of the

expression proceeds with X,._1, using EQ. (1), in the
same manner.
Case of observed variables. When

Xn is observed,

or constrained by a literal, the summation operation
reduces to assigning the observed value to each of its

and to each of the relevant clauses. In this case
(2) becomes (assume Xn == Xn and P=x, is the
function instantiated by assigning Xn to Xn):
CPTs
EQ.

).Xn

II

==

jEln

Pj=Xn,

if

iqn E

Otherwise, ).Xn = 0. Since

only if XQ.,_ -X

n

we get:

).Xn

:::

II

jEt,.

pi-::x,.

m(in A(Xn == x,.)) (3)

iq n satisfies jnA(Xn = Xn )
resolve(In, (Xn == Xn)),

satisfies /Xn

=

if iq.,_-Xn E m(i�n )

(4)

Therefore, we can extend the case of observed vari­
able in a natural way: CPTs are assigned the observed
value as usual while clauses are individually resolved
with the unit clause

(Xn

==

x,.), and both are moved

to appropriate lower buckets.
Algorithm Elim-CPE, described in Figure

21

includes

therefore a limited amount of constraint propagation
in the form of unit-resolution. Thus, for the variable
ordering of choice, once all CPTs and clauses are par­
titioned (each clause and CPT is placed in the latest

bucket of its scope), we process the buckets from last
to first. If the bucket contains a literal we assign its
value to the CPTs, resolve it with the clauses and move
the resulting functions and clauses to the appropriate
bucket.

Otherwise, in each bucket we generate the

probabilistic function. From our derivation it follows
that

THEOREM

115

Algorithm Elim-CPE

(2)

tion in EQ.

& LARKIN

In p ut : A beliefnetwork (G,P), P= {Pt, . . . ,P,..}; A
CNF formula on r propositions cp = {cq, .. . am} an or­
dering, d
Output: The belief P(cp).
1. Initialize: Place buckets with unit clauses last in
the ordering (to be processed first). Partition P and cp
into bucket, 1 • • • 1 bucketn, in the usual manner. (We
denote probabilistic functions as As and clauses by as).
Scopes of CPTs are denoted by S, of clauses by Q.
2. Backward: Process from last to first.
Let P be the current bucket.
For At, . .. ,Aj, a,, .. . ,ar in bucketp, do
• If bucketp contains Xp- = Xp (or a unit clause),
a. Assign Xp = Xp to each A;
b_ Resolve each a; with the unit clause, and put re­
solvents and probabilistic function lower buckets and
c. Move any bucket with unit clause to top of process­
mg.
P =
• Else, compute probabilistic function A

I::{rpl�upEm(cq, .. . ,ar)} n�=l Ai,
{Xp},

over Up = S U QS = U;S;, Q = U1Q1, and
place any generated function or c lause into its appro­
priate lower bucket.
3. Return
generated in the first bucket.

P( cp)

Figure 2: Algorithm Elim-CPE

get the query 'P = (B V C) A (G V D) A (...,D V -,B).
The initial partitioning into buckets along the order­
ing d = A, C1 B1 D, F, G, as well as the output buck­
ets are given in Figure 3a. In bucket G we com­
pute: >.G(f,d) = L{g/gVd=true} P(gjf,d). In bucket
F: ).F (b, c, d) = I:;1 P(fjb, c)>.G(f, d). In bucket D:
>.D(a,b,c) = I:;{dl--.dv--.b-::true} P(d[a,b)>.F(b,c,d). In
bucket B: >.B(a1 c)= I:;{blbVc=true} P(b[a)>.D(a, b, c).
In bucket C: ).C (a) = I::c P(c[a)>.B(a, c). In bucket
A: ).A =La P(a)>.c (a) P(�.p) =).A.
Let's now extend the example by adding -,Q to the
query. This will place -,Q in the bucket of G {See
Figure 3b.) The Figure shows the derived functions
and clauses, demonstrating the effect of unit resolu­
tion. Note the change in bucket ordering due to the
preference to processing buckets with unit clauses.
The following example extract clauses from the CPTs

4.1 (Correctness and Completeness)

Algorithm Elim-CPE is sound and complete for the
CPE task. o

Note that the algorithm includes also a dynamic re­
ordering of the buckets that prefers processing buckets
that include unit clauses. This may have a significant
impact on efficiency because observations (namely unit
clauses) avoid the creation of new dependencies.
Example 4.2 Lets treat the belief network in Figure
1 as if all its CPTs are pure positive, and assume we

and then applies Elim-CPE.
Example 4.3 Consider again the belief network in
Figure 1 and the query P(A[---.G) but assume the de­
terministic and mixed CPTs as described in Example
3.1. The extracted CNF is rp = (...,D V G) A (...,F V
G) A ( ...,Q V D V F) A (A V C). The initial partitioning
into buckets along the ordering d = A, B1 C, D1 F, G,
as well as the output buckets are given in Figure 4a.
In bucket G, since we have a unit clause, we compute:
>.G( f, d)
P(G = O[D, F). Applying unit resolu­
tion yields the literals --,F and -,D. Since we have
=

DECHTER

116

B1,.1cke-tG:

P(GIF,D)

Bucket. F;

PI

Buc;:kctO;

P(DfA,B)

Bucket B:

P(BIA)

(BvC)

Bucket C:

P!CIAl

A."!A.C)

Bucket A:

P(A)

&

LARKIN

(0 v D)

� l
F)�
(-.Dv--.B)

Bucket 0:

P(G(F,D)

Bu�;;kcl F:

P(

Bucket 0:

P(D�.B)

;tf(B,C,D)

�/
� l/

UAI2001

3:

v

� l
� P(rpl

A.�{A)

G)(F v D

v -.G} . --.0

�
�
A.'(D)
(-Dl

::::- \

Bucket C�

AP(A,B.C)

...t"(B,C}

.Bucket B:

P(BV'\)

Bucket A=

P(A)

...\0(.4..8).

------

.A.c-(A.B)

�l�
..to

:..t•(A)

�./
P(AI-.Gl

)
(a)

Figure

(-.Dv G)(___, ,_-

(a)

Trace of Elim-CPE ( a ) no observation

with observation

(b)

Figure

(a )

4:

Variable elimination for a hybrid network:

Elim-CPE with clause extraction (b) regular Elim­

CPE

a unit clause in bucket F, it will be assigned, yield­
ing >.F(b,c) = P(F = Olb,c). In bucket D we have
a generated unit clause -,D causing an assignment:
>.D(a,b) = P (d = Ola,b) and >.D = >..F (D = 0).
In bucket C: ;:...c (a, b)= L{blaVc=true} P(cla)>..F(b, c) .
Since the clause A V C was extracted from P(CIA)
there is a redundancy in the above computation.
Instead we will generate the function )..c (a, b) =
Lb P(cla)>.F(b,c) which may save time, depending
on the implementation. In bucket B: >.8(a) =
LcP(bla)>.c(a,b)>.0(a,b). In bucket A: >..A (a)
P(a)>.8(a)>.D. P(AI-,G) = a>.A(a). Regular Elim­
CPE, not extracting deterministic CNF information,
creates functions on 3 variables as is shown in Figure
4b.
=

Algorithm Elim-CPE-D i s geared towards processing

from last to first; when node X is processed, all its
preceding neighbors are connected. The

of a graph,

W*,

induced width

is the minimal induced width over all

its orderings [Arnborg, 1985].

As usual, the complexity of bucket elimination algo­
rithms is related to the number of variables appearing
in each bucket. The worst-case complexity is time and
space exponential in the size of the maximal bucket,
which is captured by the induced-width of the relevant
graph.

Given a belief network and a query 'f', the

mented graph

aug­

of the network is the moral graph with

additional arcs between each two variables appearing
in the same clause of the CNF.
Consider now the computation inside a bucket.

If

hybrid networks. It first extracts deterministic clauses

"/P is the CNF theory in bucket P, defined over sub­

from deterministic CPTs, and then applies Elim-CPE.

set

However, for efficiency's sake, the new clauses are used
for resolutions only in each bucket and are ignored for
function computation.
4.1

Complexity

Induced-graphs and induced width.

of a node

The

width

Qp,

and >.1,

. . .. >.i

are the probability functions

Sp, we compute: >.P =
L{xplxqEm(lp)} 0; A; whose scope is Up :::: Qp U Sp {Xr}· A brute force computation of this expression
is 0 (exp(IUr I + 1)). Since IUr I is bounded by w* (d)
of the augmented graph, along d, the complexity of
Elim-CPE is O(n exp(w*(d))).
whose union of scopes is

·

in an ordered graph is the number of the

To capture the simplification associated with observed

node's neighbors that precede it in the ordering. The

variables or unit clauses, we connect only parents

width of an ordering d,

denoted

mum width over all nodes. The

ordered graph, w* (d),

w( d), is the maxi­
induced width of an

is the width of the induced or­

dered graph obtained as follows: nodes are processed

of each non-observed variable when generating the
induced graph.

The

adjusted induced width

is the

width of this adjusted induced-graph. For details see
[Dechter and Larkin,

2001].

In summary,

DECHTER

UA12001

THEOREM

&

o,
O(n

4.4 Given a CNF 'P and an ordering

the complexity of Elim-CPE is time and space

II

served variables and unit clauses generated by unit­

4.2

Eli

ob­

resolution, in 'P· 0

Hidden :

Time

18

1 mr 1 c. I u. II

33

Figure 5: 50 test instances, network parameters of <
50, 5, 0 > and query parameters <50, 1 5 >

Bucket-elimination with hidden variables

Consider now the alternative of modeling clauses
as CPTs. It requires expressing each clause as a
CPT with a new hidden variable and the addition
of evidence to the hidden nodes. Subsequently we
can apply a regular variable elimination algorithm
([Dechter, 1996, N. L. Zhang and Poole, 1994]). We

Algorithm

II Elim-CPE:
m-

·

exp( w* ( o))), where w* ( o) is the induced width along
o of the augmented gra p h adjusted relative to the

117

LARKIN

II

Algorithm

II Elim-CPE:

Elim-Hidden:

Time
5
18

I mr I c. I u. II

Figure 6: Averages over 35 test instances, network
parameters of < 40, 5, 0 > and query parameters
< 60,10 >

call the resulting algorithm Elim-Hidden.

There is no substantial difference between Elim-CPE
and Elim-Hidden in terms of worst-case complexity.
Processing the hidden variables creates tables that cor­
responds to the clauses which are placed in the same
buckets that the original clauses occupy in Elim-CPE;
producing just a linear overhead. Subsequently, when
computing the function's bucket, Elim-Hidden uses
multiplication to factor out non-models and Elim-CPE
uses summation over models. In example 4.3, Elim­
Hidden is far inferior, unable to recognize unit clauses.
4.3

Elim-CPE with constraint propagation

Constraint propagation can, in principle, improve
Elim-CPE by inferring new unit clauses beyond
the power of unit-resolution. Furthermore, inferred
clauses correspond to infered conditional probabilities
that are either "0" or "1".
One form of constraint propagation is bounded reso­
lution [Rish and Dechter, 2000]. It applies pair-wise
resolution to any two clauses in the CNF theory iff
the resolvent does not exceed a bounding parameter,
i. Bounded-resolution algorithms can be applied until
quiesence or in a directional manner, called BDR(i).
After partitioning the clauses into ordered buckets,
each is processed by resolution with bound i.
We extend Elim-CPE into a parameterized family of
algorithms Elim-CPE(i) that incorporates BDR(i) .
The added operation in bucketp is: (If the bucket does
not have an observed variable)
For each pair { ( a V Q;), ((3 V -.Q;)} � bucket;. If the
resolvent 1 = a U (3 contains no more than i proposi­
tions, place the resolvents in the bucket of its highest
index variable. Higher levels of propagation may in­
fer more unit-clauses and general nogoods but require
more computation. It is hard to assess in advance the
right balance of constraint propagation. It is known
that the complexity of BDR(i) is O(exp(i)). There­
fore, for small levels of i the computation in non-unit

buckets is likely to be dominated by generating the
probabilistic function rather than by BDR(i).
5

Empirical Evaluation

There were four algorithms to be compared empiri­
cally: Elim-CPE (which is the same as Elim-CPE(O)),
Elim-CPE( i) , Elim-Hidden, and Elim-CPE-D. Some
random networks were tested, as well as two realistic
networks, the hailfinder and insurance networks. We
report only some of the results for space reasons. For
more information see [Dechter and Larkin, 2001].
The test generator is di­
The first creates a random be­
lief network using a tuple < n, f, d > as a parameter,
where n is the number of variables, f is the maximum
family size, and d is the fraction of deterministic en­
tries in CPTs. Parents are chosen at random from
the preceding variables in a fixed ordering. The en­
tries of the CPT's are filled in randomly. The second
part generates a 3-CNF query, using a pair of param­
eters < c, e > where c is the number of 3-CNF clauses
(clauses are randomly chosen and each is given a ran­
dom truth value) and e is the number of observations.
The random generator.

vided into two parts.

All algorithms use min-degree order, computed by re­
peatedly removing the node with the lowest degree
from the graph and connecting all its neighbors.
Results on Random networks.

We report first some of
our results on Elim-CPE vs Elim-Hidden with two
sets of random networks generated with parameters
< 50, 5, 0 > and < 40, 4, 0 >. The results of those
runs are summarized in Figures 5 and 6 respectively.
In the tables, the time is given in seconds, C stands
for derived Clauses, U stands for derived Unit clauses,
and mf is the arity of the largest function created by
the algorithm. Clearly mf � w*.
Elim-CPE vs Elim-Hidden.

We see that Elim-CPE-Hidden is slower than Elim-

DECHTER

11 8

&

LARKIN

UAI2001

300
II Algorithm
Elim-CP E n:
Elim-CPE 3:

Time

Elim-CPE 2:
Eli m-CP E 1 :

22
21

20

18

I mf I C. I U
17 23 2
17
20 2
17 17 2
17 15
2

I]
200

Elim­
CPE-D

Figure 7: Averages over 30 test instances with network
parameters of < 50, 5, 0 > and query parameters <
50,15 >
C.

U.

F.

,

100

,

,

,

,

,

..

/'

,

/'

,'

,

,

/'

,

,

.

•
•
•

0

100

200

300

Elim-CPE(O )
Figure 9: 48 test instances with network parameters
< 80, 4, 0. 75 > and query parameters < 0, 10 >

Figure 8: Averages of 50 instances with network pa­
rameters < 80, 4, 0. 75 > and varied number of evidence

CPE by a factor of 2 on the average. This is expected
because of Elim-CPE's constraint propagation, which
creates more unit variables.
Testing Elim-CPE{i). The purpose in testing Elim­
CPE ( i ) was to evaluate the effect of different levels of
bounded i-resolution. Higher values of i may produce
more clauses, especially unit clauses, which should
speed up the computation. We ran the algorithm on
networks generated by parameters of < 50, 5, 0 > and
with query parameters < 50, 15 >. The results are
summarized in Figure 7.
As we see in these tests,
higher levels of constraint propagation were not suc­
cessful in creating more unit clauses. It appears that
larger and harder CNF queries are necessary to make
stronger constraint propagation cost-effective.
Testing Elim-CPE-D. Figure 8 shows some tests of
Elim-CPE-D vs. Elim-CPE on random networks. The
difference is that Elim-CPE-D extracts deterministic
information from CPT's. 0. stands for the number
of observed variables and F. stands for the number
of clauses extracted from CPT's. We see that Elim­
CPE-D was generally superior. The results for 10 unit
clauses are also shown in the scatter diagram in Figure
9.
Realistic Benchmarks

Tests on Insurance network. Next we tested the insur­
ance network which is a realistic network for evaluating
car insurance risks that contains deterministic infor­
mation. It has 27 variables. In the experiments re­
ported in Figure 10, Elim-CPE-D outperformed Elim­
CPE substantially. Figure 12 contrasts Elim-CPE
with Elim-Hidden on the insurance network.
Testing on Hailfinder network. Finally we tested the

I Time I mf I C.
48
210
8
12
64
Elim-CPE{15):
9
6
61
9
Elim-CJ:-'E(O):
0
Elim-Hidden:
104
10

II Algorithm

Elim-CPE-D:

U.

1
1
0
0

F.

302
0
0
0

Figure 10: 50 test instances of the insurance network
(27 variables) , with query parameters < 20, 5 >
[I Algorithm
I Time
4
Elim-C PE-D:
16
.Elim-CJ:-'E( 15 ):
16
_E:lirn-::_CPE(O):
33
Elim-Hidden:

I

mf I C.
269
4
7
6
7
6
0
7

I U. I F.
1

1
1
0

501
0
0
0

Figure 1 1: 50 test instances of the hailfinder network
(56 variables ) with query parameters < 15, 15 >

400
300

Elim­
Hidden 200
100

0

100 200 300 400
Elim-CPE ( O )
Figure 12: 50 test instances of the insurance network
with query parameters < 15, 5 >

UAI2001

DECHTER

hailfinder network, another benchmark network that
has 56 variables and includes deterministic informa­
tion. It is a normative system that forecasts severe
summer hail in northeast Colorado. The results are
reported in Figure 11. Here again the results are con­
sistent with earlier observations that Elim-CPE-D was
the most efficient.
6

Discussion and related work

The most relevant work is that of Poole [Poole, 1997]
providing a rule-based description of the conditional
probability tables, and a variable elimination algo­
rithm for exploiting this rule-based representation.
When the information is deterministic, those rules are
simple clauses, and their processing may reduce to sim­
ple resolution. I An area that uses heavily both deter­
ministic and probabilistic information is planning un­
der uncertainty. Most relevant is a recent stochastic
planner called MAXPLAN [Majercik and Littman, ]
which shows how stochastic planning can be trans­
formed into an MAJSAT description and then solved
by a search-based conditioning algorithm. It would be
interesting to exploit our algorithm in the context of
these works.
The paper presents a variable elimination algorithm
called Elim-CPE, for answering Boolean CNF queries
over a belief network. The algorithm is applicable to
hybrid belief networks and to belief updating given
partial information.
The nice property of the bucket-elimination algorithms
is that their complexity is not dependent on the num­
ber of models in the CNF formula. Clearly, all the
tasks addressed here could also be solved by condi­
tioning search or by some combination of search and
inference, and should be explored further. They avoid
the space complexity of bucket elimination and may
work well in practice.

---;

The empirical results demonstrated that the proposed
algorithm Elim-CPE, is far more effective than a brute
force embedding of the CNF query into the belief net­
work (i.e., Elim-Hidden) by a factor of 2 on the av­
erage, depending on the size of the CNF formula.
When applying a variant of this algorthm to hybrid
networks (i.e., Elim-CPE-D) we observed impressive
improvement that were more significant as the portion
of the deterministic information increased. Those re­
sults were consistent for randomly generated networks
and some real benchmarks.
References

[Arnborg, 1985] S. A. Arnborg. Efficient algorithms
for combinatorial problems on graphs with bounded

&

119

LARKIN

decomposability - a survey.

BIT,

25:2-23, 1985.

[Bertele and Brioschi, 1972] U.
and F. Brioschi. Nonserial Dynamic
Academic Press, 1972.

Bertele
Programming.

[Dechter and Larkin, 2001] R. Dechter and D. Larkin.
Hybrid processing of belief and constraints. UCI
Technical report,

www.

ics. uci. edu/ dechter, 2001.

[Dechter, 1996 ] R. Dechter. Bucket elimination: A
unifying framework for probabilistic inference al­
gorithms. In Uncertainty in Artificial Intelligence
(UAI'96), pages 211-219, 1996.
[Dechter, 1999] R. Dechter. Bucket elimination: A
unifying framework for reasoning. Artificial Intel­
ligence, 113:41-85, 1999.
[Beckerman, 1989] D. Beckerman. A tractable in­
ference algorithm for diagnosing multiple diseases.
In Uncertainty in Artificial Intelligence (UAI'89),
pages 171-181, 1989.
[Majercik and Littman,] S. M. Majercik and M. L.
Littman. Maxplan: A new approach to probabilistic
planning.
[N. L. Zhang and Poole, 1994] R. Qi N. L. Zhang and
D. Poole. A computational theory of decision net­
works. International Journal of Approximate Rea­
soning, pages 83-158, 1994.
[Pavlov et al., 2000] D. Pavlov, H. Mannil a, and
P. Smyth. Probabilistic models for query approx­
imation with 20 large sparse binary data sets. In
Submitted to UAI2000, 2000.

[Pearl, 1988] J. Pearl. Probabilistic Reasoning
telligent Systems. Morgan Kaufmann, 1988.

in In­

[Poole, 1997] D. Poole. Probabilisti c partial evalua­
tion: Exploiting structure in probabilistic inference.
In IJCAI-97: Proceedings of the Fifteenth Interna­
tional Joint Conference on Artificial Intelligence,

1997.

[Portinale and Bobbio, 1999] L.
Portinale
and
A. Bobbio. Bayesian networks for dependency anal­
ysis: an application to digital control. In Proceedings
of the 15th Conference on Uncertainty in Artifi cial
Intelligence (UAI99),

pages 551-558, 1999.

[Rish and Dechter, 2000] I. Rish and R. Dechter. Res­
olution vs. search; two strategies for sat. Journal of
Automated Reasoning, 24(1/2):225-275, 2000.
(R.J. McEliece and Cheng, 1997] D.J.C.
MacKay
R.J. McEliece and J .-F. Cheng. Turbo decoding as
an instance of pearl's belief propagation algorithm.
IEEE J. Selected Areas in Communication, 1997.

