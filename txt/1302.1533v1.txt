124

Model Reduction Techniques for Computing Approximately
Optimal Solutions for Markov Decision Processes

and Robert Givan and Sonia Leach
Department of Computer Science, Brown University
[tid, rig, sml]@cs.brown.edu
http://www.cs. brown.edu/people/

Thomas Dean

Abstract

We present a method for solving implicit
(factored) Markov decision processes (MDPs)
with very large state spaces.
We intro­
duce a property of state space partitions
which we call f-homogeneity. Intuitively,
an f-homogeneous partition groups together
states that behave approximately the same
under all or some subset of policies. Borrow­
ing from recent work on model minimization
in computer-aided software verification, we
present an algorithm that takes a factored
representation of an MDP and an 0 � f � I
and computes a factored f-homogeneous par­
tition of the state space.
This partition defines a family of related
MOPs-those MOP's with state space equal
to the blocks of the partition, and transition
probabilities "appro:X:imately" like those of
any (original MDP) state in the source block.
To formally study such families of MDPs,
we introduce the new notion of a "bounded
parameter MDP" (BMDP), which is a fam­
ily of (traditional) MOPs defined by speci­
fying upper and lower bounds on the transi­
tion probabilities and rewards. We describe
algorithms that operate on BMDPs to find
policies that are approximately optimal with
respect to the original MDP.
In combination, our method for reducing
a large implicit MDP to a possibly much
smaller BMDP using an f-homogeneous par­
tition, and our methods for selecting actions
in BMDP's constitute a new approach for an­
alyzing large implicit MOP's. Among its ad­
vantages, this new approach provides insight
into existing algorithms to solving implicit
MDPs, provides useful connections to work
in automata theory and model minimization,
and suggests methods, which involve vary­
ing f, to trade time and space (specifically in
terms of the size of the corresponding state
space) for solution quality.

1

Introduction

Markov decision processes (MDP) provide a formal ba­
sis for representing planning problems involving uncer­
tainty [Boutilier et al., 1995a]. There exist algorithms
for solving MDPs that are polynomial in the size of
the state space [Puterman, 1994]. In this paper, we
are interested in MOPs in which the states are spec­
ified implicitly using a set of state variables. These
MDPs have explicit state spaces which are exponential
in the number of state variables, and are typically not
amenable to direct solution using traditional methods
due to the size of the explicit state space.
It is possible to represent some MOPs using space
polylog in the size of the state space by factoring the
state-transition distribution and the reward function
into sets of smaller functions. Unfortunately, this ef­
ficiency in representation need not translate into an
efficient means of computing solutions. In some cases,
however, dependency information implicit in the fac­
tored representation can be used to speed computa­
tion of an optimal policy [Boutilier and Dearden, 1994,
Boutilier et al., 1995b, Lin and Dean, 1995].
The resulting computational savings can be explained
in terms of finding a homogeneous partition of the state
space-a partition such that states in the same block
transition with the same probability to each of the
other blocks. Such a partition induces a smaller, ex­
plicit MDP whose states are the blocks of the partition;
the smaller MDP, or reduced model is equivalent to the
original MOP in a well defined sense. It is possible
to take an MDP in factored form and find its small­
est reduced model using a number of "partition split­
ting" operations polynomial in the size of the resulting
model; however, these splitting operations are in gen­
eral propositional logic operations which are NP-hard
and are thus only heuristically effective. The states of
the reduced process correspond to groups of states (in
the original process) that behave the same under all
policies. The original and reduced processes are equiv­
alent in the sense that they yield the same solutions,
i.e., the same optimal policies and state values.
The basic idea of computing equivalent reduced pro-

Approximate Model Reduction

cesses has its origins in automata theory [Hartmanis
and Stearns, 1966] and stochastic processes [Kemeny
and Snell, 1960] and has surfaced more recently in the
work on model checkin� in computer-aided verifica­
tion [Burch et al., 1994J [Lee and Yannakakis, 1992].
Building on the work of Lee and Yannakakis [ 1992],
we have shown [Dean and Givan, 1997] that several
existing algorithms are asymptotically equivalent to
first constructing the minimal reduced MDP and then
solving this MDP using traditional methods that op­
erate on the flat (unfactored) representations.
The minimal model may be exponentially larger than
the original compact MDP. In response to this prob­
lem, this paper introduces the concept of an (­
homogeneous partition of the state space. This re­
laxation of the concept of homogeneous partition al­
lows states within the same block to transition with
different probabilities to other blocks so long as the
different probabilities are within c For E > 0,
there are generally (-homogeneous partitions which
are smaller and often much smaller than the small­
est homogeneous partition. In this paper we discuss
appmximate model reduction-an algorithm for find­
ing an E-homogeneous partition of a factored MDP
which is generally smaller and always no larger than
the smallest homogeneous partition.
Any E-homogeneous partition induces a family of ex­
plicit MDPs, each with state space equal to the blocks
of the partition, and transition probabilities from
each block nearly identical to those of the underlying
states. To formalize and analyze such families we in­
troduce the new concept of a bounded parameter MDP
(BMDP)-an MDP in which the transition proba­
bilites and rewards are given not as point values but
as closed intervals. In Givan et al. [1997], we describe
algorithms that operate on BMDPs to produce bounds
on value functions and thereby compute approximately
optimal policies-we summarize these methods here.
The resulting bounds and policies apply to the origi­
nal implicit MDP. Bounded parameter MDPs general­
ize traditional (exact} MDPs and are related to con­
structs found in work on aggregation methods for solv­
ing MDPs [Schweitzer, 1984, Schweitzer et al., 1985,
Bertsekas and Castanon, 1989]. Although BMOPs
are introduced here to represent approximate aggre­
gations, they are interesting in their own right and are
discussed in more detail in [Givan et al., 1997], The
model reduction algorithms and bounded parameter
MDP solution methods can be combined to find ap­
proximately optimal solutions to large factored MOPs,
varying E to trade time and space for solution quality.
The remainder of this paper is organized as follows. In
Section 2, we give an overview of the algorithms and
representations in this paper and discuss how they fit
together. Section 3 reviews traditional and factored
MDPs and describes the generalization to bounded
parameter MOPs. Section 4 describes an algorithm
for €-reducing an MDP to a (possibly) smaller explicit
BMDP (an MDP if t = 0). Section 5 summarizes

125

our methods for policy selection in BMDPs, and ad­
dresses the applicability of the selected policies to any
MOP which t-reduces to the analyzed BMDP. The re­
maining sections summarize preliminary experimental
results and discuss related work.
2

Overview

Here we survey and relate the basic mathematical ob­
jects and operations defined later in this paper. We
start with a Markov decision process (MDP) M for
which we would like to compute an optimal or near
optimal policy. Figure La depicts the MDP M as a
directed graph corresponding to the state-transition
diagram, and its optimal policy 1TM as found by tradi­
tional value iteration.
We assume that the state space for M (and hence the
state-transition graph) is quite large. We therefore
assume that the states of M are encoded in terms of
state variables which represent aspects of the state;
an assignment of values to all of the state variables
constitutes a complete description of a state. In this
paper, we assume that the factored representation is in
the form of a Bayesian network, such as that depicted
in Figure l.b with four state variables {A, B, C, D}.
We speak about operations involving M, but in prac­
tice all operations will be performed symbolically us­
ing the factored representation: we manipulate sets
of states represented as formulas involving the state
variables.
Figure l.c and Figure l.d depict the unique smallest
homogeneous partition of the state space of M, where
the blocks are represented (respectively) implicitly and
explicitly. The process of finding this partition is called
(exact) model minimization. Factored model mini­
mization involves manipulating boolean formulas and
is NP-hard, but heuristic manipulation may rarely
achieve this worst case.
The smallest homogeneous partition may be exponen­
tially large, so we seek further reduction (at a cost
of only approximately optimal solutions) by finding
a smaller t-homogeneous partition, depicted in Fig­
ure I.e and Figure l.f where the blocks are again rep­
resented (respectively) implicitly and explicitly.
Any (-homogeneous partition can be used to create a
bounded parameter MDP, shown in Figure l.g and no­
tated as M -to do this, we treat the partition blocks
as (aggregate) states and summarize everything that
we know about transitions between blocks in terms of
closed real intervals that describe the variation within
a block of the transition probabilities to other blocks,
i.e., for any action and pair of blocks, we record the
upper and lower bounds on the probability of start­
ing in a state in one block and ending up in the other
block.1
1The BMDP

M naturally represents a family of MDPs,

126

Dean, Givan, and Leach

(b)

(a)

A

�:
value
tio

-<

c

factorization

D

'IT.

symbolic
€-reduction

M

�

int..,al summarizatio•

(g)

;:2'

inle<Val value itemtioo

(h)

do

�.
'IT.

Mpes

Figure 1: The basic objects and operations described in this paper: (a) depicts the state-transition diagram
for an MDP M (only a single action is shown), (b) depicts a Bayesian network as an example of a symbolic
representation compactly encoding M, (c) and (d) depict the smallest homogeneous partition in (respectively) its
implicit (symbolic) and explicit forms, similarly, (e) and (f) depict an €-homogeneous partition in its implicit and
explicit forms, (g) represents the bounded-parameter MDP M summarizing the variations in the f-homogeneous
partition, and, finally, (h), (i), and (j) depict particular (exact) MDPs from the family of MDPs defined by M.

Approximate Model Reduction

Our BMDP analysis algorithms extract particular
MDPs from M that have intuitive characterizations.
The pessimistic model Mpe• is the MDP within M
which yields the lowest optimal value VM* p•• at every
state. It is a theorem that Mpe• is well-defined, and
that vM· P•• at each state in M is a lower bound for following the optimal policy 1rM• ,.. in any MDP in M (as
well as in the original M from any state in the corresponding block). Similarly, the optimistic model Mopt
has the best value function VM.,,. VM.,, gives upper­
bounds for following any policy in M. In summary,
V,W.pes and V,W.op• give us lower and upper bounds on
the optimal value function we are really interested in,
vM· ' and following 1l"M.
, .. in M is guaranteed to achieve
at least the lower bound.
Now, armed with this high-level overview to serve as
a road map, we descend into the details.
3

Markov Decision Processes

Exact Markov Decision Processes An (exact)
Markov decision process M is a four tuple M =
(Q, A, F, R) where Q is a set of states, A is a set of
actions, R is a reward function that maps each state
to a real value R(q),2 F assigns a probability to each
state transition for each action, so that for a E A and
p, q E Q,

Fpq(a)

=

Pr(Xt+l

=

qiXt

=

p, Ut =a)

where Xt and Ut are random variables denoting, re­
spectively, the state and action at time t.

A policy is a mapping from states to actions, 1r : Q --t
A. The value function V",M for a given policy maps
states to their expected discounted cumulative reward
given that you start in that state and act according
the given policy:
v",M(P)
where
1994].

1

=

R(p) + 1

L fvq(1r(p))Vrr,M(q)

qEQ

is the disco unt rate, 0 :::;

1 <

1. [Puterman,

A bounded parame­
ter MDP (BMDP) is a four tuple M :::: ( Q, A, F, R)

Bounded Parameter MOPs

where Q and A are as for MDPs, and F and R are
analogous to the MDP F and R but yield closed real
intervals instead of real values. That is, for any action
a and states p, q, R(p) and Fp,q(a) are both closed
real intervals of the form [l, u] for l and u real numbers
with 0 :::; l :::; u :::; 1. For convenience, we define F

but note that the original M is not generally in this family.
Nevertheless, our BMDP algorithms compute policies and
value bounds which can be soundly applied to the original
M.

zThe techniques and results in this paper easily gene r­
alize to more general reward functions. We adopt a less
general formulation to simplify the presentation.

127

and F to be real valued functions which give the lower
and upper bounds of the intervals; likewise for R and
R. 3 To ensure that F admits well-formed transition
functions, we require that, for any action a and state
p, I: EQ Fp,q(a) :S 1 :S I:qEQ F p,q(a).

q
A BMDP M
( Q, A, F, R) defines a set of exact
MDPs :FM = {MIM � M} where M � M iff
M = ( Q, A, F, R ) and F and R satisfy the bounds
provided by F and R respectively. We will write
of bounding the (optimal or policy specific) value of a
state in a BMDP-by this we mean providing an up­
per or lower bound on the corresponding state value
over the entire family of MDPs :FM· For a more thor­
ough treatment of BMDPs, please see [Givan et al.,
1997].
=

Factored Representations In the remainder of
this paper, we make use of Bayesian networks [Pearl,
1988] to encode implicit (or factored) representa­
tions; however, our methods apply to other factored
representations such as probabilistic STRIPS opera­
tors [Kushmerick et al., 1995]. Let X = {Xt. ... , Xm}
be a set of state variables. We assume the vari­
ables are boolean, and refer to them also as flu­
ents. We represent the state at time t as a vector
Xt:::: (X1 t,
, Xm t) where X;' t denotes the value of
the ith st�te variabie at time t.
. • .

The state transition probabilities can be represented
using
Bayes
networks.
A two-stage temporal Bayesian network (2TBN) is a
directed acyclic graph consisting of two sets of vari­
ables {X;,t} and {Xi,t+I} in which directed arcs in­
dicating dependence are allowed from the variables in
the first set to variables in the second set and between
variables in the second set.[Dean and Kanazawa, 1989]
The state-transition probabilities are now factored as
Pr(Xt+tiXt, Ut)

m

=

IT Pr(Xi,t+IIParents(X;,t+I), Ut)

i=l

where Parents(X) denotes the parents of X in the
2TBN and each of the conditional probability distri­
butions Pr(Xi,t+11Parents(X;,t+1), Ut) can be repre­
sented as a conditional probability table or as a de­
cision tree-we choose the latter in this paper follow­
ing [Boutilier et al., 1995b]. We enhance the 2TBN
representation to include actions and reward func­
tions; the resulting graph is called an influence dia­
gram [Howard and Matheson, 1984].
Figure 2 illustrates a factored representation with
three state variables, X = { P, Q, S}, and describes the
transition probabilities and rewards for a particular ac­
tion. The factored form of the transition probabilities

3To simplify the remainder of the paper, we assume
that the reward bounds are always tight, i.e., that B.
R. The generalization to nontrivial bounds on rewards is
straightforward.
=

128

Dean, Givan, and Leach

-

--

·

Pr(PIX1_1)
PA-.P
/ QA-. Q

For conciseness, we say P is t:-homogeneous.4
Figure 3 shows two t:-homogeneous partitions for the

MDP

0.8 0.7 0.65

Figure

2.

----- ------, Pr(QIX1_1) 0.7 1
----- .. Pr(SIX1_1)

We now explain how we construct an t:-homogeneous
partition. We first describe the relationship between
every £-homogeneous partition and a particular simple
partition based on immediate reward.

R(X,)

Definition

=

_____

R

described in Figure

{

=

1
0

Q�Q
/ sA-.s

if P

0.7 1.0

else

0.5

2: A factored representation with three
P, Q and S, and reward function R.

state

variables,

2 A partition P' is a refinement of a par­
tition P if and only if each block ofP' is a subset of
some block ofP; in this case, we say that P is coarser
than P', and is a clustering of P'

Definition 3 The immediate reward partition is the
partition in which two states, p and q, are in the same
block if and only if they have the same reward.
4 A partition P is t:-uniform with respect
to a function f :Q --t n if for every two states p and
q in the same block ofP, lf(p) - f(q)l $ c

Definition

(b)

(a}
Figure

3:

Two t:-homogeneous partitions for the

MDP

described in Figure 2: (a ) the smallest exact homoge­
neous partition ( < = 0) and (b) a smaller partition for
{ =

0.05.

is

Pr(Pt+tiPt, Qt) Pr(Qt+d
Pr(St+tiSe, Qt)
·

where in this case Xt

4

=

·

a

In this section, we describe a family of algorithms that
take as input an MDP and a real value t: between 0 and

MDP where each

closed real interval has extent less than or equal to <.
The states in this BMDP correspond to the blocks of a
partition of the state space in which states in the same
block behave approximately the same with respect to
the other block s. The upper and lower bounds in the
BMDP correspond to bounds on the transition prob­
abilities ( to other blocks) for states that are grouped
together.

We first define the property sought in the desired state
space partition. Let P
{ B1,
Bn} be a partition
ofQ.
. • .

,

1 A partition P = { Bt, . .. , Bn} of the
state space of an MDP M has the property of t:­
approximate stochastic bisimulation homogeneity with
respect to M for f such that 0 $ t: $ 1 if and only if for
each B;, Bj E P, for each E A, for each p, q E B;,

Definition

a

IR(p)- R(q)l $

immediate reward partition. 5 We then refine this ini­
tial partition by splitting6 blocks repeatedly to achieve
t:-homogeneity. We can decide which blocks are can­
didates for splitting using the following local property
of the blocks of an t:-homogenous partition:

5 We say that a block C of a partition P
is t:-stable with respect to a block B iff for all actions
and all states p E C and q E C we have

Model Reduction Methods

=

t:-homogeneous partition is a refinement of some

Definition

(Pt, Qt, Se).

I and compute a bounded parameter

Every

£- uniform clustering ( with respect to reward) of the
immediate reward partition. Our algorithm starts by
constructing an £-uniform reward clustering Po of the

and

f,

ILrEBj Fpr(a)- LrEBj Fqr(a)i $

f

ILrEB

Fpr(a)-

L

rEB

l

Fqr(a) $

t

We say that C is t:-stable if C is t:-stable with respect
to every block of P and action in A.
The definitions immediately imply that a partition is<­
homogenous iff every block in the partition is t:-stable.
The model £-reduction algorithm simply checks each
block for t:-stability, splitting unstable blocks until qui­
escence, i.e., until there are no unstable blocks left to
split. Specifically, when a block C is found to be unsta­
ble with respect to a block B, we replace C in the par­
tition by a set 7 of sub-blocks ell

• • .

, ck such that each

4For the case of t = 0, t-approlcimate stochastic bisim­
ulation homogeneity is closely related to the substitution
property for finite automata developed by Hartmanis and
Stearns [1966) and the notion of lumpability for Markov
chains [Kemeny and Snell, 1960].
5There may be many such clusterings,

we currently

choose a coarsest one arbitrarily.

6The term splitting r efers to the process whereby a block

of a partition is clivided into two or more sub-blocks to
obtain a refinement of the original partition.
7There may be more than one choice, as cliscussed
below.

Approximate Model Reduction

1.0--:-r�4/\/0.71

0.7

0.70
0.69

�
03
.

$0.31
0.30
0.29

�
o.o....L

Figure 4: Clustering sub blocks that behave approxi­
mately the same. With f = 0.01 there are two smallest
clusterings.
C; is a maximal sub-block of C that is t:-stable with re­
spect to B. Note that at all times the blocks of the par­
tition are represented in factored form, e.g., as DNF
formulas over the state variables. The block splitting
operation manipulates these factored representations,
not explicit states. This method is an extension to

Markov decision processes of the deterministic model
reduction algorithm of Lee and Yannakakis [1992].

If E = 0, the above description fully defines the
block splitting operation, as there exists a unique set
of maximal, stable sub-blocks. Furthermore, in this
case, the algorithm finds the unique smallest homo­
geneous partition, independent of the order in which
unstable blocks are split. We call this partition the
minimal model (we also use this term to refer to the
MDP derived from this partition by treating its blocks
as states).
However, if I'> 0, then we may have to choose among
several possible ways of splitting C as shown in the
following example. Figure 4 depicts a block, C, and
two other blocks, B and B', such that states in C
transition to states in B and B' under some action a.
We partition C into three sub blocks { C1, C2, C3} such
that states in each sub block have the same transition
probabilities with respect to a, B, and B'. In building
an 0.01-approximate model, we might replace C by the
two blocks C1 and C2UC3, or by the two blocks C3 and
C1 U C2; it is possible to construct examples in which
each of these is the most appropriate choice because
the splits of other blocks induced later8. We require
only that the clustering selected is not the refinement
of another €-uniform clustering, i. e. , that it is as coarse
as possible.
Because we make the clustering decisions arbitrarily,
our algorithm does not guarantee finding the smallest
t:-homogenous partition when f > 0, nor that the par­
tition found for t:1 will be smaller (or even as small) as
8

The result is additionally sensitive to the order in

which unstable blocks are split�splitting one <'--unstable
block may make another become <'--Stable.

129

the partition found for f 2 < f 1• However, it is a the­
orem that the partition found will be no larger than
the unique smallest 0-homogenous partition.
For f > 0, the partition found by model
-reduction using any clustering technique is coarser
than, and thus no larger than the minimal model.

Theorem 1
t

For 0 < (2 < �'11 the smallest Et­
homogenous partition is no larger than the smallest
t2-homogenous partition. The model !'-reduction algo­
rithm, augmented by an (impractical) search over all
clustering decisions, will find these smallest partitions.

Theorem 2

Theorem 3

Given a bound and an MDP whose
smallest €-homogenous partition is polynomial in size,
the problem of determining whether there exists an !'­
homogenous partition of size no more than the bound
is NP-complete.

These theorems imply that using an f > 0 can only
help us, but that our methods may be sensitive to just
which t we choose, and are necessarily heuristic.
Currently our implementation uses a greedy cluster­
ing algorithm; in the future we hope to incorporate
more sophisticated techniques from the learning and
pattern recognition literature to find a smaller cluster­
ing locally within each SPLIT operation (though this
does not guarantee a smaller final partition).
Each !'-homogenous partition P of an MDP M =
(Q, A, F, R) induces a corresponding BMDP Mp =
(Q, A, F, R) in a straightforward manner. The states
of Mp are just the blocks of P and the actions are the
same as those in M . The reward and transition func­
tions are defined to give intervals bounding the pos­
sible reward and block transition probabilities within
each block: for blocks B and C and action a,

R(B)

FB,c(a)

=
=

[ minp EB R(p), maxpEB R(p)
[ minpEB LqEC Fp,q(a),
maXpEB LqEC Fp,q(a) ]

We can then use the methods in the next section to
give intervals bounding the optimal value of each state
in Mp and select a policy which guarantees achieving
at least the lower bound value at each state. The fol­
lowing theorem then implies the value bounds apply
to the states in M, and are achieved or exceeded by
following the corresponding policy in M.
We first note that any function on the blocks of P
can be extended to a function on the states of M: for
each state we return the value assigned to the block of
P in which it falls. In this manner, we can interpret
the value bounds and policies for Mp as bounds and
policies for M.
Theorem 4

For any MDP M and !'-homogenous par­
tition P of the states of M, sound (optimal or policy

130

Dean, Givan, and Leach

specific) value bounds for Mp apply also to M (by
extending the policy and value functions to the state
space of M according toP).

5

Interval Value Iteration

We have developed a variant of the value iteration al­
gorithm for computing the optimal policy for exact
MDPs[Bellman, 1957] that operates on bounded pa­
rameter MDPs. A BMDP M represents a family of
MDPs :FM, implying some degree of uncertainty as to
which MDP in the family actions will actually be taken
in. As such, there is no specific value for following a
policy from a start state-rather, there is a window of
possible values for following the policy in the different
MDPs of the family. Similarly, for each state there is
a window of possible optimal values over the MDPs in
the family :FM. Our algorithm can compute bounds
on policy specific value functions as well as bounds on
the optimal value function. We have also shown how
to extract from these bounds a specific "optimal" pol­
icy which is guaranteed to achieve at least the lower
bound value in any actual MDP from the family :FM
defined by the BMDP. We call this policy 11"pes, the
pessimistic optimal policy.

We call this algorithm, interval value iteration (IVI
for optimal values, and IVI .. for policy specific val­
ues). The algorithm is based on the fact that, if we
only knew the rank ordering of the states' values, we
would easily be able to select an MDP from the fam­
ily :FM which minimized or maximized those values,
and then compute the values using that MDP. Since
we don't know the rank ordering of states' values, the
algorithm uses the ordering of the current estimates of
the values to select a minimizing (maximizing) MDP
from the family, and performs one iteration of stan­
dard value iteration on that MDP to get new value
estimates. These new estimates can then be used to
select a new minimizing (maximizing) MDP for the
next iteration, and so forth.
Bounded parameter MDPs are interesting objects and
we explore them at greater length in [Givan et al.,
1997]. In that paper, we prove the following results
about IV/.
Given a BMDP M and a specific pol­
icy 11", IVI.. converges at each state to lower and up­
per bounds on the value of 1r at that state over all the
MDPs in :FM.

Theorem 5

Given a BMDP M, IVI converges at
each state to lower and upper bounds on the optimal
value of that state over all the MDPs in :FM.

Theorem 6

Given a BMDP M, the policy 11"pe• ex­
tracted by assuming that states actual values are the
IVI- converged lower bounds has a policy specific lower
bound (from IVI .. ) in M equal to the (non policy spe­
cific) IVI -converged lower bound. No other policy has
Theorem 7

a higher policy specific lower bound.
6

Related Work and Discussion

This paper combines a num her of techniques to address
the problem of solving (factored) MDPs with very
large states spaces. The definition of £-homogeneity
and the model reduction algorithms for finding (­
homogeneous partitions are new, but draw on tech­
niques from automata theor and symbolic model
checking. Burch et al. [1994 is the standard refer­
ence on symbolic model checking for computer-aided
design. Our reduction algorithm and its analysis were
motivated by the work of Lee and Yannakakis [1992]
and Bouajjani et al. [1992].

r

The notion of bounded-parameter MDP is also new,
but is related to aggregation techniques used to speed
convergence in iterative algorithms for solving exact
MDPs. Bertsekas and Castanon [1989] use the notion
of aggregated Markov chains and consider grouping
together states with approximately the same residuals
(i.e., difference in the estimated value function from
one iteration to the next during value iteration).
The methods for manipulating factored representa­
tions of MDPs were largely borrowed from Boutilier et
al. [1995b], which provides an iterative algorithm for
finding optimal solutions to factored MDPs. Dean
and Givan [1997] describe a model-minimization algo­
rithm for solving factored MDPs which is asymptot­
ically equivalent to the algorithm in [Boutilier et a/.,
1995b].
Boutilier and Dearden [?]extend the work in [Boutilier
et al., 1995b] to compute approximate solutions to fac­
tored MDPs by associating upper and lower bounds
with symbolically represented blocks of states. States
are aggregated if they have approximately the same
value rather than if they behave approximately the
same behavior under all or some set of policies, though
it often turns out that states with nearly the same
value have nearly the same dynamics.
There are two significant differences between our ap­
proximation techniques and those of Boutilier and
Dearden. First, we partition the state space and
then perform interval value iteration on the resulting
bounded-parameter MDP, while Boutilier and Dear­
den repeatedly partition the state space. Second, we
use a fixed E for computing a partition while Boutilier
and Dearden, like Bertsekas and Castanon, repartition
the state space (if necessary) on each iteration on the
basis of the current residuals, and, hence, (effectively)
they use different E's at different times and on different
portions of the state space. Despite these differences,
we conjecture that the two algorithms perform asymp­
totically the same. Practically speaking, we expect
that in some cases, repeatedly and adaptively comput­
ing partitions may provide better performance, while
in other cases, performing the partition once and for
all may result in a computational advantage.

Approximate Model Reduction

We have written a prototype implementation of the
model reduction algorithms described in this paper,
along with the BMDP evaluation algorithms (IVI) re­
ferred to. Using this implementation we have been able
to demonstrate substantial reductions in model size,
and increasing reductions with increasing L However,
the MDPs we have been reducing are still "toy" prob­
lems and while they were not concocted expressly to
make the algorithm look good, these empirical results
are still of questionable value. Further research is nec­
essary before these techniques are adequate to handle
a real-world large scale planning problem in order to
give convincing empirical data.
Finally, we believe that by formalizing the notions
of approximately similar behavior, approximately
equivalent models, and families of closely related
MDPs the mathematical entities corresponding to !'­
homogeneous partitions, !'-reductions, and bounded­
parameter MDPs provide valuable insight into fac­
tored MDPs and the prospects for solving them ef­
ficiently.
References

[Bellman, 1957] Bellman, Richard 1957. Dynamic
Programming. Princeton University Press.
[Bertsekas and Castanon, 1989] Bertsekas, D. P. and
Castanon, D. A. 1989. Adaptive aggregation for in­
finite horizon dynamic programming. lEEE Trans­
actions on A utomatic Control

34 ( 6) :589-598.
[Bouajjani et al., 1992] Bouajjani, A.; Fernandez, J.­
C.; Halbwachs, N.; Raymond, P.; and Rate!, C.
1992. Minimal state graph generation. Science of
Computer Programming 18:247-269.
[Boutilier and Dearden, 1994] Boutilier, Craig and
Dearden, Richard 1994. Using abstractions for de­
cision theoretic planning with time constraints. In
Proceedings A A A I-94. AAAI. 1016-1022.
[Boutilier et al., 1995a]
Boutilier, Craig; Dean, Thomas; and Hanks, Steve
1995a. Planning under uncertainty: Structural as­
sumptions and computational leverage. In Proceed­
ings of the Third European Workshop on Planning.

[Boutilier et al., 1995b] Boutilier, Craig; Dearden,
Richard; and Goldszmidt, Moises 1995b. Exploit­
ing structure in policy construction. In Proceedings
IJCAI 14. IJCAII. 1104-1111.
[Burch et al., 1994] Burch, Jerry;
Clarke, Edmund M.; Long, David; McMillan, Kenneth L.; and
Dill, David L. 1994. Symbolic model checking for
sequential circuit verification. IEEE Transactions
on Computer A ided Design 13(4):401-424.
[Dean and Givan, 1997] Dean, Thomas and Givan,
Robert 1997. Model minimization in Markov de­
cision processes. In Proceedings A A A I-97. AAAI.
[Dean and Kanazawa, 1989] Dean,
Thomas and Kanazawa, Keiji 1989. A model for

131

reasoning about persistence and causation. Compu­
tational Intelligence 5(3):142-150.
[Dean et al., 1995] Dean, Thomas; Kaelbling, Leslie;
Kirman, Jak; and Nicholson, Ann 1995. Planning
under time constraints in stochastic domains. Arti­
ficial Intelligence 76(1-2):35-74.

[Givan et al., 1997] Givan, Robert; Leach, Sonia; and
Dean, Thomas 1997. Bounded parameter markov
decision processes.
Technical Report CS-97-05,
Brown University, Providence, Rhode Island.
[Hartmanis and Stearns, 1966]
Hartmanis, J. and Stearns, R. E. 1966. A lgebraic
Structure Theory of Sequential Machines. Prentice­
Hall, Englewood Cliffs, N.J.
[Howard and Matheson, 1984) Howard, Ronald A.
and Matheson, James E. 1984. Influence diagrams.
In Howard, Ronald A. and Matheson, James E., ed­
itors 1984, The Principles and Applications of De­
cision A nalysis. Strategic Decisions Group, Menlo
Park, CA 94025.
[Howard, 1960] Howard, Ronald A. 1960. Dynamic
Programming and Markov Processes. MIT Press,
Cambridge, Massachusetts.
[Kemeny and Snell, 1960] Kemeny, J. G. and Snell,
J. L. 1960. Finite Markov Chains. D. Van Nos­
trand, New York.
[Kushmerick et al., 1995] Kushmerick,
Nicholas; Hanks, Steve; and Weld, Daniel 1995. An
algorithm for probabilistic planning. Artificial In­
telligence 76(1-2).
[Lee and Yannakakis, 1992] Lee, David and Yan­
nakakis, Mihalis 1992. Online minimization of tran­
sition systems. In Proceedings of 24th Annual ACM
Symposium on the Theory of Computing.

[Lin and Dean, 1995] Lin, Shieu-Hong and Dean,
Thomas 1995. Generating optimal policies for high­
level plans with conditional branches and loops. In
Proceedings of the Third European Workshop on
Planning. 205-218.
Probabilistic Reason­
ing in Intelligent Systems: Networks of Plausible In­
ference. Morgan Kaufmann, San Francisco, Califor­

[Pearl, 1988] Pearl, Judea 1988.

nia.
[Puterman, 1994] Puterman, Martin L. 1994. Markov
Decision Processes. John Wiley & Sons, New York.
[Schweitzer et al., 1985] Schweitzer, Paul J.; Puter­
man, Martin L.; and Kindle, Kyle W. 1985. Iter­
ative aggregation-disaggregation procedures for dis­
counted semi-Markov reward processes. Operations
Research 33(3):589-605.
[Schweitzer, 1984] Schweitzer, Paul J. 1984. Aggrega­
tion methods for large Markov chains. In lazola,
G.; Coutois, P. J.; and Hordijk, A., editors 1984,
Mathemaical Computer Performance and Reliabil­
ity.

Elsevier, Amsterdam, Holland. 275-302.

