194

HALPERN & PEARL

UAI2001

Causes and Explanations: A Structural-Model Approach­
Part 1: Causes

Joseph

Y. Halpern

Judea Pearl

Dept of Computer Science
University of California, Los Angeles
Los Angeles, CA 90095

Cornell University
Dept. of Computer Science
Ithaca, NY 14853
halpern®cs.cornell.edu

judea®cs.ucla.edu

http://www.cs.cornell.edu/home/halpern

http://www.cs.ucla.edu/�judea

Abstract

We propose a new definition of actual causes,
model counterfac­
tuals. We show that the definition yields a plausi­
ble and elegant account of causation that handles
well examples which have caused problems for
other definitions and resolves major difficulties in
the traditional account.
using structural equations to

1

INTRODUCTION

What does it mean that an event A actually causes event
B? This is a question that goes beyond mere philosophical
speculation. As Good [1993] argues persuasively, in many
legal settings, what needs to be established (for determining
responsibility) is not a counterfactual kind of causation, but
"cause in fact." A typical example considers two fires ad­
vancing toward a house. If fire A burned the house before
fire B, we (and many juries nationwide) would consider
fire A "the actual cause" for the damage, even supposing
the house would have definitely burned down by fire B, if
it were not for A. Actual causation is also important in arti­
ficial intelligence applications. W henever we undertake to
explain a set of events that unfold in a specific scenario, the
explanation produced must acknowledge the actual cause of
those events. The automatic generation of adequate expla­
nations, a task essential in planning, diagnosis and natural
language processing, therefore requires a formal analysis
of the concept of actual cause.
Giving a precise and useful definition of actual causality is
notoriously difficult. The philosophical literature has been
struggling with this notion since the days of Hume [ 1739].
(See [Sosa and Tooley 1993], [Hall 2002], and [Pearl2000]
for some recent discussions.) To borrow just one example
from Hall [2002], suppose a bolt lightning hits a tree and
starts a forest fire. It seems reasonable to say that the
lightning bolt is a cause of the fire. (Indeed, the description
"the lightning bolt ... starts a forest fire" can be viewed as
saying this.) But what about the oxygen in the air and the
fact that the wood was dry? Presumably, if there has not
been oxygen or the wood was wet there would not have
been a fire. Carrying this perhaps to the point of absurdity,

what about the Big Bang? This problem is relatively easy
to deal with, but there are a host of other, far more subtle,
difficulties that have been raised over the years.
Here we give a definition of actual causality based on the
language of structural equations; in a companion paper
([Halpern and Pearl 2001]; see also the full paper [Halpern
and Pearl 2000]), we give a definition of (causal) explana­
tion using the definition of causality. The use of structural
equations as a model for causa l relationships is standard
in the social sciences, and seems to go back to the work
of Sewall W right in the 1920s (see [Goldberger 1972] for
a discussion); the framework we use here is due to Pearl
[1995], and is further developed in [Galles and Pearl 1997;
Halpern 2000; Pearl 2000]. While it is hard to argue that
our definition (or any other definition, for that matter) is
the "right definition", we show that it deals well with the
difficulties that have plagued other approaches in the past,
especially those exemplified by the rather extensive com­
pendium of Hall [2002].
There has been extensive discussion about causality in the
literature, particularly in the philosophy literature. To keep
this paper to manageable length, we spend only minimal
time describing other approaches and comparing ours to
them. We refer the reader to [Hall 2002; Pearl 2000; Sosa
and Tooley 1993; Spirtes, Glymour, and Scheines 1993]
for details and criticism of the probabilistic and logical
approaches to causality in the philosophy literature. (We do
try to point out where our definition does better than perhaps
the best known approach, due to Lewis [1986, 2000] in the
course of discussing the examples.)
There has also been work in the AI literature on causal­
ity. Perhaps the closest to this are papers by Pearl and his
colleagues that use the structural-model approach. The def­
inition of causality in this paper was i n spired by an earlier
paper of Pearl's [1998] that defined actual causality in terms
of a construction called a causal beam. The causal beam
definition was later modified somewhat (see [Pearl 2000,
Chapter 10]), largely due to the considerations addressed
in this paper. The definition given here is more transparent
and handles a number of cases better (see Example 4.4).
Tian and Pearl [2000] give results on calculating the prob­
ability that A is a necessary cause of B-that is, the proba­
bility that B would not have occurred if A had not occurred.

HALPERN & PEARL

UAI 2001

Necessary causality is related to but different from actual
causality, as the definitions should make clear. Other work
(for example, [Heckerman and Shachter 1995]) focuses on
when a random variable X is the cause of a random vari­
able Y; by way of contrast, we focus on when an event
y. As we
such as X = x causes an event such as Y
shall see, many of the subtleties that arise when dealing
with events simply disappear if we look at causality at the
level of random variables. Finally, there is also a great
deal of work in AI on formal action theory (see, for exam­
ple, [Lin 1995; Sandewall1994]), which is concerned with
the proper way of incorporating causal relationships into a
knowledge base so as to guide actions. The focus of our
work is quite different; we are concerned with extracting
the actual causality relation from such a knowledge base,
coupled with a specific scenario.
=

195

The set U ofexogenous variables includes things we need to
assume so as to render all relationships deterministic (such
as whether the wood is dry, there is enough oxygen in the
air, etc.). If i1 is a setting of the exogenous variables that
makes a forest fire possible (i.e., the wood is sufficiently
dry, there is oxygen in the air, and so on) then, for example,
FF(il, L,ML) is such that F = I if L 1 or ML 1.1
=

=

Given a causal model M

= (S, :F), a (possibly empty)
vector X of variables in V, and vectors x and i1 of values
for the variables in X and U, we can define a new causal
model denoted Mx ._fi over the signature Sx
(U, V X, 'R.Iv-x ). Intuitively, this is the causal model that results
=

when the variables in X are set to x by some external
action that affects only the variables in X; we do not model
the action or its causes explicitly. Formally, Mx .... :x
=

CAUSAL MODELS: A REVIEW

2

We briefly review the basic definitions of causal models,
as defined in terms of structural equations, and the syntax
and semantics of a language for reasoning about causality.
See [Galles and Pearl 1997; Halpern 2000; Pearl 2000] for
more details, motivation, and intuition.
Causal Models: The basic picture here is that the world
is described by random variables, some of which may have
a causal influence on others. This influence is modeled by a
set of structural equations, where each equation represents
a distinct mechanism (or law) in the world, one that may be
modified (by external actions) without altering the others.
In practice, it seems useful to split the random variables into
two sets, the exogenous variables, whose values are deter­
mined by factors outside the model, and the endogenous
variables. It is these endogenous variables whose values
are described by the structural equations.

More formally, a signatureS is a tuple (U, V, 'R}, where
U is a finite set of exogenous variables, V is a finite set of
endogenous variables, and n associates with every variable
Y E U U V a nonempty set 'R(Y) of possible values for Y
(that is, the set of values over which Y ranges). A causal
model (or structural model) over signature S is a tuple
M = (S,:F), where:Fassociates with each variableX E V
a ftmction denoted Fx such that Fx : (xuEuR(U)) x
(xYEV-{x}R(Y))-+ R(X). Fx tells us the value ofX
given the values of all the other variables in U U V.
Example 2.1: Suppose that we want to reason about a forest
fire that could be caused by either lightning or a match lit
by an arsonist. Then the causal model would have the
following endogenous variables (and perhaps others):
•

F

•

L

•

for fire (F

=

1 if there is one, F

for lightning (L
otherwise)

=

ML for match lit (ML
ML 0 otherwise).
=

=

0 otherwise)

1 if lightning occurred, L

=

=

0

I if the match was lit and

(Sx, ;:x.-:x), where F� ,_fi is obtained from Fy by setting
the values of the variables in X to x.

It may seem strange that we are trying to understand causal­
ity using causal models, which clearly already encode
causal relationships. Our reasoning is not circular. Our
aim is not to reduce causation to noncausal concepts, but to
interpret questions about causes of specific events in fully
specified scenarios in terms of generic causal knowledge
such as what we obtain from the equations of physics. The
causal models encode background knowledge about the ten­
dency of certain event types to cause other event types (such
as the fact that lightning can cause forest fires). We use the
models to determine the causes and explanations of single
events, such as whether it was arson that caused the fire of
June 10, 2000, given what is known or assumed about that
particular fire.
We can describe (some salient features of) a causal model
M using a causal network. This is a graph with nodes
corresponding to the random variables in V and an edge
from a node labeled X to one labeled Y ifFy depends
on the value of X. Intuitively, variables can have a causal
effect only on their descendants in the causal network; if Y
is not a descendant of X, then a change in the value of X
has no affect on the value of Y. In this paper, we restrict
attention to what are called recursive (or acyclic) equations;
these are ones that can be described with a causal network
that is a dag. It should be clear that if M is a recursive
causal model, then there is always a unique solution to the
equations in Mx .... :r• given a setting i1 for the variables in
U (we call such a setting i1 a context).
As we shall see, there are many nontrivial decisions to be
made when choosing the structural model. The exogenous
variables to some extent encode the background situation,
that which we wish to take for granted. Other implicit back­
ground assumptions are encoded in the structural equations
themselves. Suppose that we are trying to decide whether a
lightning bolt or a match was the cause of the forest fire, and
we want to take for granted that there is sufficient oxygen
in the air and the wood is dry. We could model the dryness
of the wood by an exogenous variable D with values 0 (the
wood is wet) and I (the wood is dry).
By making D

HALPERN & PEARL

196

exogenous, its value is assumed to be given and out of the
control of the modeler. We could also take the amount of
oxygen as an exogenous variable (for example, there could
be a variable 0 with two values-0, for insufficient oxygen,
and 1, for sufficient oxygen); alternatively, we could choose
not to model oxygen explicitly at all. For example, suppose
we have, as before, a random variable ML for match lit,
and another variable WB for wood burning, with values 0
(it's not) and l (it is). The structural equation Fw 8 would
describe the dependence of WB on D and ML. By setting
Fw 8 ( 1, 1) = 1, we are saying that the wood will bum if
the match is lit and the wood is dry. Thus, the equation is
implicitly modeling our assumption that there is sufficient
oxygen for the wood to burn. If we were to explicitly model
the amount of oxygen in the air (which certainly might be
relevant if we were analyzing fires on Mount Everest), then
Fw 8 would also take values of 0 as an argument.
Besides encoding some of our implicit assumptions, the
structural equations can be viewed as encoding the causal
mechanisms at work. Changing the underlying causal
mechanism can affect what counts as a cause. Section 4
provides several examples of the importance of the choice
of random variables and the choice of causal mechanism.
It is not always straightforward to decide what the "right"
causal model is in a given situation, nor is it always obvi­
ous which of two causal models is "better" in some sense.
These may be difficult decisions and often lie at the heart
of determining actual causality in the real world. Neverthe­
less, we believe that the tools we provide here should help
in making principled decisions about those choices.
To make the definition of actual
causality precise, it is helpful to have a logic with a formal
syntax. Given a signature S = (U, V, R), a formula of
the form X = x, for X E V and x E R(X), is called a
primitive event. A basic causa/formula (overS) is one of
the form [Y, o- y1, ... , Yk o- Yk]'P where cp is a Boolean
combination of primitive events, Y,, ... , Yk,X are vari­
ables in V, with Yi, .. . , Yk are distinct, x E R(X), and
y; E R(}i). Such a formula is abbreviated as [Y o- Y1'P·
The special case where k
0 is abbreviated as cp. In­
tuitively, [Y, o- y,, . . . , Yk o- Yk]cp says that cp holds in
the counterfactual world that would arise if Y; is set toy;,
i = 1, . . . , k. A causal formula is a Boolean combination
of basic causal formulas.
Syntax and Semantics:

=

A causal formula 'lj; is true or false in a causal model, given
a context. We write (M, u) f= '1/J if '1/J is true in causal
model M given context u. (M, u) f= [Y o- Y1 (X
x)
if the variable X has value x in the (unique, since we are
dealing with recursive models) solution to the equations in
M.y_ii in context u (that is, the unique vector of values
for the exogenous variables that simultaneously satisfies
all equations Fi-ii, Z E V- Y, with the variables in U
set to ii). (M, u) F [Y ....... YJcp for an arbitrary Boolean
combination <p of formulas of the form X = x is defined
similarly. We extend the definition to arbitrary causal for­
mulas, i.e., Boolean combinations of basic causal formulas,
in the standard way.
=

UAI2001

Note that the structural equations are deterministic. We can
make sense out of probabilistic counterfactual statements,
even conditional ones (the probability that X would be 3
if Y1 were 2, given that Y is in fact 1) in this framework
(see [Balke and Pearl 1994]), by putting a probability on
the set of possible contexts. This is not necessary for our
discussion of causality, although it plays a more significant
role in the discussion of explanation.
3

THE DEFINITION OF CAUSE

With all this notation in hand, we can now give our defini­
tion of actual cause ("cause" for short). We want to make
sense out of statements of the form "event A is an actual
cause of event <p (in context ii)". As we said earlier, the
context is the background information. While this has been
left implicit in some treatments of causality, we find it use­
ful to make it explicit. The picture here is that the context
(and the structural equations) are given. Intuitively, they
encode the background knowledge. All the relevant events
are known. The only question is picking out which of them
are the causes of <p or, alternatively, testing whether a given
set of events can be considered the cause of <p.
The types of events that we allow as actual causes are
ones of the form X1
X! /1. . . . /1. Xk = Xk-that is,
conjunctions of primitive events; we typically abbreviate
this as X = x. The events that can be caused are arbitrary
Boolean combinations of primitive events.
=

Definition 3.1: (Actual cause) X =xis an actual cause of
<pin (M, it) if the following three conditions hold:

ACl. (M, it) F (X = x) /1. <p. (That is, both X= X and
<pare true in the actual world.)
AC2. There exists a partition (Z, W) of V with X � i
and some setting (.i',w') of the variables in (X, W)
such that if (M, u) F z = z* for z E i, then
(a) (M,il) 1= [ X --- .i',W--- w'j-,cp. In wards,
changing (X, w) from (x, w) to (x', w') changes
<p from true to false,
(b) (M, u) f= [X ....... x, w ....... w', i' ,_. Z*]cp for
all subsets Z' of i. In words, setting w to W1
should have no effect on <p as long as X is kept
at its current value x, even if all the variables in
an arbitrary subset of Z are set to their original
values in the context a.
AC3. X is minimal; no subset of X satisfies conditions
AC1 and AC2. Minimality ensures that only those
elements of the conjunction X = x that are essential
for changing cp cin AC2(a) are considered part of a
cause; inessential elements are pruned. I
Note that we allow X = x to be a cause of itself. While we
do not find such trivial causality terribly bothersome, it can

UAI 2001

be avoided by requiring that X
x xto be a cause ofcp.

HALPERN

=

x1\ ...,IP be consistent for

=

The core of this definition lies in AC2. Informally, the
variables in Z should be thought of as describing the "ac­
tive causal process" from X to cp. (also called "intrinsic
process" by Lewis [1986]).1 These are the variables that
mediate between X and cp. Indeed, we can define an ac­
tive causal process from X = x to 1.p as a minimal set
Z that satisfies AC2. AC2(a) is reminiscent of the tradi­
tional counterfactual criterion of Lewis [ 1986], according
to which cp should be false if it were not for X being x.
However, AC2(a) is more permissive than the traditional
criterion; it allows the dependence of cp on X to be tested
under special circumstances in which the variables W are
held constant at some setting w'. This modification of the
traditional criterion was proposed by Pearl [ 1998, 2000]
and was named structural contingency-an alteration of the
model M that involves the breakdown of some mechanisms
(possibly emerging from external action) but no change in
the context il. The need to invoke such contingencies will
be made clear in Example 3.2.
AC2(b), which has no obvious analogue in the literature,
is an attempt to counteract the "permissiveness" of AC2(a)
with regard to structural contingencies. Essentially, it en­
sures that X alone suffices to bring about the change from
1.p to -.cp; setting W to �� merely eliminates spurious side
effects that tend to mask the action of X. It captures the fact
that setting w to w' does not affect the causal process by
requiring that changingW from w to ;' has no effect on the
value of 1.p. Moreover, although the values in the variables
Z involved in the causal process may be perturbed by the
change, the perturbation has no impact on the value of 1.p.
The upshot of this requirement is that we are not at liberty to
conduct the counterfactual test of AC2(a) under an arbitrary
alteration of the model. The alteration considered must not
affect the causal process. Clearly, if the contingencies con­
sidered are limited to "freezing" variables at their actual
value, so that ( M, il) F w = W1, then AC2(b) is satisfied
automatically. However, as the examples below show, gen­
uine causation may sometimes be revealed only through a
broader class of counterfactual tests in which variables in
W are set to values that differ from their actual values. In
[Pearl 2000], a notion of contributory cause is defined as
well as actual cause. Roughly speaking, if AC2(a) holds
only with W = w' f. w, the A is a contributory cause of
B; actual causality holds only ifW
w.
=

We remark that, like the definition here, the causal beam def­
inition [Pearl 2000] tests for the existence of counterfactual
dependency in an auxiliary model of the world, modified by
a select set of structural contingencies. However, whereas
1

Recently, Lewis [2000] has abandoned attempts to define

"intrinsic process" formally. Pearl's "causal beam" [Pearl 2000,
p. 3 18] is a special kind of active causal process, in which AC2(b)

is expected to hold (with Z """ z) for all settings
necessarily the one used in (a).

w'

of W, not

&

PEARL

197

the beam criterion selects the choice of contingencies de­
pends only on the relationship a variable and its parents in
the causal diagram, the current definition selects the mod­
ifying contingencies based on the specific cause and effect
pair that is being tested. This refinement permits our def­
inition to avoid certain pitfalls (see Example 4.4) that are
associated with graphical criteria for actual causation.
AC3 is a minimality condition. Heckerman and Shachter
[ 1995] have a similar minimality requirement; Lewis [2000]
mentions the need for minimality as well. Interestingly, in
all the examples we have considered, AC3 forces the cause
to be a single conjunct of the form X
x.
Eiter and
Lukasiewicz [200 1] and, independently, Hopkins [200 1],
have recently proved that this is in fact a consequence of
our definition.
=

How reasonable are these requirements? In particular, is
it appropriate to invoke structural changes in the defini­
tion of actual causation? The following example may help
illustrate why we believe it is.
Example 3.2: Suppose that two arsonists drop lit matches
in different parts of a dry forest, and both cause trees to
start burning. Consider two scenarios. In the first, called
"disjunctive," either match by itself suffices to burn down
the whole forest. That is, even if only one match were lit,
the forest would bum down. In the second scenario, called
"conjunctive," both matches are necessary to burn down
the forest; if only one match were lit, the fire would die
down. We can describe the essential structure of these two
scenarios using a causal model with four variables:
•

an exogenous variable U which determines, among
other things, the motivation and state of mind of
the arsonists. For simplicity, assume that R(U)
{uoo, u10, uo1, u 1 I}; if U = uii, then the first arsonist
intends to start a fire iff i = 1 and the second arson­
ist intends to start a fire iff j = I. In both scenarios
U =ull·
=

•

•

endogenous variables ML1 and ML2, each either 0 or
I, where MLi = 0 if arsonist i doesn't drop the match
and ML; = I if he does, fori = 1, 2.
an endogenous variable FB for forest bums down, with
values 0 (it doesn't) and I (it does).

Both scenarios have the same causal network (see
Figure l ); they differ only in the equation for FB.
For the disjunctive scenario we have Fp 8 (u, 1, 1)
Fpa(u,O, 1) = Fpa(u, 1 , 0) = land Fpa(u,O,O) = 0
(where u E n(U)); for the conjunctive scenario we have
Fpa(u, I, l)
l and Fp8(u, 0, 0) = Fp8(u, 1, 0) =
Fp8 (u , 0, l)
0. In general, we expect that the causal
model for reasoning about forest fires would involve many
other variables; in particular, variables for other potential
causes of forest fires such lightning and unattended camp­
fires; here we focus on that part of the causal model that
involves forest fires started by arsonists. Since for causal­
ity we assume that all the relevant facts are given, we can
assume here that it is known that there were no unattended
campfires and there was no lightning, which makes it safe to
=

=

=

198

HALPERN & PEARL

ignore that portion of the causal model. Denote by M 1 and
causal models associated with the disjunctive and
conjunctive scenarios, respectively. The causal network for
the relevant portion of M 1 and M2 is described in Figure 1.

M2 the

0

ML2

FB
Figure 1: The causal network for

M1 and Mz.

Despite the differences in the underlying models, it is not
hard to show that each of ML 1 = 1 and MLz = I is a cause
of FB = 1 in both scenarios. We present the argument for

ML1
1 is a cause in M1
let Z = {ML1,FB}, so W = {ML2}. It is easy to see
that the contingency ML2 = 0 satisfies the two conditions
ML1

=

1 here. To show that

that there was a heavy rain in April and electrical storms in
the following two months; and in June the lightning took
hold. If it hadn't been for the heavy rain in April, the forest
would have caught fire in May. The question is whether
the April rains caused the forest fire. According to a naive

counterfactual analysis, they do, since if it hadn't rained,
there wouldn't have been a forest fire in June. Bennett says
"That is unacceptable. A good enough story of events and
of causation might give us reason to accept some things that
seem intuitively to be false, but no theory should persuade
us that delaying a forest's burning for a month (or indeed a
minute) is causing a forest fire."

u

ML1

=

in AC2. AC2(a) is satisfied because, in the absence of the
second arsonist (ML2 = 0), the first arsonist is necessary
and sufficient for the fire to occur (FB = 1). AC2(b)
is satisfied because, if the first match is lit (ML1 = 1) the

In our framework, as we now show, it is indeed false to say
that the April rains caused the fire, but they were a cause of
there being a fire in June, as opposed to May. This seems to
us intuitively right. To capture the situation, it suffices to use
a simple model with three endogenous random variables:
•

•

ML 1

=

of FB
1 in Mz. (Again, taking Z
W :::::: {ML2} works.)
=

I is also a cause

=

{ML,,FB}

and

This example also illustrates the need for the minimality
condition AC3. !f lighting a match qualifies as the cause of
firethen lighting a match and sneezing would also pass the

tests of AC1 and AC2 and awkwardly qualify as the cause
of fire. Minimality serves here to strip "sneezing" and other
irrelevant, over-specific details from the cause. I
This is a good place to illustrate the need for structural con­

tingencies in the analysis of actual causation. The reason
we consider ML1 = 1 to be a cause of FB = 1 in M1 is
that if ML2 had been 0, rather than 1, FB would depend on
MLt. In words, we imagine a situation in which the second

match is not lit, and we then reason counterfactually that
the forest would not have burned down if it were not for the
first match.

4

EXAMPLES

In this section we show how our definition of actual causal­
ity handles some examples that have caused problems for
other definitions. The full paper has further examples.

Example 4.1: The first example is due to Bennett (and
appears in [Sosa and Tooley 1993, pp. 222-223]). Suppose

"April showers", with two values-{) standing
for did not rain heavily in April and I standing for

ES

for "electric storms", with four possible values:

(0, 0) (no electric storms in either May or June), (1,0)
(electric storms in May but not June), (0,1) (storms
in June but not May), and (1,1) (storms in April and

May);

=

order to reveal the latent dependence of FB on ML 1· Such a
setting constitutes a structural change in the original model,
since it involves the removal of some structural equations.)

AS for

rained heavily in April;

contingency ML2 = 0 does not prevent the fire from burning
1 in Mt.
the forest. Thus, ML1 = 1 is a cause of FB
(Note that we needed to set ML2 to 0, contrary to facts, in

A similar argument shows that

UAI2001

•

andF for "fire", with three possible values: 0 (no fire
at all), 1 (fire

in May), or 2 (fire in June).

We do not describe the context explicitly, either here or
in the other examples. Assume its value ii is such that it
ensures that there is a shower in April, there are electric
storms in both May and June, there is sufficient oxygen,
there are no other potential causes of fire (like dropped
matches), no other inhibitors of fire (alert campers setting
up a bucket brigade), and so on. That is, we choose i1 so as
to allow us to focus on the issue at hand and to ensure that
the right things happened (there was both fire and rain).
We will not bother writing out the details of the structural
equations-they should be obvious, given the story (at least,
for the context il); this is also the case for all the other
examples in this section. The causal network is simple:
there are edges from AS to F and from ES to F. It is easy
to check that each of the following hold.
•

AS
W

= 1 is a cause ofthe June fire (F = 2) (taking
=

{ES}

2VF=1).

•

and

Z

=

{AS, F})

but not of fire (F

=

(1, I) is a cause of both F = 2 and (F
1 VF = 2). Having electric storms in both May and

ES

=

June caused there to be a fire.
•

I 1\ ES = ( l, 1) is not a cause of F = 2, because
it violates the minimality requirement of AC3; each
conjunct alone is a cause ofF = 2. Similarly, AS =
I 1\ ES = ( 1, 1) is not a cause of ( F = 1 VF
2).

AS =

=

UA12001

HALPERN

The distinction between April showers being a cause of the
fire (which they are not, according to our analysis) and April
showers being a cause of a fire in June (which they are) is
one that seems not to have been made in the discussion of
this problem (cf. [Lewis 2000]); nevertheless, it seems to
us an important distinction. I
Although we did not describe the context explicitly in Ex­
ample 4.1, it still played a crucial role. If the presence of
oxygen is relevant then we must take this factor out of the
context and introduce it as an explicit endogenous variables.
Doing so can affect the causality picture. The next example
already shows the importance of choosing an appropriate
granularity in modeling the causal process and its structure.
4.2: The following story from [Hall 2002] is
an example of preemption, where there are two potential
Example

causes of an event, one of which preempts the other. An
adequate definition of causality must deal with preemption
in all of its guises.
Suzy and Billy both pick up rocks and throw them
at a bottle. Suzy's rock gets there first, shattering
the bottle. Since both throws are perfectly accu­
rate, Billy's would have shattered the bottle had
it not been preempted by Suzy's throw.

Common sense suggests that Suzy's throw is the cause of the
shattering, but Billy's is not. This holds in our framework
too, but only if we model the story appropriately. Consider
first a coarse causal model, with three endogenous variables:
•

ST for "Suzy throws", with values 0 (Suzy does not

throw) and 1 (she does);

&

PEARL

199

cheating; the answer is almost programmed into the model
by invoking the relation "as a result of', which requires the
identification of the actual cause.
A more useful choice is to add two new random variables
to the model:
•

•

BHfor "Billy's rock hits the (intact) bottle", with val­
ues 0 (it doesn't) and 1 (it does); and
SHfor "Suzy's rock hits the bottle", again with values
0 and 1.

With this addition, we can go back to BS being two-valued.
In this model, we have the causal network shown in Figure
2, with the arrow SH -+ BH being inhibitory; BH BT 1\
·SH (that is, BH 1 iff BT 1 andSH 0). Note that, to
simplify the presentation, we have omitted the exogenous
variables from the causal network in Figure 2. In addition,
we have only given the arrows for the particular context of
interest, where Suzy throws first. In a context where Billy
throws first, the arrow would go from BH to SH rather than
going fromSH to BH, as it does in the figure.
=

=

=

=

Figure 2: The rock-throwing example.
Now it is the case that ST = 1 is a cause of BS == 1. To
satisfy AC2, we choose W = { BT} and w' = 0 and note
that, because BT is set to 0, BS will track the setting of
ST. Also note that BT 1 is not a cause of BS = 1; there
is no partition Z U W that satisfies AC2. Attempting the
symmetric choice W = {BT} and w' = 0 would violate
AC2(b) (with Z'
{ BH} ), because cp becomes false when
we set ST 0 and restore BH to its current value ofO.
=

•

BT for "Billy throws", with values 0 (he doesn't) and

1 (he does);

•

BS for "bottle shatters', with values 0 (it doesn't shat­
ter) and 1 (it does).

Again, we have a simple causal network, with edges from
both ST and BT to BS. In this simple causal network,
BT and ST play absolutely symmetric roles, with BS
ST V BT, and there is nothing to distinguish one from the
other. Not surprisingly, both Billy's throw and Suzy's throw
are classified as causes of the bottle shattering.
=

The trouble with this model is that it cannot distinguish
the case where both rocks hit the bottle simultaneously (in
which case it would be reasonable to say that both ST = 1
and BT 1 are causes of BS = 1) from the case where
Suzy's rock hits first. The model has to be refined to express
this distinction. One way is to invoke a dynamic model
[Pearl 2000, p. 326). Another way to gain expressiveness
is to allow BS to be three valued, with values 0 (the bottle
doesn't shatter), 1 (it shatters as a result of being hit by
Suzy's rock), and 2 (it shatters as a result of being hit by
Billy's rock). We leave it to the reader to check thatST = 1
is a cause of BS = 1, but BT = 1 is not (if Suzy hadn't
thrown but Billy had, then we would have BS 2). Thus,
to some extent, this solves our problem. But it borders on
=

=

=

==

This example illustrates the need for invoking subsets of Z
in AC2(b). I
Example 4.3: Is causality transitive? Consider the fol­
lowing story, again taken from (an early version of) [Hall

2002]:

Billy contracts a serious but nonfatal disease. He
is hospitalized and treated on Monday, so is fine
Tuesdsay morning. Had Monday's doctor for­
gotten to treat Billy, Tuesday's doctor would have
treated him, and he would have been fine Tues­
day afternoon. But there is a twist: one dose of
medication is harmless, but two doses are lethal.
The causal model for this story is straightforward. There
are three random variables: MT for Monday's treatment (1
if Billy was treated Monday; 0 otherwise), TT for Tuesday's
treatment ( 1 if Billy was treated Tuesday; 0 otherwise), and
BMC for Billy's medical condition (0 if Billy is fine both
Tuesday morning and Tuesday afternoon; 1 if Billy is sick

HALPERN

200

Tuesday morning, fine Tuesday afternoon; 2 if Billy is sick
both Tuesday morning and afternoon; 3 if Bill is fine Tues­
day morning and dead Tuesday afternoon). In the causal
network corresponding to this causal model, shown in Fig­
ure 3, there is an edge from MT to TT, since whether the
Tuesday treatment occurs depends on whether the Monday
treatment occurs, and edges from both MT and 1T to BMC,
since Billy's medical condition depends on both treatments.

&

PEARL

UAI2001

then in the context where V1 = V2 = I, it is easy to see that
each of Vi = I and V2 = I is a cause of P = l . However,
suppose we now assume that there is a voting machine that
tabulates the votes. Let M represent the total number of
votes recorded by the machine. Clearly M
V1 + V2 and
P
1 iff M � 1. The following causal network repre­
sents this more refined version of the story. In this more
=

=

MT-TT

\I
BMC

Figure 3: Billy's medical condition.
In this causal model, it is true that MT = 1 is a cause of

BMC = 0, as we would expect-because Billy is treated
Monday, he is not treated on Tuesday morning, and thus
recovers Tuesday afternoon. 2 MT = I is also a cause of
1T = 0, as we would expect, and 1T = 0 is a cause of
Billy's being alive (BMC = 0 v BMC = I v BMC = 2).
However, MT = I is not a cause of Billy's being alive. It
fails condition AC2(a): setting MT = 0 still leads to Billy's
being alive (with W = 0). Note that it would not help to
takeW = { IT} For ifTT= O,thenBilly is alive no matter
what MT is, while if 1T = 1, then Billy is dead when MT
has its original value, so AC2(b) is violated (with Z' = 0).
.

This shows that causality is not transitive, according to our
definitions. Although MT = I is a cause of 1T = 0 and
1T = 0 is a cause of BMC = 0 v BMC
1 V BMC
2,
MT= I is not a cause ofBMC = OVBMC = 1VBMC = 2.
Nor is causality closed under right weakening: MT = 1 is
a cause of BMC = 0, which logically implies BMC =
0 V BMC = 1 V BMC = 2, which is not caused by MT = 1.
=

=

Lewis [I986, 2000] insists that causality is transitive, partly
to be able to deal with preemption [Lewis 1986].
Our
approach handles preemption without needing to invoke
transitivity, which, as Lewis's own examples show, leads to
counterintuitive conclusions. I
Clause AC2(b) in the definition is complicated by the need
to check that no change in the value of the variables in Z
can affect the value of <.p. In all the previous examples,
Z = z* for each variable Z E i. Could we not just require
this? The following example shows that we cannot.
Example 4.4: Imagine that a vote takes place. For sim­
plicity, two people vote. The measure is passed if at least
one of them votes in favor. In fact, both of them vote in
favor, and the measure passes. This version of the story is
almost identical to Example 3.2. If we use V1 and Vz to
denote how the voters vote (V; = 0 if voter i votes against
and Vi = I if she votes in favor) and P to denote whether
the measure passes (P
I if it passes, P = 0 if it doesn't),
=

2
Lewis's [1986]
revised · criterion of counterfactual­
dependence-chain fails in this example; BMC does not depend
on either MT or TT in the context given.

Figure 4: An example showing the need for AC2(b).
refined scenario, vl = 1 and v2 = 1 are still both causes
of P = l . Consider V1 = I. Take Z = {Vi, M, P} and
W = V:t. Much like the simpler version of the story, if
we choose the contingency V2
0, then Pis counterfac­
tually dependent on V1, so AC2(a) holds. To check if this
contingency satisfies AC2(b), we set Vi to 1 (their original
value) and check that setting V2 to 0 does not change the
value of P. This is indeed the case. Although M becomes
1, not 2 as it is when V1 = Vz
1, nevertheless, P = 1
continues to hold, so AC2(b) is satisfied and V1 = 1 is a
cause of P = 1. However, if we had insisted in AC2(b)
that (M, u) f= [X of- x,W of- w']Z = z* for all variables
Z E Z (which in this case means that M would have to
retain its original value of2 when V1 = 1 and V2 = 0), then
neither V1 = 1 nor V2 = 1 would be a cause of P = l. I
=

=

We remark that this example is not handled correctly by
Pearl's causal beam definition. According to the causal
beam definition, there is no cause for P = 1! It can be
shown if X = x is an actual (or contributory) cause of
Y = y according to the causal beam definition given in
[Pearl 2000], then it is an actual cause according to the
definition here. As Example 4.4 shows, the converse is not
necessarily true.
Example 4.5: This example concerns what Hall calls the
distinction between causation and determination. Again,
we quote Hall [2002]:

You are standing at a switch in the railroad tracks.
Here comes the train: If you flip the switch, you'll
send the train down the left-hand track; if you
leave it where it is, the train will follow the right­
hand track. Either way, the train will arrive at
the same point, since the tracks reconverge up
ahead. Your action is not among the causes of
this arrival; it merely helps to determine how the
arrival is brought about (namely, via the left-hand
track, or via the right-hand track).

Again, our causal model gets this right. Suppose we have
three random variables:

UAI 2001

•

•

•

HALPERN

F for "flip", with values 0 (you don't flip the switch)
and 1 (you do);
T for "track", with values 0 (the train goes on the
left-hand track) and I (it goes on the right-hand track);

A for "arrival", with values 0 (the train does not arrive
at the point of reconvergence) and I (it does).

Now it is easy to see that flipping the switch (F = I) does
cause the train to go down the left-hand track (T = 0), but
does not cause it to arrive (A = I), thanks to AC2(a}-­
whether or not the switch is flipped, the train arrives.
However, our proposal goes one step beyond this simple
picture. Suppose that we model the tracks using two vari­
ables:
•

•

LT for "left-track", with values 1 (the train goes on the
left-hand track) and 0 (it does not); and
RT for "right-track", with values 1 (the train goes on
the right-hand track) and 0 (it does not).

The resulting causal diagram is shown in Figure 5 ; it is iso­
morphic to a class of problems Pearl [2000] calls "switch­
ing causation" . Lo and behold, this representation classifies
F = I as a cause of A, which, at first sight, may seem coun­
terintuitive: Can a change in representation turn a non-cause
into a cause?

& PEARL

201

this possibility that should enter our mind whenever we de­
cide to designate each track as a separate mechanism (i.e.,
equation) in the model and, keeping this contingency in
mind, it should not be too odd to name the switch position
a cause of the train arrival (or non-arrival). I
We conclude this section with an example that shows a
potential problem for our definition, and suggest a solution.
Example 4.6: Fred has his finger severed by a machine at
the factory (FS = I). Fortunately, Fred is covered by a
health plan. He is rushed to the hospital, where his finger is
sewn back on. A month later, the finger is fully functional
(FF
1). In this story, we would not want to say that
FS 1 is a cause ofFF 1 and, indeed, according to our
definition, it is not, since FF = 1 whether or not FS = 1
(in all contingencies satisfying AC2(b)).
=

=

=

However, suppose we introduce a new element to the story,
representing a nonactual structural contingency: Larry the
Loanshark may be waiting outside the factory with the in­
tention of cutting off Fred's finger, as a warning to him to
repay his loan quickly. Let LL represent whether or not
Larry is waiting and let LC represent whether Larry cuts
of the Fred's finger. If Larry cuts off Fred's finger, he will
throw it away, so Fred will not be able to get it sewn back
on. In the actual situation, LL = LC = 0; Larry is not
waiting and Larry does not cut off Fred's finger. So, intu­
itively, there seems to be no harm in adding this fanciful
element to the story. Or is there? Suppose that, if Fred's
finger is cut off in the factory, then Larry will not be able
to cut off the finger himself (since Fred will be rushed off
1 becomes a cause of FF 1.
to the hospital). Now FS
1, if FS 0
For in the structural contingency where LL
then FF
0 (Larry will cut off Fred's finger and throw it
away, so it will not become functional again). Moreover,
if FS = I, then LC
0 and FF I, just as in the actual
situation.4 I
=

=

=

=

=

A

=

Figure 5: Flipping the switch.
It can and it should! The change to a two-variable model
is not merely syntactic, but represents a profound change
in the story. The two-variable model depicts the tracks as
two independent mechanisms, thus allowing one track to be
set (by action or mishap) to false (or true) without affecting
the other. Specifically, this permits the disastrous mishap
of flipping the switch while the left track is malfunctioning.
Such abnormal eventualities are imaginable and expressible
in the two-variable model, but not in the one-variable model.
The potential for such eventualities is precisely what renders
F = I a cause ofthe A in the model of Figure 5.3
Is flipping the switch a legitimate cause of the train's ar­
rival? Not in ideal situations, where all mechanisms work
as specified. But this is not what causality (and causal
modeling) are all about. Causal models earn their value in
abnormal circumstances, created by structural contingen­
cies, such as the possibi lity of a malfunctioning track. It is
3

This can be seen by noting that condition AC2 is satisfied by

the partition

Z

=

{ F, LT, A } ,

W

=

{RT}, and choosing

the setting R T = 0. The event RT = 0 conflicts with F
normal situations.

=

w'

as

0 under

=

This example seems somewhat disconcerting. Why should
the Loanshark to the
story affect (indeed, result in) the accident being a cause of
the finger being functional one month later? While it is true
that the accident would be judged a cause of Fred's good
fortune by anyone who knew of Larry's vicious plan (many
underworld figures owe their lives to "accidents" of this
sort), the question remains how to distinguish genuine plans
that just happened not to materialize from sheer fanciful
scenarios that have no basis in reality. To some extent,
the answer here is the same as the answer to essentially all
the other concerns we have raised: it is a modeling issue.
If we know of Larry's plan, or it seems like a reasonable
possibility, we should add it to the model (in which case the
accident is a cause ofthe finger being functional); otherwise
we shouldn't.

adding a fanciful scenario like Larry

But this answer makes the question of how reasonable a
possibility Larry's plan are into an aU-or-nothing decision.
One solution to this problem is to extend our notion of
4We thank Eric Hiddleston for bringing this issue and this

example to our attention.

202

HALPERN & PEARL

causal model somewhat, so as to be able to capture more
directly the intuition that the Larry the Loanshark scenario
is indeed rather fanciful. There a number of ways of doing
this; we choose one based on Spohn's notion of a ranking
function (or ordinal conditionalfunction) [Spohn 1 988]. A
ranking K on a space W is a function mapping subsets of W
to IN* = IN U { oo} such that K(W)
0, K(0) = oo, and
K(A) = minw E A (K( { w} )) . Intuitively, an ordinal ranking
assigns a degree of surprise to each subset of worlds in W,
where 0 means unsurprising and higher numbers denote
greater surprise. Let a world be a complete setting of the
exogenous variables. Suppose that, for each context il, we
have a ranking /"£;; on the set of worlds. The unique setting
of the exogenous variables that holds in context il is given
rank 0 by /"£;;; other worlds are assigned ranks according to
how "fanciful" they are, given context il. Presumably, in
Example 4.6, an appropriate ranking K would give a world
where Larry is waiting to cut off Fred's finger (i.e., where
LL = 1 ) a rather high "' ranking, to indicate that it is rather
fanciful. We can then modify the definition of causality so
that we can talk about X = x being an actual cause of r.p in
(M, u) at rank k. The definition is a slight modification of
condition AC2 in Definition 3 . 1 so the contingency ( :2 , ,l;')
must hold in a world of rank at most k; we omit the formal
details here. We then can restrict actual causality so that
the structural contingencies involved have at most a certain
rank. This is one way of ignoring fanciful scenarios.
=

5

DISCUSSION

We have presented a principled way of determining actual
causes from causal knowledge. The essential principles
of our account include using structural equations to model
causal mechanisms; using uniform counterfactual notation
to encode and distinguish facts, actions, outcomes, and
contingencies; using structural contingencies to uncover
causal dependencies; and careful screening of these con­
tingencies to avoid tampering with the causal processes to
be uncovered. While our definitions has some unsatisfy­
ing features (see Example 4.6), we hope that the examples
presented here illustrate how well it deals with many of the
problematic cases found in the literature. As the examples
have shown, much depends on choosing the "right" set of
variables with which to model a situation, which ones to
make exogenous, and which to make endogenous. While
the examples have suggested some heuristics for making
appropriate choices, we do not have a general theory for
how to make these choices. We view this as an important
direction for future research.
Acknowledgments
We thank Christopher Hitchcock for many useful comments
on earlier versions of the paper, Zoltan Szabo for stim­
ulating discussions, and Eric Hiddleston for pointing out
Example 4.6. Halpern's work supported in part by NSF un­
der grants IRI-96-25901 and IIS-0090 145 and ONR under
grant N0001400 1 034 l . Pearl's work supported in part by
grants from NSF, ONR, AFOSR, and MICRO.

UA I 2001

References
Balke, A. and J. Pearl ( 1 994). Counterfactual probabil­
ities: Computational methods, bounds and applica­
tions. In Proc. UAI '94, pp. 4 6--54.
Eiter, T. and T. Lukasiewicz (2001 ). In Proc. IJCAI OJ
Galles, D. and J. Pearl ( 1 997). Axioms of causal rele­
vance. Artificial In telligence 97( 1 -2), 9-43 .
Goldberger, A. S. ( 1972). Structural equation methods in
the social sciences. Econometrica 40(6), 979-100 1 .
Good, I . ( 1 993). A tentative measure o f probabilistic
causation relevant to the philosophy of the law. J.
Statist. Comput. and Simulation 47, 99-105.
Hall, N. (2002). Two concepts of causation. In J. Collins,
N. Hall, and L. A. Paul (Eds.) Causation and Coun­
terfactuals, MIT Press. Forthcoming.
Halpern, J. Y. (2000). Axiomatizing causal reasoning.
Journal ofA./. Research, 3 1 7-337.
Halpern, J. Y. and J. Pearl (2000). Causes and expla­
nations: A structural-model approach. Available at
http://www.cs.cornell.edulhome/halpern.
Halpern, J. Y. and J. Pearl (200 1 ). Causes and explana­
tions: A structural-model approach - Part II: Ex­
planation. In Proc. IJCAI 'OJ .
Heckerman, D. and R. Shachter ( 1 995). Decision­
theoretic foundations for causal reasoning. Journal
ofA. I. Research 3, 405-430.
Hopkins, M. (200 I). A proof of the conjunctive cause
conjecture. Unpublished.
Hume, D. ( 1 739). A Treatise of Human Nature. John
Noon.
Lewis, D. ( 1 986). Causation. In Philosophical Papers,
Vol. II, pp. 1 59-2 1 3 . Oxford University Press.
Lewis, D. (2002). Causation as influence. In J. Collins,
N. Hall, and L. A. Paul (Eds.), Causation and Coun­
terfactuals. MIT Press. Forthcoming
Lin, F. ( 1 995). Embracing causality in specifying the
indeterminate effects of actions. In Proc. /JCAI '95,
pp. 1985-1 99 1 .
Pearl, J. ( 1 995). Causal diagrams for empirical research.
Biometrika 82(4), 669-7 1 0 .
Pearl, J . ( 1 998). O n the definition o factual cause. Techni­
cal Report R-259, Computer Science Dept., UCLA.
Pearl, J. (2000). Causality: Models, Reasining, and In­
ference. Cambridge University Press.
Sandewall, E. ( 1 994). Features and Fluents, Vol. 1 .
Clarendon Press.
Sosa, E. andM. Tooley (Eds.) ( 1 993). Causation. Oxford
readings in Philosophy. Oxford University Press.
Spirtes, P., C. Glymour, and R. Scheines ( 1 993). Causa­
tion, Prediction, and Search. Springer-Verlag.
Spohn, W. ( 1 988). Ordinal conditional functions: a
dynamic theory of epistemic states. In W. Harper
and B. Skyrms (Eds.), Causation in Decision, Belief
Change, and Statistics, Vol. 2, pp. 1 05-134. Reidel.
Tian, J. and J. Pearl (2000). Probabilities of causation:
bounds and identification. Annals of Mathematics
and A rtificial Intelligence 28, 287-3 13.
'

.

