200

KANG & TIAN

Polynomial Constraints in Causal Bayesian Networks

Changsung Kang
Department of Computer Science
Iowa State University
Ames, IA 50011
cskang@iastate.edu

1

Jin Tian
Department of Computer Science
Iowa State University
Ames, IA 50011
jtian@cs.iastate.edu

Abstract

interventional distribution, and call the distribution P(v)
non-experimental distribution.

We use the implicitization procedure to generate polynomial equality constraints on the set of
distributions induced by local interventions on
variables governed by a causal Bayesian network
with hidden variables. We show how we may reduce the complexity of the implicitization problem and make the problem tractable in certain
causal Bayesian networks. We also show some
preliminary results on the algebraic structure of
polynomial constraints. The results have applications in distinguishing between causal models
and in testing causal models with combined observational and experimental data.

The validity of a causal model can be tested only if it has
empirical implications, that is, it must impose constraints
on the statistics of the data collected. A causal BN not only
imposes constraints on the non-experimental distribution
but also on the interventional distributions that can be induced by the network. Therefore a causal BN can be tested
and falsified by two types of data, observational, which are
passively observed, and experimental, which are produced
by manipulating (randomly) some variables and observing
the states of other variables. The ability to use a mixture of
observational and experimental data will greatly increase
our power of causal reasoning and learning.

Introduction

The use of graphical models for encoding distributional and causal information is now fairly standard
[Heckerman and Shachter, 1995,
Lauritzen, 2000,
Pearl, 2000, Spirtes et al., 2001].
The most common
such representation involves a causal Bayesian network
(BN), namely, a directed acyclic graph (DAG) G which, in
addition to the usual conditional independence interpretation, is also given a causal interpretation. This additional
feature permits one to infer the effects of interventions
or actions, such as those encountered in policy analysis,
treatment management, or planning. Specifically, if an
external intervention fixes any set T of variables to some
constants t, the DAG permits us to infer the resulting
post-intervention distribution, denoted by Pt (v),1 from the
pre-intervention distribution P(v). The quantity Pt (y), often
called the “causal effect” of T on Y, is what we normally
assess in a controlled experiment with T randomized, in
which the distribution of Y is estimated for each level
t of T . We will call a post-intervention distribution an

There has been much research on identifying constraints on the non-experimental distributions implied by
a BN with hidden variables [Verma and Pearl, 1990,
Robins and Wasserman, 1997,
Desjardins, 1999,
Spirtes et al., 2001, Tian and Pearl, 2002]. In algebraic
methods, BNs are defined parametrically by a polynomial
mapping from a set of parameters to a set of distributions.
The distributions compatible with a BN correspond to a
semi-algebraic set, which can be described with a finite
number of polynomial equalities and inequalities. In
principle, these polynomial equalities and inequalities can
be derived by the quantifier elimination method presented
in [Geiger and Meek, 1999]. However, due to high computational demand (doubly exponential in the number of
probabilistic parameters), in practice, quantifier elimination is limited to models with few number of probabilistic
parameters.
[Geiger and Meek, 1998, Garcia, 2004,
Garcia et al., 2005] used a procedure called implicitization
to generate independence and non-independence constraints on the observed non-experimental distributions.
These constraints consist of a set of polynomial equalities
that define the smallest algebraic set that contains the
semi-algebraic set. [Garcia et al., 2005] analyzed the
algebraic structure of constraints for a class of small BNs.

1

[Pearl, 1995, Pearl, 2000] used the notation P(v|set(t)),
P(v|do(t)), or P(v|tˆ) for the post-intervention distribution, while
[Lauritzen, 2000] used P(v||t).

Algebraic approaches have been applied in causal
BNs to deal with the problem of the identifiabil-

KANG & TIAN
ity of causal effects [Riccomagno and Smith, 2003,
Riccomagno and Smith, 2004].
However, to the best
of our knowledge, the implicitization method has not
been applied to the problem of identifying constraints on
interventional distributions induced by causal BNs.
In this paper, we seek the constraints imposed by a causal
BN on both nonexperimental and interventional distributions. When all variables are observed, a complete characterization of constraints on interventional distributions imposed by a given causal BN has been given in [Pearl, 2000,
pp.23-4]. In a causal BN containing hidden variables, a
class of equality and inequality constraints on interventional distributions are given in [Kang and Tian, 2006]. In
this paper, we propose to use the implicitization procedure
to generate polynomial constraints on interventional distributions induced by a causal BN with hidden variables. The
main challenges in applying the implicitization procedure
on interventional distributions are:
(i) Computational complexity. The generic complexity of implicitization is known to be exponential in
the number of variables (number of parameters for
this problem). When we consider interventional distributions, the number of variables greatly increases
compared to the case of non-experimental distribution, which makes the computation infeasible even for
small causal BNs.
(ii) Understanding structures of constraints. Finding a
syntactic structure of the constraints computed by implicitization also becomes complicated.
To deal with challenge (i), we show two methods to reduce the complexity of the implicitization problem. We illustrate our method showing a model in which the generic
implicitization procedure is intractable while our methods
can solve the problem. We also show an example of new
constraints on interventional distributions that are not captured by the types of constraints in [Kang and Tian, 2006].
To deal with challenge (ii), we present some preliminary
results on the algebraic structure of polynomial constraints
on interventional distributions implied by certain classes of
causal BNs with hidden variables. We also present some
preliminary results in causal BNs without hidden variables,
which are often useful in understanding syntactic structures
of the constraints for BNs with hidden variables.

2
2.1

Preliminaries and Problem Statement
Causal Bayesian Networks and Interventions

A causal Bayesian network, also known as a Markovian
model, consists of two mathematical objects: (i) a DAG G,
called a causal graph, over a set V = {V1 , . . . , Vn } of vertices, and (ii) a probability distribution P(v), over the set

201

V of discrete variables that correspond to the vertices in
G.2 In this paper, we will assume a topological ordering
V1 > . . . > Vn in G. V1 is always a sink and Vn is always a source. The interpretation of such a graph has two
components, probabilistic and causal. The probabilistic interpretation views G as representing conditional independence restrictions on P: Each variable is independent of
all its non-descendants given its direct parents in the graph.
These restrictions imply that the joint probability function
P(v) = P(v1 , . . . , vn ) factorizes according to the product
Y
P(v) =
P(vi |pai )
(1)
i

where pai are (values of) the parents of variable Vi in G.
The causal interpretation views the arrows in G as representing causal influences between the corresponding variables. In this interpretation, the factorization of (1) still
holds, but the factors are further assumed to represent autonomous data-generation processes, that is, each conditional probability P(vi |pai ) represents a stochastic process
by which the values of Vi are assigned in response to the
values pai (previously chosen for Vi ’s parents), and the
stochastic variation of this assignment is assumed independent of the variations in all other assignments in the
model. Moreover, each assignment process remains invariant to possible changes in the assignment processes that
govern other variables in the system. This modularity assumption enables us to predict the effects of interventions,
whenever interventions are described as specific modifications of some factors in the product of (1). The simplest
such intervention, called atomic, involves fixing a set T of
variables to some constants T = t, which yields the postintervention distribution
( Q
{i|Vi <T } P(vi |pai ) v consistent with t.
Pt (v) =
(2)
0
v inconsistent with t.
Eq. (2) represents a truncated factorization of (1), with factors corresponding to the manipulated variables removed.
This truncation follows immediately from (1) since, assuming modularity, the post-intervention probabilities P(vi |pai )
corresponding to variables in T are either 1 or 0, while
those corresponding to unmanipulated variables remain unaltered. If T stands for a set of treatment variables and Y
for an outcome variable in V \ T , then Eq. (2) permits us to
calculate the probability Pt (y) that event Y = y would occur
if treatment condition T = t were enforced uniformly over
the population.
When some variables in a Markovian model are unobserved, the probability distribution over the observed variables may no longer be decomposed as in Eq. (1). Let
V = {V1 , . . . , Vn } and U = {U1 , . . . , Un′ } stand for the sets
of observed and unobserved variables respectively. If no U
2

We only consider discrete random variables in this paper.

202

KANG & TIAN

variable is a descendant of any V variable, then the corresponding model is called a semi-Markovian model. In this
paper, we only consider semi-Markovian models. However, the results can be generalized to models with arbitrary
unobserved variables as shown in [Tian and Pearl, 2002].
In a semi-Markovian model, the observed probability distribution, P(v), becomes a mixture of products:
XY
P(v) =
P(vi |pai , ui )P(u)
(3)
u

i

i

where PAi and U stand for the sets of the observed and
unobserved parents of Vi , and the summation ranges over
all the U variables. The post-intervention distribution, likewise, will be given as a mixture of truncated products
Pt (v)
 X Y


P(vi |pai , ui )P(u)


=
u
{i|V
<T
}
i


 0

v consistent with t.
(4)
v inconsistent with t.

Assuming that v is consistent with t, we can write
Pt (v) = Pt (v \ t)

(5)

In the rest of the paper, we will use Pt (v) and Pt (v \ t) interchangeably, always assuming v being consistent with t.
2.2

Algebraic Sets, Semi-algebraic Sets and Ideals

We briefly introduce some concepts related to algebraic geometry that will be used in this paper.
The set of all polynomials in x1 , . . . , xn with real coefficients is called a polynomial ring and denoted by
R[x1 , . . . , xn ].
Let f1 , . . . , f s be the polynomials in
R[x1 , . . . , xn ]. A variety or an algebraic set V( f1 , . . . , f s )
is the set {(a1 , . . . , an ) ∈ Rn : fi (a1 , . . . , an ) = 0 for all 1 ≤
i ≤ s}. Thus, an algebraic set is the set of all solutions of a
system of polynomial equations.
A subset V of Rn is called a semi-algebraic set if V =
s
i
∪i=1
∩rj=1
{x ∈ Rn : Pi, j (x) ⇔i j 0} where Pi j are polynomials
in R[x1 , . . . , xn ] and ⇔i j is one of the comparison operators
{<, =, >}. Informally, a semi-algebraic set is a set that can
be described by a finite number of polynomial equalities
and inequalities.
An ideal I is a subset of a ring, which is closed under addition and multiplication by any polynomial in the ring. The
ideal generated by a set of polynomials g1 , . . . , gn is the set
P
of polynomials h that can be written as h = ni=1 fi gi where
fi are polynomials in the ring and is denoted by hg1 , . . . , gn i.
The sum of two ideals I and J is the set I + J = { f + g :
f ∈ I, g ∈ J} and it holds that if I = h f1 , . . . , fr i and
J = hg1 , . . . , g s i, then I + J = h f1 , . . . , fr , g1 , . . . , g s i.
2.3

Problem

We now define the implicitization problem for a set of interventional distributions. We explain what the polynomial

constraints computed by the implicitization problem mean
algebraically.
Let P intv denote a set of interventional distributions. For
example, P intv ={P(v1 , v2 ), PV1 =1 (V1 = 1, v2 )} contains a
non-experimental distribution P(v1 , v2 ) and an interventional distribution PV1 =1 (V1 = 1, v2 ) where the treatment
variable V1 is fixed to 1. We will regard P(v) to be a
special interventional distribution where T = ∅ allowing
it to be in P intv . Let P∗ denote the set of all interventional distributions P∗ = {Pt (v)|T ⊂ V, t ∈ Dm(T ), v ∈
Dm(V), v is consistent with t} where Dm(T ) represents the
domain of T . For example, let V = {V1 , V2 } where both
variables are binary, then P∗ = {P(v1 , v2 ), PV1 =1 (V1 =
1, v2 ), PV1 =2 (V1 = 2, v2 ), PV2 =1 (v1 , V2 = 1), PV2 =2 (v1 , V2 =
2)}.
We can describe P intv in terms of a polynomial mapping
from a set of parameters to the distributions as follows.
First, consider a causal BN G without hidden variables.
Let V1 , . . . , Vn be the vertices of G. We denote the joint
space parameter defining Pt (v) for v consistent with t by
ptv and the model parameter defining P(vi |pai ) by qivi pai .
The model parameters are subjected to the linear relaP
tions vi qivi pai = 1. Thus, we have introduced (di −
Q
1) { j|V j ∈PAi } d j model parameters for the vertex Vi where
di = |Dm(Vi )|. Let JP intv denote the set of joint space
parameters associated with P intv and M denote the set of
model parameters. For example, consider a simple causal
BN V1 ← V2 in which both variables are binary. Let
P intv be the set of two distributions {P(v1 , v2 ), PV1 =1 (V1 =
1, v2 )}. Then, JPintv ={p11 , p12 , p21 , p22 , pV111 =1 , pV121 =1 } and
M = {q111 , q112 , q21 }. The mapping related to (2) is
φ : R M → R JPintv ,
Y
ptv =
qivi pai

(6)

{i|Vi <T }

where R M and R JPintv denote the real vector space of dimension |M| and |JP intv | respectively. (6) induces a ring homomorphism
Φ : R[JP intv ] → R[M].

(7)

Second, consider a causal BN G with hidden variables. Let
{V1 , . . . , Vn } and {U1 , . . . , Un′ } be sets of observed and hidden variables respectively. We denote the joint space parameters defining Pt (v) for v consistent with t by ptv and the
model parameters defining P(vi |pai , ui ) and P(u j ) by qiv pa ui
i

i

and ruj j respectively. The joint space parameters and the
model parameters form two rings of polynomials R[JP intv ]
and R[M]. The mapping related to (4) is
π : R M → R JPintv ,
ptv

=

X Y

u1 ...un′ {i|Vi <T }

qivi pai ui

n′
Y
j=1

ruj j .

(8)

KANG & TIAN

V4

V3
V2

V1
(a)

on the analysis of the computed constraints. In this section,
we give a preliminary result on the algebraic structure of
the constraints for a set of interventional distributions associated with causal BNs without hidden variables. The
problem of characterizing the structure of the constraints
for arbitrary set of interventional distributions is still open.
We show a few cases in which the constraints can be nicely
described by a simple set of polynomials.

V3

V2
V1

(b)

Figure 1: Two causal BNs.

3.1

(8) induces a ring homomorphism
Ψ : R[JP intv ] → R[M].

(9)

By Tarski-Seidenberg theorem, the image of φ (or π) corresponds to a semi-algebraic set, which can be described
by a set of polynomial equalities and inequalities. Finding
all of these equalities and inequalities is usually infeasible.
In this paper, we choose to find a set of polynomial equalities that define the smallest algebraic set that contains the
image of φ (or π). These polynomial equalities are a subset of the constraints that describe the image of φ (or π)
and turn out to be equal to the kernel of the ring homomorphism Φ (or Ψ). The kernel of Φ, denoted by ker(Φ) is the
ideal consisting of all polynomials f in R[JP intv ] such that
Φ( f ) = 0. Thus, the vanishing of the polynomial equalities in ker(Φ) and ker(Ψ) is a necessary condition that there
exist the model parameters in (6) and (8) respectively. The
process of computing ker(Φ) is called implicitization.
Our goal is to compute and analyze the kernels for causal
BNs with or without hidden variables.

3

203

Causal Bayesian Network with No Hidden
Variables

Consider a causal BN G and a set of interventional distributions P intv . If checking whether each Pt (v) ∈ P intv
factors as in (2) is the only goal, it is not necessary to
solve the implicitization problem since you can use the
constraints (2) given by the definition or the constraints
given in [Pearl, 2000, pp.23-4]. However, we study the
implicitization problem for a set of interventional distributions associated with a causal BN without hidden variables, since we expect that the structure of the constraints
for a causal BN without hidden variables may reveal
some syntactic structure of the constraints for a causal BN
with hidden variables. For non-experimental distribution,
[Garcia et al., 2005] showed that the constraints for a BN
without hidden variables can help finding the structure of
the constraints for a BN with hidden variables.
Since the computation of the constraints for causal BNs
without hidden variables is relatively easy, we will focus

One Interventional Distribution

Suppose P intv contains only one interventional distribution Pt (v). For non-experimental distribution P(v),
[Garcia et al., 2005] showed that
X
(10)
ker(Φ) = (Ilocal(G) : p∞ ) + h pv − 1i
v

where Ilocal(G) is the ideal associated to the local Markov
property on a BN G X
and p is the product of all linear
forms p+...+vr+1 ...vn =
pv1 ...vr vr+1 ...vn and I : f ∞ = {g ∈
v1 ,...,vr

R[J{P(v)} ] | g f N ∈ I, for some N} denotes the saturation of
I by f .
The local Markov property on G is the set of independence
statements
local(G) = {Vi y ND(Vi )|PA(Vi ) : i = 1, . . . , n}

(11)

where ND(Vi ) denotes the set of nondescendents of Vi in G
and PA(Vi ) denotes the set of parents of Vi in G.
For example, consider the causal BN G in Figure 1 (a). Assume that all variables are binary. The local Markov property on G has only one element V1 y V2 | V3 . The constraints induced by an independence statement, A y B | C
are given by the vanishing of the polynomials
P(A = a, B = b, C = c)P(A = a′ , B = b′ , C = c)
− P(A = a′ , B = b, C = c)P(A = a, B = b′ , C = c)

(12)

for all a, a′ , b, b′ , c. Thus, the ideal Ilocal(G) associated with
the local Markov property on G is
Ilocal(G) = hp111 p221 − p121 p211 , p112 p222 − p122 p212 i.
(13)
For this particular BN G, it turns out that
Ilocal(G) : p∞ =Ilocal(G) : (p111 . . . p222 p+11 . . . p+22 p++1 p++2 )∞
=Ilocal(G) .
(14)

From (10), it follows that
ker(Φ) = Ilocal(G) + h

X

pv − 1i.

(15)

v

In general, however, ker(Φ) does not coincide with Ilocal(G) .
For example, Ilocal(G) : p∞ for the causal BN G in Figure 1
(b) includes 16 additional generators other than Ilocal(G) .

204

KANG & TIAN

The above result can be applied to an arbitrary interventional distribution Pt (v). We see that the mapping in (6)
defined for Pt (v) and G is equivalent to the mapping defined for P(v \ t) and G(V \ T ) where G(C) denotes the
subgraph of G composed only of the variables in C. Thus,
the following holds.
Proposition 1 Let Φ be a ring homomorphism
Φ : R[J{Pt (v)} ] → R[M]

(16)

induced by (6). Then, we have

From (15), it follows that
ker(Φ) =Ilocal(G) + h

X

pv − 1i + hpV1v12=1
v3 −

v

X

pv1 v2 v3 : ∀v2 , v3 i.

v1

(24)

Note that the equation in (20) holds because the set {V2 , V3 }
contains its own ancestors in G. We have the following
proposition.
Proposition 2 Suppose P intv = {P(v), Pt (v)}. Let Φ and Φ′
be ring homomorphisms

(17)

Φ : R[J{P(v),Pt (v)} ] → R[M], Φ′ : R[J{P(v)} ] → R[M].

where p is the product of all linear forms p+...+vir+1 ...vik when
V \ T = {Vi1 , . . . , Vik }, Vi1 > . . . > Vik .

If V \ T contains its own ancestors in G, we have
X
ker(Φ) = ker(Φ′ ) + hptv −
pv : ∀(v \ t)i.

ker(Φ) = (Ilocal(G(V\T )) : p∞ ) + h

X

ptv − 1i

v\t

3.2

Consider the set of all interventional distributions P∗ . For
any joint space parameter ptv , we have
ptv =

Y

qivi pai =

i
pv\v
vi .

(18)

{i|Vi <T }

{i|Vi <T }

Thus, every joint space parameter can be written as the
product of some other joint space parameters. Then,
Y
i
pv\v
(19)
ker(Φ) = hptv −
vi : ∀v, ti.
{i|Vi <T }

3.3

Two Interventional Distributions

Consider the case in which P intv has two distributions. We
show some cases in which ker(Φ) can be described by a
simple set of polynomials.
Consider the causal BN G in Figure 1 (a) where all variables are binary. Suppose P intv = {P(v), PV1 =1 (v)}. We have
the following relation between pV1v12=1
v3 and pv . For any v2
and v3 ,
pV1v12=1
v3 =

X

pv1 v2 v3 .

(20)

v1

Now consider the causal BN G in Figure 1 (a) and suppose that Pintv = {P(v), PV3 =1 (v)}. In this case, PV3 =1 (v)
cannot be represented as a polynomial function of P(v).
However, we can describe the generators of ker(Φ) as follows. Given an instantiation of all the variables v and an
instantiation of treatment variables t, let Vcons = {Vi ∈
V \ T | vi pai in v is consistent with t} and cons(v, t) denote
the instantiation of V obtained by replacing the inconsistent variables in v with the values of t. For example, for
G in Figure 1 (a), if v = (V1 = 1, V2 = 1, V3 = 1) and
t = (V2 = 2), then Vcons = {V1 , V3 } and cons(v, t) = (V1 =
1, V2 = 2, V3 = 1). We have the following lemma.
Lemma 1 Suppose P intv = {P(v), Pt (v)}. Let Φ, Φ′ and Φ′′
be ring homomorphisms
Φ : R[J{P(v),Pt (v)} ] → R[M], Φ′ : R[J{P(v)} ] → R[M]
Φ′′ : R[J{Pt (v)} ] → R[M].

(i) there exist two disjoint subsets W1 = {A1 , . . . , Ai } and
W2 = {C1 , . . . , Ck } of T such that
A1 > . . . > Ai > B1 > . . . > B j > C1 > . . . > Ck

(21)

Since the joint space parameter pV1v12=1
v3 for any v2 and v3 is a
polynomial function of some of joint space parameters pv ,
we have
X
ker(Φ) = ker(Φ′ ) + hpV1v12=1
−
pv1 v2 v3 : ∀v2 , v3 i (22)
v3

(27)

If for any two vertices Vi and V j in V \ T , Vi is neither V j ’s
ancestor nor its descendent, then

Let Φ denote a ring homomorphism
Φ : R[J{P(v),PV1 =1 (v2 ,v3 )} ] → R[M].

(26)

t

The relationship between two distributions in the above
proposition is the result of Lemma 3 in Section 4.

All Interventional Distributions

Y

(25)

(28)

is a consistent topological ordering of variables in G
where V \ T = {B1 , . . . , B j } and
(ii)
ker(Φ) =ker(Φ′ ) + ker(Φ′′ )
X
X
pv −
pv : ∀vi
+ h f (v, t)
w1 ,vcons

(29)

w1

v1

where

where Φ′ denotes the ring homomorphism
′

Φ : R[J{P(v)} ] → R[M].

(23)

f (v, t) =

Y

X

{i|Vi ∈Vcons } vcons \vi

ptcons(v,t) .

(30)

KANG & TIAN
See the Appendix for the proof.

205

induced by the mapping

We can use Lemma 1 to compute ker(Φ) for the causal BN
G in Figure 1 (a) and P intv = {P(v), PV3 =1 (v)} since V1 is
neither V2 ’s ancestor nor its descendent. It turns out that
ker(Φ) =ker(Φ′ ) + ker(Φ′′ ) + hpVv13v=1
21

X

+

X

pv − 1i + Ilocal(G({V1 ,V2 }) + h

X

pv1 v2 1 − pv1 v2 1 : ∀v1 , v2 i.

X

pVv 3 =1 − 1i

v1 ,v2

For
the
non-experimental
[Garcia et al., 2005] showed that

(31)

ruj j .

(34)

j=1

Causal Bayesian Network with Hidden
Variables

ker(Ψ) = ker(Φ) ∩ R[JP intv ].

4.2

(35)

Suppose we have a causal BN G with n observed variables
V1 , . . . , Vn and n′ unobserved variables U1 , . . . , Un′ and a
set of interventional distributions P intv for G. Let Ψ be the
ring homomorphism defined in (9). We denote P U
intv be the
set of joint distributions assuming that all U1 , . . . , Un′ are
observed
(32)

Let Φ denote the ring homomorphism
(33)

(36)

Reducing the Implicitization Problem Using
Known Constraints

We can reduce the complexity of the implicitization problem by using some known constraints among interventional
distributions. Given the set of joint space parameters JP intv ,
suppose that we have some known constraints among JP intv
stating that a joint space parameter ptv can be represented as
a polynomial function of some other joint space parameters
in JP intv \ ptv . Then, the relation reduces the implicitization
problem as follows. Let f be a polynomial function such
that
ptv = f (JP intv \ ptv )

[Garcia et al., 2005] proposed a two-step method to compute ker(Ψ) for a BN with hidden variables and nonexperimental distribution. It is known that this method usually works faster than direct implicitization. We apply it to
our problem in which we have a set of interventional distributions.

Φ : R[JP Uintv ] → R[M]

P(v),

Following [Garcia et al., 2005], ker(Ψ) can be computed
in two steps. First, we compute ker(Φ) corresponding to
the case where all variables are assumed to be observed.
Then we compute the subset of ker(Φ) that corresponds to
the polynomial constraints on observable distributions. We
have implemented our method using a computer algebra
system, Singular [Greuel et al., 2005].

Two-step Method

PU
intv = {Pt (vu)|Pt (v) ∈ P intv }.

distribution

It can be naturally extended to the case of arbitrary P intv .
We have

Solving the implicitization problem for a causal BN with
hidden variables has a high computational demand. The
implicitization problem can be solved by computing a
certain Groebner basis and it is known that computing
a Groebner basis has the generic complexity mO(1) gO(N)
where m is the number of equations, g is the degree of the
polynomials and N is the number of variables. In our implicitization problems, N is the sum of the number of joint
space parameters and model parameters. Consider the implicitization for non-experimental distribution. The number of joint space parameters for non-experimental distribution is d1 . . . dn . Solving the implicitization problem becomes intractable as the number of vertices in the causal
BN and the domains of variables increase. Now consider
the cases in which we have a set of interventional distributions. The number of joint space parameters for P∗ is
d1 . . . dn (d1 . . . dn − 1). This greatly increases the complexity of the already hard problem. In this section, we show
two methods to reduce the complexity of our implicitization problem.
4.1

n′
Y

ker(Ψ) = ker(Φ) ∩ R[J{P(v)} ].

v1 ,v2

4

qivi pai ui

pv1 v2 1 − pv1 v2 1 : ∀v1 , v2 i

v

hpVv13v=1
21

=

Y

{i|Vi <T }

v1 ,v2

=Ilocal(G) + h

ptvu

(37)

and let Ψ and Ψ′ be two ring homomorphisms
Ψ : R[JP intv ] → R[M], Ψ′ : R[JP intv \ ptv ] → R[M]. (38)
Then, we have
ker(Ψ) = ker(Ψ′ ) + hptv − f (JP intv \ ptv )i.

(39)

This suggests that the more we find such relations among
parameters, the more we can reduce the implicitization
problem. The following two lemmas provide a class of
such relations.
A c-component is a maximal set of vertices such that any
two vertices in the set are connected by a path on which
every edge is of the form c U d where U is a hidden
variable. A set A ⊆ V is called an ancestral set if it contains
its own observed ancestors.

206

KANG & TIAN

procedure PolyRelations(G,JP intv )
INPUT: a causal BN G, joint space parameters JPintv associated with a set of interventional distributions P intv
OUTPUT: a subset JP′ intv ⊆ JP intv of joint space parameters and
the ideal I containing polynomial relations among the joint
space parameters
Initialization:
I←∅
JP′ intv ← JPintv
Step 1:
For each ptv ∈ JP′ intv
Let H1 , . . . , Hl be the c-components in the subgraph
G(V \ T ).
Y
i
I ← I + hptv −
pv\h
v i

U1
V1

U1

V2

V3

t\c

JP′ intv ← JP′ intv \ ptv

Figure 2: A Procedure for Listing Polynomial Relations
among Interventional Distributions

V2

V1

(a)

V3

(b)

Figure 3: Two causal BNs with one hidden variable.
computed by PolyRelations. Then,
ker(Ψ) = ker(Ψ′ ) + I

i

JP′ intv ← JP′ intv \ ptv
Step 2:
For each ptv ∈ JP′ intv
If there is a joint space parameter pcv that satisfies
(i) C ⊆ T ⊆ V
(ii) V \ T is an ancestral set in G(V \ C)
then
X
I ← I + hptv −
pcv i

V4

(42)

where Ψ′ is a ring homomorphism
Ψ′ : R[JP′ intv ] → R[M].

(43)

To illustrate the procedure, consider a causal BN G with
four observed variables V1 , V2 , V3 , V4 and one hidden variable U1 in Figure 3 (a). We will compute ker(Ψ) for the set
of all interventional distributions P∗ using PolyRelations.
In Step 1, we find that most of joint space parameters can
be represented as the product of other parameters. For example, we have
pvv1 = pvv1 v3 v4 pvv1 v2 v4 pvv1 v2 v3

Lemma 2 [Tian and Pearl, 2002] Let T ⊆ V and assume
that V \ T is partitioned into c-components H1 , . . . , Hl in
the subgraph G(V \ T ). Then we have
Y
Pt (v) =
Pv\hi (v).
(40)
i

Lemma 3 [Tian and Pearl, 2002] Let C ⊆ T ⊆ V. If V \ T
is an ancestral set in G(V \ C), then
X
Pt (v) =
Pc (v).
(41)
t\c

We give a procedure in Figure 2 that lists a set of polynomial relations among P intv based on these two lemmas.
Given a set of joint space parameters JP intv , it outputs a subset JP′ intv of JP intv which contains the joint space parameters
that cannot be represented as a polynomial function of other
joint space parameters, and the ideal I generated by all the
relations found by Lemma 2 and Lemma 3. In Step 1, we
look for the parameters that can be represented as the product of other parameters using Lemma 2. In Step 2, we find
the parameters that can be represented as the sum of other
parameters using Lemma 3. We have the following proposition.
Proposition 3 Given a set of interventional distributions
P intv , a causal BN G with hidden variables and a ring homomorphism Ψ defined in (9), let JP′ intv and I be the results

(44)

since V \ V1 = {V2 , V3 , V4 } is partitioned into three ccomponents {V2 }, {V3 } and {V4 }. Also,
pvv2 = pvv2 v4 pvv1 v2 v3

(45)

since V \ V2 = {V1 , V3 , V4 } is partitioned into two ccomponents {V1 , V3 } and {V4 }. The only joint space parameters that do not decompose in Step 1 are
pvv2 v4 , pvv1 v3 v4 , pvv1 v2 v3 , pvv2 v3 v4 and pvv1 v2 v4 .

(46)

Thus, after Step 1 we have
JP′ intv = J{Pv2 v4 (v),Pv1 v3 v4 (v),Pv1 v2 v3 (v),Pv2 v3 v4 (v),Pv1 v2 v4 (v)} .

(47)

In Step 2, we find that
pvv2 v3 v4 =

X

pvv2 v4 and pvv1 v2 v4 =

v3

X

pvv2 v4

(48)

v1

since V \ {V2 , V3 , V4 } = {V1 } and V \ {V1 , V2 , V4 } = {V3 } are
ancestral sets in G(V \ {V2 , V4 }) = G({V1 , V3 }). After Step
2, we have
JP′ intv = J{Pv2 v4 (v),Pv1 v3 v4 (v),Pv1 v2 v3 (v)}

(49)

and I is generated by all the relations found in Step 1 and
2. Finally, we have
ker(Ψ) = ker(Ψ′ ) + I

(50)

KANG & TIAN
where Ψ′ is the ring homomorphism
Ψ′ : R[JP′ intv ] → R[M].

5
(51)

Moreover, we find that ker(Ψ′ ) can be represented as
ker(Ψ1 ) + ker(Ψ2 ) + ker(Ψ3 ) where
Ψ1 : R[J{Pv2 v4 (v)} ] → R[M], Ψ2 : R[J{Pv1 v3 v4 (v)} ] → R[M]
Ψ3 : R[JP{v1 v2 v3 (v)} ] → R[M]

(52)

since the mappings inducing Ψ1 , Ψ2 and Ψ3 do not share
model parameters. This gives
ker(Ψ) = ker(Ψ1 ) + ker(Ψ2 ) + ker(Ψ3 ) + I.

(53)

Compared to the original implicitization problem of computing ker(Ψ) involving 240 joint space parameters which
is intractable, we now have three small implicitization
problems. Computing ker(Ψ1 ) involves 12 joint space
parameters and each of the computation of ker(Ψ2 ) and
ker(Ψ3 ) involves 2 joint space parameters. The reduced
problem can be solved easily.
Note that JP′ intv computed by PolyRelations in the above
example contains only the joint space parameters related to
c-components in G. This holds generally for G in which the
subgraph G(C) for each c-component C of G has no edges.
Proposition 4 Let C1 , . . . , Cl be c-components of a causal
BN G. If every subgraph G(Ci ) has no edges, then
ker(Ψ) = ker(Ψ1 ) + . . . + ker(Ψl ) + I

(54)

where

The implicitization problem for a large causal BN G is
computationally feasible if G has the structure described
in Proposition 4 and the size of each c-component in G is
small. Our method becomes infeasible as the size of each
c-component grows.
In general, there may be some constraints that are not included in the constraints for each c-component and cannot
be found by Lemma 2 and 3. For example, for the causal
BN G in Figure 3 (b), we find the following constraint by
the method in Section 4.1 using the Singular system:
2 =2 V2 =2
2 =2 V2 =1
2 =2 V2 =1
p221
p212 + p212 pV122
p211 + p222 pV122
p222 pV122

−
+
+

We obtain polynomial constraints on the interventional distributions induced by a causal BN with hidden variables,
via the implicitization procedure. These constraints constitute a necessary test for a causal model to be compatible
with given observational and experimental data. To apply
these constraints to finite data in practice, an important future work is to design test statistics for non-independence
constraints. Another future work is to study how to use
these constraints in the model selection process. We are
investigating a model selection method that uses a new
goodness-of-fit score based on the geometric distance between data and a model.
We are also working on the general characterization of
the constraints computed by implicitization for causal BNs
without hidden variables, which will be helpful in finding
the algebraic structure of the constraints implied by causal
BNs with hidden variables which typically have complicated structures.
Acknowledgments
This research was partly supported by NSF grant IIS0347846.

Appendix : Proof of Lemma 1
We define the ideal I associated with Φ.

2 =2 V2 =1 V2 =2
2 =1 V2 =2
2 =1 V2 =2
p212 p221
p221 − pV122
p221 + p222 pV212
p122 pV212
V2 =1 V2 =2
V2 =2 V2 =2
2 =1 V2 =2
p222
p212 p122 p222 − p122 p211 p222 + p222 pV212
2 =1 V2 =2 V2 =2
2 =2 V2 =2
2 =2 V2 =1 V2 =2
p221 p222
p222 − pV212
p212 p222 + p212 pV221
pV122
2 =2
2 =1
2 =1 V2 =2 V2 =2
2 =2 V2 =2
− p212 pV222
p222 p222 − p222 pV212
p222 − pV212
p212 pV222
2 =1 V2 =2
(56)
p222
pV212

which is in ker(Ψ) but cannot be induced by Lemma 2 and
3.

Y

qivi pai : ∀vi + hptv −

i

(55)

and I is the ideal computed by the procedure PolyRelations.

+

Conclusion and Future Work

I = hpv −

Ψi : R[J{Pv\ci (v)} ] → R[M]

+

207

Y

qivi pai : ∀(v \ t)i.

(57)

{i|Vi <T }

The elimination ideal I ∩ R[J{P(v),Pt (v)} ] is equivalent to
ker(Φ). The idea is that we can represent I as the sum of
three ideal I1 , I2 and I3 such that the model parameters in I1
and those in I2 are disjoint and no model parameter appears
in I3 and thus
ker(Φ) =I ∩ R[J{P(v),Pt (v)} ]
=I1 ∩ R[J{P(v)} ] + I2 ∩ R[J{Pt (v)} ] + I3
=ker(Φ′ ) + ker(Φ′′ ) + I3 .
i
i qvi pai

(58)

Q
hptv − {i|Vi <T }

Q

qivi pai

Let I1 = hpv −
: ∀vi and I2 =
:
∀(v \ t)i. We will replace each generator in I1 with two
other polynomials and add one polynomial to I3 which is
initially empty as follows.
Q
For any polynomial pv − i qivi pai , we have
pv −

Y

qivi pai

(59)

i

= pv −

 Y

qivi pai

 Y

= pv −

 Y

qivi pai

 X

{i|Vi ∈W1 }

{i|Vi ∈W1 }

qivi pai

{i|Vi ∈V\T }

pv

w1

 Y

qivi pai

{i|Vi ∈W2 }



(60)

since
X
w1

pv −

 Y

{i|Vi ∈V\T }

qivi pai



 Y

{i|Vi ∈W2 }

qivi pai



208

KANG & TIAN

is in I. Also,
X

pv −

w1

qivi pai

{i|Vi ∈V\T }

X

=

 Y

pv −

w1

 Y



 Y

qivi pai



Y

{i|Vi ∈W2 }

Y

qivi pai

{i|Vi ∈Vcons }

qivi pai

{i|Vi ∈W2 }

[Greuel et al., 2005] G.-M. Greuel, G. Pfister, and
H. Schönemann. S INGULAR 3.0. A Computer Algebra
System for Polynomial Computations, Centre for
Computer Algebra, University of Kaiserslautern, 2005.
http://www.singular.uni-kl.de.


qivi pai

{i|Vi ∈(V\T )\Vcons }





From the property that any two vertices Vi and V j in V \ T ,
Vi is neither V j ’s ancestor nor its parent, it follows that the
polynomial
X

pv −

w1 ,vcons



Y

qivi pai

{i|Vi ∈(V\T )\Vcons }

 Y

qivi pai

{i|Vi ∈W2 }



(61)

is in I. Thus,
X

pv −

w1

 Y

qivi pai

{i|Vi ∈V\T }

 Y

{i|Vi ∈W2 }

=

X

pv −



Y

qivi pai

{i|Vi ∈Vcons }

=

X

pv −



Y

X

w1

w1



qivi pai

 X

w1 ,vcons

{i|Vi ∈Vcons } vcons \vi

ptcons(v,t)

pv



 X

w1 ,vcons


pv .

(62)

We replace the polynomial (59) with the polynomials (60)
and (61) and add the polynomial (62) to I3 . After processing every polynomial in I1 , we have three ideal I1 , I2 and I3
with the desired property. 

References
[Desjardins, 1999] B. Desjardins. On the theoretical limits
to reliable causal inference. PhD thesis, University of
Pittsburgh, 1999.
[Garcia et al., 2005] L.D. Garcia, M. Stillman, and
B. Sturmfels. Algebraic geometry of bayesian networks.
Journal of Symbolic Computation, 39(3–4):331–355,
2005.
[Garcia, 2004] Luis David Garcia. Algebraic statistics in
model selection. In Proceedings of the 20th Annual Conference on Uncertainty in Artificial Intelligence (UAI04), pages 177–18, Arlington, Virginia, 2004. AUAI
Press.
[Geiger and Meek, 1998] Dan Geiger and Christopher
Meek. Graphical models and exponential families. In
Proceedings of the Fourteenth Annual Conference on
Uncertainty in Artificial Intelligence (UAI–98), pages
156–165, San Francisco, CA, 1998. Morgan Kaufmann
Publishers.
[Geiger and Meek, 1999] Dan Geiger and Christopher
Meek. Quantifier elimination for statistical problems. In
Proceedings of the Fifteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI–99), pages 226–
235, San Francisco, CA, 1999. Morgan Kaufmann Publishers.

[Heckerman and Shachter, 1995] D. Heckerman and
R. Shachter. Decision-theoretic foundations for causal
reasoning. Journal of Artificial Intelligence Research,
3:405–430, 1995.
[Kang and Tian, 2006] C. Kang and J. Tian. Inequality
constraints in causal models with hidden variables. In
Proceedings of the Seventeenth Annual Conference on
Uncertainty in Artificial Intelligence (UAI-06), pages
233–240, Arlington, Virginia, 2006. AUAI Press.
[Lauritzen, 2000] S. Lauritzen. Graphical models for
causal inference. In O.E. Barndorff-Nielsen, D. Cox,
and C. Kluppelberg, editors, Complex Stochastic Systems, chapter 2, pages 67–112. Chapman and Hall/CRC
Press, London/Boca Raton, 2000.
[Pearl, 1995] J. Pearl. Causal diagrams for empirical research. Biometrika, 82:669–710, December 1995.
[Pearl, 2000] J. Pearl. Causality: Models, Reasoning, and
Inference. Cambridge University Press, NY, 2000.
[Riccomagno and Smith, 2003] E. Riccomagno and J.Q.
Smith. Non-graphical causality: a generalization of the
concept of a total cause. Technical Report No. 394, Department of Statistics, University of Warwick, 2003.
[Riccomagno and Smith, 2004] E. Riccomagno and J.Q.
Smith. Identifying a cause in models which are not simple bayesian networks. In Proceedings of IPMU, pages
1315–1322, Perugia, 2004.
[Robins and Wasserman, 1997] James M. Robins and
Larry A. Wasserman. Estimation of effects of sequential
treatments by reparameterizing directed acyclic graphs.
In Proceedings of the Thirteenth Annual Conference on
Uncertainty in Artificial Intelligence (UAI–97), pages
409–420, San Francisco, CA, 1997. Morgan Kaufmann
Publishers.
[Spirtes et al., 2001] P. Spirtes, C. Glymour, and
R. Scheines.
Causation, Prediction, and Search.
MIT Press, Cambridge, MA, 2001.
[Tian and Pearl, 2002] J. Tian and J. Pearl. On the testable
implications of causal models with hidden variables. In
Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI), 2002.
[Verma and Pearl, 1990] T. Verma and J. Pearl. Equivalence and synthesis of causal models. In P. Bonissone et al., editor, Uncertainty in Artificial Intelligence
6, pages 220–227. Elsevier Science, Cambridge, MA,
1990.

