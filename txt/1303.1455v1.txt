12

Pearl

From Conditional Oughts to Qualitative Decision Theory

Judea Pearl

Cognitive Systems Laboratory
University of California, Los Angeles, CA 90024
judea@cs. ucla. edu
Content Areas: Commonsense Reasoning,
Probabilistic Reasoning, Reasoning about Action

Abstract
The primary theme of this investigation is a deci­
sion theoretic account of conditional ought statements
(e.g., "You ought to do A, if C") that rectifies glaring
deficiencies in classical deontic logic. The resulting ac­
count forms a sound basis for qualitative decision the­
ory, thus providing a framework for qualitative plan­
ning under uncertainty. In particular, we show that
adding causal relationships (in the form of a single
graph) as part of an epistemic state is sufficient to
facilitate the analysis of action sequences, their conse­
quences, their interaction with observations, their ex­
pected utilities and, hence, the synthesis of plans and
strategies under uncertainty.
1

INTRODUCTION

In natural discourse, "ought" statements reflect two
kinds of considerations: requirements to act in ac­
cordance with moral convictions or peer's expecta­
tions, and requirements to act in the interest of one's
survival, namely, to avoid danger and pursue safety.
Statements of the second variety are natural candi­
dates for decision theoretic analysis, albeit qualitative
in nature, and these will be the focus of our discus­
sion. The idea is simple. A sentence of the form
"You ought to do A if C" is interpreted as shorthand
for a more elaborate sentence: "If you observe, be­
lieve, or know C, then the expected utility resulting
from doing A is much higher than that resulting from
not doing A". 1 The longer sentence combines several
modalities that have been the subjects of AI investiga­
tions: observation, belief, knowledge, probability( "ex­
pected"), desirability("utility"), causation( "resulting
from"), and, of course, action( "doing A"). With the
exception of utility, these modalities have been for­
mulated recently using qualitative, order-of-magnitude
abstractions of probability theory(Goldszmidt & Pearl
1992, Goldszmidt 1992). Utility preferences them1 An alternative interpretation, in which "doing A" is
required to be substantially superior to both "not doing A"
and "doing not-A" is equally valid, and could be formulated
as a straightforward extension of our analysis.

selves, we know from decision theory, can be fairly
unstructured, save for obeying asymmetry and tran­
sitivity. Thus, paralleling the order-of-magnitude ab­
straction of probabilities, it is reasonable to score con­
sequences on an integer scale of utility: very desirable
(U = 0(1/t)), very undesirable (U = -0(1/t)), bear­
able (U = 0(1)), and so on, mapping each linguistic
assessment into the appropriate t 0(1/ti) utility rat­
ing. This utility rating, when combined with the in­
finitesimal rating of probabilistic beliefs (Goldszmidt
& Pearl 1992), should permit us to rate actions by the
expected utility of their consequences, and a require­
ment to do A would then be asserted iff the rating of
doing A is substantially (i.e., a factor of 1/t) higher
than that of not doing A.
This decision theoretic agenda, although conceptually
straightforward, encounters some subtle difficulties in
practice. First, when we deal with actions and conse­
quences, we must resort to causal knowledge of the do­
main and we must decide how such knowledge is to be
encoded, organized, and utilized. Second, while theo­
ries of actions are normally formulated as theories of
temporal changes (Shoham 1988, Dean & Kanazawa
1989), ought statements invariably suppress explicit
references to time, strongly suggesting that temporal
information is redundant, namely, it can be recon­
structed if required, but glossed over otherwise. In
other words, the fact that people comprehend, evalu­
ate and follow non-temporal ought statements suggests
that people adhere to some canonical, yet implicit as­
sumptions about temporal progression of events, and
that no account can be complete without making these
assumptions explicit. Third, actions in decision theory
predesignated explicitly to a few distinguished atomic
variables, while statements of the type "You ought to
do A" are presumed applicable to any arbitrary propo­
sition A.2 Finally, decision theoretic methods, espe­
cially those based on static influence diagrams, treat
both the informational relationships between observa­
tions and actions and the causal relationships between
actions and consequences as instantaneous (Chapter
6, Shachter 1986, Pearl 1988). In reality, the effect of
2This has been an overriding assumption in both the
deontic logic and the preference logiC literatures.

From Conditional Oughts to Qualitative Decision Theory

our next action might be to invalidate currently ob­
served properties, hence any non-temporal account of
ought must carefully distinguish properties that are
influenced by the action from those that will persist
despite the action, and must explicate therefore some
canonical assumptions about persistence.
These issues are the primary focus of this paper. We
start by presenting a brief introduction to infinites­
imal probabilities and showing how actions, beliefs,
and causal relationships are represented by ranking
functions �e(w) and causal networks r (Section 2). In
Section 3 we present a summary of the formal results
obtained in this paper, including an assertability crite­
rion for conditional oughts. Sections 4 and 5 explicate
the assumptions leading to the criterion presented in
Section 3. In Section 4 we introduce an integer-valued
utility ranking JJ(w) and show how the three compo­
nents, �e(w), r, and J.t(w), permit us to calculate, semi­
qualitatively, the utility of an arbitrary proposition <p,
the utility of a given action A, and whether we ought to
do A. Section 5 introduces conditional oughts, namely,
statements in which the action is contingent upon ob­
serving a condition C. A calculus is then developed
for transforming the conditional ranking �e(wiC) into
a new ranking KA(wiC), representing the beliefs an
agent will possess after implementing action A, hav­
ing observed C. These two ranking functions are then
combined with J.t(w) to form an assertability criterion
for the conditional statement O(AIC) : "We ought to
do A, given C". In Section 6 we compare our formu­
lation to other accounts of ought statements, in par­
ticular deontic logic, preference logic, counterfactual
conditionals, and quantitative decision theory.
2

INFINITESIMAL
PROBABILITIES, RANKING
F UNCTIONS, CAUSAL
NETWORKS, AND ACTIONS

K(<p):::

{ minw �e(w)
oo

13

for w I= <p
for w I= -.<p

K(<plt/J)::: K(<p 1\ t/J)- �e(t/J)

(2)

2. (Stratified Rankings and Causal Networks (Gold­
szmidt & Pearl 1992)). A causal network is a directed
acyclic graph (dag) in which each node corresponds to
an atomic variable and each edge X; --+ Xi asserts
that X; has a direct causal influence on Xj. Such net­
works provide a convenient data structure for encoding
two types of information: how the initial ranking func­
tion �e(w) is formed, and how external actions would
influence the agent' s belief ranking �e(w). Formally,
causal networks are defined in terms of two notions:
stratification and actions.
A ranking function �e(w) is said to be stratified relative
to a dag r if
(3)
where pa;(w) are the parents of X; in f evaluated at
state w. Given a ranking function ��:(w), any edge­
minimal dag r satisfying Eq.(3), is called a Bayesian
network of ��:(w) ( Pearl 1988). A dag r is said to be a
causal network of �e(w) if it is a Bayesian network of
�e(w) and, in addition, it admits the following repre­
sentation of actions.
3. (Actions) The effect of an atomic action do(X; =
true) is represented by adding to r a link DO; --+
X;, where DO; is a new variable taking values in
{ do(x;), do(-,xi), idle} and x; stands for X; = true.
Thus, the new parent set of X; is pai = pa; U {DO;}
and it is related to X; by

{

�e(X;(w)lpai(w)) =
�e(X;(w)lpa;(w)) if DO;= idle
oo if DO;= do(y) and X;(w) f. y
0 if DO;= do(y) and X;(w)= y

(4)

1. (Ranking Functions). Let !1 be a set of worlds,
each world w E !1 being a truth-value assignment to
a finite set of atomic variables (X1, X2, ...,Xn) which
in this paper we assume to be hi-valued, namely,
X; E {true, false}. A belief ranking function �e(w)
is an assignment of non-negative integers to the ele­
ments of !1 such that �e(w) = 0 for at least one wE !1.
Intuitively, �e(w) represents the degree of surprise asso­
ciated with finding a world w realized, and worlds as­
signed 11:= 0 are considered serious possibilities. �e(w)
can be considered an order-of-magnitude approxima­
tion of a probability function P(w) by writing P(w) as
a polynomial of some small quantity f and taking the
most significant term of that polynomial, i.e.,
(1)
P(w) � CEK(w)

The effect of performing action do(x;) is to transform
�e(w) into a new belief ranking, �e.,,(w), given by

Treating f as an infinitesimal quantity induces a condi­
tional ranking function �e(<pl'l/>) on propositions which
is governed by Spohn's calculus ( Spohn 1988):

(ii) Fixing the value of pa; (by some appropriate
choice of actions) renders X; unaffected by any
external intervention do(x"'), 1e f. i.

�e(n)= o

��:.,,

(w) _

{ �e'(wldo(x;))
oo

for w I= x;
for w I= -.x;

(5)

where �e' is the ranking dictated by the augmented
network r U {DO;____. X;} and Eqs. (3) and (4).
This representation embodies the following aspects of
actions:

(i) An action do(xi) can affect only the descendants
of X; in r.

14

3

Pearl

SUMMARY OF RESULTS

The assertability condition we are about to develop
in this paper requires the specification of an epistemic
state ES = (K(w), r, p(w)) which consists of three
components:
ii:

(w) - an ordinal belief ranking function on n.
f - a causal network of K(w).
p (w) - an integer-valued utility ranking of worlds,
where p(w) ="± i assigns to w a utility
U(w) ="± 0(1/E1), i = 0, 1, 2, ....
The main results of this paper can be summarized as
follows:
1. Let W/ and W;- be the formulas whose models
receive utility ranking +i and -i, respectively, and let
K1(w) denote the ranking function that prevails after
establishing the truth of event r.p, where r.p is an arbi­
trary proposition (i.e., K'(....,r.p) = oo and K1(r.p) = 0).
The expected utility rank of r.p is characterized by two
integers
n+ = max;[O; i- K1(W/ 1\ r.p)]
(6)
n
max;[O; i- K1(W1- 1\ r.p)]
and is given by

{ ambiguous
n+ - n-

if n+ = n- > 0
otherwise
(7)
2. A conditional ought statement O(AIC) is assertable
in ES iff
Jl(A; KA(wlC)) > p(true; K(wlC))
(8)

p[(r.p; K1(w)]

=

where A and C are arbitrary propositions and the
ranking KA(wjC) (to be defined in step 3) represents
the beliefs that an agent anticipates holding, after im­
plementing action A, having observed C.
3. If A is a conjunction of atomic propositions, A =
1\jeJ Aj, where each Aj stands for either Xj = true
or Xj =false, then the post-action ranking II:A(wlC)
is given by the formula

2:: x:(X;(w)lpai(w)) +
iE�
(9)
millw' [.z...,. Si(w,w') + x:(w'IC)]
iflJ
where R is the set of root nodes and
if X;(w) =/; X;(w') and pai = 0
if X;(w) =/; X;(w'),pa; =/; 0 and
x:(....,�;(w)jpa;(w)) = 0
(10)
otherwise
S(w,w') represents persistence assumptions: It is sur­
prising (to degree s; � 1) to find X; change from its
pre-action value of Xi(w') to a post-action value of
Xi (w) if there is no causal reason for the change.
KA(wlC) = K(w)-

If A is a disjunction of actions, A = V1 A1, where each
A1 is a conjunction of atomic propositions, then
KA(wjC) = minKAr(wjC)
(11)
I

4

FROM UTILITIES AND BELIEFS
TO GOALS AND ACTIONS

Given a proposition r.p that describes some condition or
event in the world, what information is needed before
we can evaluate the merit of obtaining r.p, or, at the
least, whether 1{>1 is "preferred" to 1{>2? Clearly, if we
are to apply the expected utility criterion, we should
define two measures on possible worlds, a probabil­
ity measure P(w) and a utility measure U(w). The
first rates the likelihood that a world w will be real­
ized, while the second measures the desirability of w.
Unfortunately, probabilities and utilities in themselves
are not sufficient for determining preferences among
propositions. The merit of obtaining r.p depends on at
least two other factors: how the truth of r.p is estab­
lished, and what control we possess over which model
of r.p will eventually prevail. We will demonstrate the�e
two factors by example.
Consider the proposition r.p = "The ground is wet". In
the midst of a drought, the consequences of this state­
ment would depend critically on whether we watered
the ground(action) or we happened to find the ground
wet (observation). Thus, the utility of a pr op osition r.p
clearly depends on how we came to know r.p, by mere
observation or by willful action. In the first case, find­
ing r.p true may provide information about the natural
process that led to the observation r.p, and we should
change the current probability from P(w) to P(wlr.p).
In the second case, our actions may perturb the natu­
ral flow of events, and P(w) will change without shed­
ding light on the typical causes of r.p. We will denote
the probability resulting from externally enforcing the
truth of r.p by Pcp(w), which will be further explicated
in Section 5 in terms of the causal network r. 3
However, regardless of whether the probability func­
tion P(wjr.p) or Pcp(w) results from learning r.p, we are
still unable to evaluate the merit of r.p unless we un­
derstand what control we have over the opportuni­
ties offered by r.p. Simply taking the expected utility
U(r.p) = Ew[P(wjr.p)U(w)] amounts to assuming that
the agent is to remain totally passive until Nature se­
lects a world w with probability P(w jr.p), as in a game of
chance. It ignores subsequent actions which the agent
might be able to take so as to change this probability.
For example, event r.p might provide tb.e agent with the
option of conducting further tests so as to determine
with greater certainty which world would eventually
be realized. Likewise, in case r.p stands for "Joe went
to get his gun" , our agent might possess the wisdom
to protect itself by escaping in the next taxicab.
3The difference between P(wlrp) and P'l'(w) is precisely
the difference between conditioning and "imaging" ( Lewis

1973),

and between belief revision and belief update (Al­
chourron et.al. 1985, Katsuno & Mendelzon 1991, Gold­

szmidt & Pearl 1992). It also accounts for the difference
between indicative and subjunctive conditionals - a topic
of much philosophical discussion ( Harper et.al. 1980).

From Conditional Oughts to Qualitative Decision Theory

In practical decision analysis the utility of being in a
situation 'fi is computed using a dynamic programming
approach, which assumes that subsequent to realizing
'fi the agent will select the optimal sequence of actions
from those enabled by 'fl· This computation is rather
exhaustive and is often governed by some form of my­
opic approximation (Chapter 6, Pearl 1988). Ought
statements normally refer to a single action A, tac­
itly assuming that the choice of subsequent actions, if
available, is rather obvious and their consequences are
well understood. We say, for example, "You ought to
get some food", assuming that the food would subse­
quently be eaten and not be left to rot in the car. In
our analysis, we will make a similar myopic approxi­
mation, assuming either that action A is terminal or
that the consequences of subsequent actions (if avail­
able) are already embodied in the functions P(w) and
t..t{w). We should keep in mind, however, that the re­
sult of this myopic approximation is not applicable to
all actions; in sequential planning situations, some ac­
tions may be selected for the sole purpose of enabling
certain subsequent actions.
Denote by P'(w) the probability function that would
prevail after obtaining <p.4 Let us examine next how
the expected utility criterion U(<p) = �P'(w)U(w)
translates into the language of ranking functions.
Let us assume that U takes on values in
{-0(1/E), 0(1), +0(1/E)}, read as {very undesirable,
bearable, very desirable}. For notational simplic­
ity, we can describe these linguistic labels as a util­
ity ranking function J.L(w) that takes on the values
-1, 0, and +1, respectively. Our task, then, is to
evaluate the rank J.L('fl), as dictated by the expected
value of U(w) over the models of 'fi·
Let the sets of worlds assigned the ranks -1, 0, and +1
0
be represented by the formulas w-' W ' and w+ ' re­
spectively, and let the intersections of these sets with
0
'fi be represented by the formulas 'fl-, <p , and 'fl+,
respectively. The expected utility of 'fi is given by
- C_jf P'(W-) + C0 P'(W0) + C+fE P'(W+),
where C_, C0, and C+ are some positive coefficients.
Introducing now the infinitesimal approximation for
P', in the form of the ranking function K1, we obtain
if K1('f!-) = 0
and K1('fl+) > 0
if "'('fl-) > 0
0(1)
and K1('f!+) > 0
U('f!)
if "'('P-) > 0
+0(1/E)
and "'('P+) = 0
ambiguous if "'('P-) = 0
(12)
The ambiguous status reflects a state of conflict
U(<p) = -C_jf + C+fE, where there is a serious possi­
bility of ending in either terrible disaster or enormous
success. Recognizing that ought statements are of­
ten intended to avert such situations (e.g., "You ought

-0(1/E)

P'(w) = P(wi�p) in case �pis observed, and P'(w)
P"'(w) in case tp is enacted. In both cases P'(�p) 1.
4

=

=

15

to invest in something safer"), we may take a risk­
averse attitude and rank ambiguous states as low as
U = -0(1/E) (other attitudes are, of course, perfectly
legitimate). This attitude, together with "'('fl) = 0,
yields the desired expression for J.L( lfli K1(w)):

(13)
The three-level utility model is, of course, only a
coarse rating of desirability. In a multi-level model,
where W/ and W;- are the formulas whose models
receive utility ranking +i and -i, respectively5, and
i = 0, 1, 2, ... , the ranking of the expected utility of
'fi is given by Eq. (7) (Section 3).
Having derived a formula for the utility rank of an
arbitrary proposition <p, we are now in a position to
formulate our interpretation of the deontic expression
O(AIC): "You ought to do A if C, iff the expected
utility associated with doing A is much higher than
that associated with not doing A". We start with a
belief ranking K(w) and a utility ranking J.L(w), and
we wish to assess the utilities associated with doing
A versus not doing A, given that we observe C. The
observation C would transform our current 1\(w) into
K(w!C). Doing A would further transform K(w!C)
into K1(w) = KA(w!C), while not doing A would ren­
der K(wiC) unaltered, so K1(w) = 1\(wiC). Thus,
the utility rank associated with doing A is given by
J.L(A; K�(w!C)), while that associated with not doing
A is given by J.L(C; K(wiC)) = J.L(true; K(w!C). Con­
sequently, we can write the assertability criterion for
conditional ought as
O(AIC) iff J.L(A; KA(wiC)) > J.L(true; K(wiC)) (14)
where the function J.L( 'Pi K(w)) is given in Eq. (13).
We remark that the transformation from K(wiC) to
x:A(wiC) requires causal knowledge of the domain,
which will be provided by the causal network r (Sec­
tion 5). Once we are given r it will be convenient to
encode both K(w) and J.L(w) using integer-valued labels
on the links of f. Moreover, it is straightforward to
apply Eqs. (7) and (14) to the usual decision theo­
retic tasks of selecting an optimal action or an opti­
mal information-gathering strategy (Chapter 6, Pearl
1988).
Example 1:

To demonstrate the use of Eq. (14), let us examine the
assertability of "If it is cloudy you ought to take an
umbrella" (Boutilier 1993). We assume three atomic
propositions, c - "Cloudy", r - "Rain", and u - "Hav­
ing an Umbrella", which form eight worlds, each corre­
sponding to a complete truth assignment to c, r, and u.
51n practice, the specification of U ( w) is done 'by defin­

ing an integer-valued variable V

( connoting

"value" ) as a

function of a select set of atomic variables. Wi would cor­
respond then to the assertion V = i, i = 0, 1, 2, ....

16

Pearl

To express our belief that rain does not normally occur
in a dear day, we assign a ,.. value of 1 (indicating one
unit of surprise) to any world satisfying r 1\ -,c and a
11: value of 0 to all other worlds (indicating a serious
possibility that any such world may be realized) . To
express the fear of finding ourselves in the rain with­
out an umbrella, we assign a I' value of -1 to worlds
satisfying r 1\ -,u and a I' value of 0 to all other worlds.
Thus, w+ = false, W0= ...,(rf\-,u),and w-= rf\-,u.
In this simple example, there is no difference between
KA(w) and ��:(w!A) because the act A = "Taking an
umbrella" has the same implications as the finding
"Having an umbrella". Thus, to evaluate the two ex­
pressions in Eq. (14), with
u and C = c, we first
note that

A=

1C(W� ju,c)= ��:(r 1\ -,uju, c)= oo
��:(wso

v w+ lu, c)

= 00

p(u; IC(w!u,c))= 0

Similarly,
hence

(15)
J.l(c; ��:(wlc)) -1
Thus, 0(u!c) is assertable according to the criterion of
Eq. (14).

=

Note that although IC(w) does not assume that nor­
mally we do not hav� an umbrella with us ( IC (u) > 0),
the advice to take an umbrella is still assertable, since
leaving u to pure chance might result in harsh conse­
quences (if it rains).
Using the same procedure, it is easy to show that the
example also sanctions the assertability of 0( -,ric, -,u),
which stands for "If it is cloudy and you don't have an
umbrella, then you ought to undo (or stop) the rain".
This is certainly useless advice, as it does not take into
account one's inability to control the weather. Con­
trollability information is not encoded in the ranking
functions 11: and J.li it should be part of one's causal
theory and can be encoded in the language of causal
networks using costly preconditions that, until satis­
fied, would forbid the action do(A) from having any
effect on A.6
5

COMBINING ACTIONS AND
OBSERVATIONS

In this section we develop a probabilistic account for
the term ICA(w!C), which stands for the belief ranking
6In decision theory it is customary to attribute direct
costs to actions, which renders tt(w) action-dependent. An
alternative, which is more convenient when actions are not
enumerated explicitly, is to attribute costs to precondi­
tions that must be satisfied before ( any ) action becomes
effective.

r
Post-action Network
I

'
r
Pre-action Network

x
x2�
········· · --�·-·· x
·l·················�
········• ···· ···················-�x--z

1

·-···· ��r��������:::���:���� -��--·ill'
x
: x
4

:

A

····

x
3

3x

=

IT.u

Figure 1: Persistence interactions between two causal
networks
that would prevail if we act A after observing C, i.e.,
the A-update of k(w !C). First we note that this up­
date cannot be obtained by simply applying the up­
date formula developed in (Eq. (2.2), Goldszmidt &
Pearl 1992),

ICA(w) =

{ :w)- IC(AjpaA(w))

�

� �A

(16)

where paA(w) are the parents (or immediate causes)
of A in the causal network r evaluated at w. The
formula above was derived under the assumption that
r is not loaded with any observations (e.g., C) and
renders "A(w) undefined for worlds w that are excluded
by previous observations and reinstated by A.
To motivate the proper transformation from IC(w) to
ICA(w!C), we consider two causal networks, r' and r
respectively representing the agent's epistemic states
before and after the action (see Figure 1). Although
the structures of the two networks are almost the same
(r contains additional root nodes representing the ac­
tion do(A)), it is the interactions between the corre­
sponding variables that determine which beliefs are
going to persist in r and which are to be "clipped" by
the influence of action A.
Let every variable Xi in f' be connected to the corre­
sponding variable X; in r by a directed link XI --+ X;
that represents persistence by default, namely, the nat­
ural tendency of properties to persist, unless there is
a cause for a change. Thus, the parent set of each X;
in r has been augmented with one more variable: Xi.
To specify the conditional probability of X;, given its
new parent set {pax; u xn, we need to balance the
tendency of X; to persist (i.e., be equal t o Xi) against
its tendency to obey the causal influence exerted by
pax;. We will assume that persistence forces yield to
causal forces and will perpetuate only those properties
that are not under any causal influence to terminate.
In terms of ranking functions, this assumption reads:

{

��:(X;(w)jpa;(w), XI(w')) =
if pa;= 0 and X;(w) 'f= X;(w')
s;
s; + ��:(X;(w)lpa,(w)) if X;(w) # x;(w') and
��:(-,X;(w)jpa,(w)) = 0
otherwise
��:(X;(w)lpa;(w))
(17)
where w' and w specify the truth values of the variables
in the corresponding networks, f' and f, and Si 2: 1 is

From Conditional Oughts to Qualitative Decision Theory

a constant characterizing the tendency of X; to persist.
Eq.(17) states that the past value of X; may affect the
normal relation between X; and its parents only when
it differs from the current value and, at the same time,
the parents of X; do not compel the change. In such
a case, the inequality X;(w) =/= XHw') contributes s;
units of surprise to the normal relation between X; and
its parents.7 The unique feature of this model, unlike
the one proposed in(Goldszmidt & Pearl 1992), is that
persistence defaults can be violated by causal factors
without forcing us to conclude that such factors are
abnormal.
Eq. (17) specifies the conditional rank �e(XIpax) for
every variable X in the combined networks and, hence,
it provides a complete specification of the joint rank
�e(w,w').8 The desired expression for the post-action
ranking KA(w) can then be obtained by marginalizing
�e(w,w') over w':

KA(w) = min�e(w,w')
'

(18)

w

need, however, to account for the fact that some
variables in network r are under the direct influence
of the action A, and hence the parents of these nodes
are replaced by the action node do(A). If A con­
sists of a conjunction of atomic propositions, A =
1\jEJ Aj, where each Aj stands for either X j = true
or Xj = false, then each X;, i E J, should be ex­
empt from incurring the spontaneity penalty speci­
fied in Eq. (17). Additionally, in calculating K(w, w')
we need to sum �e(X;(w)lpa;(w),Xt(w')) only over
i rf. J, namely, over variables not under the direct
influence of A. Thus, collecting terms and writing
�e(w) = I:; �e(X;(w)lpa;(w)), we obtain

Eq. (19) demonstrates that the effect of observations
and actions can be computed as an updating opera­
tion on epistemic states, these states being organized
by a fixed causal network, with the only varying el­
ement being �e, the belief ranking. Long streams of
observations and actions could therefore be processed
as a sequence of updates on some initial state, without
requiring analysis of long chains of temporally indexed
networks, as in Dean and Kanazawa (1989).
To handle disjunctive actions such as "Paint the wall
either red or blue" one must decide between two in­
terpretations: "Paint the wall red or blue regardless
of its current color" or "Paint the wall either red or
blue but, if possible, do not change its current color"
(see Katsuno & Mendelzon 1991 and Goldszmidt &
Pearl 1992). We will adopt the former interpretation,
according to which "do(A VB)" is merely a shorthand
for "do(A) V do(B)". This interpretation is particu­
larly convenient for ranking systems, because for any
two propositions, A and B, we have

We

tcA(wiC)

=

�e(w)-

L

i EJUR

minw'[L S;(w,w')

{

i�J

+

�e(w' IC))

(19)

where R is the set of root nodes and
s;
if X;(w) =/= X;(w') and pa; = 0
if X;(w) =/= X;(w'), pa; =/= 0 and
s;
S;(w,w')
tc(•X;(w)lpa;(w)) = 0
(20)
0 otherwise
7
This is essentially the persistence model used by Dean
and Kanazawa ( Dean & Kanazawa 1989), in which s; rep­

resents the survival function of X;. The use of ranking
functions allows us to distinguish crisply between changes

that are causally supported, ��:(-.X;(w)lpa;(w)) > 0, and
those that are unsupported, �(-.X;(w)lpa;(w)) = 0.
8
The expressions, familiar in probability theory,

P(w,w') =II P(X3(w,w')ipa1(w,w')), P(w)
j

=

��:(w,w')

=

L �(XJ(w,w'))lpaj(w,w')), ��:(w)
i

where j ranges over all variables in

=

min[�e(A); �e(B)]

(21)

Thus, if we do not know which action, A or B, will
be implemented but consider either to be a serious
possibility, then

KAvn(w)

=

min[�eA(w); �en(w)]

(22)

Accordingly, if A is a disjunction of actions, A =
V1 A1, where each A1 is a conjunction of atomic propo­
sitions, then
(23)
2

To demonstrate the interplay between actions and ob­
servations, we will test the assertability of the following
dialogue:
Robot 1: It is too dark in here.
Robot 2: Then you ought to push the switch up.
Robot 1: The switch is already up.
Robot 2: Then you ought to push the switch down.
The challenge would be to explain the reversal of the
"ought" statement in response to the new observation
"The switch is already up". The inferences involved
in this example revolve around identifying the type of
switch Robot 1 is facing, that is whether it is normal
(n ) or abnormal ( •n) (a normal switch is one that
should be pushed up (u) to turn the light on(/)). The
causal network, shown in Figure 2, involves three vari­
ables:

L P(w,w') L- the current state of the light (I vs •I) ,
w'

translate into the ranking expressions

�e(A VB)

Example

�e(X;(w)lpa;(w)) +

17

S- the current position of the switch (u vs •u) , and
T - the type of switch at hand ( vs •n ) .
n

�i,n �(w,w') The variable L stands in functional relationship to S
and T, via
(24)
the two networks.
=

18

Pearl

or, equivalently, k = oo unless I satisfies the relation
above.
Since initially the switch is believed to be normal,
we set K( -.n) = 1, resulting in the following initial
ranking:
T L
s
u
n
0
0
n -.I
1
..,n -,[
-.n I
1

T: Type of Switch

S: Switch Position

n(normal)
...,n (abnormal)

u (up)
-.u(down)

�

/

I (on)
-./(not on)

Figure 2: Causal network for Example 2
We also assume that Robot 1 prefers light to darkness,
by setting

{ -10

if w F= -.1
if wf=l

{

0
1

oo

for w = -,u 1\ n 1\ -./
for w=u/\-.n/\..,1 (26)
for all other worlds

To evaluate KA(wjC) for A = u, we now invoke Eq.
(19), using the spontaneity functions

Sr(w,w') = 1 if T(w) # T(w')
(27)
SL(w,w') = 0 if L(w) # L(w')
because L(w), being functionally determined by
paL(w) is exempt from co!lforming to persistence de­
faults. Moreover, for action A = u we also have
K(ujpaA) = K(u) = 0, hence
KA(wjC) = K(w) - K(T(w))
min {I[T(w) :f; T(w')] + K(w'IC)},
w'=w i ,w�

(28)
for w= w1, w2
where I[p] equals 1 (or 0) if pis true (or false), and

w1
u 1\ n 1\ l
w2 = u 1\ 1\ -,[
-.n

wi = -.u 1\ n 1\ -.1 (29)
w� = u 1\ A ..,[
-.n

All other worlds are excluded by either A = u or

C= -./.

(30)

We see that w2 = u A -,n A -./ is penalized with one
unit of surprise for exhibiting an unexplained change
in switch type (initially believed to be normal).
It is worth noting how w1, which originally was ruled
out (with K = oo) by the observation -./, is suddenly
reinstated after taking the action A = u. In fact, Eq.
(19) first restores all worlds to their original K(w) value
and then adjusts their value in three steps. First it
excludes worlds satisfying -,A, then adjusts the K(w) of
the remaining worlds by an amount K(AjpaA(w)), and
finally makes an additional adjustment for violation of
persistence.
sertability criterion of Eq. (14) and the first statement,
"You ought to push the switch up", is justified. At this
point, Robot 2 receives a new piece of evidence: S = u.
As a result, ,;,(wj-.1) changes to ,;,(wj-.1, u ) and the cal­
culation of K:A(wjC) needs to be repeated with a new
set of observations, C =-./Au. Since ,;,(w'j-.1, u) per­
mits only one possible world w' = u A ..,n A ..,z, the
minimization of Eq. (19) can be skipped, yielding(for
A= -.u)

(25)

The first statement of Robot 1 expresses an observa­
tion C=-./, yielding

K(wjC) =

for w = w1
for w = w2

{ 10

KA(wjC)

From Eqs. (26) and (28), we see that K:A(ljC) = 0 <
��::(ljC) = oo, hence the action A = u meets the as­

L: Light

J-t(w)

Minimizing Eq. (19) over the two possible w' worlds,
yields

{ 01 forfor

w
w

= -,u A ..,n ,\ l
= -.u A n A ...,[ (31)

which, in turn, justifies the opposite "ought" state­
ment ( "Then you ought to push the switch down").
Note that although finding a normal switch is less
surprising than finding an abnormal switch, a spon­
taneous transition to such a state would violate per­
sistence and is therefore penalized by obtaining a K
of 1.
6

RELATIONS TO OTHER
ACCOUNTS

6.1

DEONTIC AND PREFERENCE
LOGICS

Ought statements of the pragmatic variety have been
investigated in two branches of philosophy, deontic
logic and preference logic. Surprisingly, despite an
intense effort to establish a satisfactory account of
"ought" statements (Von Wright 1963, Van Fraassen
1973, Lewis 1973), the literature of both logics is
loaded with paradoxes and voids of principle. This
raises the question of whether "ought" statements are
destined to forever elude formalization or that the
approach taken by deontic logicians has been misdi­
rected. I believe the answer involves a combination of
both.

From Conditional Oughts to Qualitative Decision Theory

Philososphers hoped to develop deontic logic as a sep­
arate branch of conditional logic, not as a synthetic
amalgam of logics of belief, action, and causation.
In other words, they have attempted to capture the
meaning of "ought" using a single modal operator 0( ) ,
instead of exploring the couplings between "ought"
and other modalities, such as belief, action, causation,
and desire. The present paper shows that such an
isolationistic strategy has little chance of succeeding.
Whereas one can perhaps get by without explicit refer­
ence to desire, it is absolutely necessary to have both
probabilistic knowledge about the effect of observa­
tions on the likelihood of events and causal knowledge
about actions and their consequences.
·

We have seen in Section 3 that to ratify the sentence
"Given C, you ought to do A", we need to know not
merely the relative desirability of the worlds delineated
by the propositions A 1\ C and -.A 1\ C, but also the
feasibility or likelihood of reaching any one of those
worlds in the future, after making our choice of A. We
also saw that this likelihood depends critically on how
C is confirmed, by observation or by action. Since this
information cannot be obtained from the logical con­
tent of A and C, it is not surprising that "almost ev­
ery principle which has been proposed as fundamental
to a preference logic has been rejected by some other
source" {Mullen 1979).
In fact, the decision theoretic account embodied in Eq.
{14) can be used to generate counterexamples to most
of the principles suggested in the literature, simply by
selecting a combination of "'• JJ, and r that defies the
proposed principle. Since any such principle must be
valid in all epistemic states and since we have enor­
mous freedom in choosing these three components, it
is not surprising that only weak principles, such as
O(AJC ) =:::} -.0(-.AJC), survive the test. Among the
few that do survive, we find the sure-thing principle:
(32)
O(AJC) 1\ O(AJ-.C ) =:::} O(A)
read as "If you ought to doA given C and you ought
to doA given -.C, then you ought to do A without
examining C". But one begins to wonder about the
value of assembling a logic from a sparse collection of
such impoverished survivors when, in practice, a full
specification of,, '"'' and r would be required.
6.2

COUNTERF ACTU A L CONDITION A LS

Stalnaker {1972) was the first to make the connection
between actions and counterfactual statements, and he
proposed using the probability of the counterfactual
conditional(as opposed to the conditional probability,
which is more appropriate for indicative conditionals)
in the calculation of expected utilities. Stalnaker's the­
ory does not provide an explicit connection between
subjunctive conditionals and causation, however. Al­
though the selection function used in the Stalnaker­
Lewis nearest-world semantics can be thought of as a
generalization of, and a surrogate for, causal knowl­
edge, it is too general, as it is not constrained by the

19

basic features of causal relationships such as asym­
metry, transitivity, and complicity with temporal or­
der. To the best of my knowledge, there has been no
attempt to translate causal sentences into specifica­
tions of the Stalnaker-Lewis selection function.9 Such
specifications were partially provided in (Goldszmidt
& Pearl 1992), through the imaging function w*(w),
and are further refined in this paper by invoking the
persistence model {Eq. {19)). Note that a directed
acyclic graph is the only ingredient one needs to add
to the traditional notion of epistemic state so as to
specify a causality-based selection function.
From this vantage point, our calculus provides, in
essence, a new account of subjunctive conditionals that
is more reflective of those used in decision making. The
account is based on giving the subjunctive the follow­
ing causal interpretation: "Given C, if I were to per­
form A, then I believe B would come about", written
A > BJC, which in the language of ranking function
reads
(33)
K{-.BJC) = 0 and "-A(-.BJC) > 0
The equality states that -.B is considered a serious
possibility prior to performing A, while the inequal­
ity renders -.B surprising after performing A. This
account, which we call Decision Making Conditionals
(DMC), avoids several paradoxes of conditional log­
ics (see Nute 1992) and is further described in (Pearl
1993).
6.3

OTHER D ECISION THEOR ETIC
ACCOUNTS

Poole (1992) has proposed a quantitative decision­
theoretic account of defaults, taking the utility of A,
given evidence e, to be
JJ(AJe) =

:Ew

JJ(w,A)P(wJe)

(34)

This requires a specification of an action-dependent
preference function for each (w,A) pair. Our proposal
(in line with (Stalnaker 1972)) attributes the depen­
dence of 1-' on A to beliefs about the possible conse­
quences of A, thereby keeping the utility of each conse­
quence constant. In this way, we see more clearly how
the structure of causal theories should affect the choice
of actions. For example, suppose A and e are incom­
patible ("If the light is on ( e) , turn it off (A)"), taking
(34) literally (without introducing temporal indices)
would yield absurd results. Additionally, Poole's is a
calculus of incremental improvements of utility, while
9Gibbard and Harper ( Gibbard & Harper 1980) develop
a quantitative theory of rational decisions that is based
on Stalnaker's suggestion and explicitly attributes causal
character to counterfactual conditionals. However, they
assume that probabilities of counterfactuals are given in
advance and do not specify either how such probabilities
are encoded or how they relate to probabilities of ordinary
propositions. Likewise, a criterion for accepting a counter­
factual conditional, given other counterfactuals and other
propositions, is not provided.

20

Pearl

ours is concerned with substantial improvements, as is
typical of ought statements.
Boutilier (1993) has developed a modal logic account
of conditional goals which embodies considerations
similar to ours. It remains to be seen whether causal
relationships such as those governing the interplay
among actions and observations can easily be encoded
into his formalism.
7

CON CLUSION

By pursuing the semantics of ought statements this pa­
per develops an account of qualitative decision theory
and a framework for qualitative planning under uncer­
tainty. The two main features of this account are:
1. Order-of-magnitude specifications of probabilities
and utilities are combined to produce qualitative ex­
pected utilities of actions and consequences, condi­
tioned on observations(Eq.(7)).
2. A single causal network, combined with universal
assumptions of persistence is sufficient for specifying
the dynamics of beliefs under any sequence of actions
and observations (Eq. (9)).
Acknowledgements

This work benefitted from discussions with Craig
Boutilier, Adnan Darwiche, Moises Goldszmidt, and
Sek-Wah Tan. The research was partially supported
by Air Force granb #AFOSR 90 0136, NSF grant
#IRI-9200918, Northrop Micro grant #92-123, and
Rockwell Micro grant #92-122.
References

Alchourron, C., Gardenfors, P., and Makinson, D.,
"On the logic of theory change: Partial meet con­
traction and revision functions," Journal of Sym­
bolic Logic, 50, 510-530, 1985.
Boutilier, C., "A modal characterization of defeasi­
ble deontic conditionals and conditional goals,"
Working Notes of the AAAI Spring Symposium
Series, Stanford, CA, 30-39, March 1993.
Dean, T., and Kanazawa, K., "A model for reasoning
about persistence and causation," Computational
Intelligence, 5, 142-150, 1989.
Gibbard, A., and Harper, L., "Counterfactuals and
two kinds of expected utility," in Harper, L., Stal­
naker, R., and Pearce, G. (Eds.), Ifs, D. Reidel
Dordrecht, Holland, 153-190, 1980.
Goldszmidt, M., "Qualitative probabilities: A nor­
mative framework for commonsense reasoning,"
Technical Report (R-190), UCLA, Ph.D. Disser­
tation, October 1992.
Goldszmidt, M., and Pearl, J., "Rank-based systems:
A simple approach to belief revision, belief up­
date, and reasoning about evidence and actions,"

in Proceedings of the 3rd International Confer­
ence on Knowledge Representation and Reason­
ing, Morgan Kaufmann, San Mateo, CA, 661-672,
October 1992.
Harper, L., Stalnaker, R., and Pearce, G.(Eds.), Ifs,
D. Reidel Dordrecht, Holland, 1980.
Katsuno, H., and Mendelzon, A.O., "On the differ­
ence between updating a knowledge base and re­
vising it," in Principles of Knowledge Represen­
tation and Reasoning: Proceedings of the 2nd In­
ternational Conference, Morgan Kaufmann, San
Mateo, CA, 387-394, 1991.
Lewis, D., Counterfactuals, Harvard University
Press, Cambridge, MA, 1973.
Nute, D., "Logic, conditional," in Stuart C. Shapiro
(Ed.), Encyclopedia of Artificial Intelligence, 2nd
Edition, John Wiley, New York, 854-860, 1992.
Mullen, J.D., "Does the logic of preference rest on a
mistake?", Metaphilosophy, 10, 247-255, 1979.
Pearl, J., Probabilistic Reasoning in Intelligent Sys­
tems, Morgan Kaufmann, San Mateo, CA, 1988.
Pearl, J., "From Adams' conditionals to default ex­
pressions, causal conditionals, and counterfactu­
als," UCLA, Technical Report(R-193), 1993. To
appear in Festschrift for Ernest Adams, Cam­
bridge University Press, 1993.
Poole, D., "Decision-theoretic defaults," 9th Cana­
dian Conference on AI, Vancouver, Canada, 190197, May 1992.
Shachter, R.D., "Evaluating influence diagrams," Op­
erations Research, 34, 871-882, 1986.
Shoham, Y., Reasoning About Change: Time and
Causation from the Standpoint of Artificial Intel­
ligence, MIT Press, Cambridge, MA, 1988.
Spohn, W., "Ordinal conditional functions: A dy­
namic theory of epistemic states," in W.L. Harper
and B. Skyrms(Eds.), Causation in Decision, Be­
lief Change, and Statistics, D. Reidel, Dordrecht,
Holland, 105-134, 1988.
Stalnaker, R., "Letter to David Lewis" in Harper,
L., Stalnaker, R., and Pearce, G. (Eds.), lfs, D.
Reidel Dordrecht, Holland, 151-152, 1980.
Van Fraassen, B.C., "The logic of conditional obliga­
tion," in M. Bunge (Ed.), Exact Philosophy, D.
Reidel, Dordrecht, Holland, 151-172, 1973.
Von Wright, G.H., The Logic of Preference, Uni­
versity of Edinburgh Press, Edinburgh, Scotland,
1963.

