317

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Making Sensitivity Analysis Computationally Efficient

Uffe Kj::erulff

Linda C. van der Gaag

Department of Computer Science
Aalborg University
Fredrik Bajers Vej 7, DK-9220 Aalborg
Denmark
uk@cs.auc.dk

Department of Computer Science
Utrecht University
P.O. Box 80.089, 3508 TB Utrecht
The Netherlands
linda@cs.uu.nl

Abstract

To investigate the robustness of the output
probabilities of a Bayesian network, a sensi­
tivity analysis can be performed. A one-way
sensitivity analysis establishes, for each of the
probability parameters of a network, a func­
tion expressing a posterior marginal proba­
bility of interest in terms of the parameter.
Current methods for computing the coeffi­
cients in such a function rely on a large num­
ber of network evaluations. In this paper, we
present a method that requires just a single
outward propagation in a junction tree for es­
tablishing the coefficients in the functions for
all possible parameters; in addition, an in­
ward propagation is required for processing
evidence. Conversely, the method requires
a single outward propagation for computing
the coefficients in the functions expressing all
possible posterior marginals in terms of a sin­
gle parameter. We extend these results to an
n-way sensitivity analysis in which sets of pa­
rameters are studied.

1

INTRODUC TION

The robustness of the output probabilities of a
Bayesian network can be investigated by performing
a sensitivity analysis of the network. For mathemat­
ical models in general, sensitivity analysis serves to
identify the effects of the inaccuracies in a model's pa­
rameters on its output (Morgan & Henrion 1990). For
a Bayesian network, more specifically, performing a
sensitivity analysis yields insight in the relation be­
tween the probability parameters of the network and
its posterior marginals. The simplest type of sensi­
tivity analysis is a one-way analysis in which a single
parameter is studied; a more general n-way analysis
serves to investigate the joint effects of inaccuracies in

a set of parameters.
In the brute-force approach to performing a one-way
sensitivity analysis of a Bayesian network, each proba­
bility parameter of the network is varied systematically
and the effect on the output probabilities of the net­
work is investigated. Performing a sensitivity analysis
in this way requires thousands of network evaluations,
or (full) propagations, and is, therefore, much too time
consuming to be of any practical use.
Laskey (1995) has been the first to address the compu­
tational complexity of sensitivity analysis of Bayesian
networks. She has introduced a method for computing
the partial derivative of a posterior marginal proba­
bility with respect to a parameter under study. Her
method thus yields a first-order approximation of the
effect of varying a single probability parameter on a
posterior marginal. Compared to the brute-force ap­
proach, her method requires considerably less compu­
tational effort. The method, however, provides insight
only in the effect of small variations of parameters;
when larger variations are considered, the quality of
the approximation may rapidly break down.
The relation between a posterior marginal probability
of interest and a parameter under study can be ex­
pressed through a simple mathematical function. The
function expressing the posterior marginal is a quo­
tient of two linear functions in the parameter, as has
been shown by Castillo et al. (1996). Building upon
this property, it suffices to establish the coefficients
in this function to determine the effect of parameter
variation. Castillo et al. (1997) and Coupe & van der
Gaag (1998) have designed methods to this end. These
methods require a single network evaluation for each
coefficient to be established. Although these methods
currently are the most efficient available, they rely on
a large number of network evaluations and, as a con­
sequence, are infeasible for large realistic networks.
In this paper, we present a new method for sensitivity
analysis of Bayesian networks. Our method, like the

318

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

two methods mentioned above, exploits the property
that a posterior marginal probability relates by a sim­
ple mathematical function to a parameter under study.
It requires just a single outward propagation in a junc­
tion tree, however, to compute the coefficients in the
functions for all possible parameters; in addition, it re­
quires an inward propagation for processing evidence.
Conversely, the method requires a single outward prop­
agation for establishing the coefficients in the functions
expressing all possible posterior marginals in terms of
a single parameter. Our method can be readily ex­
tended to an n-way sensitivity analysis in which sets
of parameters are varied.
In addition to a sensitivity analysis, an uncertainty
analysis can be performed for investigating the robust­
ness of the output probabilities of a Bayesian network.
In an uncertainty analysis, all parameters are varied si­
multaneously through sampling; it therefore provides
little insight into the effects of variation of specific pa­
rameters. Experiments with uncertainty analysis have
led to the suggestion that Bayesian networks are in­
sensitive to inaccuracies in their parameters (Pradhan
et al. 1996, Henrion et al. 1996). In these experiments,
however, a measure of model robustness was obtained
by assuming a lognormal distribution for each parame­
ter and averaging over the probability of the true diag­
nosis for various diagnostic situations in a medical ap­
plication. Rather than in the average of the probabili­
ties of the true diagnosis, however, it is in the variation
of these probabilities that inaccuracies in parameters
are reflected. From these experimental results, there­
fore, no decisive conclusions can be drawn as to the
sensitivity of Bayesian networks. In fact, Coupe et al.
(1999) have reported high sensitivities in an emprical
study in the medical domain, involving real patient
data. We feel that these and emerging similar expe­
riences warrant further investigation into sensitivity
analysis of Bayesian networks.

2

THE BASIC P ROPERT Y

Sensitivity analysis of a Bayesian network basically
amounts to establishing, for each of the network's pa­
rameters, a function expressing an output probabil­
ity in terms of the parameter under study. For out­
put probabilities, we shall consider posterior marginal
probabilities of the form y = p(a I e), where a is a
value of a variable A and e denotes the evidence avail­
able. Each of the network's parameters is of the form
x = p(bi l1r), where b; is a value of a variable B and
1r is an arbitrary combination of values of the set of
parents II= pa(B) of B. We will write p(ale)(x) to
denote the function expressing the posterior marginal
p(a I e) in terms of the parameter x.
In the sequel, we will assume that in a sensitivity anal­
ysis, upon varying a parameter x p(b; l1r), each of
the other probabilitiesp(bj l1r) is co-varied accordingly,
by scaling by the ratio between the probability masses
left. More formally, let the variable B have for its do­
main dom(B)
{b1,...,bm}, m 2: 1. Note that the
parameters p(bj l1r), j =f. i , are functions of x. We now
assume for these functions that
=

=

if j= i
otherwise,
with p(b; l1r)

<

(1)

1.

With the assumption of co-variation as outlined above,
the function y(x) yielded by a sensitivity analysis is a
quotient of two linear functions in x. The following
theorem reviews this important property; the associ­
ated proof provides the basis for the algorithms pre­
sented in Sections 4 and 5.
Theorem 1 Let p be the probability function defined
by a Bayesian network over a set of variables V. Let
y = p(a I e) and x p(b; l1r) be as indicated above.
Then,
=

The paper is organised as follows. Section 2 reviews
the important basic property that a posterior marginal
probability can be expressed as a quotient of two linear
functions in a parameter under study. In Section 3, we
briefly describe currently available methods for sensi­
tivity analysis that build upon this property. In Sec­
tion 4, we present our method for computing the co­
efficients in the functions for all possible parameters,
using just one propagation in a junction tree. In Sec­
tion 5, we describe a similar method for computing
the coefficients in the sensitivity functions relating all
possible posterior marginals to a single probability pa­
rameter. These methods are generalised to an n-way
sensitivity analysis in Section 6. The paper ends with
some concluding remarks in Section 7.

y=
where a, (3,

"(,

p(a,e)(x)
p(e)(x)

ax+ f3
"(X+ 8'

(2)

and 8 are constants with respect to x.

Proof: The joint probability p(a,e) can be expressed

in terms of x as

p(a,e}(x)

�

(�:{V)) (x),

where Lv:a,...,dp(V) denotes summation over the variables V\ {A, . . . , D} with A, ... , DE V fixed at values
a, ... , d, respectively.
The sum Lv:a,ep(V) in the above equation can be
split into n+ 1 separate sums, such that the first sum

319

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

includes only terms with the value b1 for B and the
state 1r for II, the second sum includes only terms with
the value b2 for B and II in state 1r, and so on, and
the last sum includes the remaining terms. So,

(1990), as described by Castillo et a!. (1997). After
having identified the set of relevant parameters, the
sensitivity analysis can be restricted to this set.
Building upon the set of n relevant parameters,
x1, . . . , Xn, the algorithm of Castillo et a!. (1997) iden­
tifies sets of monomials for which the coefficients will
be zero in the linear function p(a, e)(x1, . . . , Xn ) · For
the resulting m monomials, the algorithm constructs
a system of m independent equations of the form
i
y
p(a, e)(xi, . . . , x�), where, for each j, xj denote
arbitrary values for parameter Xj. The corresponding
values yi, i
1, ... , m, are obtained through m net­
work evaluations. The coefficients in the function are
now determined by solving the set of equations thus
obtained. Coupe & van der Gaag independently de­
scribed a similar method, also based on the idea of
solving a system of independent equations. They fur­
ther argue that in a one-way sensitivity analysis three
network evaluations suffice per relevant parameter.
=

=

p( )

" p(V).
" l:v:a,e,b;,11' V +
L
L
1-p(b·l7r)
'
V:a,e,n'f11'
j#i
For the probability p(e) we derive a similar expression
by summing, in the above derivation, over all values of
the variable A instead of keeping it fixed at a. From
the resulting expressions p(a, e)(x) and p(e)(x), it is
readily seen that the output y p(a I e) can be written
as a quotient of two functions that are linear in x. D
=

From Theorem 1 we have that the function that ex­
presses a posterior marginal probability y in terms
of a single parameter x is characterised by at most
three coefficients. The theorem is easily extended to
n parameters. The function then includes the prod­
ucts of all possible combinations of parameters, termed
monomials, in both its numerator and its denomina­
tor. The numerator as well as the denominator are
characterised by 2n coefficients, many of which may
be zero.
3

CURRENT METHODS

The most efficient methods for sensitivity analysis of
Bayesian networks currently available exploit the basic
property reviewed in the previous section. We briefly
review these methods.
Not all parameters in a Bayesian network can influ­
ence a posterior marginal probability of interest. The
subset of parameters (possibly) influencing the poste­
rior marginal is dependent upon the evidence e. The
set of relevant parameters is easily identified using a
variation of the algorithm described by Geiger et a!.

The methods reviewed above have a computational
complexity that is considerably less than the brute­
force approach of systematic variation of parameters.
However, the methods can still be quite time consum­
ing: for a network of realistic size, it can easily require
several hundreds of network evaluations to perform a
one-way sensitivity analysis. An more general n-way
sensitivity analysis to study the joint effect of simulta­
neous variation of n parameters can in fact be so time
consuming that it is infeasible in practice.
4

ANALYSIS OF ONE OUTPUT
WRT. ALL PARAMETERS

The new methods for sensitivity analysis presented in
this paper have been tailored to Bayesian networks in
their junction-tree representation. The methods basi­
cally perform a single or a few outward propagations
in a junction tree and, as a result, are much less time
consuming than the methods reviewed in the previous
section.
In this section, we present our method for computing
the coefficients in the sensitivity functions expressing
a posterior marginal of interest y p(a I e) in terms of
all possible probability parameters x. Recall that these
functions are of the form presented in Theorem 1. Our
method now builds on the idea that, in a junction tree,
the expressions for p(a, e) and p(e) in terms of x can be
derived from the potential of a clique containing both
the variable and the parents to which the parameter x
pertains. The following theorem details the coefficients
to be computed.
=

Theorem 2 Let p be the probability function defined
by a Bayesian network and let T be a junction tree

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

320

for the network. Let y = p(aie) and x = p(b; l1r) be
as before. Suppose that, in T, an inward propagation
has been performed towards a clique containing the
variable of interest A; suppose that subsequently an
outward propagation from this clique has been per­
formed with the value a for A. Now, let K be a
clique in T containing both the variable B and its par­
ents II= pa(B); let ¢x = p(K,a,e) be the potential
of clique K after the abovementioned propagations.
Then, p(a,e)(x)= ax+ (3 with

a=

LK:b; ,,. ¢x ""'LK:b; ,,. ¢x
p(b;l1r) - �1-p(b;l7r)'

(3)

J -r- 1

(3 = L

#i

LK:b; ,,. ¢x
+ L ¢x
.
1-p(b; 17r) K:ll-=f-11"

(4)

Proof: The property follows directly from the proof
of Theorem 1 by observing that p(a,e)= 2:: K ¢ K. D

Building upon similar observations, we have the fol­
lowing corollary.
Corollary 1 Letp be the probability function defined
by a Bayesian netwerk and let T be a junction tree for
the network. Let x = p(b;l1r) and K be as before.
Suppose that the evidence e has been processed in T
by an inward and subsequent outward propagation.
Let ¢'K = p(K,e) be the potential of clique K after
the propagation. Then, p(e)(x)= "(X+ 6 with

(5)

6

=

'K
""' 'K.
""'LK:b;,,. ¢
¢
+
�
� 1- p(b1·i7r)
j-=f-i
K:0-=/-1r

4. Compute the coefficients a and (3, using the equa­
tions (3) and (6) from Theorem 2, for all relevant
parameters, locally per clique.
We would like to note that our method requires just
one inward and two outward propagations to estab­
lish all sensitivity functions for a specific posterior
marginal, whereas the methods reviewed in the pre­
vious section require three inward and outward prop­
agations per parameter.
The method described above outlines the basic idea.
The method, however, may be easier to implement in
the alternative form based upon Theorems 3 and 4.
Theorem 3 Let the junction tree T be as before.
Also, let y = p(aie) and x = p(b;l1r) be as before
and let K be a clique in T including both B and
II = pa(B). Now, let x1 be the initially specified
value for x and let x2 denote an arbitrary other value
for x. Suppose that, in T, an inward propagation has
been performed towards a clique containing the vari­
able of interest A; suppose that subsequently an out­
ward propagation has been performed with the value a
for A. Now, let ¢x =p(K,a,e) be the resulting clique
potential for clique K. Let

y1 =p(a,e)(x1)= L ¢x,
p'(Bi1r)
y2=p(a,e)(x2)= ""'
¢K
'
�
p(B i
7r)

1. Enter the evidence e into the junction tree and
perform an inward and an outward propagation
using an arbitrary root clique.
2. Compute the coefficients 'Y and 6, using the equa­
tions (5) and (6) from Corollary 1, for all relevant
parameters, locally per clique.
3. Perform an outward propagation from a clique
containing the variable of interest A, with the ad­
ditional evidence A= a.

(8)

where p(B l1r) and p'(B l1r) denote parameter vectors
with x = x1 and x = x2, respectively. Then, y =
ax+ (3 with

(6)

Theorem 2 and Corollary 1 provide the basis for our
method for computing the coefficients in the func­
tions expressing the posterior marginal of interest
y =p(a I e) in terms of all possible parameters x. The
method is composed of the following steps:

(7)

K

(9)
Since both variable B and its parents are in­
cluded in clique K, we can obtain from the parameter
vector

Proof:

p(Bi1r)

= (q1(x1 ), ... , Qi- (x1 ), x1, Qi+l (x1 ), ...,Qn(x1))
1

the parameter vector
p'(Bi1r)

(q� (x2),... 'q�- (x2),x2,q�+l (x2),...,q�(x2))
1

where q and q' are parameters co-varying according
to equation (1), by multiplication of the potential ¢x
by p'(Bl1r)jp(Bl1r). From Theorem 1, we have that
y= ax+ (3. The expressions for a and (3 now follow
D
from simple mathematical manipulation.

321

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Let the junction tree T be as before.
Also, let y = p(afe) and x = p(bi f1r) be as before.
Suppose that, in T, an inward propagation has been
performed towards a clique including the variable of
interest A. Then, p(e)(x)= "(X + 8 with

9. Compute the coefficients
(10) from Theorem 4.

Theorem 4

"(= eta + a�a and 8 =f3a + f3�a1

(10)

where aa,f3a and a�a,f3�a are as in equation (9), ob­
tained from outward propagations with the evidence
A = a and A -::/:- a, respectively.
Proof:
We begin by observing that p(e)(x) =
p(a,e)(x) + p(•a,e)(x). By entering the evidence
A = a in a clique H containing the variable A
and propagating outwards, we obtain the potential
p(K, a, e) for clique K. From this potential,
¢K
p(a, e)
Y1.
LK cPK is readily computed, as de­
scribed in equations (7) from Theorem 3. Similarly,
by entering the evidence that A does not have the
value a (that is, by multiplying the clique potential
for H with a vector over dom(A), in which the en­
try corresponding to state a is zero and all other en­
tries equal 1) and propagating outwards, we obtain
the potential ¢�
p(K, •a, e). From this poten­
tial, Y�a = p(•a,e) = LK ¢� is readily computed.
Using equation (8) from Theorem 3, we get y� and
Y�a· Now, using equation (9), we find eta, a�a, f3a,
and f3�a· Inserting these coefficients into the expres­
sion p(e)(x)= p(a, e)(x) + p(•a, e)(x) yields the result
D
stated in the theorem.
=

=

=

=

Our alternative method, building upon Theorems 3
and 4, is composed of the following steps:
1. Enter the evidence e into the junction tree and
perform an inward propagation towards a clique
H containing the variable of interest A.
2. Perform an outward propagation from H with the
additional evidence A a.
=

3. Compute y;,_ and y�, using the equations (7)
and (8) from Theorem 3.
4. Compute the coefficients a = aa and f3 = f3a,
using (9), for all relevant parameters, locally per
clique.
5. Retract the evidence A
the evidence e.

=

a without retracting

6. Perform an outward propagation from H with the
additional evidence A -::/:- a.
7. Compute Y�a and Y�a' using the equations (7)
and (8) from Theorem 3.

and 8, using equation

To allow for retracting the evidence A = a in Step 5 of
our method without retracting e, fast-retraction prop­
agation (Cowell & Dawid 1992) is used in Step 2.
Comparing the computational costs of the two al­
ternative methods, we note that they both require
one inward and two outward propagations. Consider­
ing the first method, we observe that Steps 2 and 4
are equally costly. The computation of the coeffi­
cients a and 'Y costs 2 ·Jdom(K)J/m operations, where
m = fdom(pa(B))J; the computation of j3 and 8 costs
2fdom(K) I arithmetic operations. Thus, the addi­
tional cost of the first method is roughly in the or­
der of 2 to 3 times Jdom(K)f. The additional costly
steps in the second method are Steps 3 and 7, both
costing approximately 3 ·fdom(K)J operations. Thus,
the additional cost of the second method is roughly
6 fdom(K)f arithmetic operations. This rough com­
parison of the computational costs of the two methods
only addresses the number of arithmetic operations in­
volved. The first method, however, has a much larger
overhead in terms of computing indices in performing
the various summations. Thus, depending on the im­
plementation, the two methods might very well have
comparable performance.
·

5

ANALYSIS OF ALL OUTPUTS
WRT. ONE PARAMETER

Having identified a parameter x to which an output
probability of a Bayesian network is particularly sensi­
tive, one might be interested in establishing sensitivity
functions for all possible posterior marginals in terms
of this parameter. Such an analysis amounts to com­
puting the coefficients in these functions. Note that
while the method described in Section 4 provides for
evaluating the overall robustness of a Bayesian net­
work, the method described in this section is provides
for getting insight in the spread of influence from sep­
arate parameters.
The method of Castillo et a!. (1997) and the method of
Coupe & van der Gaag (1998) can be exploited for es­
tablishing the coefficients in the sensitivity functions
for all possible output probabilities, requiring three
propagations per posterior marginal. Building upon
the ideas put forward in the previous section, how­
ever, a more efficient method is obtained. Theorem 5
provides the basis for our method.
Let the junction tree T be as before.
Also, let y
p(afe) and x
p(bl1r) be as be­
fore. Suppose that, in T, an inward propagation has
Theorem 5

=

8. Compute a�a and f3�a, using (9).

'Y

=

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

322

been performed towards a clique containing the vari­
able B to which the parameter x pertains. Then,
p(e)(x) ="(X+ J with

'Y = O:a + O:�a and J =f3a + f3�a,

(11)

where O:a,f3a and a.�a,f3�a are as in equation (9), ob­
tained from two outward propagations with two dis­
tinct values for x.
Proof: Let K be a clique containing both the variable
B and its set of parents II = pa(B). Let x1 and x2
denote two different values of the parameter x. From
the outward propagation using x1 from clique K, we
obtain the probability vector p(A,e)(x1) = (y�, Y�a)
through marginalization of the clique potential for a
clique H containing A. Similarly, from the outward
propagation with x2, we find the vector p'(A,e)(x2) =

(y�, Y�a)·

From

6

n-WAY SENSITIVITY ANALYSIS

So far, we have addressed one-way sensitivity analy­
ses only, in which the effects of separate parameters
are studied. In this section, we turn our attention to
more general n-way analyses in which the effects of
simultaneous variation of n parameters are studied.
One can regardn-way sensitivity analysis as involving
analyses of joint effects for all subsets of size n or less
of all, say m, relevant parameters. Using the method
of Castillo et al. (1997), this would involve L:::�=l

( 7)

separate analyses and I::�=I ri
probability propa­
gations to compute the 2n coefficients, assuming r-ary
variables.

( 7)

Provided the n parameters all belong to the same
clique, Theorem 6 below states that we only need
one propagation to compute the 2n coefficients, but
I::�= I
local computations involving marginaliza­
tions of clique potentials.

( 7)

=

we get the result stated in the theorem.

0

Theorem 5 provides the basis for our method for com­
puting the coefficients in the functions expressing all
possible output probabilities y = p(a I e) in a single pa­
rameter x p(blrr). The method is composed of the
following steps:
=

1. Enter the evidence e into the junction tree and
perform an inward and an outward propagation
using an arbitrary root clique.
2. Compute the probability vector p(A,e )
(y�, Y�a) through marginalization of the clique po­
tential for a clique H containing A, for all vari­
ables of interest.
3. Change the value of parameter x and perform
an outward propagation from a clique containing
both the variable B and its parents.
4. Compute the probability vector p'(A,e) =
(y�, Y�a) through marginalization of the potential
for clique H, for all variables of interest.

In essence, Theorem 1 states that the mathematical
expression for a probability, p(e)(x), of a vector of in­
stantiations, e, as a function of a probability parame­
ter, x, takes the form of a linear function of x. The­
orem 6 generalizes this statement to the case with n
parameters, x1,. . . , Xn, and states that the resulting
function is a multilinear function in x1, ... , Xn.
To simplify the exposition, we shall assume that the
parameters are independent; that is, for each pair of
parameters, x; = p(bx; lrrx;) and Xj = p(bx; lrrx;), with
the associated variables Bx;, IIx;, Bx;, and IIx;, it
holds true that rrx; # rrx;, Bx; (j. IIx;, and Bx; (j.
IIx;. To generalize the theorem to cover the case of
dependent parameters is fairly straightforward.
Let p be the probability function for a
Bayesian network, where evidence e has been prop­
agated in a junction tree, T, for the network. Let
X
{ x1, . . . , Xn} be a set of parameters of the net­
work, where, for each i = 1, . . . ,n,

Theorem 6

=

X;= p(bx; lrrx;),
with the associated variables, Bx; and IIx;, being
members of a clique, C, in T. Then

p(e)(X) =

5. Compute a.a, a.�a, f3a, and f3�a, using the equation
(9) from Theorem 3.
6. Compute

'Y

and J, using the equation (11).

We would like to note that our method requires just
one inward and two outward propagations to estab­
lish all sensitivity functions for a specific probability
parameter.

L 'Yx(Z) II

z�x

z

+

zEZ

L

1/Jc,

C:ll#1r

where II= {IIx,,...,IIxn }, rr= (rrx,,...,rrxn ), and

'Yx(Z) = (-1)1X\ZI
where

fx(Y)

(-1)1X\YI

L !x(Y),

y�z

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

323

If, instead of summing over subsets S � X\Z, we sum
over the subsets of Z and takes care that the signs of
the terms are preserved, we get the desired result. D
where¢>c = p(C,e), W = X\Y
{w1, ,wk },
k = ! W I , by= (byp···,byiY1), b� = (b:n ,,... ,b: n1w1),
and Px, denotes the initial value of Xi.
=

•

•

•

Proof: Using the same procedure as in the proof of
Theorem 1 we get

p(e)(X)

(� )
(F � c,o�,�·�. /
L L
(X)

¢>c

=

c+

=

·

b'zt

�/c) (X)

c

·· P(b�1 !1l"x,)(xr) · · · p(b�n !1l"xn )(xn)
b'Zn

Lc.·b'zl ,... ,b':l!n ,1f ¢>c
p(b�,17l"x,) ...p(b�n 11l"xJ

+

L

C:0¥71"

¢>c.

L:: li

L:: ...

z

p(b�, !1ru,)(ur)

· .

flzEZ p(bz !1rz ) fluEU p(b� i1ru )
C:07"'7r

+

p(b�, l1ru,)(ur) · p(b�1u1 l1ru1u1)(uiUI)

=

uEU
=

II

· ·

p(b� 17ru)
p(b� 17ru)
-U
1-p(bul1ru)
1 -p(bu l1ru)

�;

(b� 1l"u
1 p( u I u)
uEU

�

)

II x (13)
(-1)ISI
.
xES
sr::;u

(L

)

Inserting (13) in (12) and rearranging terms yields
p(e)(X)

=

L IT L
z

Z<;;X zEZ S<;;U

n

,

p(e)(X)=

(-1)ISI

IT x

xES

L r(Z) II

Z<;;X

z

+

zEZ

J.

(15)

Through one-way analysis involving the parameter x
we obtain the constants ax and f3x in
=

axx + f3x·

The 2n constants in (15) are related to ax and f3x
in the way that ax equals the sum of all coefficients,
r (Z), for which xE Z, and f3x equals the sum of the
remaining coefficients. That is,
ax=

f3x

(12)

¢>c,

=

(

.

and

where U = X\ Z
{u1,... ,uiU I}. Now, expand­
ing the terms p(b� 17ru)(u), uE U, using (1), an easy
calculation yields

II

•

· p(b�1u111l"uiui)(uiUI)

Lc:bz,b;_,,11" ¢>c

L

•

p(e)(x)

The multiple sum can be grouped into sums over the
subsets Z � X such that b� = bz for each zE Z:
p(e)(X)

Note that, since the computation of /X (X) ranges over
all subsets of X, rx(Z) can be computed, for each
Z C X, as a sum over a subset of the terms involved
in the computation of 'Yx(X)
.
The result presented in Theorem 6 is limited in the
sense that all parameters under investigation must
belong to the same clique in the junction tree. We
shall now present a more general method which uti­
lizes the results obtained by lower-order analyses. Let
X n } be the
parameters under investi­
X { x1 ,
gation and write

=

L
L

r (Z)

(16)

r (Z) + J.

(17)

Z<;;X:xEZ

Z<;;X:x{lZ

... , n,

2

Thus, for each one-way analysis of a parameter Xi,
i
1,
we obtain equations of the form (16)
and (17). In addition, we obtain the equation
=

p(e)

=

L

Z<;;X

r (Z)

II Zo + J,

(18)

zEZ

2n

where z0 denote the value specified for parameter z in
the Bayesian network. This gives us a total of
+ 1
equations. However, we need (at least) 2n equations
to compute the 2n coefficients.
Now, if for each parameter, z E Z, we assign a new
value and perform a full propagation, then we obtain
an additional
+ 1 equations. Thus, we can obtain
at least 2n equations by performing

2n

2
l2n : 1J

full propagations with different parameter values, in
addition to the initial propagation performed for the

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

324

one-way analyses. For example, to perform 4-way
analyses, a total of two full propagations is sufficient.
This result can be generalized very easily, since each
m-way analysis gives rise to 2m equations of the form
(16) and (17). Thus, to perform n-way analyses, where
n > m, we need at most

( )

additional propagations, as there are ;:, relevant m­
way analyses. So, for example, if we have performed
2-way analyses and want to perform 5-way analyses,
no further propagations are needed.
7

CONC LUD ING REMARKS

We have presented methods for sensitivity analysis
of Bayesian networks which are significantly more ef­
ficient than current methods. In the case of one­
way analysis, the number of probability propagations
of current methods grows linearly in the number of
relevant parameters, whereas the methods presented
above only requires one inward and one or two out­
ward propagations, no matter the number of relevant
parameters.
To substantiate the importance of this difference, we
have investigated three real-world networks to get an
idea of the typical number of relevant parameters in
a realistic scenario. All three networks are from the
medical domain: a subnetwork of Munin (Andreassen
et a!. 1989) containing 1003 variables, a network mod­
elling the pathophysiology of ventricular septal defect
(Coupe et al. 1999) containing 38 variables, and a net­
work related to disorders in the oesophagus containing
70 variables. The investigation were conducted using
real patient data involving, respectively, 15, 5, and 3
patients. The average number of relevant parameters
were found to be 16313, 738, and 992, respectively.
(Since no censoring of parameters representing func­
tional relationships were performed on parameters for
the Munin network, the figure 16313 is probably some­
what overestimated.)
Efficient methods for sensitivity analysis play an im­
portant role in both the knowledge acquisition and the
validation phases for manually constructed Bayesian­
network models.
Coupe et al. (1999) reports on an empirical study using
sensitivity analysis to focus attention on the most in­
fluential parameters in the knowledge acquisition pro­
cess, thereby considerably reducing the time required
to acquire the parameter values.
The validation phase involves two aspects: fine-tuning
and robustness analysis. The fine-tuning aspect in-

volves adjustment of the parameter values to make the
network respond correctly to a number of test cases.
A gradient descent approach is useful for that purpose
(cf. neural network type training), where the gradi­
ent of a posterior marginal with respect to a subset of
parameters can easily be computed through a minor
modification of the algorithms for sensitivity analy­
sis. Based on the work described in the present paper,
Jensen (1999) has suggested a method for gradient de­
scent training of Bayesian networks.
Once a network has been fine-tuned, and thus responds
correctly on a selection of test cases, the robustness
of the network may be investigated. This involves,
in essence, determining lower and upper bounds for
parameter values for which the output of the network
still agrees with the test cases. A parameter value
close to one of the bounds indicate a possible lack of
robustness. Given analytic expressions for the outputs
in terms of the parameters, derived by e.g. methods
described in the present paper, these bounds are easily
determined.
The time complexity ofn-way sensitivity analysis may
be fairly high for large n, even with the methods pre­
sented in this paper. Also, our method assumes that
the variables and the parents associated with then pa­
rameters reside in the same clique in a junction tree.
The method of Coupe et al. (2000) for n-way sensitiv­
ity analysis is based on propagation of tables of coeffi­
cients in a junction tree, and, therefore, has a (poten­
tially very much) larger space requirement. However,
their method is general in the sense that it does not
put any restrictions on the location of the parameters.
During the initial phase of the
work, the authors received valuable comments from
Finn V. Jensen. Also, he suggested the basis for the
general method for n-way analysis, described at the
end of Section 6.
Acknowledgements

The research has been partly funded by the Danish
National Centre for IT research, Project no. 87.2.
References

Andreassen, S., Jensen, F. V., Andersen, S. K., Falck,
B., Kj<Brulff, U., Woldbye, M., S0rensen, A. R.,
Rosenfalck, A. & Jensen, F. (1989). MUNIN
- an expert EMG assistant, in J. E. Desmedt
(ed.), Computer-Aided Electromyography and Ex­
pert Systems, Elsevier Science Publishers B. V.
(North-Holland), Amsterdam, chapter 21.
Castillo, E., Gutierrez, J. M. & Hadi, A. S. (1996). A
new method for efficient symbolic propagation in
discrete Bayesian networks, Networks 28: 31-43.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Castillo, E., Gutierrez, J. M. & Hadi, A. S. (1997).
Sensitivity analysis in discrete Bayesian networks,
IEEE Transactions on Systems, Man, and Cyber­
netics 27(4): 412-423.
Coupe, V., Jensen, F. V., Kj<Erulff, U. & van der Gaag,
L. C. (2000). Efficient n-way sensitivity analysis
of Bayesian networks, Technical report, Depart­
ment of Computer Science, University of Utrecht,
The Netherlands.
Coupe, V. M. H., Peek, N., Ottenkamp, J. &
Habbema, J. D. F. (1999). Using sensitivity anal­
ysis for efficient quantification of a belief network,
Artificial Intelligence in Medicine 17: 223-247.
Coupe, V. M. H. & van der Gaag, L. C. (1998). Prac­
ticable sensitivity analysis of Bayesian belief net­
works, in M. Huskova, P. Lachout & J. A. Visek
(eds), Proceedings of the Joint Session of the 6th
Prague Symposium of Asymptotic Statistics and
the 13th Prague Conference on Information The­
ory, Statistical Decision Functions and Random
Processes, Union of Czech Mathematicians and
Physicists, pp. 81-86.
Cowell, R. G. & Dawid, A. P. (1992). Fast retrac­
tion of evidence in a probabilistic expert system,
Statistics and Computing 2: 37-40.
Geiger, D., Verma, T. & Pearl, J. (1990). Identify­
ing independence in Bayesian networks, Networks
20(5): 507-534. Special Issue on Influence Dia­
grams.
Henrion, M., Pradhan, M., del Favero, B., Huang, K.,
Provan, G. & O'Rorke, P. (1996). Why is diag­
nosis using belief networks insensitive to impreci­
sion in probabilities?, Proceedings of the Twelfth
Conference on Uncertainty in Artificial Intelli­
gence, Morgan Kaufmann Publishers, San Fran­
cisco, California, pp. 307-314.
Jensen, F. V. (1999). Gradient descent training of
Bayesian networks, Proceedings of the Fifth Eu­
ropean Conference on Symbolic and Quantitative
Approaches to Reasoning and Uncertainty, Lon­
don, England.
Laskey, K. B. (1995). Sensitivity analysis for prob­
ability assessments in Bayesian networks, IEEE
Transactions on Systems, Man, and Cybernetics
25: 901-909.
Morgan, M. & Henrion, M. (1990). Uncertainty, a
Guide to Dealing with Uncertainty in Quantita­
tive Risk and Policy Analysis, Cambridge Uni­
versity Press, Cambridge.

325

Pradhan, M., Henrion, M., Provan, G., Favero, B. D.
& Huang, K. (1996). The sensitivity of belief net­
works to imprecise probabilities: an experimental
investigation, Artificial Intelligence 85: 363-397.

