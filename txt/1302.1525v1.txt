54

Incremental Pruning: A Simple, Fast, Exact Method for
Partially Observable Markov Decision Processes

Anthony Cassandra

Michael L.

Nevin L. Zhang

Dept. of Computer Science
Duke University
Durham, NC 27708-0129

Computer Science Dept.
The Hong Kong U. of Sci. & Tech.
Clear Water Bay, Kowloon, HK

arc@cs.brown.edu

mlittman©lcs.duke.edu

lzhang@cs.ust .hk

Abstract

Most exact algorithms for general par­
tially observable Markov decision processes
(POMDPs) use a form of dynamic program­
ming in which a piecewise-linear and con­
vex representation of one value function is
transformed into another. We examine vari­
ations of the "incremental pruning" method
for solving this problem and compare them to
earlier algorithms from theoretical and em­
pirical perspectives. We find that incremen­
tal pruning is presently the most efficient ex­
act method for solving POMDPs.

1

Littman

Computer Science Dept.
Brown University
Providence, RI 02912

INTRODUCTION

Partially observable Markov decision processes
(POMDPs) model decision theoretic planning problems
in which an agent must make a sequence of decisions
to maximize its utility given uncertainty in the effects
of its actions and its current state (Cassandra, Kael­
bling, & Littman 1994; White 1991). At any moment
in time, the agent is in one of a finite set of possible
states S and must choose one of a finite set of possible
actions A. After taking action a E A from state s E S,
the agent receives immediate reward ra(s) E �and the
agent's state becomes some states' with the probabil­
ity given by the transition function Pr(s'Js,a) E [0, 1].
The agent is not aware of its current state, and in­
stead only knows its information state x, which is a
probability distribution over possible states (x(s) is
the probability that the agent is in state s). After
each transition, the agent makes an observation z of
its current state from a finite set of possible obser­
vations Z. The function Pr(zis',a) E [0,1] gives the
probability that observation z will be made after the
agent takes action a and makes a transition to state
s'. This results in a new information state x� defined

by
a
xz (s ')

=

Pr( z j s' , a) "EsES Pr( s ' ls , a)x(s)
'
Pr(zjx, a)

(1)

where
Pr(zix,a)

=

L Pr(zls', a) L Pr(s'Js, a)x(s).

s'ES

sES

Solving a POMDP means finding a policy 1r that maps
each information state into an action so that the
expected sum of discounted rewards is maximized
(0 � 1 � 1 is the discount rate, which controls how
much future rewards count compared to near-term re­
wards). There are many ways to approach this prob­
lem based on checking which information states can
be reached (Washington 1996; Hansen 1994), search­
ing for good controllers (Platzman 1981), and using
dynamic programming (Smallwood & Sondik 1973;
Cheng 1988; Monahan 1982; Littman, Cassandra, &
Kaelbling 1996).
Most exact algorithms for general POMDPs use a
form of dynamic programming in which a piecewise­
linear and convex representation of one value func­
tion is transformed into another. This includes algo­
rithms that solve POMDPs via value iteration (Sawaki
& Ichikawa 1978; Cassandra, Kaelbling, & Littman
1994), policy iteration (Sondik 1978), accelerated
value iteration (White & Scherer 1989), structured
representations (Boutilier & Poole 1996), and ap­
proximation (Zhang & Liu 1996). Because dynamic­
programming updates are critical to such a wide ar­
ray of POMDP algorithms, identifying fast algorithms
is crucial.
Several algorithms for dynamic-programming updates

have been proposed, such as one pass (Sondik 1971),
exhaustive (Monahan 1982), linear support (Cheng
1988), and witness (Littman, Cassandra, & Kaelbling
1996). Cheng (1988) gave experimental evidence that
the linear support algorithm is more efficient than the

Incremental Pruning for POMDPS

states to value and is defined in terms of relatively
simple transformations of other value functions.

one-pass algorithm. Littman, Cassandra and Kael­
bling (1996) compared the exhaustive algorithm, the
linear support algorithm, and the witness algorithm
and found that, except for tiny problems with approx­
imately 2 observations or 2 states, which all three al­
gorithms could solve quickly, witness was the fastest
and had a number of superior theoretical properties.

The transformations preserve piecewise linearity and
convexity (Smallwood & Sondik 1973; Littman, Cas­
sandra, & Kaelbling 1996). This means that if the
function V can be expressed as V(x) = maxaESX ·a
for some finite set of [$[-vectors S (which it can in
most applications)' then we can express vza (a;) =
maxaES� X . a, va(x) = maxat=s� X . a, and V'(x) =
maxo:ES' X. a for some finite sets of /S/-vectors s�, sa'
and S' (for all a E A, and z E Z). These sets have a
unique representation of minimum size (Littman, Cas­
sandra, & Kaelbling 1996), and we assume that the
symbols S�, sa, and S' refer to the minimum-size sets.

Recently, Zhang and Liu (1996) proposed a new
method for dynamic-programming updates in POMDPS
called incremental pruning. In this paper, we analyze
the basic algorithm and a novel variation and com­
pare them to the witness algorithm. We find that
the incremental-pruning-based algorithms allow us to
solve problems that could not be solved within reason­
able time limits using the witness algorithm.

2

Here is a brief description of the set and vector nota­
tion we will be using. Vector comparisons are com­
ponentwise: a1 � a2 if and only if for all s E S,
a1(s) ::_>: a2(s). Vector sums are also componentwise.
Vector dot products are defined by a·/1 =I:. o:( s )/1( s) .
In vector comparisons and dot products, 0 is a vector
of all zeros and 1 a vector of all ones. For all s E S, the
vector e8 is all zeros except e8 (s) = 1. The cross sum
of two sets of vectors is AEB B = {a+/1/a E A, /1 E B};
this extends to collections of vector sets as well. Set
subtraction is defined by A\B = {o: E Ala� B}.

DP UPDATES

The fundamental idea of the dynamic-programming
(DP) update is to define a new value function V' in
terms of a given value function V. Value functions
are mappings from information states to expected dis­
counted total reward. In value-iteration algorithms, V'
incorporates one additional step of reward compared
to V and in infinite-horizon algorithms, V' represents
an improved approximation that is closer to the opti­
mal value function.

Using this notation, we
described earlier as

The function V' maps information states to values and
is defined by

V'(x)

=

�E�

(z=
sES

ra(s)x(s) + 'Y L Pr(zjx,a)V(x�)
zEZ

)

(2)
In words, Equation 2 says that the value for an infor­
mation state x is the value of the best action that can
be taken from x of the expected immediate reward for
that action plus the expected discounted value of the
resulting information state (x�, as defined in Equa­
tion 1).
We can break up the value function V' defined in
Equation 2 into simpler combinations of other value
functions:

V'(x)
va(x)

maxVa(x)

(3)

z= vza(x)

(4)

aEA

=

z

These definitions are somewhat novel and form an im­
portant step in the derivation of the incremental prun­
ing method, described in Section 4. Each va and
vza function is a value function mapping information

55

S'

.

=

purge

can

characterize the "S" sets

(Usa)
(ffi S�)

(6)

aEA

sa
saz

=

purge

zEZ
purge({T(a,a,z)lo:

(7)
E

S}),

(8)

where T ( a, a, z) is the lSI-vector given by

r(a,a,z)(s)

(1//ZI)ra(s) + 'Y L o:(s') P r(z/s' a) Pr(s' j s , a),

,

•'

and purge(·) takes a set of vectors and reduces it to its
unique minimum form. Equations 6 and 7 are easily
justified by Equations 3 and 4 and basic properties of
piecewise-linear convex functions. Equation 8 comes
from substituting Equation 1 into Equation 5, sim­
plifying, and using basic properties of piecewise-linear
convex functions.
The focus of this paper is on efficient implementations
for computing sa (Equation 7). Equations 6 and 8
can be implemented efficiently using an efficient im­
plementation of the purge function, described in the
next section.

56

Cassandra, Littman, and Zhang

PURGING SETS OF VECTORS

3

Given a set of lSI-vectors A and a vector a, define

R(a, A) = {xlx 2': O , x 1
·

1 ,x·a > x·a',a'

=

E A\{a}};

it is the set of information states for which vector

(9)
a is

the clear "winner" (has the largest dot product) com­
pared to all the other vectors in A. The set

R(a, A) is

called the witness region of vector a, because for any
information state

maxa'EAu{a} x

·

x

in this set maxa'EA\{a}

a ' ; in a sense,

x

x ·a' i­

can testify that a is

needed to represent the piecewise-linear convex func­
tion given by

AU{a}.

Using the definition of

R,

F�F\{w}

5

6 while F i- 0
7 do¢ E F
X� DOMINATE(c/J, W)
8
ifx=.l
9
10
then F � F\ {¢}
11
else w � argmax4>EF x ·¢
W � WU{w}
12
F � F \ {w}
13
14 return

we can define

purge(A) = { al aE

FILTER(F)
1 W�0
2 for each s in S
3 do w � argmax4>EF e, · <P
4
w�wu{w}

A, R(a,A) i-

W

0};

it is the set of vectors in A that have non-empty wit­

Figure 1: Lark's algorithm for purging a set of vectors.

ness regions and is precisely the minimum-size set for
representing the piecewise-linear convex function given
by A (Littman, Cassandra, & Kaelbling
Figure

1

1996)1.

gives an implementation of purge(F)-given

a set of vectors

F, FILTER(F)

returns the vectors in

F

that have non-empty witness regions, thereby "purg­
ing" or "filtering" or "pruning" out the unnecessary
vectors. The algorithm is due to Lark (White
Littman, Cassandra, & Kaelbling

(1996)

1991);

analyze the

algorithm and describe the way that the argmax op­
erators need to be implemented for the analysis to
hold (ties must be broken lexicographically).

DoMINATE( a, A)

procedure called in line

information state

x for

8

The

returns an

which a gives a larger dot prod­

uct than any vector in A (or .l if no such

x

DOMINATE(a, A)
1 L � LP(variables:x(s), 8; objective: max&)
2 for each a' in A\ {a}
3 do ADDCONSTRAINT(L, x ·a 2': 8 + X a')
4 AooCoNSTRAINT(L, x 1 = 1)
5 AooCoNSTRAINT(L, x 2': 0)
6 iflNFEASIBLE(L)
7
then return (.l)
else (x,8) � SOLVELP(L)
8
9
ifo>O
then return (x)
10
else return .l
11
·

·

exists)­

2:

that is, it returns an information state in the region

Figure

R(a,A).

information state in a vector's witness region.

It is implemented by solving a simple linear

program, illustrated in Figure
The

FILTER

2.

algorithm plays a crucial role in the in­

cremental pruning method, so it deserves some addi­
tional explanation. The set

w

with vectors

R(w, F);

W, initially

empty, is filled

e8

information states.

The "while" loop starting on line

</J E

3.1

USING PURGE IN DP

that have non-empty witness regions

they are the "winners." Lines 3-5 find those

winning vectors at the

6

goes through the

DOMINATE is
x E R(¢, W). If there is not,
we know R(¢, F) is empty, since x E R(¢, F) implies
x E R(</J, W) since W � F. If DOMINATE finds an x E
R(¢, W), we add the winning vector (not necessarily
¢1) at x to W and continue. Each iteration removes
vectors

Linear-programming approach to finding an

F one by one.

For each,

used to see if there is an

a vector from F, and when it is empty, every vector

FILTER procedure, it is trivial to compute
s: sets from S and to compute S' from the sa. sets
(Equations 8 and 6).
Given the

the

A straightforward computation of the sa sets from the
S� sets (Equation 7) is also easy, and amounts to an
exhaustive enumeration of all possible combinations
of vectors followed by filtering the resulting sets. This
algorithm is not efficient because the number of com­
binations of vectors grows exponentially in

IZI.

This

can be a large number of vectors even when the sa

from F will have been classified as either a winner or

sets are relatively small. This approach to computing

not a winner.

the

1

This assumes that A is a true set in that it contains
no duplicate vectors.

so.

s� sets was essentially proposed
(1982) (under the name of "Sondik's one­

sets from the

by Monahan

pass algorithm").

Incremental Pruning

3.2

INCPRUNE(Sg1, , Sgk)
1
W +- FILTER(S:1 EB SgJ

COMPLEXITY ANALYSIS

•

We seek to express the running time of algorithms
in terms of the number of linear programs they solve
and the size of these linear programs. We choose this
metric because all of the algorithms in this paper use
linear programming as a fundamental subroutine (in
the form of calls to DOMINATE( a, A)) and the solu­
tion of these linear programs is by far the most time­
consuming part of the algorithms. In addition, tra­
ditional "operation count" analyses are cumbersome
and unenlightening because of the difficulty of pre­
cisely characterizing the number of primitive opera­
tions required to solve each linear program.
We will express the running time of W +- FILTER(F)
in terms of the size of the sets F and W, the number
of states lSI, and m, the number of vectors in W that
are found by checking the e8 information states.
As is evident in Figure I, each iteration of the "while"
loop on line 6 removes one vector from F, and m vec­
tors are removed before the loop. This means the while
loop is executed precisely IFI-m times. Each iteration
of the "while" loop makes a single call to DoMINATE,
so there are IF I- m linear programs solved in all cases.
Each of these linear programs has one variable for each
state in S and one for J. The total number of con­
straints in any one of these linear programs will be
between m + 1 and /WI+ 1. In the best case, the total
number of constraints will be IFI(m + 1) - miWI +
/W/(/W/- 1)/2- m(m + 1)/2 and the worst case will
have an additional (IF/ -/W/)(/W/- m) constraints.
W hen checking the e. information states, at least one
vector in W will be found. Further, when /WI > 1 we
are guaranteed to find at least two of the vectors in
W. For the remainder of this paper, we assume that
/W/ > 1, since the case of /WI 1 is trivial.
=

The witness algorithm has been analyzed previ­
ously (Littman, Cassandra, & Kaelbling 1996), and
we list the basic results here for easy comparison.
The total number of linear programs solved by wit­
ness is O::::z jSgl - /Zi)/Sa/ + /Sa/
1; asymptoti­
cally, this is e ( ISa l Lz IS:I). Note that this is not
a worst-case analysis; this many linear programs will
always be required. The number of constraints in each
linear program is bounded by /Sal + 1. The total
number of constraints over all the linear programs is
8(1Sa/2 Lz /Sg/) asymptotically2•

-

2In the best case there are

1/2(Lz /S�/-/Z/ +1)(/S"/+

l)(JS"/ + 2)- Lz /S�/ + /Z/
worst case there are JSa/(/Sa/
constraints.

-

3 constraints and in the

+ 1)(2:. JS�/- /Z/-

57

for POMDPS

1/2)

• •

2 for i +- 3 to k
3 do W +- FILTER(W EB Sg;)

4

4

return

W

Figure

3:

The incremental pruning method.

INCREMENTAL PRUNING

This section describes the incremental pruning
method (Zhang & Liu 1996), which computes sa effi­
ciently from the s� sets.
Recall the definition for sa in Equation 7:

sa

=

purge

zEZ

here, k

=

s�

(EB )

= purge(s�l

EB

s�2

ffi

...EB s�k);

/Zj. Note that

purge(A EBB EB C)
so Equation

7

=

purge(purge(A EBB)

EB

C),

can be rewritten as

Sa =purge(... purge(purge(S �1 EB S�2) EBS�3)

• • •

ffi

S�k).
(10)

The expression for sa in Equation 10 leads to a very
natural solution method, called incremental pruning,
illustrated in Figure 3. In addition to being conceptu­
ally simpler than the witness algorithm, we will show
that it can be implemented to exhibit superior perfor­
mance and asymptotic complexity.
The critical fact required to analyze incremental prun­
ing is that if A=purge(A) and B=purge( B) (neither
contain extra vectors) and W = purge(A ffi B), then

/WI ;:: max(IAI, IB/).

(11)

Equation 11 follows from the observation that for ev­
ery w E W, every R(w, W) region is contained within
R(a,A) and R({3, B) for some a E A and j3 E B.
This means that the size of the W set in INCPRUNE
is monotonically non-decreasing; it never grows explo­
sively compared to its final size.
Figure 3 illustrates a family of algorithms that we col­
lectively call the incremental pruning method; specific
incremental pruning algorithms differ in their imple­
mentations of the FILTER procedure. The most basic
incremental pruning algorithm is given by implement­
ing FILTER by Lark's algorithm (Figure 1); we call
the resulting algorithm IP. In Section 5, we describe
several other variations.

58

Cassandra, Littman, and Zhang

R( ¢, D \ { ¢}) is empty, we need to
R(¢, A EBB) is empty. We can show this by
contradiction. Assume there is an x* E R(¢,A ffi B).
Since (D \ {¢})�A ffi B, x• E R(¢,D \ {¢}). But we
know that R( ¢,D \ { ¢>}) is empty, so this cannot be.

9(1Sal I:z jS;I) linear pro­
0(1Sal2 I:z IS: I ) constraints3. In the worst

Proof:

The complexity of IP is
grams and

case, these bounds are identical to those of the witness

3.2).

algorithm (Section

However, there are POMDPs

for which the expression for the total number of con­
straints is arbitrarily loose; the best-case total number

To prove the second part, let

of constraints for IP is asymptotically better than for

FILTER in lNCPRUNE (Figure 3)
FILTER(A EBB). This section modifies
implementation of FILTER to take advantage of
All the calls to

are

of the form

the
the

deal of regularity. The modification yields a family of

FILTER algorithms,

some of which render incremental

appearing in Figure

1

is used.

The change is to replace line

x
D

+--

8

Different choices of
in Figure

1

with

Equation

(a+ !3) =¢for a E A and j3 E B. For every
a1 E A and f3t E B, if (at+ j3t) E W, then either
(at +!3d E D, or (at + /3) E D,or (a+ f3t) E D.
Let

D

(i.e., IP,

16

17

and

algorithm. Using either

16 or 17 exclusively in the incremental prun­

constraints. Although

O(ISal L:z IS;I2 + 1Sai2IZI)

the asymptotic total number of

linear programs does not change,

that satisfy the

RR actually requires

slightly more linear programs than IP in the worst
case. However, empirically it appears that the savings

The following lemma shows that any such choice of
allows us to use the domination check in Equation

in the total constraints usually saves more time than
the extra linear programs require.
An even better variation of incremental pruning selects
whichever

16 and 17.

vector in purge( A EBB) that has not yet been
to W (note that ¢fl. W).

the

IP

algorithm

are

plicated, upper bounds are possible.

15,

The only extra work that is required is

some bookkeeping to track how vectors were created
and the sizes of the various sets that we will choose
from.

added

IS" I L:z IS� I linear program s and ISai(ISal + 1) I;, IS�I3IZI total constraints. Note that tighter, though more com­

set is smallest from among Equations

This will usually yield a faster algorithm in

practice, though it makes this variation much harder

D
12

Lemma 1 If R(c/>,D\{4>}) = 0, thenR(c/>,AEBB) = 0.
If x E R(¢,D \ {¢}), then x E R(w, W) for some
wE (A$B) \ W.

D

to analyze.

to either remove ¢ from consideration, or to find a

on

(RR)

plexity of the algorithm to

(13)
A ffi B,
({a} ffi B) U (A EB {/3}),
(14)
w,
(15)
({a} ffi B ) U {at+ /31(at + !3) E W}, (16)
(A EB {/3}) U {f3t + aj(a + /31) E W}, (17)

bounds

1991) in lNCPRUNE

ing algorithm will improve the total constraint com­

above properties. For example,

upper

filtering algorithm in

We refer to variations of the incremental pruning
as the restricted region

There are a number of choices for

(1982)

is equivalent to using Lark's

method using a combination of Equations

1. D � (AffiB).

3Simple

15

filtering algorithm (W hite

is the set of winning vectors found so far):

=

equiv­

as described earlier).

W

and

D set,

13 is

lNCPRUNE, Equation

of vectors satisfying the properties below

A ffi B

result in different incremental

alent to using Monahan's

can be used and still give a correct algorithm (recall
that we are filtering the set of vectors

D

pruning algorithms. In general, the smaller the
the more efficient the algorithm. Equation

(12)

DOMINATE(¢,D \ {¢}).

D

The lemma follows.

pruning more efficient than when the standard version

D
D
D
D
D

argmax<I>'EAffiB x ·¢'.

x·w > x·w'
w'EW. Let (at + !3t) = w' for any w' E W,
a1 E A and [31 E B and let (a + /3) = ¢> for a E A and
/3 E B. By the conditions on D, we know that either
(at + /31} E D, or (at + /3) E D, or (a+ (31) E D.
Assume (a1 +!3) ED (the other two cases are similar).
Sincex E R(¢,D\{¢}), x·¢>::: x·(a+f3) > x·(a1 +/3).
This implies that x · a > x·a1• Adding /31 to both sides
gives us that x · (a+ !3!) > x ·(a1 + /31) = x·w1• By the
definition of w, x·w � x ·(a+ 81). Hence x·w > x·w'.

fact that the set of vectors being processed has a great

2.

=

for all

GENERALIZING IP

Any set

w

The lemma is proved if we can show that

witness.

5

First, if

show that

In principle, it is also possible to choose aD set that is
the smallest set satisfying conditions

1

and 2. This ap­

pears to be closely related to the NP-hard vertex-cover
·

problem; we are investigating efficient alternatives.

Incremental Pruning for POMDPS

6

EMPIRICAL RESULTS

Although asymptotic analyses provide useful informa­
tion concerning the complexity of the various algo­
rithms, they provide no intuition about how well algo­
rithms perform in practice on typical problems. An­
other shortcoming of these analyses is that they can
hide important constant factors and operations re­

Table 2: Total execution time

Test Problem
1D maze

W itness

TTOTAL

59

(sec. )

IP

RR

9.3

2.3

2.3

2.2

4x3

727.1

346.0

157.0

>28800

4x4

3226.0

1557.0
2 15.7

909.2

216.7

351.8

203.3

>28800

5608. 4

4249.2

5226.4

1116.9

6422.9

1066.6

722.5

>28800

Exh.

quired outside of the linear programs. To address these
shortcomings, we have implemented IP and variations
and have run them on a suite of test problems to gauge
their effectiveness. All times given are in CPU seconds
on a SPARC-10.

Cheese
Part painting
Network
Aircraft ID
Shuttle
4x3 CO

found that more than
95% of the total execution time was spent solving lin­
ear programs4, verifying that the linear programs are
the single most important contributor to the complex­
ity of the algorithms.

Table 3: Total execution time (sec.) for extended tests.

417.0

234. 1

166.0

>28800

1676.7

200.8

145.9

>28800

24.6

22.8

22.7

>28800

We profiled the execution and

To ensure fairness in comparison, we embedded all of
the algorithms in the same value-iteration code and
used as many common subroutines as possible. We
also used a commercial linear programming package
to maximize the stability and efficiency of the imple­
mentation.
We ran IP, RR, exhaustive and linear support al­
gorithms on 9 different test problems listed in Ta­
ble 1 (complete problem definitions are available at
http://www.cs.brown.edu/people/arc/research/

) The "Stages" column reports
the number of iterations of value iteration we ran and
the "IVtl" column indicates the number of vectors in
the final value function5.

pomdp-examples. html .

Table 2 lists the total running time for each algorithm
on each of the 9 test problems. The results indicate
that RR works extremely well on a variety of test prob­
lems. We do not list run times for the linear support
algorithm because, in all cases, it was unable to run
to completion. This is because of memory limitations;
space requirements

for the linear support algorithm

increase dramatically as a function of the number of
states. We terminated algorithms that failed to com­
plete in 8 hours (28800 seconds); as a result, the ex­
haustive algorithm ("Exh.") was only able to complete
three of the test problems (all of which had only two
observations). On the three small test problems the
4

This profiling data was computed running witnes s, IP,
and RR on the 4x3 problem for 8 stages.
5
The number of stages was determined by finding the
maximum number of stages that the witness algorithm was
able to complete within 7200 seconds. In some of the test
problems, the witness algorithm found the optimal infinite­
horizon value function in under 7200 seconds, so we picked
the number of iterations required to conv('!rge to within
machine precision of the optimal value function.

Test Problem
Network
Shuttle

TTQTAL

Stages

W itness

IP

RR

20

>28800

4976.8

2621.3

9

>28800

5121.3

2767.7

exhaustive algorithm was able to complete, it actually
out performed all the other algorithms.
For all but two of the test problems, the witness al­
gorithm was within a factor of 5 of the performance
of RR. To highlight the advantage of the incremental­
pruning-based algorithms, we chose the two test prob­
lems for which RR was more than 5 times faster than
witness (Network, 8.9, and Shuttle, 11.5) , and ran for
a larger number of stages. As shown in Table 3, the
witness algorithm is unable to solve a problem in 8
hours that RR can solve in 43 minutes (2621 seconds).
Although linear programming consumes most of the
running time in the algorithms we examined, there
are actually three phases of the value-iteration algo­
rithm that contribute linear programs: finding the
minimum-size

sg

sets, constructing the

ga sets from

the s: sets, and constructing S' by combining the sa
sets. Of these, only constructing the sa sets is differ­
ent between witness, IP, and RR, so we have chosen to
present the execution times in two ways. The first, as
illustrated in Table 2 as TTOTAL, represents the com­
plete running time for all stages and all phases. The
second, shown in Table 4 as Tsa -BUILD, is the exe­
cution time over all stages that was devoted to con­
structing the sa sets from the s: sets.
As the data in Tables 2 and 4 show, IP performs better
than the witness algorithm on all the test problems.
These tables also show how difficult it is to analyze
the exact amount of savings IP yields; the amount of
savings achieved varies considerably across problems.

60

Cassandra, Littman, and Zhang

Table 1: Test problem parameter sizes.
Test Problem
1D maze
4x3
4x3 CO
4x4
Cheese
Part painting
Network
Shuttle
Aircraft ID

States
4
11
11
16
11
4
7
8
12

Acts.
2
4
4
4
4
4
4
3
6

Obs.
2
6
11
2
7
2
2
5
5

Stages
70
8
367
374
373
371
14
7
4

!Vtl

4
436
4
20
14
9
438
481
258

Reference
Parr & Russell (1995)
Russell & Norvig (1994)
Cassandra, Kaelbling, & Littman (1994)
McCallum (1993)
Kushmerick, Hanks, & Weld (1995)
Chrisman (1992)

Table 4: Total time (sec.) spent constructing sa sets.
Test Problem
1D maze
4x3
4x4
Cheese
Part painting
Network
Aircraft ID
Shuttle
4x3 CO

Tsa-BUILD

Witness
7.1
599.1
2252.6
221.9
5226.5
5954.7
359.1
1566.2
2.6

IP
<0.1
220.5
644. 4
84.4
3834.4
615.4
176.4
92.5
0.9

RR
<0.1
31.0
0.9
72.2
4819.5
255.4
108.3
38.1
0.9

For RR, the set D was defined by Equation 16 if
\BI < \A\ and Equation 17 otherwise; in most cases,
this is equivalent to using the equation that leads to
the smaller size for D. Looking at the data for RR, we
see that in all but one case it is faster than IP. Again,
the precise amount of savings varies and is difficult to
quantify in general.

DP-UPDATE(S)
1 for each a inA
2 do for each z in Z
do 5� t- FILTER({r(a:,a,z)ia:
3
sa t- INCPRUNE(S:,, . , s:,)
4
5 S' f-- FILTER (Ua sa)
6 returnS'
.

DISCUSSION & CONCLUSIONS

In this paper, we examined the incremental pruning
method for performing dynamic-programming updates
in partially observable Markov decision processes. In­
cremental pruning compares favorably in terms of ease
of implementation to the simplest of the previous al­
gorithms (exhal'stive), has asymptotic performance as
good as or better than the most efficient of the previ­
ous algorithms (witness), and is empirically the fastest
algorithm of its kind for solving a variety of standard
POMDP problems.

A complete incremental pruning algorithm (RR) is
shown in Figure 4.
There are several important outstanding issues that
should be explored. The first is numerical precision-

St-1})

INCPRUNE(S�,, ... , S�k)

1 W

t-

RR(S:,, S�2)

for i t- 3 to k
3 do W t- RR(W, S�;)
4 return W
2

RR(A,B)
1

2
3
4
5
6
7

7

E

.

8
9
10
u

12
13
14
15
16
17
18

Ft-AEIJB
Wt-- 0
for each s inS
do w f-- argmaxq,EF es ¢
·

Wt-W U {w}
Ft-F\{w}
while F 1:- 0
do (o: + ,B) E F
D1 t- ({o:} EBB) U {a:1 + ,Bj(o:1 +,B) E W}
D2 t- (A ffi {,8}) U {,81 + a:i(o: + ,81) E W}
if IBI < IAI
x

then D +- D1
else D t- D2
f-- DOMINATE(a:+ ,8, D)

if X = .l
then F t- F \{a:+ ,8}
else w +-- argmaxq,EF x ¢
·

W +- WU {w}
F t- F \ {w}

19

20 returnW
Figure 4: Complete RR algorithm.

Incremental Pruning for POMDPS
each of the algorithms we studied, witness, IP, and
RR, have a precision parameter c:, but the effect of
varying c on the accuracy of the answer differs from
algorithm to algorithm. Future work will seek to de­
velop an algorithm with a tunable precision parameter
so that sensible approximations can be generated.
From a theoretical standpoint, there is still smne work
to be done developing better best-case and worst-case
analyses for RR. This type of analysis might shed some
light on whether there is yet some other variation that
would be a consistent improvement over IP.
In any event, even the slowest variation of the incre­
mental pruning method that we studied is a consistent
improvement over earlier algorithms. We feel that this
algorithm will make it possible to greatly expand the
set of POMDP problems that can be solved efficiently.

C., and Poole, D. 1996. Computing optimal
policies for partially observable decision processes us­
ing compact representations. In Proceedings of the
Boutilier,

Thirteenth National Conference on Artificial Intelli­

1168-1175. AAAI Press/The MIT Press.

Cassandra, A. R.; Kaelbling, L. P.; and Littman,
M. L. 1994. Acting optimally in partially observ­
able stochastic domains. In Proceedings of the Twelfth
National Conference on Artificial Intelligence, 10231028.
Cheng, H.-T. 1988.

on

Machine

190-196. Amherst, Massachusetts: Mor­
gan Kaufmann.
Learning,

Monahan, G. E. 1982. A survey of partially observ­
able Markov decision processes: Theory, models, and
algorithms. Management Science 28(1):1-16.
Parr, R., and Russell, S. 1995. Approximating op­
timal policies for partially observable stochastic do­
mains. In Proceedings of the International Joint Con­
ference on Artificial Intelligence.

Platzman, L. K. 1981. A feasible computational ap­
proach to infinite-horizon partially-observed Markov
decision problems. Technical report, Georgia Insti­
tute of Technology, Atlanta, GA.
Russell, S. J., and Norvig, P.
ligence: A Modern Approach.

1 994. Artificial Intel­
Englewood Cliffs, NJ:

Prentice-Hall.

References

gence,

of the Tenth International Conference

61

Algorithms for Partially Observ­

Ph.D. Dissertation,
University of British Columbia, British Columbia,
Canada.
able Markov Decision Processes.

Chrisman, L. 1992. Reinforcement learning with
perceptual aliasing: The perceptual distinctions ap­
proach. In Proceedings of the Tenth National Con­
ference on Artificial Intelligence, 183-188. San Jose,
California: AAAI Press.
Hansen, E. A. 1994. Cost-effective sensing dur­
ing plan execution. In Proceedings of the Twelfth
National Conference on Artificial Intelli gence. AAAI
Press/The MIT Press. 1029-1035.
Kushmerick, N.; Hanks, S.; and Weld, D. S. 1995.
An algorithm for probabilistic planning. Artificial
Intelligence 76(1-2):239-286.
Littman, M. L.; Cassandra, A. R.; and Kaelbling,
L. P. 1996. Efficient dynamic-programming updates
in partially observable Markov decision processes.
Technical Report CS-95- 19, Brown University, Prov­
idence, RI.
McCallum, R. A. 1993. Overcoming incomplete per­
ception with utile distinction memory. In Proceedings

Sawaki, K., and Ichikawa, A. 1978. Optimal control
for partially observable Markov decision processes
over an infinite horizon. Journal of the Ope rations
Research Society of Japan 21(1):1-14.

Smallwood, R. D., and Sondik, E. J. 1973. The opti­
mal control of partially observable Markov processes
over a finite horizon. Operations Research 21:10711088.
Sondik, E. 1971.

The Optimal Control of Partially

Observable Markov Processes.

Ph.D. Dissertation,

Stanford University.
Sondik, E. J. 1978. The optimal control of partially
observable Markov processes over the infinite horizon:
Discounted costs. Operations Research 26(2):282304.
Washington, R. 1996. Incremental Markov-model
planning. In Proceedings of TAI-96, Eighth IEEE In­
ternational Conference on Tools With Artificial In­
telligence,

41-47.

White, III, C. C., and Scherer, W. T . 1989. Solu­
tion procedures for partially observed Markov deci­
sion processes. Operations Research 37(5):791-797.
White, III, C. C. 1991. Partially observed Markov
decision processes: A survey. Annals of Operations
Research 32.
Zhang, N. L., and Liu, W . 1996. Planning in stochas­
tic domains: Problem characteristics and approxi­
mation. Technical Report HKUST-CS96-31, Depart­
ment of Computer Science, Hong Kong University of
Science and Technology.

