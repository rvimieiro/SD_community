What Cannot be Learned with Bethe Approximations

Uri Heinemann
Amir Globerson
School of Computer Science and Engineering
The Hebrew University, Jerusalem 91904, Israel

Abstract
We address the problem of learning the parameters in graphical models when inference
is intractable. A common strategy in this
case is to replace the partition function with
its Bethe approximation. We show that there
exists a regime of empirical marginals where
such Bethe learning will fail. By failure we
mean that the empirical marginals cannot be
recovered from the approximated maximum
likelihood parameters (i.e., moment matching
is not achieved). We provide several conditions on empirical marginals that yield outer
and inner bounds on the set of Bethe learnable marginals. An interesting implication of
our results is that there exists a large class of
marginals that cannot be obtained as stable
fixed points of belief propagation. Taken together our results provide a novel approach
to analyzing learning with Bethe approximations and highlight when it can be expected
to work or fail.
Probabilistic graphical models [8, 23] are a powerful
tool for describing complex multivariate distributions.
They have been used successfully in a wide range of
fields, from computational biology to machine vision
and natural language processing. To use such a model
in practice, one typically needs to solve two related
tasks. The first is the inference task which involves
calculating probabilities of events under the model.
The second task involves learning the parameters of
the model from empirical data.
Unfortunately, in many models of interest the inference
problem is computationally hard, and cannot be solved
exactly in practice. This has motivated extensive research into approximate inference schemes, some of
which have been quite successful empirically. Perhaps
the most well known of these is the belief propaga-

tion (BP) algorithm, which is closely related to variational approximations based on Bethe free energies
[26]. Another variational approach, which uses convex free energies is the tree-reweighted (TRW) method
[22]. Although the TRW approach results in convex optimization problems for inference, it sometimes
yields marginals that are inferior to those obtained by
BP (e.g., see [9]).
How should one learn the parameters of a model when
inference is intractable? The typical approach to parameter learning is likelihood maximization, but when
inference is intractable it is also hard to maximize the
likelihood.1 Because of this difficulty, many methods
have been devised to approximate the learning problem. One elegant approach is to approximate the likelihood using the same variational approximation that
is employed during inference [5, 14, 16, 19].
Analyzing the performance of approximate learning
schemes is challenging, since even the accuracy of the
underlying variational approximations is hard to analyze. Furthermore, we do not generally expect the
learned model to be similar to the one obtained using exact maximum likelihood. One approach, which
has recently been introduced by Wainwright [19] is to
use the notion of moment matching. In exact maximum likelihood learning, the learned model has a nice
property: some if its marginals are guaranteed to be
identical to those of the empirical data. This property
is often referred to as moment matching. Wainwright
[19, 21] has shown that when using convex variational
approximations such as TRW, the learned model also
has the moment matching property in the following
sense: if one applies approximate inference to it (using the same variational approach that was used during learning), the resulting marginals will be equal to
1

When the data are known to be generated by a graphical model of the same structure, pseudo-likelihood [1] can
be used and is consistent. However, this assumption is
rarely met in practice, and pseudo-likelihood often does
not perform well in these cases.

the empirical ones. However, these results cannot be
applied to learning with Bethe approximations, since
the latter are not convex. Because of the success of
Bethe approximations in a wide array of applications,
it is important to understand the advantages and limitations of learning with those. This is precisely the
goal of our work.
It may initially seem like learning with Bethe approximations would also result in a moment matching property. In other words, if we use Bethe approximations
during both learning and inference, our learned model
will agree with the empirical marginals. However, as
we show here, the situation is considerably more complex. In the current work we provide some surprising
results with respect to moment matching and Bethe
approximations, that shed light on the performance of
learning with such approximations, and on properties
of the BP algorithm. Our main results are:
• We show that there exist empirical distributions for
which Bethe approximations cannot perform moment matching. In other words, if we run BP on
the optimal Bethe parameters, we will not recover
the empirical marginals. Such empirical distributions are thus bad inputs for Bethe approximations,
since the learned parameters cannot be used to reconstruct the original marginals.
• We provide inner and outer bounds on the set of
marginals for which Bethe moment matching is possible, and show that they agree with empirical behavior of Bethe learning. Surprisingly, we show that
binary attractive models cannot be learned with
Bethe approximations for certain graphs.
• Our results also provide a novel characterization of
BP fixed points. Specifically, we show that there is
a large class of marginals that cannot be obtained
as stable fixed points of BP.
Taken together, our results provide a novel way of analyzing learning with Bethe approximations.

1

Maximum Likelihood in Graphical
Models

We focus on pairwise Markov random fields for
simplicity. That is, we consider random variables
X1 , . . . , XNV and pairwise functions θij (xi , xj ) corresponding to edges E in a graph G with NV nodes. The
MRF corresponding to these parameters is given by:


NV
X
X
1
exp 
θij (xi , xj ) +
θi (xi )
p(x; θ) =
Z(θ)
i=1
ij∈E

(1)

where Z(θ) is the partition function and x corresponds
to a complete assignment to the NV variables.
We wish to learn the parameters θ from a sample of
size M given by x(1) , . . . , x(M ) . A standard approach
to parameter learning is to maximize the log likelihood
given by:2
`(θ) =

1 X
log p(x(m) ; θ) = µ̄ · θ − log Z(θ)
M m

(2)

where µ̄ are the empirical marginals given by:
µ̄ij (xi , xj )

=

µ̄i (xi )

=

1 X
δxm ,x δxm ,x
M m i i j j
1 X
δxm ,x
M m i i

∀ij ∈ E
i = 1, . . . , N

and θ is the corresponding vector with the parameters
θij (xi , xj ), θi (xi ) in appropriate coordinates.
The likelihood function `(θ) is concave in θ and thus
does not have local optima. Finding its global maximum is possible when the function, as well as its gradient, can be calculated efficiently. In these cases a
variety of methods can be used, such as gradient ascent, iterative proportional fitting or any other general
purpose first order convex optimization procedure.
A key property of the optimal parameters θ M L (µ̄) is
that they satisfy the so called moment matching condition, described next. Define the vector µθ to be the
set of marginals of the model p(x; θ) given by:
µθij (xi , xj )
µθi (xi )

= p(xi , xj ; θ) ∀ij ∈ E
= p(xi ; θ)

i = 1, . . . , N

The moment matching condition for maximum likelihood optimality is then simply:
µθM L = µ̄

(3)

The condition states the following simple fact: the optimal parameters θ M L are such that the optimal model
p(x; θ M L ) has the given empirical marginals. This is
a desirable property since the empirical marginals are
often a good approximation of the true marginals.3
1.1

Bethe Approximations of the Likelihood

The problem of maximizing the likelihood in Eq. 2 is
hard due to the general intractability of the partition
function and marginals inference problems. We shall
2

Dependence on the sample is implicit throughout.
When not enough data is available for estimating
marginals reliably from data, it is possible to use smoothing or regularization. We do not address this here, but our
results can be generalized to these cases.
3

focus on a common approach to this problem, which is
to replace log Z(θ) with an approximation F (θ). We
shall specifically be interested in the Bethe approximation [26] defined as follows. First define the negative
Bethe free energy as:

The subdifferential of F (θ) is defined as follows. For
a given θ define M (θ) to be the set of vectors µ that
maximize F (µ; θ). Namely:

F (µ; θ) = µ · θ + HB (µ)

The subdifferential of F (θ) is then Conv {M (θ)}, the
convex hull of the vectors in M (θ). The subdifferential
of `B (θ; µ̄) is thus µ̄ − Conv {M (θ)}. Taken together
we have that the optimality condition for θ B (µ̄) is:

(4)

Where the function HB (µ) is the Bethe entropy given
by:
X
X
HB (µ) =
(di − 1)
µi (xi ) log µi (xi )
xi

i∈V

−

X X

µij (xi , xj ) log µij (xi , xj )

ij∈E xi ,xj

and di is the degree of node i in the graph G.
The Bethe approximation for the log partition function
log Z(θ) is then given by:
F (θ) = max F (µ; θ)
µ∈ML

µ∈ML

µ̄ ∈ Conv {M (θ B (µ̄))}

(8)

(9)

Whenever there is a single maximizer µ(θ) in M (θ)
the condition becomes µ̄ = µ(θ), resembling the moment matching condition in Eq. 3. However, generally
the above condition is more subtle and actually means
that when there are multiple maximizers, µ̄ is not necessarily equal to any of them. In Section 2 we consider
the strong implications that this has on learning with
Bethe approximations.

(5)
1.3

Eq. 5 uses the following definitions: ML is the local
marginal polytope, which is the set of locally consistent
pseudo-marginals µ defined as:
P


µij (xi , xj ) = µi (xi ) ij ∈ E 





xj


P
µ
(x
,
x
)
=
µ
(x
)
ij
∈
E
ij
i
j
j
j
ML = µ ≥ 0
.
xi


P






µi (xi ) = 1
xi

(6)
We are thus interested in maximizing the Bethe likelihood:
`B (θ; µ̄) = µ̄ · θ − F (θ) = µ̄ · θ − max F (µ; θ) (7)
µ∈ML

One interesting property of the function `B (θ; µ̄),
which is typically overlooked, is that it is in fact a
concave function of θ and thus it does not have local
maxima. This is a simple outcome of the fact that
F (θ) is a pointwise maximum over functions that are
convex (in fact linear) in θ and is thus convex [2], so
that its negation is concave.
In what follows, we characterize the global maximizer
of the Bethe likelihood and discuss several ways of approximating it. We denote the maximizer by θ B (µ̄).
1.2

M (θ) = arg max F (µ; θ)

Optimality Conditions for Bethe
Learning

The Bethe likelihood `B (θ; µ̄) is a concave but nonsmooth function. Thus, a necessary and sufficient condition for θ B (µ̄) to maximize it is that the subdifferential of `B (θ; µ̄) at θ B (µ̄) contains the all zero vector 0
[12]. In what follows we use this to characterize θ B (µ̄).

Maximizing the Bethe Likelihood in
Practice

Although `B (θ; µ̄) is concave, finding its maximum is
still hard and there is no known closed form solution
or polynomial time algorithm for it. The key difficulty is that both evaluating `B (θ; µ̄) and calculating
a subgradient for it involve maximizing F (µ; θ) over
µ.4 Since F (µ; θ) is not concave in the general case,
this appears to be a hard problem.5
The common practice in this case is to find a stationary point of F (µ; θ) using BP [26] and using it as a
subgradient in subgradient ascent. Although it is hard
to obtain theoretical guarantees for such an approach,
it sometimes provides good empirical results [14, 16].
Another common approach comes from studying the
tree graph case. In that case, the maximum likelihood
parameters for µ̄ are given in closed form by θ c (µ̄)
defined as [21]:
θic (xi ; µ̄)
c
θij
(xi , xj ; µ̄)

=

log µ̄i (xi )
µ̄ij (xi , xj )
= log
µ̄i (xi )µ̄j (xj )

When the graph is not a tree, θ c (µ̄) is not necessarily
the maximum likelihood parameter, as we also show
in what follows. However, it is always true that µ̄
is a stationary point of F (µ; θ c (µ̄)) [21]. Specifically,
if one initializes BP on the MRF θ c (µ̄) with uniform
messages, µ̄ will be obtained as a fixed point (although
this fixed point may be unstable as we show later).
4

This maximization is subject to constraint µ ∈ ML .
In what follows we do not state this explicitly for brevity.
5
We are not aware of complexity results that prove this
fact.

2

Bethe Learnable Parameters

Our main goal in the current work is to understand when Bethe learning achieves moment matching. By moment matching we mean that inference
on the learned parameter will result in the empirical marginals (as in Eq. 3). In the context of Bethe
learning, it is natural to define inference as returning the maximizer of F (µ; θ).6 One difficulty with
this is that F is hard to maximize due to its nonconvexity.7 However, there is a more fundamental
difficulty: F (µ; θ B (µ̄)) might have multiple distinct
global maximizers. In this case, the outcome of the
inference step is not well defined, and hence moment
matching cannot be achieved. Furthermore, in this
case the empirical marginals µ̄ will generally not correspond to any of the maximizers of F (µ; θ B (µ̄)), but
will rather lie in their convex hull (see Section 1.2).
On the other hand, if F (µ; θ B (µ̄)) has a single global
maximum then by Eq. 9 this maximizer is exactly µ̄
and we have moment matching.
This brings us to our key question: for which values
of µ̄ will Bethe achieve moment matching? Given the
above discussion, these are µ̄ for which the optimal
θ B (µ̄) has a single maximum.
Definition 1. µ̄ is Bethe learnable if there exists a
θ such that µ̄ is the unique maximizer of F (µ; θ). In
this case θ maximizes the Bethe likelihood (i.e., θ =
θ B (µ̄)), and µ̄ can be recovered from θ so that moment
matching is achieved. Denote the set of such µ̄ by BL .
The definition of BL is quite implicit and it is thus
not immediately clear what can be said about this set.
For example, are there parameters that are not Bethe
learnable? Clearly there are parameters that are Bethe
learnable since Bethe is exact for trees (see Section
1.3). In what follows we show that there are in fact
µ̄ that are not in BL and characterize those in some
cases.

3

Characterizing Unlearnable
Marginals

The naive approach to testing if µ̄ ∈ BL would be
to scan all parameters values θ, and for each of those
test if µ̄ is the unique maximizer of F (µ; θ). In what
follows we provide several simpler approaches, some in
closed form. Specifically, Sections 3.1 and 3.2 provide
6

Using the same inference approximation in learning
and inference is also motivated by the results in [19, 21]
where a similar approach for convex free energies results in
moment matching.
7
In practice, maximization will be approximated by
running BP on the MRF given by θ.

outer bounds on BL and Section 3.3 provides an inner
bound.
3.1

A Condition Using Canonical Parameters

Our first observation in terms of characterizing BL is a
lemma that provides a sufficient condition for µ̄ ∈
/ BL .
In other words, we obtain an outer bound on BL .
Lemma 1. Let µ̄ ∈ ML and let θ c (µ̄) be its canonical parameter. If µ̄ is not a global maximizer of
F (µ; θ c (µ̄)) then µ̄ ∈
/ BL .
Proof. We are given the fact that:
µ̄ ∈
/ arg max F (µ; θ c (µ̄))
µ∈ML

(10)

Asssume by contradiction that µ̄ ∈ BL so that there
is a parameter θ̃ for which µ̄ ∈ arg maxµ∈ML F (µ; θ̃).
As mentioned in Section 1.3 µ̄ is a fixed point of BP for
the parameters θ c (µ̄). Since µ̄ maximizes the negative
Bethe free energy for θ̃, it is also a BP fixed point for θ̃
[26, Theorem 3]. Since BP fixed points correspond to
re-parameterizations of the MRF [20], and since both
θ c (µ̄) and θ̃ have the same BP fixed point µ̄, it follows
that θ c (µ̄) and θ̃ are re-parametrizations of the same
MRF. It is then easy to show that two parameters that
are re-parametrizations of each other share the same
BP fixed points with the same values up to a constant.
This implies that µ̄ does in fact maximize F (µ; θ c (µ̄)),
contradicting our assumption in Eq. 10.
Lemma 1 provides an easy procedure for excluding
points from BL : Given µ̄ generate the canonical parameter θ c (µ̄). Run BP with this parameter multiple
times (e.g., by initializing messages randomly). If a
fixed point with value F (µ; θ c (µ̄))) > F (µ̄; θ c (µ̄)) is
found then µ̄ ∈
/ BL .
Another interesting implication of Lemma 1 (by negation) is that if µ̄ is learnable then its maximum likelihood parameter is necessarily the canonical one θ c (µ̄).
Thus, the canonical parameters are in some sense the
best choice for maximum likelihood with a Bethe approximation. Namely, they are the correct maximum
likelihood parameters when µ̄ is learnable. When µ̄ is
not learnable using Bethe approximation in learning is
apparently not a good approach in any case.
3.2

Closed Form Bounds via the Hessian

Here we provide another outer bound on BL (i.e., a
criterion for µ ∈
/ BL ). The key idea is to find marginals
µ̄ which can never be a maximum (local or global) of
F (µ; θ) regardless of the value of θ. Clearly, such µ̄
will not be in BL .

For this purpose we will focus on binary pairwise models. Following [24, 10] we will use a minimal parameterization. In this representation we denote µi (xi = 1)
as µi and µij (xi = 1, xj = 1) as µij . The other singleton and pairwise marginals can be obtained from
these via:
µij (xi = 1, xj = 0)

= µi − µij

µij (xi = 0, xj = 1)

= µj − µij

µij (xi = 0, xj = 0)
µi (xi = 0)

1 − µi

It can be verified that a point is in ML (G) if and
only if all the above marginals are non-negative (e.g.,
see [3, 17]). The function F (µ; θ) in this case can be
expressed as a function of µij , µi alone. Furthermore,
there is no need to add non-negativity constraints since
these would result in log of a negative number in the
objective. For this unconstrained F (µ; θ), a sufficient
condition for µ̄ not to be a maximum for any θ is
that the Hessian of F (µ; θ) at µ̄ is not negative-semidefinite regardless of the value of θ. Luckily, the Hessian at F (µ; θ) does not depend on θ (since F is linear in θ) and equals the Hessian of the Bethe entropy
HB ({µij , µi }). Thus we have:
Lemma 2. For binary variables, if the Hessian of
HB ({µij , µi }) at µ = µ̄ is not negative-semi-definite
then µ̄ ∈
/ BL .
Lemma 2 provides a sufficient condition for µ̄ ∈
/ BL .
It can be easily tested for any given µ̄ by calculating
the Hessian at this point and checking its eigenvalues.
To better understand the condition on the Hessian,
we turn to a more specific scenario which results in a
closed form expression on µ̄.
We focus on marginals µ̄ that are homogenous in the
sense that all pairwise µij = µe for a constant µe and
all µi = µv for a constant µv .8 We make no restrictions
on the graph structure. The following lemma states a
simple sufficient condition for guaranteeing that µ̄ ∈
/
BL . We denote by NV the number of variables and
NE the number of edges.
Lemma 3. Assume µ̄ corresponds to homogenous binary marginals in minimal representation, and µ̄ satisfies the following condition:
µ̄e >

(1 −

NV
NV
2
NE )µ̄v + 2NE µ̄v
NV
1 − 2N
E

• The condition is independent of the graph structure. It only depends on the number of variables
and edges.
• For tree graphs and single loop graphs there are no µ̄
that satisfy the conditions. This is consistent with
the fact that the Bethe free energy has a unique
optimum in these cases and all marginals are Bethe
learnable.

= µij − µi − µj + 1
=

The proof of the lemma is given in Appendix A. This
result has several interesting implications:

(11)

Then the Hessian of HB (µ) at µ̄ is not negative-semidefinite, and hence µ̄ ∈
/ BL .
8
Note that ML (G) in this case reduces to the constraints: 2µv − 1 ≤ µe ≤ µv and µe ≥ 0.

V
• As N
NE → 0 (e.g., for complete graphs with NV →
∞) the condition becomes µ̄e ≥ µ̄2v . Perhaps surprisingly, this is a condition that is satisfied by any
ferromagnetic distribution [15, 4].9 This implies
that if µ̄ are generated from a ferromagnetic disV
/ BL . In other
tribution with N
NE → 0, then µ̄ ∈
words, ferromagnets are not Bethe learnable in this
asymptotic regime.

3.3

Inner Bounds on BL

The results in the previous sections provide outer
bounds on BL . In the current section we show how
previous results on BP convergence can be utilized
to obtain inner bounds on BL . The key idea is to
check, given some µ̄, whether the canonical parameters θ c (µ̄) yield moment matching. We know that µ̄
is a BP fixed point for θ c (µ̄). Thus a sufficient condition for moment matching to take place is that µ̄
is the unique stable fixed point of BP.10 This would
imply that at θ c (µ) we have moment matching for µ̄
and thus µ̄ ∈ BL .
Fortunately there are various results that provide sufficient conditions on a parameter θ having a unique
stable fixed point. This has been addressed by multiple works in the past (e.g., [11, 13]).11 Each of these
works provides a different condition on θ. We are not
concerned with which one is better since all can equally
well be applied to our case and we can simply take the
union of the conditions to obtain a tighter bound.
Thus, our condition for µ̄ ∈ BL is calculated as follows:
for a given µ̄, calculate θ c (µ̄). Now check whether
θ c (µ̄) has a unique BP fixed point (by using one of
the conditions in the papers above). If it does, then
µ̄ ∈ BL . In the experiments section we will specifically
look at the condition from [13].
9

The non-homogenous form is µij ≥ µi µj .
Stability is needed to guarantee that this point is in
fact a global maximum as our analysis requires [6].
11
Note also [7] which analyzes unique fixed points, but
not necessarily stable.
10

4

Related Work

1
0.9

Several works have analyzed the behavior of Bethe approximations in graphical models, and their relation to
BP. The first key work is [26], which showed that the
fixed points of BP are local optima of the Bethe free
energy. Later this result was refined in [6] to show that
the stable fixed points of BP are local minima of the
Bethe free energy.
Another line of work focuses on conditions under which
the BP has a unique fixed point (implying no local
minima via [6]). Such works (e.g., [18, 11, 13, 7]) typically provide sufficient conditions on the model structure and parameters for BP to have a unique fixed
point. As we show in Section 3.3, such results can be
used to obtain inner bounds on BL , although they were
not originally developed for this purpose.
There has been less work on understanding learning
with Bethe approximations. The canonical parameters (see Section 1.3) have been suggested in several
works [21, 23, 25]. As we show here, these parameters
are generally not the ones optimizing the Bethe likelihood, but when µ̄ is learnable, they are in fact optimal.
Much stronger theory is available for learning with
convex free energies such as tree-reweighted variants.
In [19] it is shown that such methods have desirable
stability and asymptotic properties. The performance
of Bethe learning is not analyzed in this context, since
it does not fall under the convex approximations.
Our work proposes a novel view of learning with Bethe,
which is to focus on the marginals that Bethe can
and cannot match during learning. This highlights the
regimes where Bethe cannot be expected to work well,
and those where it might work, as we show further
in our empirical results. Our results provide initial
characterization of the set BL in terms of inner and
outer bounds. We expect that these can be tightened
further.

5

Illustrating the Bounds

In this section we provide several graphical illustrations of the bounds on BL that we presented earlier. We focus on the case of a two dimensional 3 × 3
toroidal grid graph, and on homogenous parameters as
described in Section 3.2. In this case, µ̄ can be conveniently represented in two dimensions (i.e., µv , µe ).
We begin by showing a case where the empirical
marginals µ̄ are unlearnable. In this case, the maximum Bethe likelihood parameter θB (µ̄) results in a
function F (µ; θB (µ̄)) which is not maximized by µ̄
(i.e., moment matching is not achieved). Rather, the
function F has two other maxima, and µ̄ lies in their

0.8
0.7

µv

0.6
0.5
0.4
Empirical marginals

0.3
0.2

Bethe maximizers

0.1
0.1

0.2

0.3

0.4

0.5
µe

0.6

0.7

0.8

0.9

1

Figure 1: Illustration of a case where µ̄ (denoted by
empirical marginals in the figure) does not maximize
F (µ; θ B (µ̄)). However, µ̄ is obtained as a convex combination of the two global maxima of F (µ; θ B (µ̄)). The
colormap corresponds to the values of F (µ; θ B (µ̄)). In
this case θ B (µ̄) was found by exhaustive search.
convex hull, as described in Section 1.2. This is shown
in Figure 1.
Figure 2 depicts several results regarding the set BL .
The colored regions correspond to marginals which
can be obtained via some empirical distribution (i.e.,
the marginal polytope [23]). The blue region indicates marginals that are learnable according to the
inner bound in Section 3.3. The red region indicates
marginals that are unlearnable according to the outer
bounds in Sections 3.1 and 3.2. In this case, lemmas
1, 2 and 3 yield identical outer bounds. The region in
black is not covered by any of our bounds. However,
there is an easy way to check empirically whether it
might be learnable: run gradient descent on the Bethe
likelihood (using BP to approximate the objective and
gradient12 ), and check whether the parameters at convergence satisfy moment matching. It turns out that
for the region in black, moment matching is achieved.
Note that due to the potential suboptimality of BP,
this is not a theoretical guarantee that this region is indeed learnable (i.e., it could be that we have reached a
suboptimal parameter, or that the reported marginals
are not the optimal ones for Bethe). Taken together,
the results in Figure 2 suggest that our outer bounds
are tight for the given graph.

6

Discussion

This work presents an analysis of when learning with
Bethe is guaranteed to fail, in the sense of not match12
This is an approximation since BP does not necessarily
find the global optimum of the Bethe free energy.

by checking one of our outer bounds). Additionally, we
show that in the binary homogenous models, there are
graphs where ferromagnetic models are unlearnable.

1
0.9
0.8

Our analysis of the Hessian in Section 3.2 shows that
there are marginals µ̄ that cannot be local minima of
the Bethe free energy. This in turn implies that they
will not be stable fixed points of BP. This highlights a
very interesting limitation of using BP to approximate
marginals. Namely, that there regions of marginals
space which will never be obtained as a result of running BP. It will be interesting to study the practical
implications of this observations.

0.7

µv

0.6
0.5
0.4
0.3
0.2
Unlearnable
Learnable (Inner Bound)
Learnable (Grad. Opt.)

0.1
0.1

0.2

0.3

0.4

0.5
µe

0.6

0.7

0.8

0.9

1

Figure 2: Illustration of bounds on BL space. Each point
in the figure corresponds to an empirical marginal µ̄. The
blue region shows marginals that are learnable according
to Section 3.3. The red region shows marginals that are
unlearnable according to Sections 1.3 and 3.2. The black
region is not covered by any of our bounds. However,
when running gradient ascent on the Bethe likelihood with
these marginals, it turns out that moment matching is
achieved at convergence (we decide that moment matching
is achieved when the absolute difference between empirical
and Bethe marginals is less than 0.01).

ing the moments of the training data. One of the key
difficulties of the Bethe approximation of marginals is
the non-convexity of the Bethe free energy, resulting in
local optima during inference. Our results show that
when using Bethe within learning, another problem
surfaces, even in the case where exact optimization
of the Bethe free energy is possible (and hence learning is tractable since the Bethe likelihood is concave).
Namely, that for particular empirical marginals (those
outside BL ) moment matching will not be achieved.
Characterizing the set BL is a challenge, and we have
presented initial steps in this direction by showing how
to obtain inner and outer bounds on it, some in closed
form. Our analysis also highlights the fact that the
canonical parameters often used in learning are inadequate in some cases. On the other hand, they can be
used to obtain outer bounds on BL as in Lemma 1.
Many interesting questions arise from our analysis and
deserve further study: e.g., when are the inner and
outer bounds we presented tight, and whether tighter
bounds exist? When are the canonical parameters optimal and in BL ?
On a practical level our results imply that there are
regimes when Bethe learning is bound to fail, and that
in some cases they can be inferred from the empirical
marginals without running a learning algorithm (e.g.,

Finally, our results do not imply that one should not
use Bethe approximations within learning. It has
been previously shown that Bethe approximations of
marginals outperform convex variants across a wide
range of parameter settings [9]. It is thus certainly
possible that for data such that µ̄ ∈ BL the learned
parameters will perform well. We intend to address
this issue as well as other theoretical questions in future work.

A

Proof of Lemma 3

We first note that the Hessian of HB (µ) has a particularly simple form in the homogeneous case. Denote
this Hessian matrix by A. Then its elements correspond to only NV +3 unique numbers ai (i = 1, . . . , V ),
b, c , d, which are given by:
1
1
+
) − di c
µv
1 − µv
1
b = −
1 − 2µv + µe
1
1
+
c =
(µv − µe ) (1 − 2µv + µe )
1
1
d = − −
−c
µe
µv − µe

ai

=

(di − 1)(

The Hessian depends on these values via:

ak k ∈ V, l ∈ V
l=k




b
k
∈
V,
l
∈
V
l
∈
N (k)



c k ∈ V, l ∈ E
k∈l
A[k,l] =
c k ∈ E, l ∈ V
l∈k




d
k,
l
∈
E
k
=l



0
otherwise

(12)

where N (k) is the set of neighbors of node k. The
indexing scheme is as follows: by k ∈ V we mean k is
the coordinate corresponding to µk and k ∈ E means
k is the coordinate corresponding to some edge in E.
The Hessian A is negative-semi-definite iff for all z
it holds that z T Az ≤ 0. We will show that if the

condition in Eq. 11 is satisfied then we can find a z
such that z T Az > 0 and therefore A is not negative
definite, and the lemma follows. We will define such a
z in the following way: zi = 1 if i ∈ V and zi = z (for
1
we
some scalar z) if i ∈ E. Denoting â = µ1v + 1−µ
v
obtain after some algebra that:13

[10] J. Mooij and H. Kappen. On the properties of the
Bethe approximation and loopy belief propagation on
binary networks. Journal of Statistical Mechanics:
Theory and Experiment, page P11012, 2005.

z T Az = (2E − V )â − 2NE c + 2NE b + NE z 2 d + 4NE zc

[12] R. T. Rockafellar. Convex Analysis (Princeton Mathematical Series). Princeton University Press, 1970.

The above is a quadratic concave function in z. When
its discriminant is greater than zero, it will attain positive values, and A will not be negative definite. The
condition on the discriminant corresponds to:
1
NV
0 < c2 − d (â − c + b) +
dâ
2
4NE

(13)

Assigning the values of the Hessian and some more
algebra leads to the equation:
0 < (µe − µ2v )(1 −

NV
NV
)−
µv (1 − µv )
2NE
2NE

(14)

Switching sides we get the condition in Eq. 11.

References
[1] J. Besag. On the statistical analysis of dirty pictures.
Journal of the Royal Statistical Society, B-48:259–302,
1986.
[2] S. Boyd and L. Vandenberghe. Convex Optimization.
Cambridge Univ. Press, 2004.
[3] M. M. Deza and M. Laurent. Geometry of Cuts and
Metrics, volume 15 of Algorithms and Combinatorics.
Springer, 1997.
[4] C. M. Fortuin, P. W. Kasteleyn, and J. Ginibre. Correlation inequalities on some partially ordered sets.
Comm. in Math. Physics, 22:89–103, 1971.
[5] V. Ganapathi, D. Vickrey, J. Duchi, and D. Koller.
Constrained approximate maximum entropy learning
of Markov random fields. In Uncertainty in Artificial
Intelligence, pages 196–203, 2008.
[6] T. Heskes. Stable fixed points of loopy belief propagation are minima of the Bethe free energy. In Advances
in Neural Information Processing Systems 15, pages
343–350, 2003.
[7] T. Heskes. On the uniqueness of loopy belief propagation fixed points. Neural Computation, 16(11):2379–
2413, 2004.
[8] D. Koller and N. Friedman. Probabilistic Graphical
Models: Principles and Techniques. MIT Press, 2009.
[9] O. Meshi, A. Jaimovich, A. Globerson, and N. Friedman. Convexifying the Bethe free energy. In Uncertainty in Artificial Intelligence, pages 402–410. 2009.
13

Because of the structure of the matrix elements, the
dependence on the degrees di nicely cancels out after summation.

[11] J. M. Mooij and H. J. Kappen. Sufficient conditions
for convergence of the sum-product algorithm. IEEE
Trans. on Info. Theory, 53(12):4422–4437, Dec. 2007.

[13] T. Roosta, M. Wainwright, and S. Sastry. Convergence analysis of reweighted sum-product algorithms.
IEEE Trans. on Signal Proc., 56(9):4293–4305, 2008.
[14] N. Shental, A. Zomet, T. Hertz, and Y. Weiss. Learning and inferring image segmentations using the GBP
typical cut algorithm. In ICCV, pages 1243–, 2003.
[15] E. B. Sudderth, M. J. Wainwright, and A. S. Willsky.
Loop series and Bethe variational bounds in attractive
graphical models. In Advances in Neural Information
Processing Systems 20, pages 1425–1432, 2008.
[16] C. Sutton and A. McCallum. Piecewise training for
undirected models. In Uncertainty in Artificial Intelligence, pages 568–575, 2005.
[17] B. Taskar, V. Chatalbashev, and D. Koller. Learning associative Markov networks. In Proc. of the 21st
International Conference on Machine learning, 2004.
[18] S. C. Tatikonda and M. I. Jordan. Loopy belief propagation and Gibbs measures. In Uncertainty in Artificial Intelligence, pages 493–500, 2002.
[19] M. Wainwright. Estimating the ”wrong” graphical
model: Benefits in the computation-limited setting.
J. Mach. Learn. Res., 7:1829–1859, December 2006.
[20] M. Wainwright, T. Jaakkola, and A. Willsky. Treebased reparameterization framework for analysis of
sum-product and related algorithms. IEEE Trans. on
Info. Theory, 49(5):1120–1146, 2003.
[21] M. Wainwright, T. Jaakkola, and A. Willsky. Treereweighted belief propagation algorithms and approximate ML estimation by pseudomoment matching. In
AISTATS, volume 21, 2003.
[22] M. Wainwright, T. Jaakkola, and A. Willsky. A new
class of upper bounds on the log partition function.
IEEE Trans. on Info. Theory, 51:2313–2335, 2005.
[23] M. J. Wainwright and M. Jordan. Graphical models,
exponential families, and variational inference. Found.
Trends Mach. Learn., 1(1-2):1–305, 2008.
[24] M. Welling and Y. W. Teh. Approximate inference in
Boltzmann machines. Artificial Intelligence, 143:19–
50, 2003.
[25] M. Yasuda and K. Tanaka. Approximate learning algorithm in Bltzmann machines. Neural computation,
21(11):3130–3178, 2009.
[26] J. Yedidia, W. Freeman, and Y. Weiss. Constructing free-energy approximations and generalized belief
propagation algorithms. IEEE Trans. on Info. Theory, 51(7):2282– 2312, 2005.

