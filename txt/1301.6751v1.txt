696

A Method for Speeding Up Value Iteration
in Partially Observable Markov Decision Processes

Nevin L. Zhang, Stephen S. Lee, and Weihong Zhang
Department of Computer Science, Hong Kong University of Science & Technology
{lzhang, sslee, wzhang}Ccs.ust.hk

Abstract
We present a technique for speeding up
the convergence of value iteration for par­
tially observable Markov decisions processes
(POMDPs). The underlying idea is similar
to that behind modified policy iteration for
fully observable Markov decision processes
(MDPs). The technique can be easily incor­
porated into any existing POMDP value it­
eration algorithms. Experiments have been
conducted on several test problems with one
POMDP value iteration algorithm called in­
cremental pruning. We find that the tech­
nique can make incremental pruning run sev­
eral orders of magnitude faster.

1

INTRODUCTION

POMDPs are a model for sequential decision making
problems where effects of actions are nondeterministic
and the state of the world is not known with certainty.
They have attracted many researchers in Operations
Research and AI because of their potential applica­
tions in a wide range of areas (Monahan 1982, Cas­
sandra 1998b). However, there is still a significant gap
between this potential and actual applications, primar­
ily due to the lack of effective solution methods. For
this reason, much recent effort has been devoted to
finding efficient algorithms for POMDPs.
This paper is concerned with only exact algorithms.
Most exact algorithms are value iteration algorithms.
They begin with an initial value function and improve
it iteratively until the Bellman residual falls below a
predetermined threshold. See Cassandra (1998a) for
excellent descriptions, analyses, and empirical compar­
isons of those algorithms.
There are also policy iteration algorithms for
POMDPs. The first one is proposed by Sondik (1978).

A simpler one is recently developed by Hansen (1998).
It is known that, in terms of number of iterations, pol­
icy iteration for MDP converges quadratically while
value iteration converges linearly (e.g. Puterman 1990,
page 369). Hansen has empirically shown that his pol­
icy iteration algorithm for POMDPs also converges
much faster than one of the most efficient known
value iteration algorithms, namely incremental prun­
ing (Zhang and Liu 1997, Cassandra et a/1997).

Policy iteration for MDPs solves a system of linear
equations at each iteration. The numbers of unknowns
and equations in the system are the same as the size of
the state space. Consequentially, it is computationally
prohibitive to solve the system when the state space
is large. Modified policy iteration (MPI) (Puterman
1990, page 371) alleviates the problem using a method
that computes an approximate solution without actu­
ally solving the system. Numerical results reported in
Puterman and Shin (1978) suggest that modified pol­
icy iteration is more efficient than either value iteration
or policy iteration in practice.
Hansen (1998) points out that the idea of MPI can
also be incorporated into his POMDP policy iteration
algorithm and finds that such an exercise is not very
helpful (Hansen 1999).
The paper describes another way to apply the MPI
idea to POMDPs. Our method is based on the view
that MPI is also a variant of value iteration (van Nunen
1976) 1. Under this view, the basic idea is to "improve"
the current value function for several steps using the
current policy before feeding it to the next step of value
iteration. Those improvement steps are less expensive
than standard value iteration steps. Nonetheless, they
do get the current value function closer to the optimal
value function.
MPI for MDPs improves a value function at all states.
1 As a matter of fact, it was first proposed
of value iteration by van Nunen.

as

a variant

Speeding Up POMDP Value Iteration

This cannot be done for POMDPs since there are infi­
nite many belief states. Our method improves a value
function at a finite number of selected belief states. A
nice property of POMDPs is that when a value func­
tion is improved at one belief state, it is also improved
in the neighborhood of that belief state.
We call our method point-based improvement for the
lack of a better name. It is conceptually much simpler
than Hensen's policy iteration algorithm. Nonetheless,
it is as effective as, in some cases more effective than,
Hansen's algorithm in reducing the number of itera­
tions it takes to find a policy of desired quality and
hence drastically speeds up incremental pruning.
2

2.1

POMDPs

A partially observable Markov decision process
(POMDP) is a sequential decision model for an agent
who acts in a stochastic environment with only partial
knowledge about the state of the environment. The set
of possible states of the environment is referred to as
the state space and is denoted by S. At each point in
time, the environment is in one of the possible states.
The agent does not directly observe the state. Rather,
it receives an observation about it. We denote the set
of all possible observations by Z. After receiving the
observation, the agent chooses an action from a set
A of possible actions and execute that action. There­
after, the agent receives an immediate reward and the
environment evolves stochastically into a next state.
Mathematically, a POMDP is specified by the three
sets S, Z, and A; a reward function r(s,a); a transi­
tion probability P( s 'j s, a); and an observation probabil­
ity P(zjs', a). The reward function characterizes the
dependency of the immediate reward on the current
state s and the current action a. The transition prob­
ability characterizes the dependency of the next state
'
s on the current state s and the current action a. The
observation probability characterizes the dependency
of the observation z at the next time point on the next
state s' and the current action a.
2.2

contained in the current observation, previous obser­
vations, and previous actions can be summarized by
a probability distribution over the state space. The
probability distribution is sometimes called a belief
state and denoted by b. For any possible states, b(s)
is the probability that the current state is s. The set of
all possible belief states is called the belief space. We
denote it by B.
A policy prescribes an action for each possible belief
state. In other words, it is a mapping from B to A.
Associated with policy 1r is its value function V". For
each belief state b, V" (b) is the expected total dis­
counted reward that the agent receives by following
the policy starting from b, i.e.

POMDP AND VALUE
ITERATION

We begin with a brief review of POMDPs and value
iteration.

Policies and value functions

Since the current observation does not fully reveal the
identity of the current state, the agent needs to con­
sider all previous observations and actions when choos­
ing an action. Information about the current state

697

00

V"(b)

=

E,,b["L A1rt]
t=O

where r1 is the reward received at timet and A (0 <
A < 1) is the discount factor. It is known that there
'
exists a policy 7r* such that V" (b) � V"(b) for any
other policy 1r and any belief state b. Such a policy
is called an optimal policy. The value function of an
optimal policy is called the optimal value function. We
denote it by v•. For any positive number E, a policy
1r is E-optimal if
V"(b) + f?: V*(b)
2.3

Vb

E B.

Value iteration

To explain value iteration, we need to consider how
belief state evolves over time. Let b be the current
belief state. The belief state at the next point in time
depends not only on the current belief state, but also
on the current action a and the next observation z.
We denote it by b�. For any states', b�(s') is given by

baz (8')

=

E. P(s', zjs, a)b(s)
P(zjb, a)
'

(1)

and
where
P(z, s'js, a)=P(zjs', a)P(s'js,a)
P(zjb, a)= Es,s' P(z, s'js, a)b(s) is the renormalization
constant. As the notation suggests, the constant can
also be interpreted as the probability of observing z
after taking action a in belief state b.
Define an operator T that takes a value function V
and returns another value function TV as follows:
TV(b) = maxa[r(b,a) +A

L P(zjb, a)V(b�)]Vb E B

(2)

z

where r(b, a) = E. r(s, a)b(s) is the expected imme­
diate reward for taking action a in belief state b. For

698

Zhang, Lee, and Zhang

a given value function V, a policy
improving if
7r(b)

=

arg maxa[r(b, a)+ A

71"

is said to be V ­

L P(zlb, a)V(b�)]

VI:

1. Vo +- {0}, n +- 0.
2. do {
3.
n +- n + 1.
Vn +- DP-UPDATE(Vn-1).
4.
5. } while maxbiVn(b)- Vn-1(b)l > <(1- >.)/2>..
6. return Vn.

(3)

z

for all belief states b.
Value iteration is an algorithm for finding <-optimal
policies. It starts with an initial value function Vo and
iterates using the following formula:
Vn

=

TVn-1·

It is known (e.g. Puterman 1990, Theorem 6.9)
that Vn converges to V* as n goes to infinity.
Value iteration terminates when the Bellman residual
ma.xb IVn(b)- Vn-1(b)l falls below <(1- >.)/2>.. When
it does, a Vn-improving policy is <-optimal.
Since there are infinite many possible belief states,
value iteration cannot be carried explicitly. Fortu­
nately, it can be carried out implicitly. Before explain­
ing how, we first introduce several technical concepts
and notations.
2.4

Technical and notational considerations

For convenience, we call functions over the state space
vectors. We use lower case Greek letters a and (3 to
refer to vectors and script letters V and U to refer to
sets of vectors. In contrast, the upper letters V and U
always refer to value functions, i.e. functions over the
belief space B. Note that a belief state is a function
over the state space and hence can be viewed as a
vector.
A set V of vectors induces a value function as follows:
f(b)

=

maxaEva.b

Vb

E

B,

where a.b is the inner product of a and b, i.e.
a.b= Ls a(s)b(s). For convenience, we also use V(.)
to denote the value function defined above: For any
belief state b, V(b) stands for the quantity given at the
right hand side of the above formula.
A vector in a set is extraneous if its removal does not
change the function that the set induces. It is useful
otherwise. A set of vector is parsimonious if it contains
no extraneous vectors.
2.5

Implicit value iteration

A value function V is represented by a set of vectors if
it equals the value function induced by the set. When
a value function is representable by a finite set of vec­
tors, there is a unique parsimonious set of vectors that
represents the function.

Figure 1: Value iteration for POMDPs.
Sondik (1971) has shown that if a value function Vis
representable by a finite set of vectors, then so is the
value function TV. The process of obtaining a par­
simonious representation for TV from a parsimonious
representation of Vis usually referred to as dynamic­
programming update. Let V be the parsimonious set
that represents V. For convenience, we sometimes use
TV to denote the parsimonious set of vectors that rep­
resents TV.
In practice, value iteration for POMDPs is implicitly

carried in the way as shown in F igure 1. One be­
gins with a value function Vo that is representable by
a finite set of vectors. In this paper, we assume the
initial value function is 0. At each iteration, one per­
forma dynamic-programming update on the parsimo­
nious set Vn-1 of vectors that represents the previous
value function Vn-1 and obtains a parsimonious set
of vectors Vn that represents the current value nmc­
tion Vn. One continues until the Bellman residual
maxbJVn(b)- Vn-1 (b)l, which is determined by solving
a sequence of linear programs, falls below a threshold.
3

PROPERTIES OF VALUE
ITERATION

This paper presents a technique for speeding up the
convergence of value iteration. The technique is de­
signed for POMDPs with nonnegative rewards, i.e.
POMDPs such that r(s, a)?:O for all s and a. In this
section, we study the properties of value iteration in
such POMDPs and show how a POMDP with negative
rewards can be transformed into one with nonnegative
rewards that is in some sense equivalent. Most proofs
are omitted due to space limit.
We begin with a few definitions. In a POMDP, a value
function U dominates another V if U(b)?:V(b) for all
belief states b. It strictly dominates V if it dominates
V and U(b)>V(b) for at least one belief state b. A
value function is (strictly) suboptimal if it is (strictly)
dominated by the optimal value function.
A set of vectors is (strictly) dominated by a value func­
tion if its induced value function is. A set of vectors

Speeding Up POMDP Value Iteration

is (strictly) suboptimal if it is (strictly) dominated by
the optimal value function.

VI1:

1. Vo +- {0}, n +- 0.
2. do {
3.
n +- n + 1;
Un +- DP-UPDATE(Vn-di
4.
.Sf- maxb\Un(b)- Vn-I(b)\;
5.
if .s > t(1 - >..) /2>..
6.
Vn f- improve(Un)i
7.
8. } while .S > e(1 - >..)/2>...
9. Return Vn·

A set of vectors is (strictly) dominated by another set
of vectors if it is (strictly) dominated by the value func­
tion induced by the that set.
3.I

General properties of value iteration

In any POMDP, if a set of vectors V is
suboptimal, then so is TV. Moreover, if V dominates
another set of vectors V', then TV dominates TV'.

Lemma I

In any POMDP, if a set of vectors V is
strictly suboptimal, then there exist at least one belief
state b such that TV(b)>V(b).

Figure 2: A new variant of value iteration.

Lemma 2

3.2

Unless explicitly stated otherwise, all POMDPs con­
sidered from now on are with nonnegative rewards.

Properties of value iteration in POMOPs
with nonnegative rewards

4

Using Lemma 2, one can show

Consider running VI on a POMDP with
nonnegative rewards. Let Vn-! and Vn be respectively
the sets of vectors produced at the n-lth and nth iter­
ation. Then, Vn-! is strictly dominated by Vn, which
in tum is dominated by the optimal value function.

Theorem I

Note that the theorem falls short of saying that, when
the reward function is nonnegative, TV strictly domi­
nates V for any suboptimal set of vectors V. As a mat­
ter of fact, this is not always the case. As a counter ex­
ample, assume r(s0,a)=O for a certain state so regard­
less of the action. Let b0 be the belief state that is 1
at so and 0 everywhere else. Further assume V* (bo>O
and let n0 be a vector such that no(so)=V*(bo) and
n0 ( s) =0 for any other states s. It is easy to see that
if V consists of only no, then TV(bo)<V(bo).
Despite of the fact TV does not always strictly dom­
inate V, TV(b) is strictly larger than V(b) for beliefs
b in most parts of the belief space when the reward
function is nonnegative.
3.3

699

POMDPs with negative rewards

A POMDP with negative rewards can always be trans­
formed into one with nonnegative rewards by adding
a large enough constant to the reward function. It is
easy to see that an t-optimal policy for the transformed
POMDP is also t-optimal for the original POMDP
and vice versa. Moreover, the value function in the
original POMDP of a policy equals that in the trans­
formed POMDP minus C/(1->..), where Cis the con­
stant added. In other words, the original POMDP is
solved if the transformed POMDP is solved. There­
fore, we can restrict to POMDPs with nonnegative
rewards without losing generality.

S PEEDING UP VALUE
ITERATION

The section develops our technique for speeding up
value iteration in POMDPs with nonnegative rewards.
We begin with the basic idea.
4.I

Point-based improvement

Consider a suboptimal set of vectors V. By improving
V, we mean to find another suboptimal set of vectors
that strictly dominates V. By improving V at a belief
state b, we mean to find another suboptimal set of
vectors U such that U(b)>V(b).
Value iteration starts with the singleton set {0}, which
is of course suboptimal, and improves the set itera­
tively using dynamic-programming update (Theorem
1). Dynamic-programming is quite expensive, espe­
cially when performed on large sets of vectors. To
speed up value iteration, we devise a very inexpensive
technique called point-based improvement for improv­
ing a set of vectors and use it multiple times in be­
tween dynamic-programming updates. This technique
can be incorporated into value iteration as shown in
Figure 2. Applications of the technique are encapsu­
lated in the subroutine improve. The Bellman residual
li is used in improve to determine how many times the
technique is to be used.

If, for any suboptimal set of vectors U,
the output of improve(U, li) - another set of vectors
- is suboptimal and dominates U, then VIl termi­
nates in a finite number of steps.

Theorem 2

Let Vn and V� be respectively the sets of vec­
tors produced at the nth iteration of VIl and VI. From
Lemma 1 and the condition imposed on improve, we
conclude that Vn is suboptimal and dominates V�.

Proof:

700

Zhang, Lee, and Zhang

Since V� monotonically increases with n (Theorem 1)
and converges to V* as n goes to infinity, Vn must also
converge to V*. The theorem follows. D
4.2

R(a,U) = {bEBia.b>a'.b \la'EU\{a}},
R(a,U) = {bEBia.b�a'.b Va'EU\{a}}.

Improving a set of vectors at one belief
state

For the rest of this section, we let V be a fixed sub­
optimal set of vectors and let U=TV. We develop a
method for improving U.
To begin with, consider how U might be improved at a
particular belief state b. According to (2), there exists
an action a such that

U(b) = r(b,a) + A

L P(zlb,a)V(b�).

(4)

z

For each observation z, let f3z be a vector in U that has
maximum inner product with b�. Define a new vector
by

f3(s) = r(s,a) + A

L P(z,s 'ls,a)f3z(s')
z,s

'

Vs E S. (5)

We sometimes denote this vector by backup(b,a, U).
Theorem 3

For the vector f3 given by (5), we have

b.f3 = r(b,a) + A

L P(zlb,a)U(b�).

(6)

z

As pointed out at the end of Section 3.2, U(b�) is of­
ten larger than V(b�) A quick comparison of (4) and
(6) leads us to conclude that b.f3 is often larger than
U(b). When this is the case, we have found a set that
improves U at b, namely the singleton set {(3}. The
set is obviously suboptimal.
.

There is an obvious variation to the idea presented
above. Instead of using the vector backup(b, a, U) for
the action that satisfies (4), we can consider the vectors
backup(b, a',U) for all possible actions a' and choose
the one whose inner product with b is the maximum.
This should, hopefully, improve U at b even further.
We tried this variation and found that the costs are
almost always greater than the benefits.
4.3

of the belief space B respectively given by

Improving a set of vectors at multiple
belief states

It is straightforward to generalize the idea of the previ­
ous subsection from one belief state to multiple belief
states. The question is what belief states to use. There
are many possible answers. Our answer is motivated
by the properties of dynamic-programming update.
For any vector ainU, define its witness region R(a,U)
and closed witness region R(a,U) w.r.t U to be regions

During dynamic-programming update, each vector a
in U is associated with a belief state that is in the
closed witness region of a. We say that the belief
state is the anchoring point provided for a by dynamic­
programming update and denote it by point(a). The
vector is also associated with an action, which we de­
note by action(a). It is the action prescribed for the
belief state point(a) by a V(.)-improving policy. Be­
cause of those, equation (4) is true if b is point(a) and
a is action(a).
We choose to improve U on the anchoring points using

U1

=

{backup(point(a),action(a),U)Ia E U}.

(7)

According to the discussions of the previous subsec­
tion, the value function U1 (.) is often larger than U(.)
at the anchoring points. When a value function is
larger than another one at one belief state, it is also
larger in the neighborhood of the belief state. There­
fore, the value function U1 (.) is often larger than U(.)
in regions around the anchoring points. Our experi­
ence reveal that it is often larger in most parts of the
belief space. The explanation is that the anchoring
points scatter "evenly" over the belief space w.r.t U in
the sense that there is one in the closed witness region
of each vector of U.
There is one optimization issue. Even that the inner
product of the vector backup(point(a), action(a),U)
with the belief state point(a) is often larger than that
of a with the belief state, it might be smaller some­
times. When this is the case, we use a instead of
backup(point(a), action(a),U) so that the value at
the belief state is as large as possible.
4.4

Relation to modified policy iteration for
MDPs

Point-based improvement is closely related to MPI for
MDPs (Puterman 1990, page 371). It can be shown
that, for each anchoring point b,
ul (b)

=

r(b,7r(b)) + A

L P(zlb,7r(b))U(b�(b)),

(8)

z

where 1r is a V(.)-improving policy. This formula is
very similar to formula (6.37) of Puterman (1990).
MDP modified policy iteration uses the latter formula
to "improve" the value of each possible state of the
state space. We cannot apply the above formula to all
possible belief states since there are infinite many of

Speedi ng Up POMOP Value Iteration

improve(U):
1. Uo +-- U, k t-- 0.
2. do {
3. k +-- k+ 1, uk +-- 0, w +-- 0.
4. for each vector a in Uk-1
a' +--backup(point(a),action(a),Uk-1 U W).
5.
6.
if a. point(a)> a' .point(a)
7.
a' +--a .
else W +-- WU {a'}.
8.
point(a') t-- point(a),
9.
10. action(a') +--action(a);
11. uk +--uk u {a'}.
12.} while stop(Uk,Uk-d =false.
13. return uk u u.
Figure 3: The improve subroutine.
them. So, we choose to use the formula to improve the
values of a finite number of belief states, namely the
anchoring points.
4.5

Repeated improvements

We now know how we might improve U at the anchor­
ing points. In hope to get as much improvement as
possible, we want, of course, to apply the technique
on U1 and try to improve it further. This can easily
be done. Observe that there is a one-to-one correspon­
dence between vectors in U and U1: for each vector a
in U, we have backup(point(a),action(a),U) in U1.
We associate the latter with the same belief state and
action as the former. Then we can improve U1 at the
anchoring points the same way as we improve U. The
process can of course be repeated for the resulting set
of vectors and so on.
The above discussions lead to the routine shown in Fig­
ure 3. The routine improves the input vector set at the
anchoring points iteratively. Improvement takes place
at line 5. Lines 6 and 7 guarantee that the values of the
anchoring point never decrease. The improved vector
a' is added to W at line 8 so that better improve. ­
ments can be achieved for vectors yet to be processed.
At lines 9 and 10, the belief state and action associated
with a vector of the previous iteration are assigned to
the corresponding vector at the current iteration.
The stopping criterion we use is

where E1 is a positive number smaller than 1. In our
experiments, E1 is set at 0 .. Compared with the stop­
ping criterion of value iteration, the stopping criterion
is stricter. The reason for this is that the improvement
step is computationally cheap.

701

Finally, the union UkUU is returned instead of Uk for
the following reason. While Uk(b) is no smaller than
U(b) at the anchoring points, it might be smaller at
some other belief states. In other words, Uk might
not dominate v. If improve simply returns uk, the
conditions of Theorem 2 are not met. Consequently,
the union ukuu is returned.
4.6

Pruning extraneous vectors

The union UkUU usually contains many extraneous
vectors. They should be pruned to avoid unnecessary
computations in the future. One way to doing so is to
simply apply Lark's algorithm (White 1991 ) .
Lark's algorithm solves a linear program for each input
vector. It is expensive when there is a large number of
vectors. We use a more efficient method. The moti­
vation lies in two observations: First, most vectors in
Uk are not extraneous. Second, many vectors in U are
componentwise dominated by vectors in uk and hence
are extraneous. The method is to replace line 13 with
the following lines:
13. Prune from U vectors that are componentwise
dominated by vectors in uk.
14. Prunes from U vectors a such that
R(a,UkUU) is empty.
15. return ukuu.
At line 14, a linear program is solved for each vector in
U. Since no linear programs are solved for vectors in Uk
and the set U usually becomes very small in cardinality
after line 13, the method is much more efficient than
simply applying Lark's algorithm to the union UkUU.
4. 7

Recursive calls to

improve

Consider the set U after line 14 of the algorithm seg­
ment given in the previous subsection. If it is not
empty, then every vector a in the set is useful. This is
determined by solving a linear program. In addition
to determining the usefulness of a, solving the linear
program also produces a belief state b that is in the
closed witness region R(a,UkUU).
The facts that a is from the input set U and that b
is in R(a,UkUU) imply that the value at b has not
been improved. To achieve as much improvement as
possible, we improve the value by a recursive call to
improve. To be more specific, we reset point(a) to b
at line 14 and replace line 15 with the following:
15. if U 'f 0, return improve(UkUU,8).
16. else return uk.

702

5

Zhang, Lee, and Zhang

EMPIRICAL RESULTS

ng.,

Experiments have been conducted to empirically de­
termine the effectiveness of point-based improvement
in speeding up value iteration and to compare it im­
provement with Hansen's policy iteration algorithm.
Four problems were used in the experiments for both
purposes. The problems are commonly referred to as
Tiger, Network, Shuttle, and Aircraft ID in the litera­
ture and were downloaded from Cassandra's POMDP
page 2• Information about their sizes is summarized
in the following table.

IISIIIZIIIAII
Tiger
Network
Shuttle
Aircraft ID
5.1

2
7
8
12

2
2
2
5

VI -+-�·
Vl1 -+-

0.001

�.,.........._�....._...
....
..._ ....
._ c...
..
....1

0.0001
0.01

The effectiveness of point-based improvement is deter­
mined by comparing VI and VI1. We borrowed an
implementation of VI by Cassandra and VIl was im­
plemented on top of his program. Cassandra's pro­
gram provides a number of algorithms for dynamic­
programming update. For our experiments, we used
a variation of incremental pruning called restricted re­
gion (Cassandra et a/1997). The discount factor was
set at 0.95 and experiments were conducted on an UJ­
traSparc II.
For the purpose of comparison, we collected informa­
tion about the quality of the policies that VI and VIl
produce as a function of the times they take. The
quality of a policy is measured by the distance be­
tween its value function to the optimal value function,
i.e. the minimum € such that the policy is €-optimal.
The smaller the distance, the better the policy. Since
we do not know the optimal value function, the dis­
tance cannot be exactly computed. We use an upper
bound derived from the Bellman residual.
One experiment was conducted for each algorithm­
problem combination. The experiment was terminated
when either an 0.01-optimal policy was produced or
the run time exceeded 24 hours, i.e. 86400 seconds,
CPU time.

1

10

100 1000

100000

CPU time In seconds
Nelworl<

i
..

f

0

3
4
3
6

Effectiveness of point-based improvement

0.1

100

-

10

:-\\

V11 -+0.1

__

0.01
0.001
O.o1

1

0.1

CPU time in seconds

10

100 1000

100000

0.1

CPU time In seconds

10

100

1000

100000

1000

§"

8.
..
.�

�

0

10

0.1
0.01

1

Aircraft IO

10000
1000

i
!
..

100
10
VI -+-·
VII -+-

1
0.1
0.01
0.001
0.0001
1e-05
O.o1 0.1

1

10

100

10000

CPU time In seconds

10+06

Figure 4: Empirical results. See text for explanations.
We see that VIl was able to produce a 0.01-optimal
policy for all four problems in a few iterations. On the
other hand, VI took 215 iterations to produce a 0.01optimal policy for Network. Within the time limit, VI
completed only 35, 16, and 10 iterations respectively
for Tiger, Shuttle, and Aircraft ID. Those suggest that
the technique proposed in this paper is very effective
in reducing the number of iterations that is required
to produce good policies.

It is also clear that VIl is much faster than VI. For
Network, VI took about 17,000 seconds to produce
a 0.01-optimal policy, while VIl took only about 350
seconds. The speedup is about 50 times. VI was not
able to produce "good" policies for Tiger, Shuttle, and
Aircraft ID within the time limit, while VIl produced
2
0.01-optimal or better policies for them in 0.53, 130,
http://vvw.cs.brovn.edu/research/ai/pomdp/index.html and 38,424 seconds respectively.

The data are summarized in the four charts in Figure
4. Note that both axes are in logarithmic scale. There
is one chart for each problem. In each chart, there are
two curves: one for VI and one for VI 1. On each curve,
there is data point for each iteration taken.

Speeding Up POMDP Value Iteration

5.2

Comparisons with Hansen's policy
iteration algorithm

In his implementation, Hansen used standard in­
cremental pruning, instead of restricted region, for
dynamic-programming update. Moreover, while the
round-off threshold is set at w-9 in Cassandra's pro­
gram, Hansen set it at w-6 probably because the rou­
tines for solving linear equations cannot handle pre­
cision beyond w-6• For fairness of comparison, we
implemented VI1 on top Hansen's code.
The following table shows the numbers of iterations
and amounts of time VI1 and Hansen's algorithm took
to find 0.01-optimal policies. We see that VI1 took
fewer iterations than Hansen's algorithms on all prob­
lems. It took less on the first two problems and took
roughly the same time on the last two problems.

Tiger
Network
Shuttle
Aircraft ID

6

VI1

I Hansen

4
10
6
8

14
18
9
9

Time
VI1 I Hansen
3.3
0.51
1122
395
65
73
66,964
72,377

CONCLUS IONS

We have developed a technique, namely point-based
improvement, for speeding up the convergence of value
iteration for POMDPs. The underlying idea is simi­
lar to that behind modified policy iteration for MDPs.
The technique can easily be incorporated into any ex­
isting POMDP value iteration algorithms.
Experiments have been conducted on several test prob­
lems. We found that the technique is very effective in
reducing the number of iterations that is required to
obtain policies with desired quality. Because of this, it
greatly speeds up value iteration. In our experiments,
orders of magnitude speedups were observed.
Acknowledgement

Research is supported by Hong Kong Research Grants
Council Grant HKUST6125 /98E. The authors thank
Cassandra and Hansen's for sharing with us their pro­
grams and the anonymous reviewers for useful com­
ments.
References

Cassandra, A. R. (1998a), Exact and approximate al­
gorithms for partially observable Markov decision
processes, PhD thesis, Department of Computer
Science, Brown University, Providence, Rhode Is­
land, USA.

703

Cassandra, A. R. (1998b), A survey of POMDP ap­
plications, in Working Notes of AAAI Jggs Fall
Symposium on Planning with Partially Obseroable
Markov Decision Processes, 17-24.
Cassandra, A. R., Littman, M. 1., & Zhang, N. L.
(1997). Incremental pruning: A simple, fast, ex­
act method for partially observable Markov deci­
sion processes. In Proceedings of Thirteenth Con­
ference on Uncertainty in Artificial Intelligence,
54-61.
Hansen, E. A. (1998), Finite-memory control of par­
tially obseroable systems, Ph.D. Diss., Depart­
ment of Computer Science, University of Mas­
sachusetts at Amherst.
Hansen, E. A. (1999), Personal communication.
G. E. Monahan (1982), A survey of partially observ­
able Markov decision processes: theory, models,
and algorithms, Management Science, 28 (1), 116.
Puterman, M. L. (1990), Markov decision processes,
in D. P. Heyman and M. J. Sobel (eds.), Hand­
books in OR & MS., Vol. 2, 331-434, Elsevier
Science Publishers.
Puterman, M. L. and M. C. Shin (1982), Action elim­
ination procedures for modified policy iteration
algorithms, Operations Research, 30, 301-318.
Sondik, E. J. (1971). The optimal control of partially
observable Markov processes. PhD thesis, Stan­
ford University, Stanford, California, USA.
Sondik, E. J. (1978), The optimal control of partially
observable Markov processes over a finite horizon,
Operations Resear-ch, 21, 1071-1088.
van

Nunen, J. A. E. E. (1976), A set of successive ap­
proximation methods for discounted Markov de­
cision problems, z. Operations Research, 20, 203208.

White III, C. C. (1991). Partially observed Markov
decision processes: A survey. Annals of Opera­
tions Research, 32.
Zhang, N. L. and W. Liu (1997), A model approxi­
mation scheme for planning in stochastic domains,
Journal of Artificial Intelligence Research, 7, 199230.

