arXiv:1205.0288v2 [cs.LG] 7 Jan 2013

A Randomized Mirror Descent Algorithm
for Large Scale Multiple Kernel Learning

Arash Afkanpour

AndraÌs GyoÌˆrgy

Csaba SzepesvaÌri

Michael Bowling

Department of Computing Science
University of Alberta
Edmonton, AB
{afkanpou,gyorgy,szepesva,mbowling}@ualberta.ca
Abstract
We consider the problem of simultaneously learning to linearly combine a very large
number of kernels and learn a good predictor based on the learnt kernel. When the
number of kernels d to be combined is very large, multiple kernel learning methods whose
computational cost scales linearly in d are intractable. We propose a randomized version
of the mirror descent algorithm to overcome this issue, under the objective of minimizing
the group p-norm penalized empirical risk. The key to achieve the required exponential
speed-up is the computationally efficient construction of low-variance estimates of the
gradient. We propose importance sampling based estimates, and find that the ideal
distribution samples a coordinate with a probability proportional to the magnitude of
the corresponding gradient. We show the surprising result that in the case of learning
the coefficients of a polynomial kernel, the combinatorial structure of the base kernels
to be combined allows the implementation of sampling from this distribution to run
in O(log(d)) time, making the total computational cost of the method to achieve an optimal solution to be O(log(d)/2 ), thereby allowing our method to operate for very large
values of d. Experiments with simulated and real data confirm that the new algorithm is
computationally more efficient than its state-of-the-art alternatives.

1

Introduction

We look into the computational challenge of finding a good predictor in a multiple kernel
learning (MKL) setting where the number of kernels is very large. In particular, we are
interested in cases where the base kernels come from a space with combinatorial structure
and thus their number d could be exponentially large. Just like some previous works (e.g.
Rakotomamonjy et al., 2008; Xu et al., 2008; Nath et al., 2009) we start with the approach
that views the MKL problem as a nested, large scale convex optimization problem, where
the first layer optimizes the weights of the kernels to be combined. More specifically, as the
1

objective we minimize the group p-norm penalized empirical risk. However, as opposed to
these works whose underlying iterative methods have a complexity of â„¦(d) for just any one
iteration, following (Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; RichtaÌrik and
TakaÌcÌ‚, 2011) we use a randomized coordinate descent method, which was effectively used in
these works to decrease the per iteration complexity to O(1). The role of randomization in our
method is to use it to build an unbiased estimate of the gradient at the most recent iteration.
The issue then is how the variance (and so the number of iterations required) scales with d.
As opposed to the above mentioned works, in this paper we propose to make the distribution
over the updated coordinate dependent on the history. We will argue that sampling from a
distribution that is proportional to the magnitude of the gradient vector is desirable to keep
the variance (actually, second moment) low and in fact we will show that there are interesting
cases of MKL (in particular, the case of combining kernels coming from a polynomial family
of kernels) when efficient sampling (i.e., sampling at a cost of O(log d)) is feasible from this
distribution. Then, the variance is controlled by the a priori weights put on the kernels,
making it potentially independent of d. Under these favorable conditions (and in particular,
for the polynomial kernel set with some specific prior weights), the complexity of the method
as a function of d becomes logarithmic, which makes our MKL algorithm feasible even for
large scale problems. This is to be contrasted to the approach of Nesterov (2010, 2012) where
a fixed distribution is used and where the a priori bounds on the methodâ€™s convergence rate,
and, hence, its computational cost to achieve a prescribed precision, will depend linearly on d
(note that we are comparing upper bounds here, so the actual complexity could be smaller).
Our algorithm is based on the mirror descent (or mirror descent) algorithm (similar to the
work of RichtaÌrik and TakaÌcÌ‚ (2011) who uses uniform distributions).
It is important to mention that there are algorithms designed to handle the case of
infinitely many kernels, for example, the algorithms by Argyriou et al. (2005, 2006); Gehler
and Nowozin (2008). However, these methods lack convergence rate guarantees, and, for
example, the consistency for the method of Gehler and Nowozin (2008) works only for â€œsmallâ€
d. The algorithm of Bach (2008), though practically very efficient, suffers from the same
deficiency. A very interesting proposal by Cortes et al. (2009) considers learning to combine
a large number of kernels and comes with guarantees, though their algorithm restricts the
family of kernels in a specific way.
The rest of the paper is organized as follows. The problem is defined formally in Section 2.
Our new algorithm is presented and analyzed in Section 3, while its specialized version
for learning polynomial kernels is given in Section 4. Finally, experiments are provided in
Section 5.

2

Preliminaries

In this section we give the formal definition of our problem. Let I denote a finite index set,
indexing the predictors (features)
the set of predictors considered
 to be combined, and define
P
over the input space X as F = fw : X â†’ R : fw (x) = iâˆˆI hwi , Ï†i (x)i , x âˆˆ X . Here Wi
is a Hilbert space over the reals, Ï†i : X â†’ Wi is a feature-map, hx, yi is the inner product
.
over the Hilbert space that x, y belong to and w = (wi )iâˆˆI âˆˆ W = Ã—iâˆˆI Wi (as an example,
Wi may just be a finite dimensional Euclidean space). The problem we consider is to solve

2

the optimization problem
minimize Ln (fw ) + Pen(fw )

subject to w âˆˆ W ,
and Ln (fw ) = n1
convex losses `t

(1)
Pn

where Pen(fw ) is a penalty that will be specified later,
t=1 `t (fw (xt ))is the
empirical risk of predictor fw , defined in terms of the
: R â†’ R (1 â‰¤ t â‰¤ n)
and inputs xt âˆˆ X (1 â‰¤ t â‰¤ n). The solution wâˆ— of the above penalized empirical risk
minimization problem is known to have favorable generalization properties under various
conditions, see, e.g., Hastie et al. (2009). In supervised learning problems `t (y) = `(yt , y)
for some loss function ` : R Ã— R â†’ R, such as the squared-loss, `(yt , y) = 21 (y âˆ’ yt )2 , or the
hinge-loss, `t (yt , y) = max(1 âˆ’ yyt , 0), where in the former case yt âˆˆ R, while in the latter
case yt âˆˆ {âˆ’1, +1}. We note in passing that for the sake of simplicity, we shall sometimes
abuse notation and write Ln (w) for Ln (fw ) and even drop the index n when the sample-size
is unimportant.
As mentioned above, in this paper we consider the special case in (1) when the penalty
is a so-called group p-norm penalty with 1 â‰¤ p â‰¤ 2, a case considered earlier, e.g., by Kloft
et al. (2011). Thus our goal is to solve
!2
p
1 X p
p
minimize Ln (w) +
Ïi kwi k2
,
(2)
wâˆˆW
2
iâˆˆI

where the scaling factors Ïi > 0, i âˆˆ I, are assumed to be given. We introduce the notation
u = (ui ) âˆˆ RI to denote the column vector obtained from the values ui .
The rationale of using the squared weighted p-norm is that for 1 â‰¤ p < 2 it is expected to
encourage sparsity at the group level which should allow one to handle cases when I is very
large (and the case p = 2 comes for free from the same analysis). The actual form, however,
is also chosen for reasons of computational convenience. In fact, the reason to use the 2-norm
of the weights is to allow the algorithm to work even with infinite-dimensional feature vectors
(and thus weights) by resorting to the kernel trick. To see how this works, just notice that
the penalty in (2) can also be written as
!2
(
)
p
2 kw k2
X p
X
Ï
i
2
i
Ïi kwi kp2
= inf
: Î¸âˆˆâˆ† p
,
2âˆ’p
Î¸i
iâˆˆI

iâˆˆI

where for Î½ â‰¥ 1, âˆ†Î½ = {Î¸ âˆˆ [0, 1]|I| : kÎ¸kÎ½ â‰¤ 1} is the positive quadrant of the |I|-dimensional
`Î½ -ball (see, e.g., Micchelli and Pontil, 2005, Lemma 26). Hence, defining
1 X Ï2i kwi k22
J(w, Î¸) = L(w) +
2
Î¸i
iâˆˆI

for any w âˆˆ W, Î¸ âˆˆ [0, 1]|I| , an equivalent form of (2) is
minimize J(w, Î¸)

wâˆˆW,Î¸âˆˆâˆ†Î½

(3)

where Î½ = p/(2 âˆ’ p) âˆˆ [1, âˆž) and we define 0/0 = 0 and u/0 = âˆž for u > 0, which implies
that wi = 0 if Î¸i = 0. That this minimization problem is indeed equivalent to our original
task (2) for the chosen value of Î½ follows from the fact that J(w, Î¸) is jointly convex in (w, Î¸).1
1

Here and in what follows by equivalence we mean that the set of optimums in terms of w (the primary
optimization variable) is the same in the two problems.

3

Let Îºi : X Ã— X â†’ R be the reproducing kernel underlying Ï†i : Îºi (x, x0 ) = hÏ†i (x), Ï†i (x0 )i
(x, x0 âˆˆ X ) and let Hi = HÎºi the corresponding reproducing kernel Hilbert space (RKHS).
Then, for any given fixed value of Î¸, the above problem becomes an instance
P of a standard
penalized learning problem in the RKHS HÎ¸ underlying the kernel ÎºÎ¸ = iâˆˆI Î¸i Ïâˆ’2
i Îºi . In
particular, by the theorem on page 353 in Aronszajn (1950), the problem of finding w âˆˆ W
for fixed Î¸ can be seen to be equivalent to minimizef âˆˆHÎ¸ L(f ) + 21 kf k2HÎ¸ , and thus (2) is
seen to be equivalent to minimizef âˆˆHÎ¸ ,Î¸âˆˆâˆ†Î½ L(f ) + 21 kf k2HÎ¸ . Thus, we see that the method
can be thought of as finding the weights of a kernel ÎºÎ¸ and a predictor minimizing the HÎ¸ norm penalized empirical risk. This shows that our problem is an instance of multiple kernel
learning (for an exhaustive survey of MKL, see, e.g., GoÌˆnen and AlpaydÄ±n, 2011 and the
references therein).

3

The new approach

When I is small, or moderate in size, the joint-convexity of J allows one to use off-the-shelf
solvers to find the joint minimum of J. However, when I is large, off-the-shelf solvers might
be slow or they may run out of memory. Targeting this situation we propose the following
approach: Exploiting again that J(w, Î¸) is jointly convex in (w, Î¸), find the optimal weights
by finding the minimizer of
.
J(Î¸) = inf J(w, Î¸),
w

.
or, alternatively, J(Î¸) = J(wâˆ— (Î¸), Î¸), where wâˆ— (Î¸) = arg minw J(w, Î¸) (here we have slightly
abused notation by reusing the symbol J). Note that J(Î¸) is convex by the joint convexity of
J(w, Î¸). Also, note that wâˆ— (Î¸) exists and is well-defined as the minimizer of J(Â·, Î¸) is unique
for any Î¸ âˆˆ âˆ†Î½ (see also Proposition 3.2 below). Again, exploiting the joint convexity of
J(w, Î¸), we find that if Î¸âˆ— is the minimizer of J(Î¸), then wâˆ— (Î¸âˆ— ) will be an optimal solution
to the original problem (2). To optimize J(Î¸) we propose to use stochastic gradient descent
with artificially injected randomness to avoid the need to fully evaluate the gradient of J.
More precisely, our proposed algorithm is an instance of a randomized version of the mirror
descent algorithm (Rockafellar, 1976; Martinet, 1978; Nemirovski and Yudin, 1998), where in
each time step only one coordinate of the gradient is sampled.

3.1

A randomized mirror descent algorithm

Before giving the algorithm, we need a few definitions. Let d = |I|, A âŠ‚ Rd be nonempty
with a convex interior Aâ—¦ . We call the function Î¨ : A â†’ R a Legendre (or barrier) potential
if it is strictly convex, its partial derivatives exist and are continuous, and for every sequence
{xk } âŠ‚ A approaching the boundary of A, limkâ†’âˆž kâˆ‡Î¨(xk )k = âˆž. Here âˆ‡ is the gradient
âˆ‚
operator: âˆ‡Î¨(x) = ( âˆ‚x
Î¨(x))> is the gradient of Î¨. When âˆ‡ is applied to a non-smooth
0
convex function J (Î¸) (J may be such without additional assumptions) then âˆ‡J 0 (Î¸) is defined
as any subgradient of J 0 at Î¸. The corresponding Bregman-divergence DÎ¨ : A Ã— Aâ—¦ â†’ R
is defined as DÎ¨ (Î¸, Î¸0 ) = Î¨(Î¸) âˆ’ Î¨(Î¸0 ) âˆ’ hâˆ‡Î¨(Î¸0 ), Î¸ âˆ’ Î¸0 i. The Bregman projection Î Î¨,K :
Aâ—¦ â†’ K corresponding to the Legendre potential Î¨ and a closed convex set K âŠ‚ Rd such
that K âˆ© A 6= âˆ… is defined, for all Î¸ âˆˆ Aâ—¦ as Î Î¨,K (Î¸) = arg minÎ¸0 âˆˆKâˆ©A DÏˆ (Î¸0 , Î¸).
Algorithm 1 shows a randomized version of the standard mirror descent method with an
unbiased gradient estimate. By assumption, Î·k > 0 is deterministic. Note that step 1 of the
4

Algorithm 1 Randomized mirror descent algorithm
1: Input: A, K âŠ‚ Rd , where K is closed and convex with K âˆ© A 6= âˆ…, Î¨ : A â†’ R Legendre,
step sizes {Î·k }, a subroutine, GradSampler, to sample the gradient of J at an arbitrary
vector Î¸ â‰¥ 0
2: Initialization: Î¸ (0) = arg minÎ¸âˆˆKâˆ©A Î¨(Î¸), k = 0.
3: repeat
4:
k = k + 1.
(kâˆ’1) )
5:
Obtain gÌ‚k = GradSampler(Î¸

(k)
6:
Î¸Ìƒ = arg minÎ¸âˆˆA Î·kâˆ’1 hgÌ‚k , Î¸i + DÎ¨ (Î¸, Î¸(kâˆ’1) ) .
7:
Î¸(k) = Î Î¨,K (Î¸Ìƒ(k) ).
8: until convergence.
algorithm is well-defined since Î¸Ìƒ(k) âˆˆ Aâ—¦ by the assumption that kâˆ‡Î¨(x)k tends to infinity
as x approaches the boundary of A. The performance of Algorithm 1 is bounded in the next
theorem. The analysis follows the standard proof technique of analyzing the mirror descent
algorithm (see, e.g., Beck and Teboulle, 2003), however, in a slightly more general form than
what we have found in the literature. In particular, compared to (Nemirovski et al., 2009a;
Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; RichtaÌrik and TakaÌcÌ‚, 2011), our
analysis allows for the conditional distribution of the noise in the gradient estimate to be
history dependent. The proof is included in Section A in the appendix.
Theorem 3.1. Assume that Î¨ is Î±-strongly convex with respect to some norm k Â· k (with
dual norm k Â· kâˆ— ) for some Î± > 0, that is, for any Î¸ âˆˆ Aâ—¦ , Î¸0 âˆˆ A
Î¨(Î¸0 ) âˆ’ Î¨(Î¸) â‰¥ âˆ‡Î¨(Î¸), Î¸0 âˆ’ Î¸ + Î±2 kÎ¸0 âˆ’ Î¸k2 .

(4)

Suppose, furthermore, that Algorithm 1 is run for T time steps. For 0 â‰¤ k â‰¤ T âˆ’ 1 let Fk
denote the Ïƒ-algebra generated by Î¸1 , . . . , Î¸k . Assume that, for all 1 â‰¤ k â‰¤ T , gÌ‚k âˆˆ Rd is an
unbiased estimate of âˆ‡J(Î¸(kâˆ’1) ) given Fkâˆ’1 , that is,
E [ gÌ‚k | Fkâˆ’1 ] = âˆ‡J(Î¸(kâˆ’1) ).

(5)

Further, assume that there exists a deterministic constant B â‰¥ 0 such that for all 1 â‰¤ k â‰¤ T ,


E kgÌ‚k k2âˆ— Fkâˆ’1 â‰¤ B a.s.
(6)
Finally, assume that Î´ = supÎ¸0 âˆˆKâˆ©A Î¨(Î¸0 ) âˆ’ Î¨(Î¸(0) ) is finite. Then, if Î·kâˆ’1 =
k â‰¥ 1, it holds that
!#
"
r
T
1 X (kâˆ’1)
2BÎ´
E J
Î¸
âˆ’ inf J(Î¸) â‰¤
.
Î¸âˆˆKâˆ©A
T
Î±T

q

2Î±Î´
BT

for all

(7)

k=1

Furthermore, if
kgÌ‚k k2âˆ— â‰¤ B 0

5

a.s.

(8)

B0

for some deterministic constant
and Î·kâˆ’1 =
it holds with probability at least 1 âˆ’  that
J

T
1 X (kâˆ’1)
Î¸
T

q

2Î±Î´
B0T

!

r
âˆ’ inf J(Î¸) â‰¤

k=1

for all k â‰¥ 1 then, for any 0 <  < 1,

2B 0 Î´
Î±T

Î¸âˆˆKâˆ©A

s
+4

B 0 Î´ log 1
.
Î±T

(9)

The convergence rate in the above theorem can be improved if stronger assumptions are
made on J, for example if J is assumed to be strongly convex, see, for example, (Hazan et al.,
2007; Hazan and Kale, 2011).
Efficient implementation of Algorithm 1 depends on efficient implementations of steps
1-1, namely, computing an estimate of the gradient, solving the minimization for Î¸Ìƒ(k) , and
projecting it into K. The first problem is related to the choice of gradient estimate we use,
which, in turn, depends on the structure of the feature space, while the last two problems
depend on the choice of the Legendre function. In the next subsections we examine how these
choices can be made to get a practical variant of the algorithm.

3.2

Application to multiple kernel learning

It remains to define the gradient estimates gÌ‚k in Algorithm 1. We start by considering
importance sampling based estimates. First, however, let us first verify whether the gradient
exist. Along the way, we will also derive some explicit expressions which will help us later.
Closed-form expressions for the gradient. Let us first consider how wâˆ— (Î¸) can be
calculated for a fixed value of Î¸. As it will turn out, this calculation will be useful not
only when the procedure is stopped (to construct the predictor fwâˆ— (Î¸) but also during the
iterations when we will need to calculate the derivative of J with respect to Î¸i . The following
proposition summarizes how wâˆ— (Î¸) can be obtained. Note that this type of result is standard
(see, e.g., Shawe-Taylor and Cristianini, 2004; SchoÌˆlkopf and Smola, 2002), thus we include
it only for the sake of completeness (the proof is included in Section A in the appendix).
Proposition 3.2. For 1 â‰¤ t â‰¤ n, let `âˆ—t : R â†’ R denote the convex conjugate of `t : `âˆ—t (v) =
0 )i, and let K =
supÏ„ âˆˆR {vÏ„ âˆ’ `t (Ï„ )}, v âˆˆ R. For i âˆˆ I, recall that Îºi (x, x0 ) = hÏ†i (x), Ï†i (xP
i
(Îºi (xt , xs ))1â‰¤t,sâ‰¤n be the n Ã— n kernel matrix underlying Îºi and let KÎ¸ = iâˆˆI ÏÎ¸2i Ki be the
i
P
Î¸i
kernel matrix underlying ÎºÎ¸ =
Îº
.
Then,
for
any
fixed
Î¸,
the
minimizer
wâˆ— (Î¸) of
i
2
iâˆˆI Ïi
J(Â·, Î¸) satisfies
n
Î¸i X âˆ—
wiâˆ— (Î¸) = 2
Î± (Î¸)Ï†i (xt ), i âˆˆ I ,
(10)
Ïi t=1 t
where

(
Î±âˆ— (Î¸) = arg min
Î±âˆˆRn

n

1 >
1X âˆ—
Î± KÎ¸ Î± +
`t (âˆ’nÎ±t )
2
n

)
.

(11)

t=1

Based on this proposition, we can compute the
Ppredictor fwâˆ— (Î¸) usingPthe kernels {Îºi }iâˆˆI
and the dual variables (Î±tâˆ— (Î¸))1â‰¤tâ‰¤n : fwâˆ— (Î¸) (x) = iâˆˆI hwiâˆ— (Î¸), Ï†i (x)i = nt=1 Î±tâˆ— (Î¸)ÎºÎ¸ (xt , x) .

6

Let us now consider the differentiability of J = J(Î¸) and how to compute its derivatives.
Under proper conditions with standard calculations (e.g., Rakotomamonjy et al., 2008) we
find that J is differentiable over âˆ† and its derivative can be written as2

 âˆ— >
âˆ‚
Î± (Î¸) Ki Î±âˆ— (Î¸)
J(Î¸) = âˆ’
.
(12)
âˆ‚Î¸
Ï2i
iâˆˆI
Importance sampling based estimates. Let d = |I| and let ei , i âˆˆ I denote the ith unit
vector of the standard basis of Rd , that is, the ith coordinate of ei is 1 while the others are
0. Introduce
D
E
gk,i = âˆ‡J(Î¸(kâˆ’1) ), ei , i âˆˆ I
(13)
to denote the ith component of the gradient of J in iteration k (that is, gk,i can be computed
based on (12)). Let skâˆ’1 âˆˆ [0, 1]I be a distribution over I, computed in some way based on
the information available up to the end of iteration k âˆ’ 1 of the algorithm (formally, skâˆ’1 is
Fkâˆ’1 -measurable). Define the importance sampling based gradient estimate to be
gÌ‚k,i =

I{Ik =i}
gk,Ik ,
skâˆ’1,Ik

i âˆˆ I, where Ik âˆ¼ skâˆ’1,Â· .

(14)

That is, the gradient estimate is obtained by first sampling an index from skâˆ’1,Â· and then
setting the gradient estimate to be zero at all indices i âˆˆ I except when i = Ik in which
gk,I
case its value is set to be the ratio skâˆ’1,Ik . It is easy to see that as long as skâˆ’1,i > 0 holds
k

whenever gk,i 6= 0, then it holds that E [ gÌ‚k | Fkâˆ’1 ] = âˆ‡J(Î¸(kâˆ’1) ) a.s.
Let us now derive the conditions under which the second moment of the gradient estimate
stays bounded. Define Ckâˆ’1 = âˆ‡J(Î¸(kâˆ’1) ) 1 . Given the expression for the gradient of J
shown in (12), we see that supkâ‰¥1 Ckâˆ’1 < âˆž will always hold provided that Î±âˆ— (Î¸) is continuous
since (Î¸(kâˆ’1) )kâ‰¥1 is guaranteed to belong to a compact set (the continuity of Î±âˆ— is discussed
in Section B in the appendix).
1
Define the probability distribution qkâˆ’1,Â· as follows: qkâˆ’1,i = Ckâˆ’1
|gk,i | , i âˆˆ I. Then
2
qkâˆ’1,I

2
2
keIk k2âˆ— = s2 k Ckâˆ’1
keIk k2âˆ— . Therefore, it also holds
it holds that kgÌ‚k k2âˆ— = s2 1 gk,I
k
kâˆ’1,Ik
kâˆ’1,Ik
2


P
qkâˆ’1,i
qkâˆ’1,i
2
2
2
2
that E kgÌ‚k k2âˆ— Fkâˆ’1 = Ckâˆ’1
iâˆˆI skâˆ’1,i kei kâˆ— â‰¤ Ckâˆ’1 maxiâˆˆI skâˆ’1,i kei kâˆ— . This shows that


q
supkâ‰¥1 E kgÌ‚k k2âˆ— Fkâˆ’1 < âˆž will hold as long as supkâ‰¥1 maxiâˆˆI skâˆ’1,i
< âˆž and supkâ‰¥1 Ckâˆ’1 <
kâˆ’1,i
âˆž. Note that when skâˆ’1 = qkâˆ’1 , the gradient estimate becomes gÌ‚k,i = Ckâˆ’1 I{It =i} . That is,
in this case we see that in order to be able to calculate gÌ‚k,i , we need to be able to calculate
Ckâˆ’1 efficiently.

Choosing the potential Î¨. The efficient sampling of the gradient is not the only practical
issue, since the choice of the Legendre function
and the convex set K may also cause some
P
complications. For example, if Î¨(x) =
iâˆˆI xi (ln xi âˆ’ 1), then the resulting algorithm
is exponential weighting, and one needs to store and update |I| weights, which is clearly
infeasible if |I| is very large (or infinite). On the other hand, if Î¨(x) = 12 kxk22 and we project
2

For completeness, the calculations are given in Section B in the appendix.

7

Algorithm 2 Projected stochastic gradient algorithm.
1:
2:
3:
4:
5:
6:

(0)

Initialization: Î¨(x) = 21 kxk22 , Î¸i = 0 for all i âˆˆ I, k = 0, step sizes {Î·k }.
repeat
k = k + 1.
Sample a gradient estimate gÌ‚k of g(Î¸(kâˆ’1 ) randomly according to (14).
Î¸(k) = Î Î¨,âˆ†2 (Î¸(kâˆ’1) âˆ’ Î·kâˆ’1 gÌ‚k ).
until convergence.

to K = âˆ†2 , the positive quadrant of the `2 -ball (with A = [0, âˆž)I ), we obtain a stochastic
projected gradient method, shown in Algorithm 2. This is in fact the algorithm that we
use in the experiments. Note that in (2) this corresponds to using p = 4/3. The reason we
made this choice is because in this case projection is a simple scaling operation. Had we
chosen K = âˆ†1 , the `2 -projection would very often cancel many of the nonzero components,
resulting in an overall slow progress. Based on the above calculations and Theorem 3.1 we
obtain the following performance bound for our algorithm.
Corollary 3.3. Assume that Î±âˆ— (Î¸) is continuous on âˆ†2 . Then there exists a C > 0 such
q
âˆ‚
that k âˆ‚Î¸
J(Î¸)k1 â‰¤ C for all Î¸ âˆˆ âˆ†2 . Let B = 21 C 2 maxiâˆˆI,1â‰¤kâ‰¤T skâˆ’1,i
. If Algorithm 2 is run
kâˆ’1,i
âˆš
for T steps with Î·kâˆ’1 = Î· = 1/ BT , k = 1, . . . , T , then, for all Î¸ âˆˆ âˆ†2 ,
!#
"
r
T
1 X (kâˆ’1)
B
âˆ’ J(Î¸) â‰¤
Î¸
.
E J
T
T
k=1

Note that to implement Algorithm 2 efficiently, one has to be able to sample from skâˆ’1,Â·
and compute the importance sampling ratio gk,i /sk,i efficiently for any k and i.

4

Example: Learning polynomial kernels

In this section we show how our method can be applied in the context of multiple kernel learning. We provide an example when the kernels in I are tensor products of a set of base kernels
(this we shall call learning polynomial kernels). The importance of this example follows from
the observation of GoÌˆnen and AlpaydÄ±n (2011) that the non-linear kernel learning methods of
Cortes et al. (2009), which can be viewed as a restricted form of learning polynomial kernels,
are far the best MKL methods in practice and can significantly outperform state-of-the-art
SVM with a single kernel or with the uniform combination of kernels.
Assume that we are given a set of base kernels {Îº1 , . . . , Îºr }. In this section we consider the
set KD of product kernels of degree at most D: Choose I = {(r1 , . . . , rd ) : 0 â‰¤ Q
d â‰¤ D, 1 â‰¤ ri â‰¤ r}
and the multi-index r1:d = (r1 , . . . , rd ) âˆˆ I defines the kernel Îºr1:d (x, x0 ) = di=1 Îºri (x, x0 ).
For d = 0 we define Îºr1:0 (x, x0 ) = 1. Note that indices that are the permutations of each
other define the same kernel. On the language of statistical modeling, Îºr1:d models interactions of order d between the features underlying the base kernels Îº1 , . . . , Îºr . Also note that
|I| = Î˜(rD ), that is, the cardinality of I grows exponentially fast in D.
We assume that Ïr1:d depends only on d, the order of interactions in Îºr1:d . By abusing

8

Algorithm 3 Polynomial kernel sampling. The symbol
denotes the Hadamard product/power.
1: Input: Î± âˆˆ Rn , the solution to the dual problem; kernel matrices {K1 , . . . , Kr }; the
degree
of the polynomial kernel, the weights (Ï20 , . . . , Ï2D ).
PD
r
>
2: S â†
j=1 KDj , M â† Î±Î±
E
0
âˆ’2
3: Î´(d0 ) â† Ïd0 M, S d , d0 âˆˆ {0, . . . , D}
PD
4: Sample d from Î´(Â·)/ d0 =0 Î´(d0 )
5: for i = 1 to d do
tr(M S (dâˆ’i) K )
6:
Ï€(j) â† tr(M S (dâˆ’i+1) j) , j âˆˆ {1, . . . , r}
7:
Sample zi from Ï€(Â·)
8:
M â† M K zi
9: end for
10: return (z1 , . . . , zd )
notation, we will write Ïd in the rest of this section to emphasize this.3 Our proposed
algorithm to sample from qkâˆ’1,Â· is shown in Algorithm 3. The algorithm is written to return
a multi-index (z1 , . P
. . , zd ) that isP
drawn from qkâˆ’1,Â· . The key idea underlying the algorithm
r
d
is to exploit that ( j=1 Îºj ) = r1:d âˆˆI Îºr1:d . The correctness of the algorithm is shown in
Section 4.1. In the description of the algorithm
denotes the matrix entrywise product
(a.k.a. Schur, or Hadamard product) and A s denotes A
.{z
. . A}, and we set the priority
|
s

of
A

to be higher than that of the ordinary matrix product (by definition, all the entries of
0 are 1).
Let us now discuss the complexity of Algorithm 3. For this, first note that computing all
0
the Hadamard products S d , d0 = 0, . . . , D requires O(Dn2 ) computations. Multiplication
with Mkâˆ’1 can be done in O(n2 ) steps. Finally, note that each iteration of the for loop takes
O(rn2 ) steps, which results in the overall worst-case complexity of O(rn2 D) if Î±âˆ— (Î¸kâˆ’1 ) is
readily available. The computational complexity of determining Î±âˆ— (Î¸kâˆ’1 ) depends on the
exact form of `t , and can be done efficiently in many situations: if, for example, `t is the
squared loss, then Î±âˆ— can be computed in O(n3 ) time. An obvious improvement to the
approach described here, however, would be to subsample the empirical loss Ln , which can
bring further computational improvements. However, the exploration of this is left for future
work.
Finally, note that despite the exponential cardinality of |I|, due to the strong algebraic
structure of the space of kernels, Ckâˆ’1 can be calculated
In fact, it is not hard to
P efficiently.
0 ). This also shows that if Ï
see that with the notation of the algorithm, Ckâˆ’1 = D
Î´(d
0
d
d =0
decays â€œfast enoughâ€, Ckâˆ’1 can be bounded independently of the cardinality of I.

4.1

Correctness of the sampling procedure

In this section we prove the correctness of Algorithm 3.
As said earlier, we assume that Ïr1:d depends only on d, the order of interactions in Îºr1:d
3

Using importance sampling, more general weights can also be accommodated, too without effecting the
results as long as the range of weights (Ïr1:d ) is kept under control for all d.

9

and, by abusing notation, we will write Ïd to emphasize this. Let us
P how one can
Pnow consider
sample from qkâˆ’1,Â· . The implementation relies on the fact that ( rj=1 Îºj )d = r1:d âˆˆI Îºr1:d .
Remember that we denoted the kernel matrix underlying some kernel k by Kk , and recall
that Kk is an n Ã— n matrix. For brevity, in the rest of this section for Îº = Îºr1:d we will write
Kr1:d instead of KÎºr1:d . Define Mkâˆ’1 = Î±âˆ— (Î¸kâˆ’1 )Î±âˆ— (Î¸kâˆ’1 )> . Thanks to (12) and the rotation
property of trace, we have
(15)
gk,r1:d = âˆ’Ïâˆ’2
d tr(Mkâˆ’1 Kr1:d ) .
P
The plan to sample from qkâˆ’1,Â· = |gk,Â· |/ r1:d âˆˆI |gk,r1:d | is as follows: We first draw the order
of interactions, 0 â‰¤ dË† â‰¤ D. Given dË† = d, we restrict the draw of the random multi-index
Ë†
R1:d to the set {r1:d âˆˆ I}. A multi-index will be sampled in a d-step
process: in each step
we will randomly choose an index from the indices of base kernels according to the following
distributions. Let S = K1 + . . . + Kr , let


Ïâˆ’2 tr(Mkâˆ’1 S d )
P dË† = d|Fkâˆ’1 = PD d âˆ’2
d0 )
d0 =0 Ïd0 tr(Mkâˆ’1 S
and, with a slight abuse of notation, for any 1 â‰¤ i â‰¤ d define


P Ri = ri |Fkâˆ’1 , dË† = d, R1:iâˆ’1 = r1:iâˆ’1




i
(dâˆ’i)
tr Mkâˆ’1
K
S
j=1 rj




= P
r
iâˆ’1
(dâˆ’i)
0
tr
M
K
K
S
0
r
kâˆ’1
r
j
r =1
j=1
i
i

where we used the sequence notation (namely, s1:p denotes the sequence (s1 , . . . , sp )). We
have, by the linearity of trace and the definition of S that
r
X


tr Mkâˆ’1



iâˆ’1
j=1 Krj



Kri0

S

(dâˆ’i)



ri0 =1


= tr Mkâˆ’1



iâˆ’1
j=1 Krj



S

(dâˆ’i+1)



Thus, by telescoping,


P dË† = d, R1:d = r1:d |Fkâˆ’1
=

Ïâˆ’2
. . . Krdâˆ’1 Krd )
d tr(Mkâˆ’1 Kr1
.
PD
âˆ’2
d0 )
d0 =0 Ïd0 tr(Mkâˆ’1 S

as desired. An optimized implementation of drawing these random variables is shown as
Algorithm 3. The algorithm is written to return the multi-index R1:d .

5

Experiments

In this section we apply our method
P to the problem of multiple kernel learning in regression
with the squared loss: L(w) = 21 nt=1 (fw (xt ) âˆ’ yt )2 , where (xt , yt ) âˆˆ Rr Ã— R are the inputoutput pairs in the data. In these experiments our aim is to learn polynomial kernels (cf.
Section 4).
10

We compare our method against several kernel learning algorithms from the literature
on synthetic and real data. In all experiments we report mean squared error over test sets.
A constant feature is added to act as offset, and the inputs and output are normalized to
have zero mean and unit variance. Each experiment is performed with 10 runs in which we
randomly choose training, validation, and test sets. The results are averaged over these runs.

5.1

Convergence speed

In this experiment we examine the speed of convergence of our method and compare it against
one of the fastest standard multiple kernel learning algorithms, that is, the p-norm multiple
kernel learning algorithm of Kloft et al. (2011) with p = 2,4 and the uniform coordinate
descent algorithm that updates one coordinate per iteration uniformly at random (Nesterov,
2010, 2012; Shalev-Shwartz and Tewari, 2011; RichtaÌrik and TakaÌcÌ‚, 2011). We aim to learn
polynomial kernels of up to degree 3 with all algorithms. Our method uses Algorithm 3
for sampling with D = 3. The set of provided base kernels is the linear kernels built from
input variables, that is, Îº(i) (x, x0 ) = x(i) x0(i) , where x(i) denotes the ith input variable. For
the other two algorithms the kernel set consists of product kernels from monomial terms for
D âˆˆ {0, 1, 2, 3} built from r base kernels, where r is the number of input variables. The
number of distinct product kernels is r+D
D . In this experiment for all algorithms we use
ridge regression with its regularization parameter set to 10âˆ’5 . Experiments with other values
of the regularization parameter achieved similar results.
We compare these methods in four datasets from the UCI machine learning repository
(Frank and Asuncion, 2010) and the Delve datasets5 . The specifications of these datasets are
shown in Table 1. We run all algorithms for a fixed amount of time and measure the value
Table 1: Specifications of datasets used in experiments.
Dataset
german
ionosphere
ringnorm
sonar
splice
waveform

# of variables
20
34
20
60
60
21

Training size
350
140
500
83
500
500

Validation size
150
36
1000
21
1000
1000

Test size
500
175
2000
104
1491
2000

of the objective function (1), that is, the sum of the empirical loss and the regularization
term. Figure 1 shows the performance of these algorithms. In this figure Stoch represents
our algorithms, Kloft represents the algorithm of Kloft et al. (2011), and UCD represents
the uniform coordinate descent algorithm. The results show that our method consistently
outperforms the other algorithms in convergence speed. Note that our stochastic method
updates one kernel coefficient per iteration, while Kloft updates r+D
kernel coefficients
D
per iteration. The difference between the two methods is analogous to the difference between stochastic gradient vs. full gradient algorithms. While UCD also updates one kernel
4

Note that p = 2 in Kloft et al. (2011) notation corresponds to p = 4/3 or Î½ = 2 in our notation, which
gives the same objective function that we minimize with Algorithm 2.
5
See, www.cs.toronto.edu/~delve/data/datasets.html

11

10

10

german

objective function

ionosphere

6

10
Kloft
Stoch
UCD

ringnorm

6

10

4

4

10

waveform

6

10

4

10

10

5

10

2

2

10

0

10

0

10

0

2

10

0

10

10

10

âˆ’2

âˆ’2

10
âˆ’5

10

0

âˆ’4

50
100
time (sec.)

150

10

0

âˆ’2

10

10

âˆ’4

5

10
15
time (sec.)

20

10

0

âˆ’4

100

200
300
time (sec.)

400

10

0

100
200
time (sec.)

300

Figure 1: Convergence comparison of our method and other algorithms.
coefficient per iteration its naive method of selecting coordinates results in a slower overall convergence compared to our algorithm. In the next section we compare our algorithm
against several representative methods from the MKL literature.

5.2

Synthetic data

In this experiment we examine the effect of the size of the kernel space on prediction accuracy
and training time of MKL algorithms. We generated data for a regression problem. Let r
denote the number of dimensions of the input space. The inputs are chosen uniformly at
random from [âˆ’1, 1]r . The output of each instance is the uniform combination of 10 monomial
terms of degree 3 or less. These terms are chosen uniformly at random among all possible
terms. The outputs are noise free. We generated data for r âˆˆ {5, 10, 20, . . . , 100}, with 500
training and 1000 test points. The regularization parameter of the ridge regression algorithm
was tuned from {10âˆ’8 , . . . , 102 } using a separate validation set with 1000 data points.
We compare our method (Stoch) against the algorithm of Kloft et al. (2011) (Kloft),
the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical
kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear
kernels built from the input variables.
P Recall that the method of Cortes et al. (2009) only
considers kernels of the form ÎºÎ¸ = ( ri=1 Î¸i Îºi )D , where D is a predetermined integer that
specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent
to adding polynomial kernels of degree less than D to the combination too. We provide all
possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011).
For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3.
The results are shown in Figure 2, the mean squared errors are on the left plot, while the
training times are on the right plot. In the training-time plot the numbers inside brackets
6
While several fast MKL algorithms are available in the literature, such as those of Sonnenburg et al. (2006);
Rakotomamonjy et al. (2008); Xu et al. (2010); Orabona and Luo (2011); Kloft et al. (2011), a comparison of
the reported experimental results shows that from among these algorithms the method of Kloft et al. (2011)
has the best performance overall. Hence, we decided to compare against only this algorithm. Also note
that the memory and computational cost of all these methods still scale linearly with the number of kernels,
making them unsuitable for the case we are most interested in. Furthermore, to keep the focus of the paper
we compare our algorithm to methods with sound theoretical guarantees. As such, it remains for future work
to compare with other methods, such as the infinite kernel learning of Gehler and Nowozin (2008), which lack
such guarantees but exhibit promising performance in practice.

12

1

250
Kloft
Stoch
Cortes
Bach
Uniform

0.9
0.8

200
training time (sec.)

0.7

MSE

0.6
0.5
0.4
0.3
0.2

150

100

50

0.1
0

20
40
60
80
number of dimensions of input space

0

100

20
[1,771]

40

60

80

100

[12,341]

[39,711]

[91,881]

[176,851]

Figure 2: Comparison of kernel learning methods in terms of test error (left) and training
time (right).
indicate the total number of distinct product kernels for each value of r. This is the number of
kernels fed to the Kloft algorithm. Since this method deals with a large number of kernels,
it was possible to precompute and keep the kernels in memory (8GB) for r â‰¤ 25. Therefore,
we ran this algorithm for r â‰¤ 25. For r > 25, we could use on-the-fly implementation of this
algorithm, however that further increases the training time. Note that the computational cost
of this method depends linearly on the number of kernels, which in this experiment, is cubic
in the number of input variables since D = 3. While the standard MKL algorithms, such
as Kloft, cannot handle such large kernel spaces, in terms of time and space complexity,
the other three algorithms can efficiently learn kernel combinations. However their predictive
accuracies are quite different. Note that the performance of the method of Cortes et al.
(2009) starts to degrade as r increases. This is due to the restricted family of kernels that
this method considers. The method of Bach (2008), which is well-suited to learn sparse
combination of product kernels, performs better than Cortes et al. (2009) for higher input
dimensions. Among all methods, our method performs best in predictive accuracy while its
computational cost is close to that of the other two competitors.

5.3

Real data

In this experiment we aim to compare several MKL methods in real datasets. We compare
our new algorithm (Stoch), the algorithm of Bach (2008) (Bach), and the algorithm of
Cortes et al. (2009) (Cortes). For each algorithm we consider learning polynomial kernels
of degree 2 and
P 3. We also include uniform combination of product kernels of degree D,
i.e. ÎºD = ( ri=1 Îºi )D , for D âˆˆ {1, 2, 3} (Uniform). To find out if considering higherorder interaction of input variables results in improved performance we also included a MKL
algorithm to which we only feed linear kernels (D = 1). We use the MKL algorithm of Kloft
et al. (2011) with p âˆˆ {1, 2} (Kloft).
We compare these methods on six datasets from the UCI machine learning repository

13

ionosphere

german

ringnorm

0.78

Bach (d=2)
Bach (d=3)

MSE

Stoch (d=2)
Stoch (d=3)
Stoch (d=3, prior)

0.8

0.7

0.76
0.74

0.6

0.6
0.72
0.7

0.4

0.5

0.68
0.2

Cortes (d=2)
Cortes (d=3)
Kloft (p=1)
Kloft (p=2)
Uniform (d=1)
Uniform (d=2)
Uniform (d=3)

sonar

splice

waveform

0.9

0.8

0.6

0.5

0.55

0.45

0.5

0.4

0.7

0.45
0.4

0.35

0.6

0.35

0.3

Figure 3: Prediction error of different methods in the real data experiment
and Delve datasets. In these datasets the number of dimensions of the input space is 20
and above. The specifications of these datasets are shown in Table 1. The regularization
parameter is selected from the set {10âˆ’4 , . . . , 103 } for all methods using a validation set. The
results are shown in Figure 3.
Overall, we observe that methods that consider non-linear variable interactions (Stoch,
Bach, and Cortes) perform better than linear methods (Kloft). Among non-linear methods, Cortes performs worse than the other two. We believe that this is due to the restricted
kernel space considered by this method. The performance of Stoch and Bach methods is
similar overall.
We observe that our method overfits when it considers kernels of degree 3. However, one
can easily prevent overfitting by assigning larger Ï values to higher-degree kernels such that
the stochastic algorithm selects lower-degree kernels more often. For this purpose, we repeat
this experiment for D = 3 with a modified set of Ï values, where we use Ï2d = 1 for kernels of
degree 2 or less and Ï2d = 4 for kernels of degree 3. With the new Ï coefficients we observe an
improvement in algorithmâ€™s performance. See Stoch (D = 3, prior) error values in Figure 3.

6

Conclusion

We introduced a new method for learning a predictor by combining exponentially many linear
predictors using a randomized mirror descent algorithm. We derived finite-time performance
bounds that show that the method efficiently optimizes our proposed criterion. Our proposed
method is a variant of a randomized stochastic coordinate descent algorithm, where the main
trick is the careful construction of an unbiased randomized estimate of the gradient vector
that keeps the variance of the method under control, and can be computed efficiently when
the base kernels have a certain special combinatorial structure. The efficiency of our method
14

was demonstrated for the practically important problem of learning polynomial kernels on a
variety of synthetic and real datasets comparing to a representative set of algorithms from
the literature. For this case, our method is able to compute an optimal solution in polynomial
time as a function of the logarithm of the number of base kernels. To our knowledge, ours is
the first method for learning kernel combinations that achieve such an exponential reduction
in complexity while satisfying strong performance guarantees, thus opening up the way to
apply it to extremely large number of kernels. Furthermore, we believe that our method is
applicable beyond the case studied in detail in our paper. For example, the method seems
extendible to the case when infinitely many kernels are combined, such as the case of learning
a combination of Gaussian kernels. However, the investigation of this important problem
remains subject to future work.
Acknowledgements
This work was supported by Alberta Innovates Technology Futures and NSERC.

A

Proofs

In this section we present the proofs of Theorem 3.1 and Proposition 3.2. The proof of
Theorem 3.1 is based on the standard proof of the convergence rate of the proximal point
algorithm, see, for example, (Beck and Teboulle, 2003), or the proof of Proposition 2.2 of
Nemirovski et al. (2009b), which carry over the same argument to solve very similar but less
general problems. We also provide some improvements and simplifications at the end. Before
giving the actual proof, we need the following standard lemma:
Lemma A.1 (Lemma 2.1 of Nemirovski et al. 2009b). Assume that Î¨ is Î±-strongly convex
with respect to some norm k Â· k (i.e., (4) holds). Let Î¸1 âˆˆ K âˆ© Aâ—¦ , Î¸ âˆˆ K âˆ© A, and g âˆˆ Rd .
Define Î¸2 = arg minÎ¸0 âˆˆKâˆ©A {hg, Î¸0 i + DÎ¨ (Î¸0 , Î¸1 )}. Then
hg, Î¸1 âˆ’ Î¸i â‰¤ DÎ¨ (Î¸, Î¸1 ) âˆ’ DÎ¨ (Î¸, Î¸2 ) +

kgk2âˆ—
.
2Î±

We provide an alternate proof that is based on the so-called 3-DIV lemma. The 3-DIV
lemma (e.g., Lemma 11.1, Cesa-Bianchi and Lugosi, 2006) allows one to express the sum of
the divergences between the vectors u, v and v, w in terms of the divergence between u and
w and an additional â€œerror termâ€, where u âˆˆ A, v, w âˆˆ Aâ—¦ :
DÎ¨ (u, v) + DÎ¨ (v, w) = DÎ¨ (u, w) + hâˆ‡Ïˆ(w) âˆ’ âˆ‡Ïˆ(v), u âˆ’ vi .
Proof. Note that Î¸2 âˆˆ Aâ—¦ due to behavior of Î¨ at the boundary of A. Thus, Î¨ is differentiable
at Î¸2 and
âˆ‡1 DÎ¨ (Î¸2 , Î¸1 ) = âˆ‡Ïˆ(Î¸2 ) âˆ’ âˆ‡Ïˆ(Î¸1 ) ,

(16)

where âˆ‡1 denotes differentiation of DÎ¨ w.r.t. its first variable. Let f (Î¸0 ) = hg, Î¸0 i+DÎ¨ (Î¸0 , Î¸1 ).
By the optimality property of Î¸2 and since Î¸ âˆˆ K âˆ© A, we have
hâˆ‡f (Î¸2 ), Î¸2 âˆ’ Î¸i â‰¤ 0 .
15

Plugging in the definition of f together with the identity (16) gives
hg + âˆ‡Ïˆ(Î¸2 ) âˆ’ âˆ‡Ïˆ(Î¸1 ), Î¸2 âˆ’ Î¸i â‰¤ 0 .

(17)

Now, by the 3-DIV Lemma,
DÎ¨ (Î¸, Î¸2 ) + DÎ¨ (Î¸2 , Î¸1 ) = DÎ¨ (Î¸, Î¸1 ) + hâˆ‡Î¨(Î¸1 ) âˆ’ âˆ‡Î¨(Î¸2 ), Î¸ âˆ’ Î¸2 i
= DÎ¨ (Î¸, Î¸1 ) + hg + âˆ‡Î¨(Î¸2 ) âˆ’ âˆ‡Î¨(Î¸1 ), Î¸2 âˆ’ Î¸i + hg, Î¸ âˆ’ Î¸2 i .
Hence, by reordering and using the inequality (17) we get
DÎ¨ (Î¸, Î¸2 ) âˆ’ DÎ¨ (Î¸, Î¸1 ) â‰¤ hg, Î¸ âˆ’ Î¸2 i âˆ’ DÎ¨ (Î¸2 , Î¸1 )
= hg, Î¸1 âˆ’ Î¸2 i âˆ’ DÎ¨ (Î¸2 , Î¸1 ) + hg, Î¸ âˆ’ Î¸1 i
â‰¤

kgk2âˆ—
+ hg, Î¸ âˆ’ Î¸1 i ,
2Î±

where in the last line we used Youngâ€™s inequality7 and that due to the strong convexity of Î¨,

DÎ¨ (Î¸2 , Î¸1 ) â‰¥ Î±2 kÎ¸2 âˆ’ Î¸1 k2 .
Theorem 3.1. Assume that Î¨ is Î±-strongly convex with respect to some norm k Â· k (with
dual norm k Â· kâˆ— ) for some Î± > 0, that is, for any Î¸ âˆˆ Aâ—¦ , Î¸0 âˆˆ A
Î¨(Î¸0 ) âˆ’ Î¨(Î¸) â‰¥ âˆ‡Î¨(Î¸), Î¸0 âˆ’ Î¸ + Î±2 kÎ¸0 âˆ’ Î¸k2 .

(4)

Suppose, furthermore, that Algorithm 1 is run for T time steps. For 0 â‰¤ k â‰¤ T âˆ’ 1 let Fk
denote the Ïƒ-algebra generated by Î¸1 , . . . , Î¸k . Assume that, for all 1 â‰¤ k â‰¤ T , gÌ‚k âˆˆ Rd is an
unbiased estimate of âˆ‡J(Î¸(kâˆ’1) ) given Fkâˆ’1 , that is,
E [ gÌ‚k | Fkâˆ’1 ] = âˆ‡J(Î¸(kâˆ’1) ).

(5)

Further, assume that there exists a deterministic constant B â‰¥ 0 such that for all 1 â‰¤ k â‰¤ T ,


E kgÌ‚k k2âˆ— Fkâˆ’1 â‰¤ B a.s.
(6)
q
Finally, assume that Î´ = supÎ¸0 âˆˆKâˆ©A Î¨(Î¸0 ) âˆ’ Î¨(Î¸(0) ) is finite. Then, if Î·kâˆ’1 = 2Î±Î´
BT for all
k â‰¥ 1, it holds that
"
!#
r
T
1 X (kâˆ’1)
2BÎ´
E J
Î¸
âˆ’ inf J(Î¸) â‰¤
.
(7)
Î¸âˆˆKâˆ©A
T
Î±T
k=1

Furthermore, if
kgÌ‚k k2âˆ— â‰¤ B 0 a.s.
(8)
q
2Î±Î´
for some deterministic constant B 0 and Î·kâˆ’1 = B
0 T for all k â‰¥ 1 then, for any 0 <  < 1,
it holds with probability at least 1 âˆ’  that
s
!
r
T
B 0 Î´ log 1
1 X (kâˆ’1)
2B 0 Î´
J
Î¸
âˆ’ inf J(Î¸) â‰¤
+4
.
(9)
Î¸âˆˆKâˆ©A
T
Î±T
Î±T
k=1

7

Youngâ€™s inequality states that for any x, y vectors and Î± > 0, hx, yi â‰¤ kxkâˆ— kyk â‰¤

16

1
2



kxk2
âˆ—
Î±


+ Î±kyk2 .

P
(T )
Proof. Introduce the average learning rates Î· k = Î·k / Tk=1 Î·kâˆ’1 , k = 1, . . . , T , the averaged
parameter estimates
T
X
(T )
Î¸Ì„(T âˆ’1) =
Î· kâˆ’1 Î¸(kâˆ’1)
k=1

and choose some Î¸âˆ— âˆˆ K âˆ© A. To prove the first part of the theorem,
it suffices to show that

(T
âˆ’1)
âˆ—
(kâˆ’1)
the bound holds for J(Î¸Ì„
) âˆ’ J(Î¸ ). Define gk = âˆ‡J Î¸
. By the convexity of J(Î¸),
we have
T


 


X
(T )
Î· kâˆ’1 J Î¸(kâˆ’1) âˆ’ J(Î¸âˆ— )
J Î¸Ì„(T âˆ’1) âˆ’ J(Î¸âˆ— ) â‰¤

â‰¤

=

k=1
T
X
k=1
T
X

D
E
(T )
Î· kâˆ’1 gk , Î¸(kâˆ’1) âˆ’ Î¸âˆ—
T
E X
E
D
D
(T )
(T )
Î· kâˆ’1 gÌ‚k , Î¸(kâˆ’1) âˆ’ Î¸âˆ— +
Î· kâˆ’1 gk âˆ’ gÌ‚k , Î¸(kâˆ’1) âˆ’ Î¸âˆ— (18)
k=1

k=1

Notice that the first term on the right hand side above is the sum of linearized losses appearing
in the standard analysis of the proximal point algorithm with loss functions gÌ‚k and learning
(T )
rates Î· kâˆ’1 , and the second sum contains the term that depends on how well gÌ‚k estimates
the gradient gk . Thus, in this way, it is separated how the proximal point algorithm and the
gradient estimate effect the convergence rate of the algorithm. The first sum can be bounded
by invoking the standard bound for the proximal point algorithm (we will give the very short
proof for completeness, based on Lemma A.1), while the second sum can be analyzed by
noticing that, by assumption (5), its elements form an {Fk }-adapted martingale-difference
sequence.
To bound the first sum, first note that the conditions of Lemma A.1 are satisfied for
(T )
Î¸1 = Î¸(kâˆ’1) , Î¸ = Î¸âˆ— , g = Î· kâˆ’1 gÌ‚k , since Î¸1 âˆˆ K âˆ© Aâ—¦ (as mentioned beforehand, this follows
from the behavior of Î¨ at the boundary of A). Further, note that due to the so-called
projection lemma (i.e., the DÎ¨ -projection of the unconstrained optimizer is the same as the
optimizer of the constrained optimization problem),we can conclude that Î¸(k) = Î¸2 , where Î¸2
is defined in Lemma A.1. Thus, Lemma A.1 gives
D
E
Î· 2 kgÌ‚k k2âˆ—
Î·kâˆ’1 gÌ‚k , Î¸(kâˆ’1) âˆ’ Î¸âˆ— â‰¤ DÎ¨ (Î¸âˆ— , Î¸(kâˆ’1) ) âˆ’ DÎ¨ (Î¸âˆ— , Î¸(k ) + kâˆ’1
.
2Î±
Summing the above inequality for k = 1, . . . , T , the divergence terms cancel each other,
yielding
!
T
T
D
E
X
X
1
1
(T )
2
Î· kâˆ’1 gÌ‚k , Î¸(kâˆ’1) âˆ’ Î¸âˆ— â‰¤ PT
Î·kâˆ’1
kgÌ‚k k2âˆ— .
DÎ¨ (Î¸âˆ— , Î¸(0) ) âˆ’ DÎ¨ (Î¸âˆ— , Î¸(T ) ) +
2Î±
Î·
k=1 kâˆ’1
k=1
k=1
(19)
Let us now turn to the second sum. We start with developing a bound on the expected
(T )
regret. For any 1 â‰¤ k â‰¤ T , by construction Î· kâˆ’1 and Î¸(kâˆ’1) are Fkâˆ’1 -measurable. This,
together with (5) gives
D
D
h
E
i
E
(T )
(T )
E Î· kâˆ’1 gk âˆ’ gÌ‚k , Î¸âˆ— âˆ’ Î¸(kâˆ’1) Fkâˆ’1 = Î· kâˆ’1 gk âˆ’ E [ gÌ‚k | Fkâˆ’1 ] , Î¸âˆ— âˆ’ Î¸(kâˆ’1) = 0 . (20)
17

Combining this result with (18) and (19) yields
h 

i
E J Î¸Ì„(T ) âˆ’ J(Î¸âˆ— ) â‰¤
â‰¤

1

âˆ—

DÎ¨ (Î¸ , Î¸
Î·
kâˆ’1
k=1
1 PT
2
Î´ + 2Î±
k=1 Î·kâˆ’1 B
,
PT
k=1 Î·kâˆ’1
PT

(0)

âˆ—

) âˆ’ DÎ¨ (Î¸ , Î¸

(T )

!
T
 

1 X 2
2
)+
Î·kâˆ’1 E E kgÌ‚k kâˆ— Fkâˆ’1
2Î±
k=1

(21)

where we used the tower rule to bring in the bound (6), the nonnegativity of Bregman
divergences, and DÎ¨ (Î¸, Î¸(0) ) â‰¤ Î¨(Î¸) âˆ’ Î¨(Î¸(0) ); the latter holds as âˆ‡Î¨(Î¸(0) ), Î¸ âˆ’ Î¸(0) â‰¥ 0
q
since Î¸(0) minimizes Î¨ on K. Substituting Î·kâˆ’1 = Î· = 2Î±Î´
BT , k = 1, . . . , T finishes the proof
of (7).

To prove the high probability result (9), notice that thanks to (5) Î·kâˆ’1 gk âˆ’ gÌ‚k , Î¸âˆ— âˆ’ Î¸(kâˆ’1)
is an {Fk }-adapted martingale-difference sequence (cf. (20)). By the strong convexity of Î¨
we have
Î± (kâˆ’1)
kÎ¸
âˆ’ Î¸âˆ— k2 â‰¤ Î¨(Î¸(kâˆ’1) ) âˆ’ Î¨(Î¸âˆ— ) â‰¤ Î´.
2
Furthermore, conditions
(5) and (8) imply that kgk k2âˆ— â‰¤ B 0 a.s., and so by (8) we have
âˆš
kgk âˆ’ gÌ‚k kâˆ— â‰¤ 2 B 0 a.s. Then by HoÌˆlderâ€™s inequality
r
E
D
2B 0 Î´
âˆ—
(kâˆ’1)
âˆ—
(kâˆ’1)
gk âˆ’ gÌ‚k , Î¸ âˆ’ Î¸
â‰¤ kgk âˆ’ gÌ‚k kâˆ— kÎ¸ âˆ’ Î¸
kâ‰¤2
.
Î±
Thus, by the Hoeffding-Azuma inequality (see, e.g., Lemma A.7, Cesa-Bianchi and Lugosi,
2006), for any 0 <  < 1 we have, with probability at least 1 âˆ’ ,
v
!
u
T
T
D
E
u B0Î´ X
X
4
1
(T )
âˆ—
(kâˆ’1)
t
2
Î· kâˆ’1 gk âˆ’ gÌ‚k , Î¸ âˆ’ Î¸
â‰¤ PT
Î·kâˆ’1 ln .
(22)
Î±

Î·
kâˆ’1
k=1
k=1
k=1
Combining (19) with (8) implies an almost sure upper bound on the first sum on the right
hand side of (18) as in (21) with B 0 in place of B. This, together
q with (22) proves the required
high probability bound (9) when substituting Î·kâˆ’1 = Î· 0 =

2Î±Î´
B0T .


Proposition 3.2. For 1 â‰¤ t â‰¤ n, let `âˆ—t : R â†’ R denote the convex conjugate of `t : `âˆ—t (v) =
0 )i, and let K =
supÏ„ âˆˆR {vÏ„ âˆ’ `t (Ï„ )}, v âˆˆ R. For i âˆˆ I, recall that Îºi (x, x0 ) = hÏ†i (x), Ï†i (xP
i
(Îºi (xt , xs ))1â‰¤t,sâ‰¤n be the n Ã— n kernel matrix underlying Îºi and let KÎ¸ = iâˆˆI ÏÎ¸2i Ki be the
i
P
Î¸i
âˆ—
kernel matrix underlying ÎºÎ¸ =
iâˆˆI Ï2i Îºi . Then, for any fixed Î¸, the minimizer w (Î¸) of
J(Â·, Î¸) satisfies
n
Î¸i X âˆ—
wiâˆ— (Î¸) = 2
Î± (Î¸)Ï†i (xt ), i âˆˆ I ,
(10)
Ïi t=1 t
where

(
âˆ—

Î± (Î¸) = arg min
Î±âˆˆRn

n

1 >
1X âˆ—
Î± KÎ¸ Î± +
`t (âˆ’nÎ±t )
2
n
t=1

18

)
.

(11)

Proof. By introducing the variables Ï„ = (Ï„t )1â‰¤tâ‰¤n âˆˆ Rn and using the definition of L we can
write the optimization problem (3) as the constrained optimization problem
n

minimizen

wâˆˆW,Ï„ âˆˆR

1X
1 X Ï2i kwi k22
`t (Ï„t ) +
n
2
Î¸i
t=1

s.t. Ï„t =

X

hwi , Ï†i (xt )i ,

(23)

iâˆˆI

iâˆˆI

In what follows, we call this problem the primal problem. The Lagrangian of this problem is
(
)
n
n
X
1 X Ï2i kwi k22 X
. 1X
hwi , Ï†i (xt )i ,
L(w, Ï„, Î±) =
`t (Ï„t ) +
+
Î±t Ï„t âˆ’
n
2
Î¸i
t=1

t=1

iâˆˆI

iâˆˆI

Rn

where Î± = (Î±t )1â‰¤tâ‰¤n âˆˆ
is the vector of Lagrange multipliers (or dual variables) associated
.
with the n equality constraints. The Lagrange dual function, g(Î±) = inf w,Ï„ L(w, Ï„, Î±), can be
readily seen to satisfy
!
n
1 >
1X âˆ—
g(Î±) = âˆ’
Î± KÎ¸ Î± +
`t (âˆ’nÎ±t ) .
2
n
t=1

Now, since the objective function of the primal problem is convex and the primal problem
involves only affine equality constraints and the primal problem is clearly feasible, by Slaterâ€™s
condition (p.226, Boyd and Vandenberghe, 2004), if Î±âˆ— (Î¸) is the maximizer of g(Î±) then
wâˆ— (Î¸) = arg min infn L(w, Ï„, Î±âˆ— (Î¸))
wâˆˆW Ï„ âˆˆR
(
)
n
X Ï2 kwi k2 X
2
i
= arg min
âˆ’
Î±t hwi , Ï†i (xt )i .
2Î¸i
wâˆˆW
t=1

iâˆˆI

The minimum of the last expression is readily seen to be equal to the expression given in (10),
thus finishing the proof.


B

Calculating the derivative of J(Î¸)

In this section we show that under mild conditions the derivative of J exist and we also give
explicit forms. These derivations are quite standard and a similar argument can be found in
the paper by (e.g.) Rakotomamonjy et al. (2008) specialized to the case when `t is the hinge
loss.
As it is well-known, thanks to the implicit function theorem (e.g., Brown and Page,
âˆ‚2
âˆ‚
1970, Theorem 7.5.6), provided that J = J(w, Î¸) is such that âˆ‚Î¸âˆ‚w
J(w, Î¸) and âˆ‚w
J(w, Î¸)
are continuous, the gradient of J(Î¸) can be computed by evaluating the partial derivative
âˆ‚
âˆ‚
âˆ—
âˆ‚Î¸ J(w, Î¸) of J(w, Î¸) with respect to Î¸ at (w (Î¸), Î¸)), that is, âˆ‚Î¸ J(Î¸) = âˆ‚Î¸ J(w, Î¸)|w=wâˆ— (Î¸) .Note
that the derivative is well-defined only if Î¸ > 0, that is, when no coordinates of Î¸ is zero, in
which case
 2 âˆ—

Ïi kwi (Î¸)k22
âˆ‚
âˆ—
J(w (Î¸), Î¸) = âˆ’
.
(24)
âˆ‚Î¸
Î¸i2
iâˆˆI
If Î¸i = 0 for some i âˆˆ I, we define the derivative in a continuous manner as
âˆ‚
J(Î¸) =
âˆ‚Î¸

lim
0

Î¸ â†’Î¸

Î¸0 âˆˆâˆ†,Î¸0 >0

19

âˆ‚
J(Î¸0 )
âˆ‚Î¸

(25)

assuming that the limit exists. From (10) we get, for any i âˆˆ I, kwiâˆ— (Î¸)k22 =
Combining with (24) we obtain

 âˆ— >
âˆ‚
Î± (Î¸) Ki Î±âˆ— (Î¸)
âˆ—
.
J(w (Î¸), Î¸) = âˆ’
âˆ‚Î¸
Ï2i
iâˆˆI

Î¸i2 âˆ—
Î± (Î¸)> Ki Î±âˆ— (Î¸).
Ï4i

Now, by (25) and the implicit function theorem, Î±âˆ— (Î¸) is a continuous function of Î¸ provided
that the functions `âˆ—t (1 â‰¤ t â‰¤ n) are twice continuously differentiable. This shows that under
the conditions listed so far, the limit in (25) exists. In the application we shall be concerned
with, these conditions can be readily verified.

References
Argyriou, A., Hauser, R., Micchelli, C., and Pontil, M. (2006). A DC-programming algorithm for kernel selection. In Proceedings of the 23rd International Conference on Machine
Learning, pages 41â€“48.
Argyriou, A., Micchelli, C., and Pontil, M. (2005). Learning convex combinations of continuously parameterized basic kernels. In Proceedings of the 18th Annual Conference on
Learning Theory, pages 338â€“352.
Aronszajn, N. (1950). Theory of reproducing kernels. Transactions of the American Mathematical Society, 68(3):337â€“404.
Bach, F. (2008). Exploring large feature spaces with hierarchical multiple kernel learning. In
Advances in Neural Information Processing Systems, volume 21, pages 105â€“112.
Beck, A. and Teboulle, M. (2003). Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167â€“175.
Boyd, S. and Vandenberghe, L. (2004). Convex optimization. Cambridge University Press.
Brown, A. and Page, A. (1970). Elements of Functional Analysis. Van Nostrand Reinhold
Company, Windsor House, 46 Victoria Street, London S .W.1, England.
Cesa-Bianchi, N. and Lugosi, G. (2006). Prediction, Learning, and Games. Cambridge
University Press, New York, NY, USA.
Cortes, C., Mohri, M., and Rostamizadeh, A. (2009). Learning non-linear combinations of
kernels. In Advances in Neural Information Processing Systems, volume 22, pages 396â€“404.
Frank, A. and Asuncion, A. (2010). UCI machine learning repository.
Gehler, P. and Nowozin, S. (2008). Infinite kernel learning. Technical Report 178, Max
Planck Institute For Biological Cybernetics.
GoÌˆnen, M. and AlpaydÄ±n, E. (2011). Multiple kernel learning algorithms. Journal of Machine
Learning Research, 12:2211â€“2268.
Hastie, T., Tibshirani, R., and Friedman, J. (2009). The Elements of Statistical Learning:
Data Mining, Inference, Prediction. Springer, 2nd edition.
20

Hazan, E., Agarwal, A., and Kale, S. (2007). Logarithmic regret algorithms for online convex
optimization. Machine Learning Journal, 69(2-3):169â€“192.
Hazan, E. and Kale, S. (2011). Beyond the regret minimization barrier: an optimal algorithm
for stochastic strongly-convex optimization. In Proceedings of the 24th Annual Conference
on Learning Theory, volume 19 of JMLR Workshop and Conference Proceedings, pages
421â€“436.
Kloft, M., Brefeld, U., Sonnenburg, S., and Zien, A. (2011). lp -norm multiple kernel learning.
Journal of Machine Learning Research, 12:953â€“997.
Martinet, B. (1978). Perturbation des meÌthodes dâ€™optimisation. Applications. RAIRO Analyse NumeÌrique, 12:153â€“171.
Micchelli, C. and Pontil, M. (2005). Learning the kernel function via regularization. Journal
of Machine Learning Research, 6:1099â€“1125.
Nath, J., Dinesh, G., Raman, S., Bhattacharyya, C., Ben-Tal, A., and Ramakrishnan, K.
(2009). On the algorithmics and applications of a mixed-norm based kernel learning formulation. In Advances in Neural Information Processing Systems, volume 22, pages 844â€“852.
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. (2009a). Robust stochastic approximation approach to stochastic programming. SIAM J. Optimization, 4:1574â€“1609.
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. (2009b). Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574â€“
1609.
Nemirovski, A. and Yudin, D. (1998). Problem Complexity and Method Efficiency in Optimization. Wiley.
Nesterov, Y. (2010). Efficiency of coordinate descent methods on huge-scale optimization
problems. CORE Discussion paper, (2010/2).
Nesterov, Y. (2012). Subgradient methods for huge-scale optimization problems. CORE
Discussion paper, (2012/2).
Orabona, F. and Luo, J. (2011). Ultra-fast optimization algorithm for sparse multi kernel
learning. In Proceedings of the 28th International Conference on Machine Learning, pages
249â€“256.
Rakotomamonjy, A., Bach, F., Canu, S., and Grandvalet, Y. (2008). SimpleMKL. Journal
of Machine Learning Research, 9:2491â€“2521.
RichtaÌrik, P. and TakaÌcÌ‚, M. (2011). Iteration complexity of randomized block-coordinate
descent methods for minimizing a composite function. (revised July 4, 2011) submitted to
Mathematical Programming.
Rockafellar, R. (1976). Monotone operators and the proximal point algorithm. SIAM Journal
on Control and Optimization, 14(1):877â€“898.

21

SchoÌˆlkopf, B. and Smola, A. (2002). Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA.
Shalev-Shwartz, S. and Tewari, A. (2011). Stochastic methods for l1 -regularized loss minimization. Journal of Machine Learning Research, 12:1865â€“1892.
Shawe-Taylor, J. and Cristianini, N. (2004). Kernel Methods for Pattern Analysis. Cambridge
Univ Press.
Sonnenburg, S., RaÌˆtsch, G., SchaÌˆfer, C., and SchoÌˆlkopf, B. (2006). Large scale multiple kernel
learning. The Journal of Machine Learning Research, 7:1531â€“1565.
Xu, Z., Jin, R., King, I., and Lyu, M. (2008). An extended level method for efficient multiple
kernel learning. In Advances in Neural Information Processing Systems, volume 21, pages
1825â€“1832.
Xu, Z., Jin, R., Yang, H., King, I., and Lyu, M. R. (2010). Simple and efficient multiple
kernel learning by group lasso. In Proceedings of the 27th International Conference on
Machine Learning, pages 1175â€“1182.

22

