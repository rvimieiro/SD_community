195

An ordinal view of independence with application to plausible reasoning

Didier Dubois, Luis Farinas del Cerro, Andreas Herzig and Henri Prade
Institut de Recherche en Informatique de Toulouse- C.N.R.S.

1 18 route de Narbonne
3 1062 TOULOUSE Cedex.- FRANCE
Email: {dubois, farinas, herzig, prade} @irit.fr, Fax: (+33) 6 1.55.62.58
Universite Paul Sabatier,

Abstract

negation ...,. True and False are propositional constants
denoting the true and false events respectively.

An ordinal view of independence is studied in the

1

framework of possibility theory. We investigate
three possible definitions of dependence, of

1. 1

increasing strength. One of them is the counterpart
to the multiplication law in probability theory, and
the

two

others

are based on the

notion

The Multiplication Law

via
Multiplication Law:A and C are probabilistically
independent iff Prob(AAC ) = Prob(A) * Prob(C). A priori
we find it more natural to define independence as : C is
probabilistically independent of A iff Prob(CIA)
Prob(C ), relying thus on conditioning. In fact this

The standard definition of probabilistic independence is

of

the

conditional possibility. These two have enough
expressive power to support the whole possibility
theory, and a complete axiomatization is provided
for the strongest one. Moreover we show that weak

=

independence is well-suited to the problems of

conditioning-based notion of independence is equivalent to

belief change and plausible reasoning, especially to

the multiplication law. It follows from the axioms of

address the problem of blocking of property

probability theory that independence defined in this way is

inheritance

a symmetric relation, and that it is not sensitive to

in

exception-tole rant

taxonomic

reasoning.

0

PROBABILISTIC INDEPENDENCE

negation. In other words:
(symmetry)

INTRODUCTION

The notion of epistemic independence can be studied in the

(negation)

C

is independent of A

then

A is

"C is independent ofAiff

one's opinion about C is not affected by the fact of

knowing A." Dually, we say that C depends onAif it is
not the case that C is independent of A. A synonymous
expression for "C depends on A" is

"A is relevant for C'.

Traditionally, the formal basis of the dependence relation
(also called relevance relation) is conditional probability:
Given two events A and C and a probability measure
Prob, C is (probabilistically)
Prob(CIA) = Prob(C).

independent of A iff

In this paper we show that independence based on
possibility theory (Zadeh 1978, Dubois and Prade 1988)
has quite different properties. We present three definitions
of independence and study their formal properties. We call
them unrelatedness, strong and weak independence. We

If

C

is independent

of A then

C

IS

independent of ...,A.

framework of reasoning under uncertainty. It can be derived
naturally using conditioning:

If

independent of C.

Symmetry justifies to say "A and C are independent"
instead of "A is independent of C". Moreover we have
A and True are independent.

(truth)

Note that there are no such simple properties governing
the interplay of independence with conjunction and
disjunction.
The above properties are not enough to completely
characterize the notion of probabilistic independence. To
get completeness we need two more axioms (Kolmogorov
1956), cited in (Fine 1973):
(prob v) If A and B are independent, A and C are
independent and B and C are mutually exclusive,
then A and BvC are independent.

show that strong independence has enough expressive

(prob A) If A and B are independent, C and D are

power to support the whole possibility theory, and we

independent, B � D and A � C then AAB �

give a complete ax.iomatization not explicitly involving
the underlying uncertainty theory. Then we apply these
notions to the problems of belief revision and exception­
tolerant reasoning.
Throughout the paper A, B, C, D and E stand for events

CAD.
Hence the ax.iomatisation of dependence involves not only
logical truth, but also a qualitative probability relation
"�". It follows that - at least for the simple definition via

belonging to a Boolean algebra of subsets of a set Q.We

conditioning - we cannot study the formal properties of
probabilistic dependence separately from the probabilistic

use the symbols: conjunction

framework.

A,

disjunction

v,

and

196

Dubois, Farinas del Cerro, Herzig, and Prade

The Multiplication Law has been criticized by several
authors. R. von Mises (1964) has argued that the
Multiplication Law could be fulfilled just because of "pure
numerical accidents", although A and B are not intuitively
independent in the sense of 'being separated' or 'not
influencing each other'. He gives an example where
Prob(A) = Prob(B) = Prob(C) Prob(D)
1/4, and A, B,
C and D are pairwise mutually exclusive. Then he
=

=

investigates whether AvB and BvC are independent.
According to von Mises they are intuitively dependent
(because they have B in common), whereas the
Multiplication Law says that they are independent.
H. Reichenbach ( 1949) has argued that dependence and
independence should be ternary rather than binary relations,
where the third element in the relation is the evidence on
which we declare that A and C are independent. From the
ternary relation we can get back the binary one as follows:

Aand C are independent if! there is some evidence E such
thatA and C are independent on evidence E. We do not
treat ternary relations in the sequel except in the last
section. Nevertheless our analysis carries over to the
ternary case. In the rest of the section we formally present
two important principles that are not validated by the
Multiplication Law.

2

POSSIBILITY THEORY

We introduce the notions of possibility measure and
distribution and of conditional possibility.

2. 1

Possibility

Measures

Possibility measures allow to associate an uncertainty
degree to the elements of the set of events. Following
e.g., Dubois and Prade (1986), a function ll from 20. into
the real interval [0, 1] is a possibility measure if it satisfies
the following axioms:
I1(True) > IT(False)

;

I1(AvB)

=

max(TI(A),TI(B))

Any totally ordered set can stand for the unit interval, that
we keep here for the sake of simplicity. By convention
I1(True) == 1, IT(False) = 0. ITCA) = 1 only means that A is
possibly true, while I1(A)
0 means that A is certainly
false. Particularly, when ITCA)
TIC-.A)
1, it means
total ignorance about the truth of A. The basic max­
decomposability axiom is due to the purely ordinal
setting. It can also be viewed as enforcing IT(A u B) to its
=

=

=

lowest bound since for any reasonable confidence measure,
we should have "if A implies B then IT(A) � I1(B)". Note
that n cannot be decomposable with respect to
conjunction because TI(AA...,A)
0 for all A, but IT(A)
f1(-,A) = 1 is permitted. Moreover, max(IT(A),IJ(-,A))
=

1. 2

Conjunction and (in)dependence

A formal objection to the multiplication Law has been
given by J. M. Keynes (1921, cited in (Giirdenfors 1978)).
According to Keynes, the following conjunction criterion
for dependence (called conjunction criterion for relevance in
(Giirdenfors 1978)) should be valid:
(CCD)

If C depends on A, and C depends on B then C
depends on At\13.

He notes that (CCD) is not validated by the Multiplication
Law. Keynes proposes a stronger definition of
probabilistic independence that does it: Cis independent of

A iff there is noBsuch that A impliesBand Prob(CIB) ;t:
Prob(C). R. Carnap (1950) has shown that this definition
leads to a trivial notion of independence: It entails that C
depends on A as soon as C and A are consistent.
Giirdenfors (1978) has suggested a conjunction criterion for

independence dual to (CCD):
(CCI)

=

=

If C is independent of A, and C is independent of
B then C is independent of AAB.

is a natural principle that should be valid. Gardenfors
criticizes the Multiplication Law because it does not
guarantee this principle, and investigates a series of
stronger definitions of independence. He finally comes up
with: C is independent of A iff
Prob(A)
0, or
==

Prob(CA
I B
A ) = Prob(C)forallBsuch that Prob(AAB) > 0
and Prob(CIB) = Prob(C .) This definition validates (CCI).
Note that Gardenfors' independence relation is non­
symmetric. Here we show that in an ordinal setting where
uncertainty is described by ordering the states of the world
by their plausibility, we capture similar regularities in
terms of disjunction, in a much simpler way.

I1(True), since Av-,A

=

True. The quantity N(A)

==

1

-

IJ(..,A) is called the necessity of A, and represents a level
of c ertainty, or acceptance of A. Especially N(A) > 0

means that A is accepted, while ..,A is not (since N(A) > 0
entails N(•A)
0 in the absence of inconsistency). And
we have the reasonable axiom of acceptance saying that if
A is accepted and so is B, then AAB is accepted too, since
N(AAB) == min(N(A),N(B)) holds. Note that the fact that A
is not accepted (N(A) 0) does not entail that it is rejected
(which is expressed by N(...,A) > 0).
=

=

In the finite case a possibility measure can be represented
by a possibility distribution 1t on 0. the set of
interpretations of the language. Namely IT(A) =
sup{n{ro) I w I= A}. 1t encodes a complete partial ordering
of interpretations, with the intended meaning that if n(ro)
> n(w'), ro is a more plausible description of the current
situation than ro'. Reasoning in the setting of possibility
theory comes down to assume that the current situation is
always one of the most plausible ones. Accepting A (N(A)
> 0) means that I1(A) > f1(-,A), i.e., that "normally A
should be true". This way of modelling uncertainty is in
full accordance with Shoham (1988)'s view of preferential
logic, Lehmann (1989)'s notion of ranked models, also
encoded in Pearl (1990)'s system Z. See (Benferhat et al.
1992). Especially N(A) > 0 can be written True lv A in
the terminology of conditional assertions in Lehmann's
rational nonmonotonic setting. True lv A means "A is
plausibly true" (unconditionally).
Every possibility measure induces a relation "�" defined
by A � B if and only if ll(A) � I1(B). We call � a
relation agreeing strictly with IT. A � B is read "A is at

An Ordinal View of Independence with Application to Plausible Reasoning

least as possible as B". This relation is called qualitative
possibility ordering and satisfies the following conditions:
{non triviality)
(tautology)
(transitivity)
(disjunctiveness)
(dominance)

True >False
True;:: A
if A;:: B and B ;:: C then A ;:: C
AvC � A or AvC �C.
If A implies B then A � B

Remark. Often, (disjunctiveness) is replaced by the two
axioms of connectedness {A ;:: B or B ;:: A) and disjunctive
stability (if A;:: B then AvC;:: BvC)
That these conditions are sufficient follows from the
following formal relation between possibility theory and
qualitative possibility orderings (Dubois 1986): The only
functions mapping events into [0, 1] which strictly agree
with qualitative possibility orderings are possibility
measures, and a strictly agreeing possibility measure
always exists. In our presentation of possibility theory,
this result can be expressed as follows:
Theore m (soundness and completeness of qualitative
possibility orderings). Let IT be any measure on n, and;::
any binary relation on the subsets of n. Then IT is a
possibility measure iff ;:: is a qualitative possibility
ordering.
Conditional possibility relations first appear in Lewis
(1973)'s logics of counterfactuals whose semantics are
systems of spheres. But as indicated in Dubois and Prade
(1991) a system of spheres is equivalent to a possibility
distribution. Formal links between possibility theory and
conditional logics are studied in Farinas del Cerro and
Herzig (1991). Lastly, the dual qualitative certainty
relation, equivalent to necessity measures (A ;;>:N C iff
...,c ;:: -,A) is closely related to expectation-orderings of
Gardenfors and Makinson (1994). The characteristic axiom
of ;::N is
(conjunctiveness)

2.2

Conditional

Following Hisdal ( 1978) and Dubois and Prade (1986,
1992) the conditional possibility IT(CIA) is defined as the
maximal solution of the equation IT(AAC) = min(fl(CIA),
IT(A)). This definition is clearly inspired from Bayes'
Rule, where min corresponds to the product. The choice of
the maximal solution is due to the principle of minimal
specificity which urges to select the least committed
possibility measures, i.e. the one which allows each event
to have the greatest possibility level:
IT(CIA)= 1
IT(CIA)= Il(A..\C)

if fl(A) = fl(A·'.C)
if fl(A) > fl(AAC)

Facts.
1. If Il(A) 0 then fl(CIA) I
2. If fl(AAC) = I then fi(CIA) 1
3. If fl(A) > 0 and IT(C) 0 then IT(CIA)
4. IT(ChC) = 1 iff fi(-q = 0
5. Il(ChC) = 0 iff fi(...,C) > 0
=

=

=

=

Some of these facts deserve some comments. Fact 1
suggests that nothing is sure when assuming that a
certainly false proposition is true (since in this case
anything and its contrary is plausible). This leads to the
convention IT(False I False)= I which does not agree with
the non-triviality axiom (it is not compulsory anyway).
Fact 2 says that if A and C are fully consistent, assuming
A true keeps C possibly true. Fact 3 says that a certainly
false proposition remains false via conditioning by a non­
certainly false proposition. However if IT(A) = 0 then the
conditional possibility again disagrees with the n on­
triviality axiom.
In the next sections we present three different ordinal
definitions of (in)dependence. Two of them are based on
the notion of conditional possibility. We show that these
two can express qualitative possibility, and that complete
axiomatizations is given for one of them. We conjecture
that this is not possible for the third one, originally due to
Zadeh. In all three cases, a necessary condition for the
independence of A and C will be that the conjunction AAC
can be interpreted truth-functionally, in the sense that
fl(AAC) =min (fl(A),fl(C)) for these particular events.
The conditional necessity is N(...,C IA) = 1 - IT(CIA),
defined by duality. Note that N(CIA) = N(-,AvC) if
N(CIA) > 0. The following property will be used at length
in the sequel:
N(CIA) > 0 iff fl(AAC) > fl(AN·•C)
N(CIA) > 0 means that C is accepted as true when A is
assumed to be true. It corresponds to the conditional
assertion A tv C in the sense of Lehmann's rational
inference, and can be viewed as the (nonmonotonic)
plausible entailment of C from A in the presence of an
ordering of interpretations. The above clearly show that
A tv C means AAC is more plausible than AA..,C (or
equivalently fi(AAC) > fl(AA..,C) in terms of possibility
measure).

3

Possibility

=

0

197

(UN)RELA TEDNESS

Zadeh (1978) has introduced a symmetric definition of
independence called "non-interactivity" between
possibilistic variables that is not based on conditional
possibilities.This notion has also been studied by
Nahmias (1978) for events, under the name
"unrelatedness".
(Def ""z) A and C are related propositions in lildeh's
sense (denoted by A ""z C) iff Il(AAC) *
min(fi(A),fl(C)).
It is interesting to characterize the constraints induced by
unrelatedness on the ordering of interpretations AAC ,
..,AAC, AA..,C, ...,AA..,C respectively.
Proposition. A and C are unrelated if and only if
fl(AAC);:: min(fl(AA-,C),IT(...,AAC)).
Proof IT ( A A C) = min(max(Il(AA C ),Il( A A...,C ) ) ,
max(fl(AAC),IT(..,AAC))). Clearly as soon as Il(AAC) :2:
fl(AA..,C), unrelatedness holds. And the same when
fl(AAC) ;;>: fiC-,AAC). However if Il(AAC) < IT(AA...,C )

198

Dubois, Farinas del Cerro, Herzig, and Prade

and Il(A/\C) < IT(...,A/\C), then ITCA) = Il(A/\•C) and
TICC)=I1(-,A/\C), and A and C are related.

Il(CIA)

=

I1(C)

<

1 implies [1(-,CIA) = ITC...,C)

=

1

1 is a
but not the converse. Hence fl(-,CIA) = ITC•C)
very weak statement saying that not accepting C (i.e.
N(C) = 0) is not questioned by fact A. In particular,
I1(CIA) I1(C) IJ(-,CIA) Il(...,C) 1 (which is never
met in the probabilistic case), means that in the presence
of A, C, which was originally ignored, is still ignored. In
this paper we shall restrict to independence of accepted
propositions with respect to other propositions;
independence of ignored propositions turns out to be a
very distinct issue, as suggested by the following result:
=

Clearly, A and C are related implies that A/\C is an
implausible situation (since in any case it holds
Il(A /\C) :::.:; min(IT(A),IT(C)) ), i.e., A and C are (more or
less) mutually exclusive:

Corollary. A and C are related if and only if N(...,CIA) >
0 and N(-,AIC) > 0.
On the contrary when A and C are unrelated the two
propositions are totally allowed to be true together.
Zadeh's independence is an extension of the logical notion
of consistency. This notion is not very demanding.
Moreover this notion is local in the sense that it is
sensitive to negation: if A and C are unrelated, it does not
say anything about the other literals ..,A and C, C and ..,A,
-,cand -,A, Other properties are as follows.

Facts.
l . A ""z C iff C ""z A
2. If A ""z BvC then A ""z B or A
3. If AvB ""z C then A ""z C or B

C
C
(due to symmetry)
4. If A ""z C and B ""z C then AvB ""z C
5. If A ""z B and A ""z C then A ""z BvC
(due to symmetry)
6. False ::tz A (where ::tz means "not(-=z ))"
7. True ::tz A
8. A ::tz A
9. A ::tz -,A iff IT(A) 0 or I1(-,A) 0 ; 10. AvC "1:-z A
=

""z

""z

=

Facts 2 and 3 are disjunction-oriented. However, none of
the two conjunction criteria (CCD) and (CCI) are valid
with unrelatedness. Note also that facts 8 and 10 is
certainly a strange property for an independence relation.
There seems to be no way to express I1 by means of ""z,
the reason being that we cannot express Il(A)
1.
Therefore, we conjecture that (just as for probabilistic
independence) ""Z cannot be axiomatized alone.
=

4

STRONG INDEPENDENCE

It is tempting to define dependence in possibility theory in
a way similar to probability theory, namely to define C as
independent of A when the conditional measure of C given
A is equal to the unconditional measure of C. Here we
have two uncertainty functions I1 and N. Hence we can
defin e independ e n ce as ITCCIA) = Il(C) or
N(CIA) = N(C). Notice that N(CIA)=N(C) is equivalent
to IJ(-,CIA) = Il(...,C). In (Farinas and Herzig 1994a) the
independence relation defined by ITCCIA) [l(C) is
studied. A complete axiomatisation has been given.
=

Note that if Il(CIA) = Il(C) < 1 then we are in the
situation where C is plausibly rejected (since fl(...,C) = 1 >
ITCC)). Hence the meaning of IT(CIA) Il(C) < 1 is that
when A is assumed to be true, it does not affect the
plausible rejection of C. This expresses the negative
statement that accepting ...,c is independent of A. It
suggests to use N(CIA) N(C) in order to express a
positive statement. Note that we also have
=

=

=

=

=

=

Proposition 4. 1. N(CIA) = N(C) iff either (i) 1 =
max(I1(-,A/\...,C),I1(A/\-,C)), and Il(A/\...,q � Il(A/\C), or
(ii) I1(A/\C) > IT(A"...,q � IJ(-,A/\...,C)
Moreover, (i) is equivalent to N(CIA) N(C) 0, and (ii)
is equivalent to N(CIA)= N(C) > 0. Note that the two
situations (i) and (ii) correspond to (almost) reversed
orderings of interpretations. We give the following
definition of the strong independence relation:
=

=

(Def "1:->) C is strongly independent of A (denoted by
A "1:-> C) iff N(CIA) N(C) > 0.
=

Note that A "1:-> C indicates that in the context where A is
true C is accepted. Due to what we said above, C is
strongly independent of A iff ITC...,CIA) Il(•C) < I .
=

In the next theorem we characterize a dependence relation
"">=not("#>) without using conditional necessities.

Theorem ( construction of ""> from IT). Let IT be a
possibility measure, and let""> be defined from its dual N
through (Def"l:->).
iff I1(A) > I1(-,q= IT(A/\•C)
l. A"�:-> C
iff [l(A) S: TIC•C) or IT( ...,C) > IT(A/\•C)
2. A ""> C
Proof Follows directly from Proposition 4.1.
Corollary. Let IT be a possibility measure, and let =>
be defined from IT through (Def "1:->).
l . A "1:-> C iff Il(A/\•C) min(IT(A),[1(-,C)) and IT(•C)
< I1(A).
2. A""> C iff IT(A/\...,q :F- min(I1(A),I1(.C)) or [1(-,q �
[1(A).
3. If A "1:-> C then IT(A/\C) = min(Il(A),IT(C)).
4. If IlC•C) � I1(A) then A""> C.
=

Fa c ts.
1. If A""> B/\C then A -=> B or A""> C
2. If A v B => C then A""> C or B ""> C
3.1f A""> C and B -=> C then AvB ""> C
4. If A""> B and A ""> C then A""> B"C
6. True "1:-> C iff N(C) > 0
5. False""> C
7. A => False
8. A "1:-> True iff [l(A) > 0
9. A/\B ""> •B"C 10. If A implies -.c then A ""> C
11. AvC "1:-> .C iff IJ(A) > IT(C)
12. A"�:-> A iff N(A) = 1 ; 13. If Il(A) 0 then A""> C
14. If IT(C) = 1 then A "'> ·C
15. A""> cor -,c""> •A
=

Let us comment on these facts. Facts 2 and 3 are similar
to the (CCI) and (CCD) axioms except that disjunction is
used instead of conjunction. Facts 1 and 4 are also similar

An Ordinal View of Independence with Application to Plausible Reasoning

but the conjunction of influenced facts is considered
instead of influencing facts. Fact 5 means that assuming a
contradiction holds destroys all previously plausible
propositions. On the contrary tautologies never affect the
plausibility of already plausible propositions (Fact 6).
Fact 7 is simply due to the impossibility to assert false
propositions. Fact 8 says that we can only assert a
tautology is plausible when taking for granted an
impossible proposition. Fact 9 and 10 express equivalent
properties. N amely if A implies that C is false then when
learning that A is true affects our opinion about C when C
was previously supposed to be plausible. Fact 11 shows
that the possibilistic ordering can be translated in terms of
strong independence. Fact 12 claims that the only case
when the truth of A is independent of itself is when A is a
tautology. Fact 13 is a more general statement than fact 5.
Fact 14 holds because it cannot be the case that D(A) >
I1(C). Similarly the reason for fact 15 is that TI(A) >
D(•C) cannot go along with IT(.C)> TI(A).
Clearly, probabilistic dependence and possi bilistic
dependence are quite different concepts. Probabilistic
properties such as symmetry("If B depends on A then A
depends on B") or transparency w.r.t. negation ("If B
depends on A then B depends on •A'') do not hold in the
possibilistic case. In other words, A i:-> B neither implies
B i:-> A nor A i:-> •B. On the other hand, possibilistic
dependence has some "nice" regularities such as 1., 2., 3.,
4., none of which holds in the probabilistic case. These
regularities are quite close to the criteria(CCD) and(CCI).
Concerning the expressivity of the dependence relation it
is interesting to observe that it possesses the same
expressivity as possibility theory itself. This follows from
the next result.
Theorem (construction of TI from i:->). Let TI be a
possibility measure, and let i:->be defined from TI.
1. I1(A) >TI(C) iff AvC *>.C.
2. ITCA) ::::rrcq iff Ave""> ·A.
Proof By previous fact I I.

The theorem can be read as follows: C is strictly less
possible than A if and only if learning that AvC is true
does not change my rejection of C. The theorem should
not be surprizing since the meaning of independence is to
enforce constraints on the ordering between interpretations
as shown in Proposition 4.1. It turns out that such
constraints are enough to identify a single ordering, i.e. a
comparative necessity relation.
Thus we are able to express qualitative possibility by
means of strong independence. In a trivial manner, this
correspondence enables us to obtain an axiomatization of
the (in)dependence relation by translating the qualitative
counterpart of possibility theory. N ote that this is in
contrast with probability theory: There, the independence
relation cannot completely capture qualitative probability
(which in turn determines the probability measure). Here
we give a simpler axiomatization of"">:
("">1) True *>True
(o:>2) A"">False

199

(=>3) If AvB "">•B and BvC =>....C then AvC""> -.c
(=>4) A =>•A
(=>5) If A =>BAC then A"'> B or A"">C
Theorem (soundness and completeness of the axiomatics

of "'> w.r.t. possibility theory). Let => be a relation on
events, and n a mapping from the set of events to [0,1]
such that A i:-> C iff N(CIA) N(C) > 0. Then => is a
dependence relation iff N is a necessity measure.
Proof From the right to the left, it is sufficient to prove
that the above axioms(rewritten as qualitative necessities)
are valid. Then we can use the soundness of qualitative
necessity orderings w.r.t. possibility theory. From the left
to the right, we prove that the axioms for qualitative
necessity orderings are derivable from the above
axiomatics(and then use the completeness of qualitative
necessity orderings w.r.t. possibility theory). Using the
previous theorem in terms of necessities, namely N(A) >
N(C) iff •Av•C *>A; N(A);:: N(C) iff •Av•C "'> C
we express qualitative necessities with "'>:
=

1. (non triviality) True >N False becomes

•Falsev•True *>True.
It is equivalent to True*>True which is an instance of
(=>1).
2. (transitivity) if A :=:N B and B :=:N C then A :=:N C
becomes: If •Av•B "'>B and •Bv•C "'>C
then •Av•C => Cwhich is("'> 3).
3. (tautology) A �N True becomes •Av•True => True
which is nothing else but(""> 2).
4. (conjunctiven ess) AA C :=:N A or AA C :=:N C
becomes •(AAC)v•A =>A or •(AAC)v•C =>C, hence
•Av....C "">A or •Av•C "">C. The latter can be proved
combining •Av•C "'> AAC which is an instance of
(""> 4), and: If •Av•C => AAC then •Av-,C "'> A or
•Av-,C =>C which is an instance of("'> 5).
5. {dominance) can be replaced by
(equivalence)
If A H C then A �N C and
(monotony)
A :=:N AAC.
The latter is translated to • Av•(AAC) =>AAC, which is
an instance of(=>4). Hence what remains to prove is
If A H C then •Av....C => C.
N ow from A H C we get •C H •Av•C. From the latter
we get (•C "'> C iff •Av•C "'>C). Then •Av•C "'> C
follows from("">4).
Remark. It is important to note that => is quite close to

a qualitative possibility ordering: Replacing A "='> C by
TI(A) � TI(•C) all our principles are possibilistically
valid. In particular(connectedness) can be deduced from the
axioms: From('"">4) and("'>5) we can get AvC =>·A
or AvC =>•C(see above).
The other way round, the only(qualitative) axiom for �N
that apparently does not follow from the above axioms is
that of transitivity. As on the other hand we know by the
above Corollary that A i:->C implies TI(A) >TIC.....C), we
obtain that for a given n. *> is a fragment of the

200

Dubois, Farinas del Cerro, Herzig, and Prade

corresponding strict possibility ordering. This fragment is
closed under all the axioms of possibility theory except

that of transitivity.

5

because what we may wish to express is a more qualitative

notion of independence. Now, strong independence requires
that not only C remains more plausible than

-,c when A

is known to be true, but its level of acceptance should not
be altered. This last requirement forces the inequality

fl(A/\...,C );:: fi(-,A/\...,C) which implies that in the context
where C would be false, it is forbidden to conclude that

-,A should be accepted (see Fact 15 of the previous

section). Hence we have the property

C is strongly independent of A if and only if N(CIA) >
and ...,(N( ·AhC) >

0).

0

A milder notion of independence is that if C is accepted

unconditionally, then if A is true, C remains accepted;

then we do away with any commitment in the case when

C would turn out to be false. Hence the following
definition:

Proposition 5.1.

A #>w C iff I1(A/\ C) > fl(A/\ • C )

I1(A/\..., C ), I1CC)
max(I1(C/\A), I1(C/\-,A)) >
1
I1(A/\...,C) and max(I1(C/\A),I1(C/\•A)) > I1C•A/\...,C), the
=

first of which is redundant.

Q.E.D.

A -:t=> C iff A #>w C and

N(C).

...,c

strongly independent of A as soon as

A. As a

1--

consequence, it is easy to see that the theorem that

constructs a possibility measure from the independence

relation

also

holds

when

we

change

the

strong

independence into the weak independence. In fact only the

part of the strong independence relation that is equivalent

to weak independence is useful to recover the underlying

possibility measure. However if

....C

1--

A does not hold, A

#>w C does not enforce an inequality between I1(C) and
fl(...,A) generally. Finally the six axioms that characterize

strong independence with respect to possibilistic semantics
also hold for weak independence, but more axioms are

necessary to completely characterize weak independence.
Let us show how weak independence can

be related to the

framework of belief revision (Gii.rdenfors, 1988). A central

problem for the theory of belief revision is what is meant

by a minimal change of a state of belief. As pointed out in

based on almost exclusively logical considerations.

Proof Indeed A#>w C is equivalent to I1(A/\C) >

Proposition 5.2.

=

been used [in the models for belief change] have been

0 andN(C) > 0.

and max(I1(C/\A),I1(C/\-,A)) > fl(...,A/\•C ).

=

N((...,A/\C)vC)

Gardenfors (1990), "the criteria of minimality that have

is weakly independent of A

(denoted A #>w C) iffN(CIA) >

=

0 is such that

Hence when C is weakly independent of A then it is also

The notion of strong independence may be felt too strong

#>w) C

Indeed if Av...,C #>w C, then N(CIAv....C) >
N(CIAv....C)

WEAK INDEPENDENCE

(Def

VA,C, Av-,C #>w C iff Av....C #> C

fl(A/\•C)=

However, there are a number of non-logical factors that

should be important when characterizing a process of

belief revision". Gardenfors focuses the notion of

dependence (he uses the synonymous term 'relevance') and

proposes the following criterion:

If a belief state K is
revised by a sentence A, then all sentences in K that are
independent of the validity of A should be retained in the
revised state of belief

IJ(...,C). (Obvious using Proposition 4.1.)

This seems to be a very natural requirement for belief

Proposition 5.3. A #w C implies A #>z C.
Proof Obvious since I1(A/\C) > fl(A/\...,q ,

to implement belief change operations. As noted by

fl(A)

=

fl(A/\C) :::; I1(C).

However it

revision operations, as well as a useful tool when it comes

and then

technical formulation in a model based on belief sets built

up from sentences in a simple propositional language

is not true that, as for strong independence

A #>w C implies I1(A/\-,q

=

because the notion of relevance is not available in such a

min(I1(A),f1C...,C))

language." However the above criterion does make sense

in the ordinal setting of possibility theory.

since weak independence does not involve ll(A/\..., C).
It can be checked that weak independence satisfies Facts

1,

namely Fact 12, which becomes A #>w A iff N(A) >

0,

2, etc. of the previous section except for a few ones,
and Fact 15. The latter is not surprizing since weak

independence is meant to let the relationship between

N(AIB) >

0 and N(...,BhA) >

to have A #>w C and

Gardenfors, "a criterion of this kind cannot be given a

0 loose. Hence it is possible

-,c #>w ...,A. This occurs precisely

when IT(..., A/\C) > max(Il(A/\C), fi(...,A/\...,C)) and

min(I1(A/\C),IJ(-,A/\-,C)) > I1(A/\...,C). Besides, weak
independence satisfies stronger forms of Facts 3 and 4 :

3'.

if A "">w C or B "">w C then AvB "'>w C

4'.

if A "'>w B orA ""->w C then A "">w B/\C

Lastly we have the following remarkable property

We suppose given a theory
operation

K

and an AGM revision

* (Giirdenfors, 1988). K*A represents the result

of revising K by A. According to Gardenfors and
Makinson's characterization theorem,

K

and

*

can be

represented equivalently by an epistemic entrenchment

ordering, which in turn is nothing else than a qualitative

necessity ordering. It can be proved that in terms of
possibility theory the fact that C belongs to
equivalent to having N(CIA) >

0

K* A

is

(Dubois and Prade,

1992); moreover C belongs to K is equivalent to N(C) >

0. If we translate the definition of the weak independence
relation #>w in terms of revision we get
A #>w C iff C E

K and C E K*A

which is exactly Gardenfors' above requirement for
revision-based independence. Clearly, a companion

An Ordinal View of Independence with Application to Pl ausible Reasoning

201

definition of a dependence relation "">- associated to a
given qualitative necessity ordering can be defined via the
following condition from a given AGM contraction
operation (-):

simplicity we have restricted our analysis to binary
dependence relations here, but it is clear that a ternary
relation is certainly the most general one. This will be
subject of further investigations.

(Cond ::=>-) A ::=>- C iff C E K and C tl: K-A
iff N(C) > 0 and N(A) � N(A vC).

7

This alternative notion is studied in (Farinas and Herzig,
1994b). The comparative analysis of revision-based and
contraction-based notions of independence is beyond the
scope of this paper.

Possibility theory is a natural framework for handling
nonmonotonic reasoning problems, because it embeds
what Lehmann calls rational inference (see Benferhat et al.,
1992). Given a set of rules modelled by pairs of
propositional formulas, it is possible to rank-order these
exception-tainted rules in terms of their relative
specificity. This ranking of rules generates an ordering of
interpretations that can be encoded as a possibility
distribution.

6

COMPARATIVE DISCUSSION

We have analysed three notions of (in)dependence that can
be defined in possibility theory. A common feature to all
of them is that the independence of A and C requires that
the conjunction of A and C is interpreted truth­
functionally. In other words, we have

A :F> C implies A :t>w C ; A :F.>w C implies A :F.z C
Hence, all notions of independence share the property
Il(AAC) = min(IT(A),Il(C)). Moreover, we have shown
that
A :t:.> C iff A :F.z •C and fl(•C) < fl(A)
A :F.> C iff A :F.>w C and IT(AA•C)= fl(.C)
We now examine the validity of Keynes-Giirdenfors criteria
of Section l in the ordinal setting of possibility theory.
Namely the following requirements:
If A ""> C and B => C then AAB => C
If A :F> C and B :F> C then AAB :t> C

(CCD)
(CCI)

Also consider symmetric counterparts of CCD and CCI:
(CCD-r)
(CCI-r)

If A ::=> B and A => C then A => BAC
If A :F> B and A :F> C then A :t> BAC

and the corresponding properties changing conjunction
into disjunction (DCD, DCI, etc).
If A :t> C and B :t> C then AvB :t> C

(DCI)

If A :F.> B and A :F.> C then A :F.> BvC

(DCI-r)
(DCD)
(DCD-r)

If A ""> C and B ""> C then AvB ""> C
If A ""> B and A ""> C then A => BvC

First the relatedness property of Zadeh :tz satisfies the four
above criteria concerning disjunctions. (CCD-r), (CCI-r),
(DCI) (DCD) hold for strong and weak independence. The
weak independence has the following stronger property:

A :F>w BAC iff A :t>w B and A :F.>w C
AvB :F->w C iff A :t>w C and B :t>w C
that is (DCI) and (CCI-r) with equivalence, due to Facts 3'
and 4' of Section 5. This is natural if weak independence is
considered in terms of belief revision: if we continue to
accept BAC upon learning A we should continue to accept
C and B as well.
We could have introduced as well a ternary dependence
relation "B and C are independent, given A", as studied by
Giirdenfors (1978, 1990) and Pearl ( 1988). For reasons of

APPLICATION TO EXCEPTION­
TOLERANT REASONING

The algorithm for ranking rules (or interpretations) has
been proposed by Lehmann, and also in a different form by
Pearl. Benferhat et al. (1992) have shown that this
ordering can be retrieved by means of the least specific
possibility distribution that is consistent with the rules.
Namely let K be a conditional knowledge base where rules
are of the form Ai l'v Bi (read if A 1 is true, B1 is plausibly
true). Each rule is interpreted as the constraint N(BiiAj) >0
or equivalently IT(A1AB1) > IT(A1A•B1). Then the ranking
of the interpretation obtained by considering the maximal
element of the set { 1t, fl(AiAB1) > Il(AiA'Bi), V'i=l ,n}.
This possibility distribution is unique and is denoted 1t*.
Then the level of priority of rule (A1,B1) is simply
computed as N*(•A1vB1) (computed from n*). Then given

evidence A, and knowledge K, B is a plausible conclusion
of A in the context B if and only if N*(BIA) > 0, i.e.
Il*(BAA) > IT*(•BAA). This procedure suffers from the
problem of blocking of property inheritance as shown in
the following example.

K = (p 1'v ..,f, b 1'v f, p fv b, b l'v I} where p =penguin, b =
bird, f
fly, I
legs. It is well-established that the
rational inference method classifies the rules of K into 2
sets of rule: {b 1'v f, b 1'v I} have lower priority than {p l'v
•f, p l'v b }. It can be encoded in possibilistic logic as
N(•bvl) � o:; N(•bvf) � o:; N(•pv•f) � p; N(pvb) � p,
with p > 0:.
=

=

The corresponding minimally specific ranking is such that
IT*(pAI) = IT*(pA...,l), hence forbidding the conclusion that
penguins have legs, despite the fact that the rule b l'v I is
not involved in the conflict between penguins and birds
with respect to flying. Several solutions have been
suggested to solve this problem including maximal
entropy rankings, lexicographic methods and others. Here
we suggest that weak independence solves the problem.
Consider the graph induced by the rules of K.

202

Dubois, Farinas del Cerro, Herzig, and Prade

Any Bayesian-oriented AI researcher would suggest that I

conditional possibility distribution into a global joint one

(which is clearly not true for f). This is intuitive as well:

independence relation is non-symmetric, i.e will not be a

is conditionally independent of p in the context of birds

(see, e.g., Fonck

1993). A third reason is that the weak

If we learn that some bird is in fact a penguin, this does
not influence our belief that it has legs. The conditional

graphoid. Hence the mastering of weak conditional

extension of weak independence reads

handling exception-tolerant rule-bases is an open line of

N(llb) >

independence in the possibilistic setting for the purpose of
research, although a promising one.

0 and N(llb/\p) > 0.

Here it leads to add the rule P-"b 1--- I to K, i.e., to select

8

Il(-..1-"P-"b) the level of priority of this rule will be the

This paper has provided a preliminary but systematic study

another ranking of worlds that satisfies also 00-"P-"b) >
same as p 1--- b and p
K u {p-"b 1---

1--- -.f. It is clear then that from p and

I} one can deduce I plausibly.

There is no space to develop this point in detail here.
However we plan to develop this methodology in the
future (see, e.g. (Benferhat et al.

1994) for preliminary

results). A first remark is that we do not use strong

independence here. Strong independence would have two
drawbacks

1) It would introduce equality constraints (here of the

form N(llp/\b);;:; N(llb)) whose nature is different from

that of the rules. As a consequence looking for the
minimally

specific possibility

distribution

that

satisfies both rule-constraints and independence
constraints may not lead to a unique solution. This is
the problem already encountered by Goldzsmidt and
Pearl

(1992) with stratified rankings. The weak

independence notion avoids this drawback.

2) It forbids the possibility of adding some contraposed
rules since N(llp/\b) ;;:; N(llb) > 0 implies that
Il(p-"-..1/\b) � Il(-..p/\-.)1\b), i.e., it is forbidden to
claim that "birds without legs are not penguins" which

seems to be a natural claim in the context of birds.
The idea of adding weak independence relationships to a
rule base is to take advantage of the graphical structure of
the knowledge base, as Bayesians do, and add just what is
necessary. Part of the work is already done by the rational
monotony property, i.e. N(AIB) > 0 and N(-.CIB);;:;
imply

N(AIB/\C)

0 does

> 0. However more conditional

independence assertions are needed to overcome problems
such as blocking of property inheritance. The problem is
not

to

add

too

many

assertions

so

as

to

avoid

inconsistencies. Clearly we should stop imperatively once
a total ordering of worlds is obtained. On the other hand
the specification of conditional independence relation is
extremely flexible and would enable to have tailored
solutions to many inheritance problems. For instance if
we add a bird that has no legs (n) to the above knowledge
base, with rules saying that n 1--- -.1, and n fv b, we can

solve the problem by "reading on the graph" the proper
conditional independence assertions while most other
approaches would fail due to the presence of two conflicts.

However we cannot adapt the Bayesian methods readily for
several reasons: here nodes of the graph are literals (not
propositional variables), and cycles should be allowed (we
must be able to say that "students are young" but "young
people are not usually students"). Moreover there is no
result that allow us to aggregate (via the min operation) a

CONCLUSION

of independence in the framework of possibility theory
when conditioning is defined in an ordinal way (via the
min-operation). The case where conditional possibility is
defined as O(A"C);;:; il(CIA)·O(A) using product instead
of min has been left for further research. It is also worth
noticing that we have been working with events (or
formulas) and not with variables (see (Studeny

1993) for

an overview of the latter approach). It is well-known that

in the probabilistic framework, the independence of A and
B means, in terms of relative frequency, that the number
of cases where A is true over the number of cases where A
is false is left unchanged when B is known to be true. In
the view of independence presented here, it can

be checked

that an analog property holds in terms of orderings: The
possibilistic ordering between the interpretations with the
greatest possibility which make A true and those which
make A false is left unchanged when we restrict ourselves
to

interpretations

where

B

is

true.

Besides,

the

transparency of probabilistic conditioning with respect to
negation is closely related to the compositionality of
probabilities with respect to negation. Similarly, the
remarkable behavior of the possibilistic dependence and
independence with respect to disjunction or conjunction
stems from the fact

that

possibility measures are

compositional with respect to disjunction, and necessity
measures with respect to conjunctions.

REFERENCES
S. Benferhat, D. Dubois, H. Prade

(1992) Representing

default rules in possibilistic logic. Proc.

KR'92

(B. Nebel et al. eds.), Cambridge, MA, Morgan
Kaufmann, San Mateo,

&

673-684.

S. Benferhat, D. Dubois, H. Prade

(1994) Expressing

independence in a possibilistic framework and its
application to default reasoning. Proc. E CA/'94,

150-154.
R. Carnap

( 1950) Logical Foundations of Probability.

University of Chicago Press.
D. Dubois

(1986) Belief structures, possibility theory,

decomposable confidence measures on finite sets.
Computer and Artificial Intelligence,

5(5), 403-417.

D. Dubois, H. Prade (1986) Possibilistic inference under
matrix form. In:

F uzzy

Logic

in

Knowledge

Engine ering (H. Prade, C.V. Negoita, eds.), Verlag

TOV Rheinland, Ktiln, 112-126.
D. Dubois, H. Prade ( 1 988) Possibility Theory. Plenum
Press, New York.

An Ordinal View of Independence with Application to Plausible Reasoning

203

D. Dubois, H. Prade ( 1991) Epistemic entrenchment and
possibilistic logic. J. of AI, 50, 223-239.

H. Reichenbach ( 1949) Theory of Probability. University
of California Press, Berkeley.

D. Dubois, H. Prade ( 1 992) Belief change and possibility
theory. In: Belief Revision (P. Gardenfors, ed.),
Cambridge University Press, 142-182.

D. Scott (1964) Measurement structures and linear
inequalities. J. of Math. Psychology, I , 233-247.

L.

Farinas del Cerro, A. Herzig ( 1 991) A modal analysis
of possibility theory. LNCS, Vol. 535, Springer
Verlag, Berlin, 11- 1 8.

L.

Farinas del Cerro, A. Herzig (1993) Interference logic =
conditional logic + frame axiom. LNCS, VoL 747,
Springer Verlag, Berlin, 105- 1 12.

L . Farinas del Cerro, A. Herzig ( 1994a) Conditional
possibility and dependence. Proc. IPMU-94, Paris,
July 4-8.
L.

Farinas del Cerro, A. Herzig ( 1994b) Belief change and
dependence. Internal Report, IRIT, Feb. 1 994.

T. Fine (1973) Theories of Probability. Academic Press,
New York.
P. Fonck (1993) Reseaux d'inference pour le raisonnement
possibiliste. Dissertation, Univ. of Liege, Belgium.
P. Gardenfors (1978) On the logic of relevance. Synthese,
35 1-367.
P. Giirdenfors (1990) Belief revision and irrelevance. PSA,
2 , 349-356.

P. Gardenfors, D. Makinson ( 1 994) Nonmonotonic
inference based on expectation ordering. J. of AI, 65,
197-245.
M. Goldzsmidt, J. Pearl (1992) Rank-based systems: A
simple approach to belief revision, belief update, and
reasoning about evidence and actions. Proc. KR '92 (B.
Nebel et al. eds. ) , Cambridge, MA, Morgan &
Kaufmann, San Mateo, 661-672.
E.

Hisda1 (1978) Conditional possibilities, independence
and noninteraction. Fuzzy Sets & Syst. , 1, 283-297.

J. M. Keynes (1921) A Treatise on Probability.
MacMillan, London.
A. Kolmogorov ( 1956) Foundations of the Theory of
Probability. Chelsea, Bronx, New York.
D. Lehmann ( 19 89) What does a conditional knowledge
base entail? Proc. KR'89 (R.I. Brachman et al. eds.),
Morgan & Kaufmann, San Mateo, CA, 2 1 2-222.
D. Lewis ( 1 973) Counteifactuals. Blackwell, Oxford.
R. von Mises, H. Geiringer (1964) The Mathematical
Theory of Probability and Statistics. Academic Press,
New York, 7-43.
S. Nahmias ( l978) Fuzzy variables. Fuzzy Sets and
Systems, 1 (2), 97- 1 1 0.
J. Pearl ( 1 988) Probabilistic Reasoning in Intelligent
Systems Morgan & Kaufmann, San Mateo, CA.
J. Pearl ( 1990) System Z: A natural ordering of defaults
with tractable applications to default reasoning. Proc.
TARK-3 (M. Vardi, ed.), Morgan & Kaufmann, San
Mateo, CA, 12 1-135.

K. Segerberg (1971) Qualitative probability in a modal
setting. Proc. 2nd Scandinavian Logic Symp. (J.E.
Fenstad, ed.), North-Holland, Amsterdam.
M. Studeny (1993) Formal properties of conditional
independence in different calculi of AI. LNCS, Vol.
747, Springer Verlag, Berlin, 34 1 -348.
Y. Shoham (1988) Reasoning About Change. The MIT
Press, Cambridge, MA.
L.A. Zadeh (1978), Fuzzy sets as a basis for a theory of
possibility. Fuzzy Sets and Systems, 1, 3-28.

