244

On testing whether an Embedded Bayesian Network represents a
probability m o del

Dan Geiger•

Azaria Pazt

Judea Pearlt

Technion
CS Dept
Israel, 32000

Technion
CS Dept
Israel, 32000

UCLA
CS Dept
LA, CA, 90024

Abstract

Testing the validity of probabilistic models
containing unmeasured (hidden) variables is
shown to be a hard task. We show that
the task of testing whether models are struc­
turally incompatible with the data at hand,
requires an exponential number of indepen­
dence evaluations, each of the form: "X is
conditionally independent of Y, given Z . In
contrast, a linear number of such evaluations
is required to test a standard Bayesian net­
work (one per vertex). On the positive side,
we show that if a network with hidden vari­
ables G has a tree skeleton, checking whether
G represents a given probability model P
requires the polynomial number of such in­
dependence evaluations. Moreover, we pro­
vide an algorithm that efficiently constructs a
tree-structured Bayesian network (with hid­
den variables) that represents P if such a net­
work exists, and further recognizes when such
a network does not exist.
"

1

Introduction

Bayesian Networks possess several desirable properties
for representing uncertain knowledge, especially in sys­
tems that perform diagnosis and forecasting. One such
property is the ability to encode and update prob­
abilistic knowledge economically, using modular rep­
resentation and distributed computations. Another
property is the ability to represent causal knowledge
of the domain in a way that supports a wide vari­
ety of inferences, including prediction, abduction and
the control of actions. A third property, which is the
central topic of this paper, is the possibility of model
validation, namely, testing by objective measurements
•some of this work was done while the author visited
Microsoft Research Center. Email: dang@cs.technion.ac.il.
tsome of this work was done while the author visited

UCLA. Email: paz@cs.technion.ac.iL
�E-mail: judea@cs.ucla.edu

whether a Bayesian network, constructed by a domain
accurately represents the target domain. Much
of the basic work on Bayesian networks can be found
in [Pe88J, and more recent advances are summarized
in [Pe93a, Pe93b].

expert

The appeal of the Bayesian network model has
prompted researchers to suggest several useful exten­
sions one of which is Embedded Bayesian Networks (e­
BN) [PV91]. E-BNs enhance the language of Bayesian
networks by allowing bidirected edges in addition to
the directed edges permitted in Bayesian networks;
each bidirected edge represents a pair of variables that
are correlated but have no causal influence on each
other. Such symmetrical correlations normally em­
anate from causal factors which the analyst chooses to
exclude from formal analysis, either because they lie
beyond the scope of the domain, or because they are
inaccessible to direct measurement. In this paper, we
address the task of validating Embedded Bayesian net­
works subject to the constraint that all causal factors
responsible for the bidirected arcs are unobservable.
Model validation involves two subtasks; validating the
qualitative graph structure provided by the expert and
validating the numerical parameters associated with
the edges of the graph. Each parameterized graph
structure defines a joint probability distribution on the
observed variables which, in principle, can be tested for
compatibility with the observed data. A graph struc­
ture is said to be valid if it can be parameterized so
as to define a probability distribution compatible with
an observed distribution P. 1
When the structure is valid, then iterative learning
technique can be employed for tuning the parameters
so as to fit a given stream of empirical observations
[La91] However, if the structure itself is erroneous, no
parameter tuning can ever render the model compat­
ible with the data. We, therefore focus our attention
on the task of validating the structure of an embedded
Bayesian network .
.

1We assume that the entire distribution P is observed

directly, which is a good approximation when the sample
size is large.

Testing Embedded Bayesian Networks

Testing structural validity of an e�BN is much harder
than that of a Bayesian network. In BN, the values of
all parameters are uniquely determined from the ob­
served distribution P by mere projection and, so, we
can test for structural validity by checking whether the
parameterized structure defines a probability distribu­
tion compatible with the observed data. Alternatively,
a BN is known to be valid if and only if all the inde­
pendencies implied by the BN hold in the observed
distribution P and, moreover, this matching of inde­
pendencies can be verified by testing a linear number
of conditional independence statements (one per ver­
tex).
Things are different in e-BNs. First, the set of pa­
rameters defining an e-BN model cannot be obtained
by projection. Moreover, the domain of the hidden
variables may be unbounded or undefined, so one can­
not test model validity by first fitting parameters (say
by maximum-likelihood techniques) and then testing
the resulting model for agreement with P. Second,
matching independencies is a necessary but not suf­
ficient condition for e-BN validity. In other words,
even if all independencies implied by the structure of
a given e-BN hold in a distribution P, it is still possible
that the e-BN is incapable of generating P (see [Verma
and Pearl 1991) for a counterexample). Finally, while
matching independencies could serve as a potentially
quick way of screening invalid models, it is not clear
how one would test the validity of all independencies
in a given e-BN, because, in principle, the number of
(X, Y, Z) triplets to be tested is exponential with the
number of vertices .
This last problem is the central theme of our paper to
which we find a negative result. We prove that given
an embedded Bayesian network G, checking whether
G faithfully represents independencies of an empiri­
cal distribution P, requires a number of evaluations of
independence statements, each of the form: "X is con­
ditionally independent of Y, given that we observe Z",
which grows exponentially in the number of vertices.
This result is rather surprising since for Bayesian net­
works this task requires only a linear number of such
evaluations. Obviously, to verify even one indepen­
dence statement that contains n variables requires to
verify an exponential number of equalities, however,
when the number of parents of each vertex is a fixed
constant, then a linear number of statements can be
verified by a linear number of hypothesis tests one per
equality.
Notably, our negative result (Theorem 4) does not ex­
clude the possibility that given additional properties
of P which are not shared by all probability models, a
test of independence will be devised which requires
only a polynomial number of independence evalua­
tions. Consequently, it is advisable to consider proba­
bility distributions which are faithful [SV92] for which
a test of independence may still be feasible.
On the positive side, we show that if G has a tree

245

skeleton, checking whether G represents P requires
the evaluation of only polynomial number of indepen­
dence assertions. Moreover, we provide an algorithm
that efficiently constructs a tree-structured embedded
Bayesian network that represents P if such a network
exists, and further recognizes when such a network
does not exist.
2

Preliminaries

Let U be a finite set of variables {u1 ...Un } and let the
domain of each Ui be d( u; ) . A probability model over
U is a probability distribution of the form P : d( ul ) x
... x d(un) --+ [0, 1) where each d(u;), i = 1, .. . , n,
is a finite set. The class of probability models over
U is denoted by P. A probability model over U is
strictly-positive if every combination of U's values has
a probability greater than zero. The class of strictly­
positive probability models is denoted by p+. Note
that p+ � P.
An expression !(X, Z, Y) where X # 0, Y # 0, and
Z (possibly empty) are disjoint subsets of a finite set
U is called an independence statement. A set of inde­
pendence statements is called a dependency model. An
independence statement I(X, Z, Y) is said to hold for
a probability model P if for every value X, Y, and Z
of X, Y, and Z, respectively,

P(X, Y, Z) P(Z) = P(X, Z) P(Y, Z).
(1)
The set of statements that hold for P is denoted by
M(P) and is called the dependency model induced by
P.
·

·

When !(X, Z, Y) E M(P), then X and Y are condi­
tionally independent relative to P, and if in addition
Z = 0, then X and Y are marginally independent rel­
ative toP.
Eqs. (2) through (5) below are properties of condi­
tional independence that hold for every probability
model and Eq. (6) is a property that holds for every
strictly positive probability model. Variants of these
properties were first introduced by Dawid (1979) and
further studied by Spohn (1980), Pearl and Paz (1985),
Pearl (1988), and Geiger (1990).
Symmetry

(2)

I(X, Z, Y):::} I(Y, Z, X)
Decomposition
!(X, Z, Y U W)

�

!(X, Z, Y)

(3)

Contraction
I(X, Z, Y) & I(X, Z
Weak-union

U

Y, W) �I(X, Z, Y

I( X, Z, Y U W)

�

U

I( X, Z U W, Y)

W) (4)

(5)

Intersection
I(X, ZUW, Y )&I(X, ZUY, W)

=>

I(X, Z, YUW) (6)

246

Geiger, Paz, and Pearl

The interpretation of each of these properties is
straight forward. For example, Weak-union states that
if X, Y, Z, and W are sets of variables such that
I(X, Z, Y U W) is in M(P) for some probability model
P, then I( X, ZUW, Y) must also be in M(P). In oth er
words, according to Weak-union, if X and Y U W are
conditionally in dependent given Z is known, then X
and Y are conditionally independent given Z U W is
known.
Every d epen d ency model that satisfies Eqs . (2)
through (5) is called a graphoid. Hence, every prob­
ability model induces a graphoid. A Bayesian net­
work and the corresp ond ing d-separation criteria, de­
fined below, is another example of a dependency model
which is a graphoid. A graphoid defined by a Bayesian
network usually serves to represent graphoids induced
by probability models.

The primary advantage of a Bayesian network is that it
allows a wide spectrum of independence assumptions
to be conveniently considered by a model builder so
that a p ract ical balance can be established between
computational needs and adequacy of conclusions. We
now give a definition of a Bayesian network.
A dag (directed acyclic graph) is a directed graph with
no parallel edges and no directed cycles. A trail in a
dag is a subgraph whose underlying graph is a path
in which no vertex appears twice. A vertex b is called
a sink on a trail t if there exist two consecutive edges
a - b and b +-- c on t . A trail t is active by a set of
vertices Z if ( 1) every sink with respect tot either is in
Z or has a descendant in Z and (2) every other vertex
along t is outside Z. A trail is said to be blocked by
Z if it is not active by Z. An independence statement
I(X, Z, Y) holds in D if X, Y, and Z are disjoint sub­
sets of vertices, and every trail between an element in
X and an element in Y is blocked by Z. The set of all
ind ependence statements that hold in D are denoted
by M(D). The set M(D) is calle d the dependency
model induced by D. A dag D is an !-map of a p rob­
ability model over U if I( X, Z, Y ) E M(D) implies
I(X, Z, Y) E M(P). A dag D is a minimal !-map of
P if whenever an edge is removed from D, the result­
ing dag is not anI-map of P. A dag D = (U, E) is a
Bayesian network of a probability model P over U
if Dis a minimal I-map of P.

Lemma 1 ([Pe88]) A dependency model induced

by

a dag is a graphoid.

Bayesian networks as defined above would have re­
mained merely interesting mathematical objects un­
less an efficient procedure existed for testing whether
a given dag is an I-map of a probability model. Since
such a procedure does exist [ Pe88), it became possible
to construct a dag from causal knowledge and then test
whether the dag constructed actually reflects reality as
sensed by measured data.
A known example of a Bayesian network is given in
Figure 1. It is depicted using the assertions that a

Earthquake

Radio

0

0 Burglary

c{ v

Ala<m

Figure 1

Burglary (B) and an Earthquake (E) may each ac­
tivate an Alarm (A) and that Radio announcement
( R) may follow an earthquake. Examining the de­
pendency model induced by this dag we see, for ex­
ample, that Br = {I(B, 0, E),I(R, E, {A, B})} is a
subset of M(D). The first statement asserts that bur­
gl aries and earthquakes are independent. The second
statement asserts that once an earthquake is known
to happen or not to happen with certainty, whether
or not a radio announcement is broadcasted is in­
dependent of both the alarm system and a burglary
occurrence. Suppose P(B, E, A, R) is a probability
model describing the 16 possible outcomes, then one
can test whether I(B, 0, E) E M(P) and whether
I(R, E, {A, B})} � M(P). If it is not the case, ad­
ditional edges, such as an edge from E to B, could be
added to reflect reality more accurately.
Pearl and Verma [PV91] show that if Br � M(P),
then M(D) � M(P) and therefore D is an 1-map of
P. In fact , each dag defines a linear-sized set i
l ke Br,
called a recursive basis, such that Br � M(P) im­
plies M(D) � M(P). Thus, 1-mapness can be tested
efficiently. Furthermore, Pearl and Verma show that
M(D) is precisely the set of statements that can be
derived from a recursive basis by repeated application
of Eqs. (2) through (5). For example, by applying,
Decomposition, Symmetry, and Contraction one can
derive I(B,0,R) E M(D) from Br. It is worthy to
mention that the definition of active trails cannot be
enhanced because every statement entailed from a re­
cursive basis can be derived using Eqs. (2) through (5)
[GV P90].
3

Embedded Bayesian Nets

In real-life applications each edge in a Bayesian net­
work is usually directed from cause to effect. However,
two variables are often correlated without having a
causal influence of one on the other. Such a situation
arises when a latent variable, not modeled by the net­
work, is a common cause of the two observable vari­
ables. Embedded Bayesian networks, defined b elow,
encode such a situation with a bidirected edge , x +-+y.
Each such edge is equivalent to a trail x +- a - y in
a Bayesian network in which the variable a is latent
(unobservable). For example, two symptoms x andy
of a disease a can be modeled by the network x +--+ y.

Testing Embedded Bayesian Networks

An embedded directed graph is a graph with two types
of edges: directed edges, x-+ y, and bidirected edges,
A path a1, . . . , an is directed from a1 to an
x +4 y.
if each edge ( ai, ai+l ) is a directed edge from a; to
ai+l· A directed cycle is a directed path where a1 = an.
An E-dag is an embedded directed graph that has no
parallel edges and no directed cycles. A descendant
y of a vertex x is any vertex such that there exists
a directed path from x to y. A trail in a dag is a
subgraph whose underlying graph is a path in which
no vertex appears twice. A vertex b is called a sink
on a trail t if there exist two consecutive edges ( a, b)
and (b, c) on t such that none of these two edges is a
directed edge that points away from b. (If b is incident
with only one edge on t, then b is not a sink). A trail
t is active by a set of vertices Z if ( 1) every sink on t
either is in Z or has a descendant in Z and (2) every
other vertex along t is outside Z. A trail is said to be
blocked by Z if it is not active by Z.
An independence statement I(X, Z, Y) holds in an E­
dag G if X, Y, and Z are disjoint subsets of vertices,
and every trail between an element in X and an ele­
ment in Y is blocked by Z. The set of independence
statements that hold in an E-dag G are denoted by
M(G). The set M(G) is called the dependency model
induced by G. An E-dag G is an !-map of a prob­
ability model over U if l(X, Z, Y) E M(D) implies
I(X, Z, Y) E M(P). An E-dag G is a minimal !­
map of P if whenever an edge is removed from G,
the resulting E-dag is not an 1-map of P. An E-dag
G = (U, E) is an Embedded Bayesian network of a
probability model P over U if G is a minimal I-map of
P.
Similar to dags, we have:

Lemma 2 A dependency model induced by an E-dag
is a graphoid.
Proof. Let D be an E-dag and let D1 be a dag formed
from D by replacing each bidirected edge x +4 y with a
new vertex a and two directed edges a -+ x and a -+ y.
It is easy to verify that whenever X, Y and Z are
disjoint sets of vertices of D, then I(X, Z, Y) E M(D)
if and only if I(X, Z, Y) E D'. The lemma now follows
directly from Lemma 1. 0
In order to check whether G is an I-map of P one could
naively test that each element of M(G) is in M(P). We
call each such test a membership test. Analogously to
what we have claimed in the previous section for dags,
one can check whether an E-dag G is an I-map of P
using less than IM(G)I membership tests. The rest of
this section shows how.
A set of statements 1: entails a set of statements r
if for every probability model P, E � M(P) implies
f � M(P). The set 1: positively entails r if for ev­
ery strictly-positive probability model P, 1: � M(P)
implies r � M(P). A set B � M(G) is called a proba­
bilistic basis of an E-dag G if B entails M(G). The set
B is a minimum probabilistic basis if there exists no

247

other probabilistic basis B' of G satisfying IB'I < IBI.

Lemma 3 Let B be a basis of an E-dag G and let P
be a probability model. If B � M(P), then M(G) �
M(P).
Proof. The proof follows from the definition of entail­
ment where I: = B and f = M(G). D
Lemma 4 Let B be a minimum basis of an E-dag
G and let P be a probability model. The number of
membership tests needed in order to determine whether
M(G) � M(P) is equal to IBI .
Proof. Let k be the smallest number of membership
tests needed in order to determine whether M(G) �
M(P). First we show that k ::; IB\. If B � M(P),
then M(G) � M(P) because B is a probabilistic basis
of G (Lemma 3). Testing whether B � M(P) requires
at most IBI membership tests-one for each statement
in B. Thus k ::; IBI. Next we show that k � \BI which
implies the claim.
Let R � M(G) be a smallest set of independence
statements upon which the assertion M(G) � M(P)
is made. By the definition of k, IRI = k. Further­
more, R is a probabilistic basis of G because otherwise
M(G) � M(P) cannot have been asserted. Since IBI
is a minimum basis, k 2. IBI. D
We note that a recursive basis of a dag is in fact a
probabilistic basis of at most n statements-one per
vertex in G. Hence, according to Lemma 4, testing
whether a dag is an 1-map of a probability model P
requires only n membership tests. This property of
having a poly-sized basis, as we show in the next sec­
tion, does not extend to E-dags, and thus, the number
of membership tests needed in order to check whether
an E-dag G is an I-map of some probability model P
may grow exponentially in the number of vertices of
G.
This result is rather discouraging because it says that
if the number of variables is large enough and some
variables are unobservable, then one cannot verify in
reasonable time that a given causal description is cor­
rect by simply observing the world. In Section 5 we
show that some E-dags can be verified using a polyno­
mial number of membership tests.
To prove the non-existence of a poly-sized basis in
some E-dags we need the following completeness the­
orem concerning independence statements of the form
I(X, 0, Y) which we call marginal statements.

Theorem 1 ([GPP91]) Let B be a set of marginal
statements and let B* be the set of all marginal state­
ments that B entails. Then u E B* if and only if u
can be derived from B by repeated applications of the
following properties:
M-symmetry
I(X, 0, Y)

:::?

J(Y, 0, X)

(7)

248

Geiger, Paz, and Pearl

M-decomposition

I(X, 0, Y U W)::} I(X, 0,Y)

(8)

M-mixing
I(X, 0,Y) & !(XUY, 0, Z)::} !(X, 0,Y

u

Z)

(9)

This theorem states that the above three properties,
which are satisfied by any graphoid (M-Mixing is de­
rived from Weak-union and Contraction), are sound
and complete for the set of probability models P; Ev­
ery marginal statement that is entailed can be derived
and vice versa. Theorem 1 remains correct when B*
is redefined to be the set of statements positively en­
tailed by B. Thus, strictly-positive probability models
do not share any property for marginal statements that
is not already shared by all probability models.

Verifying Embedded Bayesian Nets

4

We now construct a sequence of E-dags, G�:, k 2: 1,
such that each Gk has 2k + 2 vertices and the cardi­
nality of any of G k 's probabilistic bases is larger or
equal to 21:. Consequently, due to Lemma 4, testing
whether Gk is an I-map of a given probability model

P

requires a number of membership tests that grows
exponentially in the number of vertices.

We define G k as follows. The vertices of G k are CUD
where C == {c0,c1,...,ck} and D
{d0,d1 ... ,dk}.
All edges in Gk are bidirected. All vertices in C are
connected with a bidirected edge to each other and
all vertices in D are connected with a bidirected edge
to each other. That is, C and D are cliques. For
=

i

=

1 ..

.k

(but not for i=O), c; and d; are connected

with a bidirected edge.

Theorem 2 If I(X, Z, Y) E M(Gk), then there exists
a partition Z', zu of Z, such that I(XUZ', 0, YUZ") E
M(Gk)·
Suppose <r = I(X, Z,Y) E M(Gk)· The state­
ment <r has the form I(X, C' U D',Y) where C' � C,
and D' �D. It cannot be the case that both XnC # 0
andY n C # 0 because any two vertices inC are con­
nected with an edge. Similarly, it cannot be the case
that both X n D # 0 andY n D =/; 0. Suppose now,
with out loss of generality, that X n D = 0. Hence,
since X is not the empty set, X n C # 0, and there­
fore, Y n C = 0. So <r has the form I(C", C' UD', D")
where C11 C C and D' C D. The set C' U D' con­
tains no p�r (ci, d;), i j. 0, because otherwise each
element x E C11 would be connected to each element
y E D11 via the active trail x <---> c; ....... d; <---> y.
Similarly, the set C" U D' contains no pair (c;, d;),
i # 0, because otherwise c; E C" would be con­
nected to each element y E D" via the active trail
c; <-+ d; <---> y.
Thus, I({c},0,{d}) E M(Gk) for
every c E C' U C" and for every d E D'. Hence,
I(C' U C", 0, D') E M(Gk)· Symmetric arguments im­
ply I(D' U D11, 0, C') E M(G�<)· The last two state-

Proof.

ments together with I(C", C' U D', D") E M(Gk) de­
rive I(C' UC",0, D' U D" ) E M(Gk), using Symmetry,
Weak-union and Contraction, and so, {C',D'} is the
desired partition of Z. 0

Theorem 3 Every probabilistic basis B of Gk satis­
fies IBI 2: 21:.
Proof. First we show that it is sufficient to prove this
theorem for probabilistic bases that consist solely of
marginal statements.
Suppose B is a probabilistic basis of Gk and let 0' =
I(X, Z,Y) be a statement in B. According to Theo­
rem 2, there exists a partition Z', Z" of Z, such that

= I(X U Z',0,Y U Z") is in M(G�:).
Note that
entails <r due to Symmetry and Weak-union. Let
Bm be a set of marginal statements obtained by re­
placing each statement <r in B with a statement <Tm

O'm
O'm

of the form just defined. It follows that Bm entails B
and that IBm I= IBI. Consequently, Bm is a basis that
has the same size of B. Hence, if every probabilistic
basis consisting solely of marginal statements satisfy
IB I > k , then every probabilistic basis satisfies this
inequality as well.

Next we define a set T of marginal statements of size
that consists
solely of marginal statements must have a size larger
than ITI. Recall that the vertices in G�: are C U D.
Let T be the set of all marginal statements having the
form I( {co} U C', 0, {do} U D') where C' � C \{co},
D' � D \ {d0}, and C' U D' contains precisely one
vertex of each pair { Ci, di}, i = 1 .. . k. Note that the
cardinality of T is indeed 2�< and that T � M(Gk)

2k and show that every probabilistic basis

because each trail between a vertex in {co} U C' and a
vertex in {d0}U D' contains a vertex that is a sink on
that trail.
Let B be a probabilistic basis of G k that consists solely
of marginal statements. Since T � M(G�:) and B is
a basis, it follows that B entails T. Due to the com­
pleteness theorem 1, B entails T if and only iffor each
0' E T, there exists a derivation of <r from B using M­

symmetry, M-decomposition and M-mixing. Let 0' be
any statement in T such that 0' has a derivation chain
from B of length l. Define the symmetric image of
a statement I(X, 0,Y) to be the statement I(Y, 0, X).
We now prove by induction on l that either <r E B

or the symmetric image sym(<r) of <r is in B. Conse­
quently, IBI 2: ITI 2: 2" which proves the Theorem.

The case 1
0 is trivial; a derivation chain of length
zero means that <r E B. Assume the inductive claim
holds for l :::; s. Let <r E T be a statement whose short­
est derivation chain has a length of s + 1. At the last
derivation step either M-symmetry, M-decomposition
or M-mixing where used. We consider each case sepa­
rately. If <r is derived from 0'1 by Symmetry, then <r1 is
the symmetric image of <r. By the induction hypoth­
esis, either 0'1 or sym( <r') is in B. Hence, either <r or
sym(<r) is in B as well. Next suppose <r is derived by
decomposition from <r1• If 0'1 has the same number of
=

Testing Embedded Bayesian Networks

vertices as (1, then (1 and (11 are the same statements
and therefore by the induction hypothesis, either (11 or
sym((11) is in B. Hence, either (1 or sym((1) is in B as
well. However, the number of vertices in (11 cannot be
strictly greater than that in (1; Assume it were greater.
Then (11 is not in M(G) because the addition of any
vertex to a statement in T creates an active trail since
for some i, c; would belong to C1 and d; would belong
to D1• But (11 is derived from a probabilistic basis
of Gk using properties that hold in every E-dag, and
therefore, (11 must be in M(G). Contradiction.
F inally, suppose (1 = I({co}UC1,0,{d0} U D1) is
derived using Mixing from two previous statements
in the derivation chain 11 = I(U, 0, V) and 12 =
I(U U V, 0, W). Both 1'1 and 1'2 must be in M(G�c)
because they were derived from a probabilistic basis
of G�c using properties that hold in every E-dag. If M­
mixing is applied, the resulting statement has the form
I(U, 0, V U W) where V U W equals {do} U D'. Thus ,
either V or W must be empty or else I(U U V, 0, W) f/:.
M(G�c). Consequently, either /l = (1 or r2 = (1. Thus,
by the induction hypothesis, (1 or sym((1) is in B. 0

We

have thus derived the

main claim of this section.

Theorem 4 Testing whether an E-dag G is an !-map
of a probability model P requires, in the worst case, a
number of membership tests that grows exponentially
in the number of vertices of G.
Co nsid er the E-dags Gk. k � 1. Accord ing
to Theorem 3, each basis B of G�c , in particular a
minimum basis, satisfi es IBI ;::: 2" where 2k + 2 is the
number of ver t ices in G�c. Lemma4 shows that IBI is
the required number of membership tests, thus proving
the claim. 0

Proof

5

Verifying Embedded Bayesian Trees

An E-tree is an E-dag whose underlying gra p h is a tree.
In this section we show that each E-tree has a proba­
bilistic basis of a polynomial size and that such a basis
can be easily found. Hence according to Lemma 4
testing whether an E-tree is an I-map of a probabil­
ity m o del P can be done using polynomial number of
membership tests.
Let T = ( U, E) be an arbitrary E-tree. Let x be a
vertex in T. Let s1,
, s1 be the set of vertices such
that there exists a directed edge from each s; to x.
Let q1, ... , qk be all o ther vertices in T which are con­
nected to x with an edge. Let S; be the set of vertices
such that for each y E 5; the single trail connecting y
with x passes through s;. Similarly, let Q; be the set
of vertices such that for each y E Q; the single trail
connecting y with x passes through q;.

249

not in Q;. Let Br be the union over all vertices x in T
of the set { (1f, 1j I i = 1 ...1, j = 1 ...k}. Clearly the
size of Br is no greater than n2 where n is the number
of vertices in T because l + k, the number of edges
adjacent with x, is less or equal to n. Furthermore,

Br t;;;; M(T).

The rest of this section shows that Br is a probabilistic
basis ofT. Consequently, in order to test whether Tis
an I-map of some probability model Pone must merely
test that Br t;;;; M(P) using at most n2 membership
tests.
A statement I(X, Y, Z) is simple if X and Y are sin­
gletons.
The set of independence statements that
can be derived from a statement (1 using Weak-union
and Decomposition is denoted by A((1). (Note that
(1 E A( (1) ) . The set of all simple statements in A( (1) is
denoted by A. ((1).

Lemma 5 Let (1 be an independence statement and
P be a probability model. If A,((1) t;;;; M(P), then (1 E
M(P).

Proof. Let (1 = I(X, Z, Y) and let (11 = I(X1, Z1, Y')
be an element of A((1). We prove by induction on the
size of X1UY1 that if A,((1) t;;;; M(P), then (11 E M(P).
Since (1 is in A( (1), it follows from this induction that
u E M(P). Indu ction basis: If IX' U Y'I = 2, then (11 is
in A,((1) and hence in M(P) as well. Induction step:
Assume that all (11 in A.((!') with IX' U Y11 ::=; k are in
M(P) and let (111 I(X", Z",Y") be a statement in
A,(e1) withY"=Y'U{a}, a is a singleton and IX" U
Y(ll= k+ 1. Consider the statements O"r = I(X", Z"U
{a},Y1) and (1� = I(X", Z", {a}). Both (1�1 and (1� can
=

be derived by Weak-union and Decomposition from (111
which is either equal to (1 or can be derived from (1 by
Weak-union and Decomposition. Thus (1� and (1� can
be derived from (1 by Weak-union and Decomposition
and are therefore in A( (1). But I X" U {a} I ::=; k and
IX" U Y11 ::=; k so that, by the induction hypothesis,
both (1�1 and (1� are in M(P). Finally(!'" can be derived
from (1� and (1� by Contraction and M(P) is closed
under Contraction thus implying that (111 is in M(P).
A si mi l ar argument proves the symmetric case when
X"= X' U {a} and IX"UY"I= k + 1. D
Lemma 6 Let

D be an E-DA G and let P be a proba­
bility model. If all the simple statements in M(D) are
in M(P) then M(D) � M(P).

. . •

The set of all vertices in T is denoted by U. For each
vertex x in T and for each si let (1f = I(Si, {x}, U\5; \
{x}). For each q£, let 1f
I(Q; , 0, Ri) where Rf is
set of all vertices that are not descendants of x and are
=

Proof. If (1 E M(D), then A,((1) E M(D) be­
cause, according to Lemma 2, M(D) is closed un­
der Decomposition and Weak-union. If all the simple
statements in M(D) are in M(P), then in particular
A,(a) E M(P). Hence, by Lemma 5, (1 E M(P).
Thus, M( D) � M(P). D
The next theorem
basis ofT.

shows

that Br is a

pr ob abilistic

Theorem 5 Let T be an E-tree and P be a probability
model. Then, Br t;;;; M(P) implies M(T) � M(P).

250

Geiger, Paz, and Pearl

Proof. Let u = I({a},Z, {b}) be an arbitrary simple
statement in M(T). We will show that if Br <;;; M(P),
then I({a},Z,{b}) E M(P). Consequently, due to
Lemma 6, M(T) <;;; M(P) which is what we need to
show. Since T is a tree, there is a unique trail t in T
connecting a with b. Since this trail is blocked by Z,
there are two cases to consider. Either (1) some sink x
on t and all x's descendants are not in Z, or (2) some
vertex z that is not a sink on t is in Z.
If the first case occurs then consider the independence
statement u' = I(Q;, 0, Rf) E BT where a E Q; and
b E Rf . Such a statement exists in BT according to
BT 's definition. Furthermore, Rf does not contain x
or any of x's descendants and nor does Q;. Thus, Z �
Q;URf. The statement u is therefore derivable from (!'1
by Symmetry, Decomposition, and Weak-union which
hold for M(P). Hence, if BT <;;; M(T), then (!' is in
M(P).
If the second case occurs then consider the indepen­
dence statement u1 = I(S;, { x}, U \ S; \ { x}) which by
definition is in BT where a E S; and b E U \ S; \ {x}.
Recall that all vertices of T appear in (!'1• Thus, if
BT <;;; M(P), then (!' E M(P) since (!' can be derived
from (!'1 by Symmetry, Decomposition and Weak-union
which hold in M(P). D
It is worthy to note that one can define for each E-tree
another polynomial basis B, that has the same num­
ber of independence statements as BT but some state­
ments include less vertices. B, is defined as follows.
For each x in T, let B,(x) be the set {I(S;, {x}, U \
S, \ { x}) I i = 1, . . , /} (as in BT), and let B� ( x) be the
set {I(Q;,{x},U#;Qj) l i = l, . . . ,k}. Let B, be the
union over all vertices x ofT of the set B, (x) U B� ( x ) .
The set B. is a probabilistic basis of T because B,
entails BT using Eqs. (7) through (9) . In fact, ev­
ery marginal statement in M(T) can be derived from
B, using these three properties. The proofs of these
claims are omitted.
.

6

Learning Embedded Bayesian Trees

In section 5 we have analyzed the task of testing
whether a given E-tree is anI-map of a given proba­
bility model P. Now we are concerned with the much
more complicated task of synthesizing an E-tree that
represents a given probability model, if such an E­
tree exists, and recognizing when one does not exist.
To facilitate our investigation we make two assump­
tions. F irst we consider only strictly positive prob­
ability models. This assumption is justified whenever
categorical relationships can b e excluded from the rep­
resentation (as often happens, for example, in medical
domains). Second, we only search for E-trees that rep­
resent P well, as defined below.
A trail tis called a trek if no vertex oft is a sink on t.
An E-dag D that is a minimalI-map of a probability
model P is said to represent P well if whenever two

vertices a and b are connected with a trek in D, then
a and bare marginally dependent, i.e., I({a},0, {b})
does not hold in P. Equivalently, we will say that P
is well-represented by D.
The assumption of well-representation is quite natural
because one expects that changes in a variable a on
one side of a trek will reflect through the trek towards
b on the other end of the trek, thus making the two
variables dependent.
The algorithm below determines whether a given
strictly positive probability model P can be well­
represented by an E-tree and it finds such an E-tree
if one exists. This result generalizes our claims in
[GPP90, GPP93) in the sense that we now deal with
embedded Bayesian networks instead of just Bayesian
networks. There are examples in which the algorithm
below recovers an E-tree 1-map while our previous al­
gorithm fail to recover a treeI-map because none ex­
ists. (For example, a chain of three bidirected edges).
The Recovery Algorithm
Input: A strictly-positive probability model P over U
Output: An E-tree that represents P well if such ex­
ists, or acknowledgment that no such network exists.
1. Start with a complete undirected graph having U
as its vertex set.
2. Remove every edge a- b for which I({a},U \
{a, b}, {b}) holds in P.
3. Remove every edge a - b for which I({a}, 0, { b})
holds in P.
4. Let Ra be the resulting graph. If Ra is not a tree,
then "FAIL".
5. Orient every pair of edges (a, b) and (b, c) towards
b whenever I( {a}, 0, { c}) holds in P. (Note that
each edge can be oriented twice-once to each di­
rection).
6. Orient the remaining edges without introducing
new sinks on any trail.
7. If the resulting E-tree does not represent P well
then "FAIL". Otherwise, output the resulting net­
work.

Step 7 is done using polynomial number of indepen­
dence statements as shown in Section 5.
The following claims establish the correctness of the
algorithm. First we define a skeleton of an E-tree T to
be the underlying undirected graph of T.
Theorem 6 Let P be a strictly-positive probability
model. If P can be well-represented by an E-tree T,
then the skeleton ofT is equal to Ra-the graph con­
structed m step 3.
Theorem 6 shows that step 3 of the algorithm identifies
the skeleton of an E-tree that represents Pwell, if such

Testing Embedded Bayesian Networks

exists. Thus, if P can be well-represented by an E­
tree, then it must be one of the orientations of the
u ndirected graph Ra produced by step 3. Hence by
checking all possible orientations of this graph, one can
decide whether a strictly-positive model can be well­
represented by an E-tree. Consequently, all E-trees
t h at represent a strictly positive model P must have
the same skeleton .
The next theorem justifies an efficient way of estab­
lishing some orientations of the skeleton of Ra .

Theorem 7 Let P be a strictly-positive probability
m odel. If T is an E-tree that represents P well, and
a - b - c is a chain in the skeleton of T, then b is a
sink on a - b - c if and only if I( {a}, 0, { c}) holds in
P.

Proof. If b is a sink on a - b - c, then I ( { a } , 0, { c}) is
in M (T) and is therefore i n M ( P ) . Otherwise a and
c are connected by a trek in T which implies, by the
well-representation assumption, that I( { a } , 0, { c}) is
not in M ( P ) . 0
Step

6

leaves us freedom to choose the orientation of

some edges in the skeleton . For example, the E-trees:
a ....-. b --+ c, a <-- b ....-. c, and a <---- b --+ c are three
possible orientations of a - b - c. However, these three
E-trees are indistinguishable (isomorphic) in the sense
that they induce the same dependency models. Hence
no algorithm that relies on independence statements
can distinguish between them. On the other hand,
the E-trees a ...... b <---- c and a ....... b +-+ c, are distin­
guishable from the previous three E-trees because both
portray a new independence assertion , I({a} , 0, {c}),
w hich is not represented i n either o f the former three
E-trees. Our algorithm uses this distinction to orient
these edges.
Two Embedded Bayesian networks D1 and D2 are iso­
morphic if M(Dl ) = M ( D2 ) . Isomorphism defines
the theoretical limitation on the ability to identify di­
rectionality of edges using information about indepen­
dence.

Theorem 8 Two E-trees T1 and T2 are isomorphic
iff they share the same skeleton and each of their cor­
responding trails have the same sinks on them.
If T1 and T2 share the same skeleton
and have the same sinks on their corresponding trails
then every active trail in T1 is an active trail in T2 and
vice versa. Thus, M(Tt ) and M(T2 ) , the dependency
m odels corresponding to T1 and T2 respectively, are
equal .

Sufficiency:

Necessity: T1 and T2 must have the same set of ver­
tices U , for otherwise their dependency models are not
equal. If a --+ b is an edge in T1 and not in T2 , then the
statement /({a} , U \ {a, b } , {b}) is in M ( Tl ) but not in
M (T2 ) . Thus, if M(Tt ) and M ( T2 ) are equal, then T1
and T2 must have the same skeleton. Assume T1 and

251

T2 have the same skeleton and that

a - c - b is a trail in
these trees but that c is a sink on that trail in T1 while
not being a sink on that trail in T2 . The trail a - c - b
is the only trail connecting a and b in T2 because T2 is
an E-tree and it has the same skeleton as T1 . Since c is
not a sink on this trail in T2 , I( {a}, { c}, {b}) E M(T2).
However, I( {a}, { c} , { b}) � M (Tt ) because the tr ai l
a -> c <-- b is active by {c} . Thus, if M(T1 ) and
M ( T2 ) are equal, then T1 and T2 must h ave the same
sinks on the each of the corresponding trails. 0

Theorem 8 shows that all orientations of step 6 that do
not introduce a new sink on any trail yield isomorphic
E-trees because these E-trees satisfy the requirement
of the theorem. Thus, in order to decide whether or
not P can be well-represented by an E-tree it is suf­
ficient to examine one E-tree produced by step 6 , as
performed by step 7 , because all other E-trees are iso­
morphic.
Note that it is not only sufficient but actually nec­
essary to examine one E-tree produced by step 6 for
1-mapness because there are cases where no orientation
of Ra yields an I-map, as shown by the following exam­
ple. Let M(P) ;:::; { I({x1 } , 0, { x2}), /({xt } , 0, { xa} ) +
symmetric images} . Then the undirected graph Rc
consists of three vertices { x 1 , x2, x3} and one undi­
rected edge ( x 2 , x3) but none of its three orientations
yields an I-map of M ( P ) . Furthermore, note that The­
orem 8 cannot be extended from E-trees to E-dags (al­
though it does extend to every dag) because there are
examples where two E-dags which have different skele­
tons are both minimal 1-maps of the same dependency
model M ( P ) .

7

D iscussion

This paper exposes some of the basic difficulties in
testing the validity of probabilistic models containing
unmea.�ured (hidden) variables. On one h and, such
models allow greater freedom in p ar ameteri zi ng the
hidden links, requiring only that the chosen param­
eters be compatible with the observed relationships.
On the other hand , and this is where the results de­
rived in this paper fi t , there is no simple way of testing
whether a compatible parameterization of the hidden
links exists. Even an attempt to rule out models that
display structural incompatibility turns out to be hard
when hidden variables are included.
Our results should also be viewed in the context of
recent works on causal discovery, namely, the discov­
ery of causal graphs structures containing hidden vari­
ables, which m atch independencies found in empiri­
cal data [VP91, SV92] . The b asic assumption behind
these works is that of structural stability [VP91], also
called " faithfulness" [SV92] , which amounts to con­
sidering all independencies as produced by the graph
topology, rather than by accidental matching of nu­
merical parameters. Our negative results no longer
apply when the data is generated by such structurally

252

Geiger, Paz, and Pearl

stable process, and it might still be possible (though
unlikely) that with such assurance, one can verify
model validity (or at least 1-mapness) by t esting a
polynomial number of independence claims. However,
real-life data tends not to exhibit structural stabil­
ity because its source may be from several recursive
processes and may involve aggregated variables. Un­
der such conditions, stru ct ural stability cannot b e as­
sumed and the results established in this paper, both
negative and positive, provide theoretical limits on the
complexity of model validation in such cases.

References
[Ge90] Geiger, D. 1990. Graphoids: A Qualitative
Framework for Probabilistic Inference. PhD thesis,
UCLA Computer Scien ce Department. Also appears
as a Technical Report (R-142) Cognitive Systems
Laboratory, CS, UCLA.
[GPP90] Geiger, D . ; Paz, A.; and Pearl , J. 1990.
Learning causal trees from dependence information.
In AAAI, pp. 770-776, Boston, M assach u setts .
[GPP9 1 ] Geiger, D . ; Paz , A . ; and Pearl, J . 1991. Ax­
ioms and algorithms for inferences involving prob­
abilistic independence. Information a n d Computa­
tion, 9 1 , pp. 128-14 1 .
[GPP93] Geiger, D . ; Paz, A.; and Pearl , J . 1993.
Learning si mple causal structures. International
Journal of intelligent Systems, Vol. 8, 23 1-24 7.
[GP88] Geiger D . ; and Pearl J. 1988. On the logic of
causal models. In Uncertainty in Artificial Intelli­
gence 4, Shachte r R. D. ; Levitt T.S . ; Kana) L . N . ;
and Lemmer J . F. (Editors). Elsevier Science Pub­
lishers B.V. ( N ort h-H o lland) , pp . 3-12. 1990.
[GVP90] Geiger, D . ; Verma, T.S.; and Pearl , J. 1990.
Identifying independence in Bayesian networks. Net­
works, 20, pp. 507-534.
[SGS93] Spirtes, P. ; Glymour , C.; Scheines, R. 1993.
Causation, prediction, and search. Springer Verl ag ,
New York , 199 3.
[La91] S. L. Lauritzen . 1991. The EM algorithm for
graphical association models with missing data.
Technical Report R 9 1 -05 , Institute for Electronic
Systems, Aaalborg University. To appear in Com­
putational Statistics a nd Data Analysis.
[LS88] Lauritzen, S. L . , an d Spiegelhalter, D. J. 1988 .
Local computations with probabilities on grap hi cal
structures an d their application to expert systems.
J. R. Statst. Soc. B, 50(2) , pp. 157-224.
[Pe93a] J. Pearl. 1993. Belief N etworks Revisited, Ar­
tificial Intelligence, 59, Elsevi er , 49-56.
[Pe93b] J . Pearl. 1993. Comment: Graphical Mod­
els, Causality, and Intervention, Statistical Science,
8(2), 266-269 .
[Pe93c] J . Pearl. 1993. l,From Conditional O ught s to
Qualitative Decision Theory, in D . Heckerman and

A. Mamdani (Eds.), Proceedings of the Ninth Con­
ference on Uncertainty in Artificial Intelligence,
Washington , D.C. , 12-20.
[Pe88] Pearl , J. 1988. Probabilistic Reasoning in Intel­
ligent Systems. Morgan-Kaufman, San Mateo.
[PP89] Pearl, J . ; and Paz , A. 1989. Graphoids: A
graph-based logic for reasoning about relevance rela­
tions. In B. Du Boulay et a!. ( eds . ) , A dvances in Ar­
tificial Intelligence-II, pp. 357-363. North Holland,
Ams terd am.
[P V 9 1] Pearl, J . , Verma, T. 1991. A theory of inferred
causation, in Al len J . A . , Fikes R., and E. Sande­
wal l ( Editors), Principles of knowledge representa­
tion and reasoning: Proceedings of the 2nd interna­
tional Conference, Morgan Kaufmann, San Mateo,
CA, pp. 441-452.
[Si54] Sim on, H . 1954. Spurious cor rel ati ons : A causal
interpretation. Journal American Statistical Associ­
ation, 49:469-492.
[SV92] P. Spirtes and T. Verma. 1992. Equivalence of
causal models with latent variables. Technical report
CMU-PHIL-33, Carnegie Mellon university, Philos­
ophy departm ent .
[Sp80] Spohn, W . 1980. Stochastic independence,
causal independence, and shieldability. Journal of
philosophical logic, 9 , pp. 73-99.
[Su70] Suppes, P. 1 9 70 . A Probabilistic Theory of Cau­
sation. North H ol l and , Amsterdam.
[V93] T. Verma. 1993. Graphical aspects of causal
models PhD thesis, UCLA Computer Science De­
partment . In preparat ion .
[VP9 1] T . Verma, and J . Pearl. 1991. Equivalence and
Sy nthes i s of Causal Models. in Uncert ainty in Artifi­
cial Intellzgence 6, Cambridge, MA, Elsevier Science
Publishers, 220-227.
[VP88] Verma, T. S . ; and Pearl J. 1988. Causal
networks: Semantics and expressiveness. In Uncer­
tainty in Artificial Intelligence 4, Shachter R. D . ;
Levitt T.S.; Kana! L.N.; and Lemmer J .F. (Editors) .
Elsevier Science Publishers B.V. (North-Holland),
pp. 69-76 . 19 9 0.

