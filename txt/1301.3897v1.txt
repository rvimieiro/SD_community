UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

580

A Branch-and-Bound Algorithm for MDL Learning Bayesian
Networks

Jin Tian

Cognitive Systems Laboratory
Computer Science Department
University of California, Los Angeles, CA 90024
jtian@cs.ucla. edu

1

Abstract

complexity, some algorithms require as input a strict
ordering of variables.

This paper extends the work in [Suzuki, 1996]
and presents an efficient depth-first branch­
and-bound algorithm for learning Bayesian
network structures, based on the minimum
description length (MDL) principle, for a
given (consistent) variable ordering. The
algorithm exhaustively searches through all
network structures and guarantees to find the
network with the best MDL score. Prelimi­
nary experiments show that the algorithm is
efficient, and that the time complexity grows
slowly with the sample size. The algorithm is
useful for empirically studying both the per­
formance of suboptimal heuristic search algo­
rithms and the adequacy of the MDL princi­
ple in learning Bayesian networks.

Assuming a consistent variable ordering is given,
[Suzuki, 1996] developed a branch-and-bound algo­
rithm using MDL scoring function, which exhaustively
searches through all network structures and guaran­
tees to find the best scored network. In this paper,
we extend Suzuki's work and present a more efficient
branch-and-bound algorithm making use of the spe­
cial properties of the MDL scoring function. We find
bounds for the MDL scores of complex network struc­
tures using inequalities from information theory. A
greedy search is applied before the branch-and-bound
procedure to speed up the pruning process. We show
that the MDL scoring function will not select networks
with parent sets containing more than llog ,;tN j vari­
ables where N is the sample size. Preliminary test re­
sults show that the proposed algorithm is efficient, and
that the time complexity grows slowly with respect to
the sample size. The algorithm is useful for studying
the performance of suboptimal heuristic search algo­
rithms and it paves the way for empirically investigat­
ing the MDL principle in learning Bayesian networks.

Introduction

Learning the structures of Bayesian networks from
data has become an active research area in recent years
[Heckerman, 1995, Buntine, 1996]. One approach to
this problem is to turn it into an optimization exer­
cise. A scoring function is introduced that evaluates a
network with respect to the training data and outputs
a number that reflects how well the network scores
relative to the available data. We then search through
possible network structures for the best scored network
and take it as the network learned from the data.
Different scoring functions have been applied
such as Bayesian [Cooper and Herskovits, 1992,
Heckerman et al., 1995] and minimum
description
length
(MDL)
[Bouckaert, 1994a,
Lam and Bacchus, 1994, Suzuki, 1996] scoring func­
tions, and various search algorithms have been devel­
oped. Since, in general, the search problem is NP­
hard [Chickering, 1996], most algorithms use heuristic
search methods. Additionally, to reduce the search

The paper is organized as follows. Section 2 briefly
describes the procedure of learning Bayesian networks
from data using the MDL principle. Section 3 reviews
some related previous work. Section 4 formally defines
the search space of our problem. Section 5 presents
in detail our depth-first branch-and-bound algorithm.
Section 6 analyzes the time complexity of our algo­
rithm. Section 7 gives the test results of applying the
algorithm to several databases. Section 8 concludes
with discussions of future research.
2

The Learning Problem

A Bayesian network is a directed acyclic graph G that
encodes a joint probability distribution over a set of
random variables U
{X1, ... , Xn}. Each node of
the graph G represents a variable in U. If there is a

=

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

directed edge from node X; to XJ, X; -7 Xj, we say
that X; is a parent of Xi. The graph encodes that each
variable is independent of its non-descendants given
its parents in the graph. In this paper we only con­
sider discrete random variables and we assume that
each variable X; may take on values from a finite set,
{ x;1, . . . , Xir;}. A network is quantified by a set of
conditional probability tables, P(x;ipa;), one table for
each node-parents family, where we use PA; to denote
the parent set of X;, x; and pa; denote an instantia­
tion of values of X; and PA;, and the probability table
enumerates all possible instantiations. A Bayesian net­
work specifies a unique joint probability distribution
over the set of random variables U as [Pearl, 1988]:
n

P(Xt, . ..,Xn)

= II P(X;IPA;).
i=l

=

Assume we are given a training data set, D
{ u1, u2, ... , uN}, where each ui is a particular instan­
tiation over the set of variables U. We only consider
situations where the data are complete, that is, every
variable in U is assigned a value. To learn a Bayesian
network from the training database D, we use a scoring
function to score each network relative to D and then
search through possible network structures for the best
scored network. The scoring function we use in this pa­
per is based on the MDL principle [Bouckaert, 1994a,
Lam and Bacchus, 1994, Suzuki, 1996] and is given by
[Friedman and Getoor, 1999]:

MDL(G,D)

=

L MDL(X;JPA;)

MDL(X; /PA;)

=

H(X;jPAi) + -2-K(Xi/PA;J2)

H(X;/PA;)
K(X;/PA;)

(1)

logN

Nx· pa· )
= - L Nx; ,pa; log(�
pai
Xi,pai
(r; - 1) II rz

=

(3)
(4)

XtEPA;

where H(X;IPA;) is called the empirical entropy term
and Io�N K(X;IPA;) the penalty term. K(X;IPA;)
represents the number of parameters needed to repre­
sent P(X;IPA;) and r; is the number of possible states
of X;. N is the total number of samples, Nxi,pa; is the
sufficient statistics of the variables X; and PA; in D,
that is, the number of samples in D which match the
particular instantiation x; and pa;, and the summa­
tion in (3) is over all the possible instantiations of X;
and PA;. We have
Xiri

L

Nx;,pa;

= Npa;·

581

In this paper we assume that a consistent variable
ordering is given as X1 < X2 < ... < Xn, where
X; < Xi means that the edge between X; and Xi
can only be directed as X; � Xi, not the other way.
The scoring function given in equation (1) is decom­
posable, that is, it is decomposed into a sum of local
scores over each node-parents family. This decompos­
ability plus node ordering greatly reduces the search
complexity. The score M DL( G, D) is minimized if
and only if each local score M DL(X;IPA;) is individ­
ually minimized. Thus each parent set PA; may be
independently selected. The network learning prob­
lem reduces to that of for each variable X; finding a
subset of {X1,X2, ...,X;_t} as PA; that minimizes
the score M DL(X;IPAi)· For each variable X;, we
need to search through 2i-l sets. For a Bayesian net­
work of n variables, to find the best scored network
we need to search through 2:.:7=1 2i-l
2n - 1 sets.
The search space is still exponential in the number of
variables.

=

3

Previous Work

In this section we examine some previous work
that is directly related to the result presented in
this paper.
Since the possible network struc­
tures to search are exponential in the number
of variables even if an ordering on the variables
is given, [Cooper and Herskovits, 1992] developed a
greedy search algorithm called K2 (which used a
Bayesian scoring function) . To find the parent set
for the variable Xj, K2 starts with the empty par­
ent set, successively adds to the parent set the vari­
able within {X1,X2, . ..,Xi- d that maximally im­
proves a Bayesian score, and stops when no vari­
able can be added such that the score improves.
[Bouckaert, 1994a] replaced the Bayesian scoring func­
tion in the K2 algorithm with a MDL scoring function
and called the resulting algorithm K3. K3 is presented
in Figure 1, which will be used for preprocessing in our
proposed algorithm.
[Suzuki, 1996] developed an exhaustive search algo­
rithm which uses branch-and-bound (BnB) technique
and guarantees to find the parent set with the mini­
mum MDL score. He noticed that in theM DL scoring
function

(5)

When the MDL scoring function is defined as in equa­
tions (1)-(4), we wish to find the network structure
that minimizes the MDL score.

when adding a node Xq into PAj, K(XJIPAj) in­
creases by K(X3IPA3) * (rq- 1) . On the other hand,
the empirical entropy term can decrease at most by the
current value H(XJIPAj) since the value of empirical

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

582

Algorithm K3
INPUT: Node Xi;
OUTPUT: Parent set PAj, with its MDL score

minMDL.

=

Predi {X1,X2, . . . ,Xi-d i
PAi 0;
minMDL MDL(XiiPAi) i
WHILE (PAi =J= Predi){
let Xz be the node in Predj \ PAj that minimizes
MDL(XiiPAi U {Xz});
MDLnew MDL(XiiPAi U {Xz});
IF ( MDLnew < minMDL) THEN
minMDL MDLnewi
PAi PAi U {Xz};
ELSE
RETURN;
}

=

=

=

=

=

Figure 1: The K3 algorithm.
entropy is nonnegative. Hence, if we already have
logN
H(XiiPAi) � - -K(XiiPAi) * (rq- 1),
2

(6)

adding nodes to the parent set PAi will always in­
crease the MDL value. Therefore further search along
this branch is unnecessary. Based on this observation,
Suzuki developed a BnB algorithm termed B&B..D
( which is not presented here due to space limit) .
Essentially, Suzuki used a heuristic H(XiiPAi) � 0,
and found a lower bound for the MDL score:
logN
MDL(XiiPAj) � - -K(XiiPAi) ·
2

(7)

In this paper we will improve the B&B..D algorithm
in several ways: find better lower bounds for the MDL
score, use thus far found minimum MDL score to speed
up pruning, and use node ordering etc. But first to
clarify the problem we formally define the search space.
4

The Search Space

The problem that we are facing is for a variable Xi
to find a subset of Ui {X1, . .. ,Xi-d as its parent
set that minimizes the MDL score. To formulate it as
a search problem first we define the search space. A
search space is composed of a set of states and a set of
operators that change the states. A state represents a
single configuration of the problem. An operator takes
a state and maps it to another state. For our problem
a state is naturally defined as a subset of Uj which
represents a parent set of Xi and is represented by a

=

Figure 2: A search tree for finding the parent set of
variable x5.

set of variables. An operator is defined as adding a
single variable to a set of variables.
A search space can be represented by a search-space
graph. The states are represented by nodes of the
graph and the operators by edges between nodes.
When a search-space graph is formed as a tree, sys­
tematic search algorithms can be applied which are
guaranteed to find a solution if one exists. For the
current problem we start searching from the empty
set, that is, we define the empty set as the root of the
search tree. To avoid repeated visit of the same states
we give an order to the variables in Ui as
(8)
We will call this order as tree order. The tree order
could be arbitrary and has nothing to do with the given
variable ordering. We will see later that our algorithm
is more efficient under some particular tree order. A
state T with l variables is represented by an ordered
list {xil' xi2' 'xi,} where xil < xi2 <
< xi,
in the tree order. The legal operators for this state T
are restricted to adding a single variable that is after
Xi, in the tree order. Under the above definitions our
search-space graph forms a tree. A search tree for
finding the parent set of variable x5 is shown in the
Figure 2. The search tree for variable Xj has 2i-l
nodes and the tree depth is j - 1.
0 0

0

0

0 0

For each state T we can compute a MDL score:
MDL(XiiT)

= H(XiiT) + lo�N K(XiiT).

Systematic search algorithms can be applied to the
search tree, which exhaustively search through all
states and find the state with the minimum MDL
score. We describe our algorithm in the next section.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

5

Depth-First Branch-and-Bound
Algorithm

Inspired by Suzuki's work [Suzuki, 1996] we developed
an efficient depth-first branch-and-bound (DFBnB) al­
gorithm. Our algorithm improves Suzuki's B&B_D al­
gorithm in several ways.
1. To determine if a branch of the search tree can
be pruned we use thus far found minimum MDL
value (denoted by minMDL) to compare with the
lower bound for the MDL values of that branch.
2. To speed up the pruning, we first run the greedy
search algorithm K3 and output the minimum
MDL value found by K3 into the DFBnB pro­
cedure as the initial minMDL value. Since K3
works very well practically in finding the global
minimum MDL value, this leads the DFBnB pro­
cedure to prune the search tree nodes using the
optimal minMDL value from the starting point
most of the time.
3. We find better lower bounds for the MDL values.
Assume we are visiting a state T
{. .. ,Xk,}
in the search tree. Let W denote the set of
variables after xk, in the tree order, w =
{Xk,+P ...,Xk;_J. We want to decide if we need
to visit the branch below T's childTU{Xq}, where
Xq E W. Since the entropy term is nonnega­
tive and K(XjiPAj) increases with adding vari­
ables to the parent set P Aj, Suzuki obtained a
lower bound for the MDL values of all states be­
low T U {Xq} in the search tree as:

=

logN
MDL(XjiT,Xq,.. . ) � -- K(XjiT) * Tq· (9)
2
Note that all states below T U {Xq} contain
the set T U {Xq} as a subset. We notice that
since the empirical entropy is nonincreasing with
respect to adding variables to the parent set
[Gallager, 1968], that is,

583

then all states along the branch below T U {Xq}
can be pruned.
4. We use a node ordering to further speed up
the pruning process. So far, the tree order (8)
is arbitrary. The entropy terms that are fre­
quently used as lower bounds in equation (12)
are (ref. Figure 2):
H(Xj!Xk1,. . .,Xk;_1),
H(Xj! XkpXk3 . . . ,Xk;-1), H(Xj! Xk2,... ,Xk;-1),
'
H(Xj! Xk3,... ,Xk;_1) etc .. Now the tree order
would influence the value distributions of these
terms. For example, if the entropy values tend to
be lower when the parent sets contain xkj-1' all
these lower bounds would be small since all these
parent sets contain Xk;_1• On the other hand, if
the entropy values tend to be lower when the par­
ent sets contain Xk1, most of the lower bounds
would be large because most of these parent sets
do not contain Xk1• The frequencies that the vari­
ables appear in these terms are opposite to their
tree order, that is, Xk;_1 appears most frequently
and Xk1 appears least. Thus naively, if we decide
the tree order such that the variables that tend
to reduce the entropy are ordered earlier, most of
these lower bounds would have larger values than
ordered the other way around. In our algorithm,
the tree order is determined such that
H(XiiXk1) :S H(XiiXkJ :S ... :S H(Xi1Xk;_1).

(14)
Our tests show that our algorithm will visit far
fewer states in this order than when the given vari­
able ordering is also used as tree order.
Our proposed algorithm DFBnB_K3 is presented in
Figure 3. In the main program we call procedure K3
to find a minMDL value, order the nodes according to
equation (14), and call the procedure DFBnB starting
from the empty set. The DFBnB procedure is a stan­
dard recursive depth-first search algorithm and uses
equation (13) as pruning condition.

Some improvement to the DFBnB procedure is possi­
ble. The same lower bound terms H(Xj!T,W) may
be computed several times. This repeated computa­
(10)
H(Xji T) � H(XjjT, Xq),
tion may be avoided by passing their values. Our tests
show that this only leads to minor improvement. We
all the states below T satisfy
used the fixed tree order to visit the search tree. It
is possible to dynamically order which branch to visit
(11)
H(Xj! T,. . .) � H(Xj! T,W),
first according to their MDL values. This way it may
because they are all subsets ofT UW. Thus a bet­
be possible to find a lesser minMDL earlier and thus
ter lower bound for the MDL values of all states
to
speed up the pruning. But since we already run K3
below T U {Xq} is
procedure which performs very well practically in find­
logN
MDL(XiiT,Xq, ...) 2: H(XiiT, W)+-2 - K (XiiT)*rq. ing the minimum MDL values, the redundancy of dy­
namical ordering outweights its possibility of pruning
(12)
more states. Note that tree order makes the search­
Before computing the entropy term for the state
T U {Xq}, if we already have
space graph form a tree and restricts which variables
can
be added to a state, but we are still free to choose
!o N
K (XiiT)Hq, (13)
minMDL < H(XiiT, W)+
an order to visit the tree.

;

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

584

Algorithm DFBnB_K3
INPUT: Node X1;
OUTPUT: Parent set PA1, which has the minimum

MDL score minMDL.

main(){
CALL K3 which outputs minMDL and PA1;
order variables according to equation (14);
lo� N (ri - 1); T= 0;
Pl =
CALL procedure DFBnB(T,Xko,Pl);
}

(17)

MDL= H(XJIT) +P1;
IF(MDL < minMDL)
THEN minMDL= MDL;PAJ= T;
Let W be the set of variables after Xk, in tree order;
lowbound= H(XJIT, W);
FOR(each Xq E W){
P2= P1 * rq;
IF (minMDL > p2 + lowbound)
THEN CALL DFBnB(T U {Xq},Xq,p2);
}
}
Figure 3: Our proposed algorithm DFBnB_K3.
In our algorithm, when we visit a state T, we need to
compute one extra entropy term H(XJIT, W) for the
use of the lower bound. This computation is costly
and at least doubles the time to visit a state. Noticing
that
(15)

we could use a fixed lower bound H(XiiX1,... ,X1-d
in the whole computation, which is still better than
using zero as lower bounds. Our tests showed that for
small sample size (such as N = 500) this could save
us time in spite of more states were visited. But for
large sample size and number of variables the power
of H(X1jT, W) in pruning states outweights the com­
putation redundancy.
6

Complexity Analysis

In this section, we study how many states in the search
tree will be visited by our algorithm and thus estimate
the time complexity.
Consider a state T = {Xiu Xi2, ,Xid_J at depth
d- 1 of the search tree. Assume W is the set of vari­
ables that are ordered after Xid-l in the tree order. We
• • •

logN
.
H(XJIT, W)+- -K(XJIT,XiJ � mmMDL (16)
2
from the pruning condition (13). Since the number of
possible states ri 2::2,i = 1, 2, . . , j - 1, we have
.

Procedure DFBnB(T, Xk,Pl){
INPUT: state T, the last variable Xk, in T in the tree
order, penalty term value Pl;

H(XiiT,...) 2:: H(XiiX1, ...,Xj-1),

will expand (i.e. , call DFBnB procedure for) a child of
T, T U {Xid}, if and only if

thus
log N
.
- - (ri - 1)2d � mmMDL- H (X1 I T, W) .
2
minMDL satisfies
minMDL:::; MDL(XJi0)

=

H(Xil0) +

(18)

lo;N (ri -1).
(19)

This leads to

logN
logN
- -{rj-1)2d :::; - -{rj-l)+ H(X1!0) - H(X1!T,W)
2
2
(20)
From information theory we have [Gallager, 1968]

(21)
and
(22)
Thus we obtain
lo N
lo N
(r1- 1) +N logri,
(r1 -1)2d �

;

;

(23)

or

2 logrj N
< 1+
--.
(24)
r1- 1 logN
We conclude that for large N, the search tree for a
variable Xi is searched at most to the depth of
2d

--

D1= Llog

N
2 logrj
log 1
J,
og N
r1- 1 +

--

-

(25)

where LxJ stands for the largest integer not greater
than x. Our algorithm will not visit parent sets of
X1 which contain more than D1 variables. Using the
inequality
logr � r -1,for r 2::2,
(26)
equation (24) becomes
2d � 1 +

2N
logN"

(27)

Thus for any variable our algorithm will not visit par­
ent sets that contain more than
2N
(28)
D= Llog
J
log N
variables. This is consistent with the theorem in
[Bouckaert, 1994b] that the MDL measure will not se­
lect network structures that contain parent sets with
more than log N parents. Variable ordering does not
influence the result given by equation (28) since it
holds under any variable ordering.

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

The MDL scoring function given in
equations (1}-(4) will not select network structures
that contain parent sets with more than l log 1;::N J
variables.

Theorem 1

now estimate how many states the search tree
contains to the depth Di. The states at depth k
are those subsets of {X1,X 2, ,Xi-d which contain
i 1
exactly k variables. Thus there are ( k' ) states at
depth k. The total number of states to depth Dj is
mi
2::�,:;,0 (ik'1). Assume Dj ::; (j- 1)/2, we have
i 1
m i < Di( .iJ ). Each state takes O(N(j- 1 + rj))
J
O(N(j-1)) time to compute the entropy terms, where
j- 1 comes from that parent sets can contain at most
j- 1 variables and we assume using hash tables to store
the sufficient statistics. Thus the time complexity of
finding the minimum scored parent set for variable Xi
is
j- 1
Ti O (N(j- 1)Di
).
(29)
Di
We

•

.

.

=

far better than the time complexity estimation (33).
The actual time grows much slower with respect to
N. Practically we can get an empirical estimation for
D after the K3 procedure has returned a minMDL
value. From equation (18) and (15) we have
logN
.
2d
2 -(ri-1) :::; mmMDL-H(XiiX1, X2,

( )

For a Bayesian network of n variables, the time com­
plexity to reconstruct the whole structure for large n
and N is
T(n,N) < n * Tn = O(n2ND

(; ) )

(30)

where D is given in equation (28) and we assume D <
�· We may estimate (�) using Stirling's formula x! �
J27fX(; ) x. It can be shown that

Thus

To see clearly how T(n,N) changes with respect to N,
using x10g Y ylog x and ignoring the floor function in
the expression (28) for D, the above formula can also
be expressed as

=

In deriving equation (28) forD, we used equation (22),
that is, we take zero as the lower bounds for the en­
tropy terms. In practice the values of H(XJIT, W)
used in equation (16) could be large and compara­
ble with minMDL sometimes, especially after we de­
termined the tree order according to equation (14).
Thus not all branches are searched to the depth D and
some branches are pruned after only a few steps. The
fact that many variables have more than two states
(ri 2': 2) also leads to that the actual search depth is
less than the estimation (28). Our test results in Sec­
tion 7 show that the DFBnB_K3 algorithm performs

• • •

, Xj-1},

thus a good estimation for DJ is
DJ

=

=

585

7

=

Llog

2 (minMDL-H(XiiXl, X2, ... , Xi-d)
(rj-l)IogN

J

·

(34)

Test Results

We applied the DFBnB_K3 algorithm to train­
ing data generated from the following networks:
ALARM [Beinlich et al., 1989] which contains 37 vari­
ables and 46 edges, Boerlage92 [Boerlage, 1992] which
has 23 variables and 36 edges, Car..Diagnosis..2
which has 18 variables and 20 edges, Hailfinder2.5
[Abramson et al., 1996] which has 56 variables and
66 edges, A [Kozlov and Singh, 1996] which has
54 variables, and B [Kozlov and Singh, 1996] which
has 18 variables and 39 edges.
Boerlage92 and
Car..Diagnosis..2 were downloaded in the N etica for­
mat from the web site of Norsys Software Corpo­
ration, http:/jwww.norsys.com. Hailfinder2.5, A,
and B were obtained with GeNie modeling envi­
ronment distribution developed by the Decision Sys­
tems Laboratory of the University of Pittsburgh
(http:/jwww.sis.pitt.edu/"'dsl) and were transformed
to Netica format using GeNie. We used the ALARM
database generated by Herskovits [Herskovits, 1991]
which contains 20000 cases and which gives a vari­
able ordering that is the same order used by
[Cooper and Herskovits, 1992] and [Suzuki, 1996]. A
database containing 20000 cases was generated for
each other network using a demo version of Net­
ica API developed by Norsys Software Corporation
(http:jjwww.norsys.com). In the following experi­
ments by "the sample size is N" we mean that the
first N cases in the databases were used.
To show the efficiency of the algorithm DFBnB_.K3, we
compared it with Suzuki's B&B..D using the ALARM
database. Table 1 shows our test results. "k" col­
umn stands for that the computation is up to xk in
the variable ordering. "States" column denotes how
many times the entropy terms are computed. For
B&B..D algorithm, this number is the same as the
number of states visited in the search tree, while for
DFBnB_K3 algorithm it is approximately double of
the number of states visited. Both algorithms were
implemented with C++ language and "Time" column

586

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

gives the actual run time in a Sun Workstation Ultra5.
Table 1 shows that our algorithm indeed improves over
Suzuki's B&B_D, and more importantly, the time com­
plexity increases more slowly in the sample size N.
Table 2 gives the test results of applying DFBnB_K3
algorithm on various databases with various sample
sizes, which lists "States" and "Time" for recovering
the full network structures. It shows that the number
of states visited increases slowly with sample size N,
and since the time spent in each state is O(N), the
overall time complexity is a low polynomial in N. This
slow increase rate in N makes it possible to exploit
large databases using our algorithm. The table didn't
give results for A network because it took DFBnB_K3
more than 48 hours to recover the full structure even
with N
250. A is a randomly generated network
originally, has 54 variables and is densely connected.

=

Since DFBnB_K3 is an exhaustive search algorithm
which returns with the global minimum MDL score,
it paves the way for studying some other problems.
One interesting problem is how well the greedy search
algorithm K3 does in finding parent sets with the
global minimum score. Our test results show that K3
performed very well and found the global minimum
most of the time regardless of the sample sizes and
databases. For the ALARM database, K3 was trapped
in a local minimum twice for N 1000 and once for
each other sample size in finding the parent sets for
the 36 variables. For the A database K3 was trapped
5 times for 54 variables with N = 250. For all other
databases and sample sizes shown in the Table 2 K3
found the global minimum all the time.
=

Another interesting problem is how well the MDL scor­
ing function is in recovering the original network struc­
ture. Table 3 gives our test results. It lists in the
networks with the minimum MDL score how many
edges are extra to or missing from the original net­
works from which the training databases were gener­
ated. The results show that the MDL score tends to
miss edges when the training data size is small and
will add few extra edges even if the data size is small.
The number of missing edges decreases as the data size
increases, as expected. [Bouckaert, 1994b] has shown
that the MDL scoring function will pick the minimal
I-map under given variable order when the data size is
large enough. The MDL scoring function seems having
trouble in recovering structures for B network. B is a
randomly generated network and is densely connected.
It has several variables having more than 4 parents but
the empty set had the minimum MDL score (thus was
picked up as the parent sets) for all of them up to
N 16000.

=

Table 1: Comparison of Suzuki's algorithm with ours
(ALARM database).
N 250
B&B_D
DFBnB_K3
k
States
Time
Time
States
(minute)
(minute)
1
10
649
190
1
15
6317
1
704
1
20
26760
1
2401
1
25
57928
1
4843
1
30
104129
1
6760
1
31
154298
2
9134
1
32
165052
2
11373
1
33
2
197883
13357
1
215289
2
34
13879
1
35
2
236506
14695
1
36
3
264405
15990
1
37
324472
3
17660
1

=

k
10
15
20
25
30
31
32
33
34
35
36
37

N = 500
B&B_D
States
Time
(minute)
807
1
10350
1
54841
1
2
123138
4
241565
374421
7
416052
7
498796
9
547686
9
601752
10
695805
12
920851
16

DFBnB_K3
States
Time
(minute)
220
1
782
1
3153
1
6489
1
9091
1
12215
1
19350
2
21244
2
21988
2
23410
2
25582
2
29142
2

=

k
10
15
20
25
30
31
32
33
34
35
36
37

N 1000
B&B_D
Time
States
(minute)
908
1
14397
1
101555
4
259142
9
532494
18
808540
28
946658
33
1208414
42
1372767
48
1567279
54
1780786
61
2423177
85

DFBnB_K3
Time
States
(minute)
230
1
1114
1
3929
1
8649
1
11358
2
15177
2
28374
4
29912
4
30596
4
32406
4
35674
5
39382
5

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

N
250
500
1000
2000
4000
8000
16000
nodes

587

Table 2: The efficiency of DFBnB_K3 algorithm. ( time is in minutes)
ALARM
Boerlage92
Car_Diagnosis_2
B
Hailfinder2.5
States Time
States Time States
Time States Time
States Time
1
1
1
81,695
1,954
2,209
17,660
18
3
277,492
1
2 111,076
1,724
2
29,142
7
7,120
72
621,488
1
19
2 1,876,046
403
2,731
5 169,320
39,382
7,717
11 272,136
1
52, 047
2,913
2
7,725
60
1 21,232
75,113
4,026
9
32 320,038
148
2 21,926
93 429,900
4,144
15
112,599
391
182,627
4, 376
310
5 43,886
74
37
23
18
18
56

Table 3: The performance of MDL scoring function in recovering network structures.
Boerlage92
Car_Diagnosis_2
ALARM
B
Hailfinder2.5
N extra missing extra missing extra missing extra missing extra missing
2
1
10
38
15
9
250
11
0
0
33
37
8
26
500
7
7
0
0
0
12
0
1
6
0
0
35
18
5
9
1000
6
0
34
1
5
0
0
4
8
0
2000
3
28
0
0
0
3
8
0
4000
1
25
1
0
8
0
0
8000
0
1
21
0
0
1
0
16000
39
20
36
edges
46
66

8

Conclusion

We developed an efficient depth-first branch-and­
bound search algorithm for learning Bayesian network
structures from data when a consistent variable order­
ing is given. Preliminary test results are promising.
The time complexity of our algorithm grows slowly
with the sample size. Our algorithm finds the global
optimum networks according to the MDL scoring func­
tion, thus it can be used to empirically measure the
performance of the MDL principle in learning Bayesian
network structures. We also showed that the subop­
timal algorithm K3 performs very well practically in
finding the global optimum.
The branch-and-bound technique is limited to learn­
ing based on the MDL principle since it relies on the
nature of the MDL scoring function. On the other
hand, the technique can be extended to the general
cases of not requiring variable ordering, where the
greedy search is the common practice. As the DF­
BnB_K3 algorithm presented in this paper, we may run
a greedy search procedure before a depth-first branch­
and-bound procedure. Since the search space is huge,
it would be impractical to visit all states even if good
pruning is applied. The purpose would be to find a
better solution than what greedy search found if more
time is spent, since simple greedy search cannot benefit

from more time. When we are willing to spend more
time, iterative greedy search can be applied though.
It would be interesting to compare the performance of
branch-and-bound algorithm with the iterative greedy
search algorithm. We are currently working on the
problem.
Acknowledgements

The author was supported by a Microsoft Fellowship.
References

(Abramson et al. , 1996] B. Abramson, J. Brown,
W. Edwards, A. Murphy, and R. Winkler. Hail­
finder: A Bayesian system for forecasting severe
weather. International Journal of Forecasting,
12( 1) :57-72, 1996.
(Beinlich et al. , 1989] I. A. Beinlich, H. J. Suermondt,
R. M. Chavez, and Cooper G. F. The ALARM
monitoring system: A case study with two prob­
abilistic inference techniques for belief networks. In
Proceedings of the second European conference on
Artificial Intelligence in Medicine, pages 247-256,

London, England, 1989.

588

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

[Boerlage, 1992] B. Boerlage.
Link strength in
Bayesian networks. Master's thesis, Computer Sci­
ence Department, Univ. of British Columbia, BC,
1992. Also Tech. Report 94-17, Dept. of Computer
Science, Univ. of British Columbia.
[Bouckaert, 1994a] R. R. Bouckaert. Probabilistic net­
work construction using the minimum description
length principle. Technical Report RUU-CS-94-27,
Utrecht University, Netherlands, July 1994.
[Bouckaert, 1994b] R. R. Bouckaert. Properties of
Bayesian belief network learning algorithms. In
R. Lopez de Mantaras and D. Poole, editors, Pro­
ceedings of the tenth conference on uncertainty in
artificial intelligence (UAI '94), pages 102-109.

Morgan Kaufmann, 1994.

[Buntine, 1996] W. Buntine. A guide to the literature
on learning probabilistic networks from data. IEEE
Trans. on Knowledge and Data Engineering, 8:195210, 1996.
[Chickering, 1996] D. M. Chickering.
Learning
Bayesian networks is NP-complete. In D. Fisher and
H.-J. Lenz, editors, Learning from Data: Artificial
Intelligence and Statistics V. Springer Verlag, 1996.
[Cooper and Herskovits, 1992] G. F. Cooper and
E. Herskovits. A Bayesian method for the induc­
tion of probabilistic networks from data. Machine
Learning, 9:309-347, 1992.
Friedman
[Friedman and Getoor, 1999] N.
and L. Getoor. Efficient learning using constrained
sufficient statistics. In Proc. Seventh International
Workshop on Artificial Intelligence and Statistics,

1999.
[Gallager, 1968] R. G. Gallager. Information theory
and reliable communication. John Wiley & Sons,
Inc., 1968.
[Beckerman et al., 1995] D. Beckerman, D. Geiger,
and D.M. Chickering. Learning Bayesian networks:
The combination of knowledge and statistical data.
Machine Learning, 20:197-243, 1995.
[Beckerman, 1995] D. Beckerman. A tutorial on learn­
ing Bayesian networks. Technical Report MSR-TR95-06, Microsoft Research, Redmond, Washington,
1995.
[Herskovits, 1991] E. H. Herskovits. Computer-Based
Probabilistic-Network Construction. PhD thesis,
Medical Information Sciences, Stanford University,
1991.

[Kozlov and Singh, 1996] Alexander V. Kozlov and
Jaswinder Pal Singh. Parallel implementations of
probabilistic inference. IEEE Computer, pages 3340, 1996.
[Lam and Bacchus, 1994] W. Lam and F. Bacchus.
Learning Bayesian belief networks: An approach
based on the MDL principle. Computational Intel­
ligence, 10:269-293, 1994.
[Pearl, 1988] J. Pearl. Probabilistic Reasoning in In­
telligence Systems. Morgan Kaufmann, San Mateo,
CA, 1988.
[Suzuki, 1996] J. Suzuki. Learning Bayesian belief net­
works based on the minimum description length
principle: An efficient algorithm using the B&B
technique. In L. Saitta, editor, Proceedings of the
Thirteenth International Conference on Machine
Learning, pages 462-470. Morgan Kaufmann, 1996.

