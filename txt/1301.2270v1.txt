FRIEDMAN ET AL.

152

UAI2001

Multivariate Information Bottleneck

Nir Friedman

Ori Mosenzon

Noam Slonim

Naftali Tishby

School of Computer Science & Engineering, Hebrew University, Jerusalem 91904, Israel
{ nir, mosenzon, noamm, tishby } @cs.huji.ac.il
Abstract

The Information bottleneck method is an unsu­
pervised non-parametric data organization tech­
nique. Given a joint distribution P(A, B), this
method constructs a new variable T that extracts
partitions, or clusters, over the values of A that
are informative about B. The information bot­
tleneck has already been applied to document
classification, gene expression, neural code, and
spectral analysis. In this paper, we introduce
a general principled framework for multivariate
extensions of the information bottleneck method.
This allows us to consider multiple systems of
data partitions that are inter-related. Our ap­
proach utilizes Bayesian networks for specify­
ing the systems of clusters and what informa­
tion each captures. We show that this construc­
tion provides insight about bottleneck variations
and enables us to characterize solutions of these
variations. We also present a general frame­
work for iterative algorithms for constructing so­
lutions, and apply it to several examples.
1

Introduction

Clustering, or data partitioning, is a common data anal­
ysis paradigm. A central question is understanding gen­
eral underlying principles for clustering. One informa­
tion theoretic approach to clustering is to require that clus­
ters should capture only the "relevant" information in the
data, where the relevance is explicitly determined by var­
ious components of the data itself. A common data type
which calls for such a principle is co-occurrence data, such
as verbs and direct objects in sentences [7], words and doc­
uments [1, 4, 11]. tissues and gene expression patterns [14],
galaxies and spectral components [10], etc. In most such
cases the objects are discrete or categoric and no obvious
"correct" measure of similarity exists between them. Thus,
we would like to rely purely on the joint statistics of the
co-occurrences and organize the data such that the "rele­
vant information" among the variables is captured in the
best possible way.
Formally, we can quantify the relevance of one variable,
A, with respect to another one, B, in terms of the mutual
information, I(A; B). This well known quantity, defined

as,
I(A; B)

=

""'
L..J P(a, b) log
a,b

P(a, b)
P(a)P(b)

is symmetric, non-negative, and equals to zero if and only
if the variables are independent. It measures how many bits
are needed on the average to convey the information A has
about B (or vice versa). The aim of information theoretic
clustering is to find (soft) partitions of A's values that are
informative about B. This requires balancing two goals:
we want to lose irrelevant distinctions made by A, and at
the same time maintain relevant ones. A possible principle
for extracting such partitions is the information bottleneck
(IB) method [13]. Clustering is posed as a construction
of a new variable T that represents partitions of A. The
principle is described by a variational tradeoff between the
information we try to minimize, I(A; T), and the one we
try to maximize I(T; B). We briefly review this principle
and its consequences in the next section.
The main contribution of this paper is a general formula­
tion of a multivariate extension of the information bottle­
neck principle. This extension allows us to consider cases
where the clustering is relevant with respect to several vari­
ables, or where we construct several systems of clusters at
the same time.
To give concrete motivation, we briefly mention two ex­
amples that we treat in detail in later sections. In symmetric
clustering (also called two-sided or double clustering) we
want to find two systems of clusters: one of A and one of
B that are informative about each other. A possible appli­
cation is relating documents to words, where we seek clus­
tering of documents according to word usage, and a corre­
sponding clustering of words. This procedure aims to find
document clusters that correspond to different topics and
at the same time identify cluster of words that characterize
these topics [11]. Clearly, the two systems of clusters are
in interaction, and we want a unifying principle that shows
how to construct them simultaneously.
In parallel clustering we attempt to build several systems
of clusters of the values of A. Our aim here is to capture
independent aspects of the information A conveys about
B. A biological example is the analysis of gene expression
data, where multiple independent distinctions about tissues
(healthy vs. tumor, epithelial vs. muscle, etc.) are relevant
for the expression of genes.
We present such tasks, and others, in our framework by

UAI 2001

FRIEDMAN ET AL

specifying a pair of Bayesian networks. One network,

Gin,

represents which variables are compressed versions of the

straints, Tishby et al. show that the optimal partition satis­
fies,

observed variables (each new variable compresses its par­
ents in the network). The second network,

153

P(t I a) =

G out• represents

which relations should be maintained or predicted (each

Z�;��)

D(PIIQ) =

exp

( -fJD(P(B I a)IP(B I t)))

GJ

variable is predicted by its parents in the network). We

where

formulate the general principle as a tradeoff between the

gence [3]. This equation must be satisfied self consistently.

information each network carries. We want to minimize
the information maintained by G;n and to maximize the
information maintained by

Gout.

as a tradeoff between compression of the source (given by

G;n) and fitness to a target model, where the model is de­
scribed by Gout. Using this interpretation we can think
of our new principle as a generalized compression distor­
tion tradeoff (as in rate-distortion theory [3]).

This in­

terpretation may allow us to investigate the principle in a
general parametric setup.

In addition, we show that, as

with the original IB, the new principle provides us with
self-consistent equations in the unknown probabilistic par­
tition(s) which can be iteratively solved and shown to con­
verge. We show how to combine this in a deterministic

The practical solution of these equations can be done by

words based on multiple parts of speech, complex gene­
expression data analysis, and neural code analysis.

2

3

Bayesian Networks and
Multi-Information

A Bayesian network structure over a set of random vari­

ables

X1, ... , X n

is a DAG

G

we denote by Pax, the (potentially empty) set of parents

of Xi in G. We say that a distribution Pis consistent with

G, if P can be factored in the form:

P(X1,...,Xn) =IT P(X; I Pa�J
i

and use the notation P

amount of information that variables

A,B, T, X, for random variable names and lowercase
a, b, t, x to denote specific values taken by those

variables. Sets of variables are denoted by boldface cap­
ital letters T, X, and assignments of values to the vari­
ables in these sets are denoted by boldface lowercase let­

I b)

is used as a shorthand for

Tishby et al. [13] considered two variables, A and B, with

their (assumed given) joint distribution

P(A,B).

Here

A

is the variable we try to compress, with respect to the "rel­
evant" variable

A

B.

Namely, we seek a (soft) partition of

through an auxiliary variable

mapping

P(T I A),

T

and the probabilistic

such that the the mutual information

I(A; T) is minimized (maximum compression) while the
relevant information

I(T; B)

is maximized. The depen­

dency relations between the 3 variables can be described
by the relations:

T independent of B

X1, ...,Xn

contain

D(P(X1,...,Xn)IP(Xl) · · ·P(Xn))
P(X1, ...,Xn)
EP [Iog
.
P(X1) ···P(Xn) )

I(X1,... , Xn)

letters

ters t, x. The statement P(a

f= G to denote that.

One of the main issues that we will deal with is the

information given by

We start with some notation. We use capital letters, such

P(A =a I B =b).

G in which vertices are anno­

tated by names of random variables. For each variable x.•'

about each other. A quantity that captures this is the multi­

The Information Bottleneck

as

similar to clustering by determinis­

(generally local) optimum was proven in [13] as well.

mation tradeoff in an hierarchical manner. There are many
To mention just a few, we consider semantic clustering of

fJ,

tic annealing [8]. The convergence of these iterations to a

annealing procedure which enables us to explore the infor­
possible applications for our new principle and algorithm.

is the familiar KL diver­

repeated iterations of the self-consistent equations, for ev­
ery given value of

We further give another interpretation to this principle,

Ep[log

given A; and on the

other hand we want to predict B from T.

By introducing a positive Lagrange multiplier fJ, Tishby

The multi-information captures how close is the distri­
bution

P(X1, ... , Xn)

to the factored distribution of the

marginals. This is a natural generalization of the pairwise
concept of mutual information . If this quantity is small, we

do not lose much by approximating P by the product dis­

tribution. Like mutual information, it measures the average
number of bits that can be gained by a joint compression of
the variables vs. independent compression.
When

P

has additional known independence relations,

we can rewrite the multi-information in terms of the de­
pendencies among the variables:

G

Proposition 3.1 : Let
be a Bayesian network structure
over X = {X1, ... , Xn}, and let P be a distribution over
X such that P f=
Then,

G.

I(Xl,···,Xn) = Ll(Xi;Pa�.).

et al. formulate this tradeoff by minimizing the following
That is, the multi-information i s the sum o f local mutual

Lagrangian,

.C[P(T I A)] = I(A; T) - fll(T; B)
where we take P(A,B, T)

= P(A,B)P(T I A).

By taking the variation (i.e derivative in the finite case)

of L w.r.t.

P(T I A),

under the proper normalization con-

information terms between each variable and its parents.
We denote the sum of these informations with respect to a
network structure as:

1 54

FRIEDMAN ET AL

When P is not consistent with the DAG G, we often want
to know how close is
with

G.

P to

a distribution that is consistent

That is, what is the distance (or

distortion)

of

P

UAI 2001

set of variables Pa� '"

; � X.

from its projection onto the sub-space of distributions con­

P(X, T)

sistent with G. We naturally define this distortion as

D(PIIG)

=

min

Qf=G

Once these are set, we have a

joint distribution over the combined set of variables:

=

P(X) IJ P(Tj I Paf'n)

(1)

j

Analogously to the original IB formulation, the informa­

D(P�Q) .

tion that we would like to minimize is now given by

This measure has two immediate interpretations in terms of
the graph G.
Proposition 3.2:

Let G be a Bayesian network structure
over X = {Xb .., Xn}. and let P be a distribution over
X. Assume that the order X1, ... , Xn is consistent with
the DAG, then
.

dependent of each other as possible. (Note that since we
only modify conditional distributions of variables in T, we

cannot modify the dependencies among the original vari­
ables.)
The "relevant" information that we want to preserve is
specified by another DAG,
each

D(P�G)

yG;,.

Minimizing this quantity attempts to make variables as in­

Ti

children in
(or

T j)

Gout·

This graph specifies, for

which variables it predicts. These are simply its

Gout.

Conversely, we want to predict each Xi

by its parents in

Gout·

Thus, we think of

'I0""'

as a measure of how much information the variables in

T

maintain about their target variables. This suggest that we
Thus, we see that

D(PIIG)

can be expressed as a sum

wish to maximize is

sponds to a Markov
to the order

independence assumption with respect
X1, ... , Xn. Recall that the Markov indepen­

dence assumptions (with respect to a given order) are nec­
essary and sufficient to require the factored form of distri­
butions consistent with G

[6]. We see that D{PIIG) is mea­

sured in terms of the extent these independencies are vio­

I(X;; {x1' ... 'Xi-1} - Pa�. I Pa�.) = 0
if and only if X; is independent of {X1, ... , Xi-d-Pa�,
given its parents. Thus, D(PIIG)
0 if and only if P is
consistent with G.
lated, since

=

An alternative representation of this distance measure is
given in terms of multi-informations, since we can think of

D(P�G)

as the

amount of information between the vari­

ables that cannot be captured by the dependencies of the
structure G.

4

1
£( )[ P(Tl I Pa�1'n), . .,P(Tk I Pa��n)] ='IG'n-(jyG""',
(2)
.

and the variation is done subject to the normalization con­
straints on the partition distributions. It leads to tractable
self-consistent equations, as we henceforth show.
It is easy to see that the form of this Lagrangian is a direct

generalization of the original IB principle. Again, we try to

balance

Example

specifies that

we want

up" of the original IB variational principle to the multi­

variate case, using the semantics of Bayesian networks of

the previous section. Given a set of observed variables,
instead of one partition variable

we now consider a set

T

=

{T1, . , Tk},
. .

T,

which corre­

spond to different partitions of various subsets of the ob­
served variables. More specifically, we want to "construct"
new

variables, where the relations between the observed

variablesland these new compression variables are speci­
fied using a DAG

G;n

over

XU T. S ince we assume that

the new variables in T are functions of the original vari­
ables, we restrict attentions to DAGs where the variables in

T are l�afs.1 Thus, each TJ is a stochastic function of a
1It

Jm be convenient

to think of cases where G;, restricted

to X forms a complete graph. However, this is not crucial in the

following development. To simplify the technical details, we do

assume that P(X) is consistent with

G;,.

+

I(A; B)

Since,

=

I(A; B)

��� of Figure l .

T compresses A and G��� specifies that

T to predict B.

fCll

The multi-information allows us to introduce a simple "lift­

Gin

4.1 : As a simple example, consider application

of the variational principle with Gin and G

Gin

I(T; A)

X = {X1, ... ,Xn},

between the information T l oses about X in

and the information it preserves with respect to Gout.

Lagrangian is

Multi-Information Bottleneck Principle

yG""'.

The generalized Lagrangian can be written as

of conditional information terms, where each term corre­

and

For this choice of DAGs,

yc""' = I(T; A).

I(T; A)+ I(A; B)

-

I0•n

=

The resulting

j3I(T; B)

is constant, we can ignore it, and we end

up with a Lagrangian equivalent to that of the original IB
method.

5

Analogous Variational Principle

We now describe a closely related variational principle.
This one is based on approximating distributions by a class
defined by the Bayesian network

Gout.

rather than on

preservation of multi-information.
We face the problem of choosing the conditional distribu­

tions P(T

•
j I Pa?3 · ) . Thus, we also need to specify what is

exactly our target in constructing these variables. As with

the original IB method, we are going to assume that there
are two goals.
On the one hand, we want to compress, or partition,
the original variables. As before the natural multivariate

UAI 2001

FRIEDMAN ET AL.

155

I(A; B)+ I(T; A) and yc , = l(T; A)+ I(T; B). Using
c .
Proposition 3.2, we have that D(PIIGout) =zein- y
Putting, these together, we get the Lagrangian
••

•• ,

_c_(2l
Figure 1: The source and target networks for the original
lB.
form of this is to minimize the multi-information of\ P.
Since P is consistent with Gin• we can denote this multi­
information as yc '".
While in the previous se·ction the second goal was to pre­
serve the multi-information to other (target, relevant) vari­
ables, here we think of a target class of model distributions,
specified by a target Bayesian network. In that interpreta­
tion the compressed variables should help us in describing
the joint distribution in a different desired structure. We
specify this structure by another DAG Gout that represents
which independencies we would like to impose.
To make this more concrete consider the simple two­
variable case shown in Figure 1. In this example, we are
given the distribution of two variables A and B. The DAG
Gin specifies that Tis a compressed version of A. On the
other hand, we would like this T to make A and B as in­
dependent as possible. A way of formally specifying this
desire, is to specify the DAG G��t of Figure 1. In this
DAG, T separates between A and B.
The question now is how to force the construction of
P(T I A) such that it will lead to the independencies that
are specified in the target DAG. Notice that these two DAGs
are, in general, incompatible: Except for trivial cases, we
cannot achieve both sets of independencies simultaneously.
Instead, we aim to come as close as possible to achieving
this by a tradeoff between the two. We formalize this by
requiring that P can be closely approximated by a distribu­
tion consistent with Gout. As previously discussed, a natu­
ral information theoretic measure for this approximation is
D(PIIG), the minimal KL divergence from P to distribu­
tions consistent with G out.
As before, we introduce a Lagrange multiplier that con­
trols the tradeoff between these objectives. To distinguish
it from the parameter we use above, we denote this param­
eter by 'Y. The Lagrangian we want to minimize in this
formulation is thus:
(3)

where the parameters that we can change during the mini­
mization are again over the conditional distributions P(Ti I
Pa�·�).
The range of 'Y is between 0, in which case we
J
have a trivial solution in which the Tj 's are independent of
their parents, and oo, in which we strive to make Pas close
as possible to Gout.
5.1 : Consider again the example of Figure 1
with Gin and G��t· In this case, we have that IG'"

Example

=

=

I(T; A) - "fl(T ; B) + (1 + -y)I(A; B)

Since, I(A; B) is constant, we can ignore it, and we end
up with a Lagrangian equivalent to that of the original IB
method (setting 'Y = (3). Thus, we can think of the IB as
finding a compression T of A that results in a joint distri­
bution that is as close as possible to the DAG where A and
B are independent given T.
Going back to the general case, we can apply Propo­
sition 3.2 to rewrite the Lagrangian in terms of multi­
informations:

_c_(2)

zGin

-y(IG'" yc.,,)
(1 + 'Y)Ic'" - "fLG""'
+

_

which is equivalent to the Lagrangian £(1) presented in
the previous section, under the transformation (3 = ibi.
Where the range 'Y E [0, oo) corresponds to the range
(3 E [0, 1). (Note that when (3 = 1, we have that
£(1)
D(P�G out). which is the extreme case of £(2) .)
Thus, from a mathematical perspective, .c<2l is a special
case of .c<1l with the restriction f3 � 1.
This transformation raises the question of the relation be­
tween the two variational principles. As we have seen in
Examples 4.1 and 5 .1, we need different versions of Gout
in the two Lagrangians to reconstruct the original lB. To
better understand the differences between the two, we con­
sider the range of solutions for extreme values of (3 and 'Y·
When f3 ---+ 0 and 1---+ 0, both Lagrangians minimize the
term yG;". That is, the emphasis is on loosing information
in the transformation from X toT.
In the other extreme case, the two Lagrangians differ.
When f3 ---+ oo, minimizing £(1) is equivalent to maximiz­
ing ya , . That is, the emphasis is on preserving informa­
tion about variables that have parents in Gout. For exam­
ple, in the application of £(1) in Example 4.1 with G��t>
this extreme case results in maximization of I(T; B). On
the other hand, if we apply £(1) with G��t• then we maxi­
mize l(T; A) + I(T; B). In this case, when (3 approaches
oo information about A will be preserved even if it is irrel­
evant to B.
When 'Y ---+ oo, minimizing £(2) is equivalent to min­
imizing D(PIIGout). By Proposition 3.2 this is equiva­
lent to minimizing the violations of conditional independencies implied by Gout. Thus, for G��t, this mini­
mizes I(A; B I T). Using the structure of P, we can
write I(A; B I T ) = l(A; B) - I(T; B) (as implied by
Proposition 3.2), and so this is equivalent to maximizing
l(T; B). If instead we use G���· we minimize the infor­
mation l(A; B, T) = I(A; B)+ I(T; A)- I(T; B). Thus,
we minimize I(T; A) while maximizing I(T; B). Unlike,
the application of £(1) to G��t• we cannot ignore the term
l(A;T).
=

••

156

FRIEDMAN ET AL.

al
c(out
(a) Parallel Bottleneck

G(b)
out

c(al
out
(b) Symmetric Bottleneck

db)
out

M.
® @

UAI2001

other interpretation for the above optimization is that we
aim to find Tt and T2 that together try to compress A, pre­
serve the information about B and remain independent of
each other as possible. In this sense, we can say that we
are trying to decompose the information A contains about
B into two "orthogonal" components.
Recall, that using £(2) we aim at minimizing violation
of independencies in Gout· Th i s suggests that the DAG
G��� of Figure 2(a) captures our intuitions above. In this
DAG, A and B are independent given T1 and T2. Moreover, here again G�b2t specifies an additional independence
require ment over T1 and T2• To see that we examine the
Lagrangian defined by the principle. In this case, I?""'
I(T1, T2; A)+ I(T1, T2; B). Using Eq.(S) and dropping the
constant term I(A; B), the Lagrangian £(2) can be written
as
=

Figure 2: The source and target networks for the parallel
and symmetric Bottleneck examples.
To summarize, we might loosely say that £(1) focuses
on the edges that are present in Gout• while £(2) focuses
on the edges that are not present in Gout (or more pre­
cisely, the conditional independencies they imply). This
explains the somewhat different intuitions that apply to un­
derstanding the solutions found by the variational princi­
ples. Thus, although both variants can be applied to any
choice of Gout, some choices might make more sense for
2
£(1} than for £( ), and vice versa.
6

Bottleneck Variations

We now consider two examples of these principles applied
to different situations.
Example 6.1: We first consider a simple extension of the
original lB. Suppose, we introduce two variables T1 and
T2. As specified in G;n of Figure 2(a), both of these vari­
ables are stochastic functions of A. In addition, similarly
to the original IB, we want T1 and T2 to extract informa­
tion about B from A. We call this example the parallel
bottleneck, as T1 and T2 compress A in "parallel".
The DAG G��t specifies that T1 and T2 should predict B.
Based on these two choices, I0•" I( A ; B)+ I(T2; A)+
I(T1; A) and T!f·••
I(T1,T2; B). After dropping the
constant term I(A; B), the Lagrangian £(1) can be written
as
=

=

Thus, we attempt to minimize the information between
A and T1 and T2 while maximizing the information they
preserve about B. Since T1 and T2 are independent given
A, we can also rewrite2
I(T1, T2; A)

=

I(T1; A)+ I(T2; A) - I(T1; T2) . (5)

Thus, minimizing I(T1; A) + I(T2; A) is equivalent for
minimizing I(T1, T2; A)+ I(T1; T2). In other words, anP(T!,To,A)

2pTOOf·· I(T1, T2,·A)- E[Iog P(A)P(T1,T2) J
P(A)P(TtiA)P(ToiA) . P(T!)P(T2)
_ E[lo
g
P(A)P(T, ,T,)
P(T,)P(T,) ]
=

I(T1; A)+ I(T2; A)- I(T1; T2).

Thus, again, we attempt to minimize the information be­
tween T1 and T2 while maximizing the information they
together contain about B.
Example 6.2: We now consider the symmetric bottleneck.
In this case, we want to compress A into TA and B into TB
so that TA extracts the information A contains about B, and
at the same time TB extracts the information B contains
about A. The DAG Gin of Figure 2(b) captures the form of
the compression. The choice of Gout is less obvious.
One alternative, shown as G��t in Figure 2(b), attempts
to make each of TA and TB sufficient to separate A from
B. As we can see, in this network A is independent of
B and TB given TA. Similarly, TB separates B from the
other variables. The structure of the network states that
TA and TB are dependent of each other. Developing the
Lagrangian defined by this network, we get:

C�2l

=

I(TA; A)+ I(TB; B) - {I(TA; TB)

Thus, on one hand we attempt to compress, and on the other
hand we attempt to make TA and TB as informative about
each other as possible. (Note that ifTA is informative about
TB, then it is also informative about B.)
Alternatively, we might argue that TA and TB should each
compress different "aspects" of the connection between A
and B. This intuition is specified by the target network
G�b2t of Figure 2(b). In this network TA and TB are inde­
pendent of each other, and both are needed to make A and
B conditionally independent. In this sense, our aim is to
find TA and TB that capture independent attributes of the
connection between A and B. Indeed, following arithmetic
similar to that of Example 6. 1, we can write the Lagrangian
as:
I(TA; A)+ I(TB; B)+
!(2I(TA; TB) - I(TB; A) - l(TA; B))

That is, we attempt to maximize the information TA main­
tains about B and TB about A, and at the same time try to
minimize the information between TA and TB.

FRIEDMAN ET AL.

UAI 2001

7

Characterization of the Solution

In the previous sections we stated a variational principle.
In this section we consider the form of the solutions of the
principle. More precisely, we assume thatGin, Gout, andf3
(or/) are given. We now want to describe the properties of
) . We present this characterthe distributionsP(Tj IPa�•·
'
ization for the Lagrangians of the form of £(1). However,
we can easily recover the corresponding characterization
for Lagrangians of the form£(2) (using the transformation
(3 = if.y).
In the presentation of this characterization, we need some
additional notational shorthands. We denote by Ui =
0••' and Vx. = PaX;
0'• VT,. = PaT;
0•••. We also dePaT;
•

•

•

noteV7{; = VT1 - {Ti} and similarly forv;}';. To sim­
p! ify the presentation, we also assume that Ui n VT; = 0.
In addition, we use the notation
EP(.Iu;)[D(P(Y I Z, u;)IP(Y I Z, t;))]

LP(Z I Uj)D(P(Y I Z, u;)IIP(Y I Z, tj))
z

=

EP(·Iu;)[log

P(Y I Z, u;)
]
P(Y I Z, t;)

where Y and Z are variables (or sets of variables) and
P( · I u;) is thejoint distribution over all variables given
the specific value of U i. Note that this terms implies aver­
aging over all values ofY and Z using the conditional dis­
tribution. In particular, if Y or Z intersects with Ui, then
only the values consistent with u; have positive weights in
this averaging. Also note that ifZ is empty,then this is term
reduces to the standardKL divergence betweenP(Y I Uj)
andP(Y I ti)·
The main result of this section is as follows.
Theorem 7.1: Assume that P(X), Gin. Gout. and {3 are
given. The conditional distributions { P(Ti I Ui)} are a
stationary point of ,C(l) I0•n - {3I0••< if an only if

1 57

See AppendixA for a proof outline.
The essence of this theorem is that it definesP(t; I u;) in
terms of the distortion d(tj, u1). This distortion measures
how close are the conditional distribution in which t; is
involved into these where we replace t; with ui. In other
words, we can understand this as measuring how well ti
perfonns as a"representative" of the particular assignment
u;. The conditional distributionP(T1 I u 1 ) depends on the
differences in the distortion for different values ofTj.
The theorem also allows us to understand the role of f3,
When /3 is small, the conditional distribution is diffused,
sincej3 reduces the differences between the distortions for
different values ofT1. On the other hand, whenf3 is large,
the exponential term acts as a "softmax" gate, and most
of the conditional probability mass will be assigned to the
valueti with the smallest distortion. This behavior matches
the intuition that when f3 is small, most of tlle emphasis is
on compressing the input variablesU i into T; and when f3
is large, most of the emphasis is on predicting the"outputs"
variables ofTj, as specified by Gout.
7.2: To see a concrete example, we reconsider
the parallel bottleneck of Example 6.1. Applying the tlle­
orem to £(1) of Eq. (4), we get that the distortion term for
T1 is

Example

This tenn corresponds to the information of B andT1, T2.
We see that P(t1 I a) increases when the predictions of B
given t1 are similar to those given a (when averaging over
T2). The distortion forT2 is defined analogously.
7.3 : Consider now the symmetric bottleneck
case of G��t inExample 6.2. Applying the theorem, we
get that the distortion term forTA is

Example

d(tA,a)

=

=

where Zr; ( u;, /3) is a normalization term, and d(ti, u;) is
given by

L
i:T;EVx;

EP(.Iu;)[D(P(Xi I v�:j, u;)IIP(Xi I

v�:j, t;))]

Ept.la)[D(P(TB I a)W(TB ItA))]+

EP(·Ia)[D(P (A IaHP(A ItA))]

The first term is a simple KL divergence,and last term can
be simplified to-logP(a I tA)= -logp(tA)-logP(a)+
logP(tA I a). By simple arithmetic operations, and using
"f = � ·we get the set of self-consistent equations
P(tA I a)
P(tB I b)

=

P(tA) e--rD(P(TBia)IIP(TB lt.-�))
Z .-� (a,/)
T
P(tB)

Zr8

(b, 1)

e-1D(P(T.-�Ib)!P(T.-�1tB))

Thus, TA attempts to make predictions as similar to these of
+Ept.lu;J[D(P(Vr; 1 uj)IIP(Vr; 1 tj))], AaboutTB,and similarlyTB attempts to make predictions
as similar to these ofB aboutTA.
where all probabilities in this term are der ived from the
8 Iterative Optimization Algorithm
definition of the model in Eq. ( 1 ). 3
3This

can
procedures.

be

done

by

standard

variable

elimination

We now consider algorithms for constructing solutions of
the variational principle.

158

FRIEDMAN ET AL.

UAI 2001

Symme!rlc compression, lr.formaHon cur,res, synthetic exam;piE

Pl..\ B) sorted by the d�o�l1ering aolution

0.6
0.4
Compression factor

(b)

(a)

0.8

(c)

Figure 3: Application of the symmetric bottleneck on a simple synthetic example. (a) input joint distribution, (b) the
same joint distribution where rows and columns were permuted to match the clustering found; dotted lines show cluster
boundaries. (c) information curves showing the progression along the information tradeoff graph for increasing (3. The
x-axis is the fraction of the information about the original variable that is maintained by the compressed variable, and
they-axis is the fraction of the information between A and B that is captured by I(TA; B) or I(TB; A). Circles denote
bifurcation events.
We start with the case where f3 is fixed. In this case, fol­
lowing standard strategy in variational methods, we sim­
ply apply the self-consistent equations. More precisely, we
use an iterative algorithm, that at the m'th iteration main­
tains the conditional distributions {P(m)(TJ I UJ) : j =
1, ... , k}. At the m + l'th iteration, the algorithm applies
an update step

where P(tj)(m} and d(m)(tj, uj) are computed with re­
spect to the conditional probabilities {P(m)(T1 I U 1) :
j = l, . k}.
There are two main variants of this algorithm. In the syn­
chronous variant, we apply the update step for all the con­
ditional distributions in each iteration. That is, each con­
ditional probability P(Tj I Ui) is updated by computing
the distortion based on the conditional probabilities of the
previous iterations. In the asynchronous variant, we choose
one variable Tj, and perform the update only for this vari­
able. For all f-:/= j, we set p(m+ll(Te I Ue)
p(m)(Tt I
Ut)- The main difference between the two variants is that
the update of T1 in the asynchronous update incorporates
the implications of the updates of the other variables.
. .

=

Theorem

8.1

:

Asynchronous

iterations of the

self­

consistent equations converge to a stationary point of the
optimization problem.

See Appendix A for a proof for the case f3 < 1. The con­
vergence proof for the general case is more involved and
will appear in the full version of this paper.
At the current stage we do not have a proof of conver­
gence for the synchronous case, although in all our experi­
ments, synchronous updates converge as well.

A key question is how to initialize this procedure, as dif­
ferent initialization points can lead to different solutions.
We now describe a deterministic annealing like procedure
[8, 13]. This procedure works by iteratively increasing the
parameter f3 and then adapting the solution for the previous
value of j3 to the new one. This allows the algorithm to
"track" the changes in the solution as the system shifts its
preferences from compression to prediction.4
Recall that when f3 -t 0, the optimization problem tends
to make T1 independent of its parents. At this point the so­
lution consists of essentially only one cluster for each TJ
which is not predictive about any other variable. As we in­
crease (3, we suddenly reach a point where the values of TJ
diverge and show two different behaviors. This phenomena
is a phase-transition of the system. Successive increases
of f3 will reach additional phase transitions in which addi­
tional splits of values of Tj emerge. The general idea of
this annealing procedure is to identify these bifurcations of
clusters. At each step of the procedure, we maintain a set of
values for each T1. Initially, when f3 = 0, each Tj has a sin­
gle value. We then progressively increase f3 and try to de­
tect bifurcations. At the end of the procedure we record for
each T1 a bifurcating tree that traces the sequence of solu­
tions at different values of fJ (see for example Figure 4(a)).
The main technical problem is how to detect such bi­
furcations. We adopt the methods of Tishby et al. [13]
to multiple variables. At each step, we take the solution
from the previous step (i.e., for the previous value of j3 we
considered) and construct an initial problem in which we
duplicate each value of each T1. To define such an ini­
tial solution we need to specify the conditional probabili­
ties of these "doubled" values given each value UJ. Sup4

In detenninistic annealing terminology,

!

is the "tempera­

ture" of the system, and thus increasing /3 corresponds to "cool­
ing" the system.

UAI 2001

FRIEDMAN ET AL.

159

Symmetric compression, Information cur.-es,

0.4

0.8

Compression lactor

(a)

20NG example

0.8

(b)

Figure 4: Application of the symmetric bottleneck to the 20 newsgroup data set with 300 informative words. (a) The
learned cluster hierarchy of categories. (b) information curves showing the progression along the information tradeoff
graph. Note that with 16 word clusters we preserve most of the information present in the data.
pose that tj,a and t3,b are the two copies of the value ti.
Then we set P*(tj,a I uj)
P(tj I uj) (! + o:E(uj))
and P*(tj,b I uj)
P(tj I uj) {!- n:E(uj)) where
E(uj) ,.., U[-!, !J is a noise term and 0 < a :::; 1 is a scale
parameter. Thus, each copy tj,a and tj,b is a perturbed ver­
sion of ti. If (3 is high enough, this random perturbation
suffices to allow the two copies of t1 to diverge. If (3 is too
small to support such bifurcation, both perturbed versions
will collapse to the same solution.
After constructing this initial point, we iteratively per­
form the update equations of (7) until convergence. If at
the convergence point the behavior of fj,a and tj,b is iden­
tical, then we declare that the value t3 has not split. On the
other hand, if the distribution P(tj,a I uj) is sufficiently
different from P(tj,b I Uj) for some values of uj. then we
declare that the value ti has split, and incorporate lj,a and
fj,b into the bifurcation we construct for T1 . Finally, we
increase (3 and repeat the whole process.
We stress that this annealing procedure is heuristic in na­
ture. We do not have formal guarantees that it will find the
global optima. Nonetheless, it has the distinct advantage in
that charts the behavior of the system at different values of
=

=

(3.

An alternative and simpler approach that proved useful
for the original IB formulation employed agglomerative
clustering techniques to find a bifurcating tree in a bottom­
up fashion [9]. Such an approach can also applied to the
multivariate case and will be presented elsewhere.
9

Examples

To illustrate the ideas described above, we now examine
few applications of symmetric and parallel versions of the
bottleneck.
As a simple synthetic example we produced a joint prob­
ability matrix P(A, B) (see Figure 3(a)) where IAI
80
and IBI = 20. Using the symmetric compression cd2) of
Example 6.2) we find 6 natural clusters for A and 3 nat­
ural clusters for B. Sorting the joint probability matrix
=

by this solution illustrates this structure (Figure 3(b)). It
is also interesting to see the fraction of information pre­
served by our clusters. One way of presenting these results
is by considering the fraction of information preserved by
TA about B, and analogously, the fraction of information
preserved in TB about A. This amount of preserved infor­
mation should be plotted with respect to the compression
factor, i.e., how compact is the new clusters representation.
This is given of course by I(TA; A) and I(T8; B) respec­
tively. In Figure 3(c) we present these two information
curves. In both curves we see that splitting the current clus­
ters set increase the amount of information preserved about
the relevant variable, and simultaneously reduces the corn­
pression (since more clusters induces less compression). In
the first split we find 2 clusters in TA and 2 in T8. The sec­
ond split results with 4 clusters in TA and 3 in T8. The next
split finds 6 clusters in TA and leaves TB with 3 clusters.
This is indeed the "real" structure of this data. Interestingly,
due to this last split, the information TB preserve about A
is increased, though there was no split in T8. The rea­
son, of course, is that TB predicts A through TA, thus the
split in TA increases I(TB; A). On the other hand, since
there was no split (at this step) in TB, I(TA; B) remains
unchanged. The next splits are practically overfitting ef­
fects and accordingly there is no real information gain due
to these splits.
As a more realistic example we used the standard 20
newsgroups corpus. This natural language corpus con­
tains about 20, 000 articles evenly distributed among 20
USENET discussion groups [5] and has been employed for
evaluating text classification techniques (e.g., [12]). Many
of these groups have similar topics. Five groups discuss
different issues concerning computers, three groups discuss
religion issues, etc. Thus, there is an inherent hierarchy
among these groups.
To model this domain in our setting, we introduce two
random variables. We let W denote words, and C denote a
category (i.e., a newsgroup). The joint probability P(w, c)

FRIEDMAN ET AL.

1 60

Parallel comprusion,

lnfOfm�tton C\Jr'¥88. 20NG example

0.4
0.6
Comprusion lector

0.8

Figure 5: Information curves for parallel compression of
the 20 newsgroup data set showing the progression along
the information tradeoff graph.

sion variables, T1 , w , T2,w• that try simultaneously (and in­
dependently), to cluster the set of words W in a way that
will preserve the information about the category variable
C, as high as possible. In Figure 5, we present the infor­
mation curves for these two cluster sets. Clearly, using the
combination of the compression variables is much more in­
formative than using each one of them independently. For
example, after the second split, I Tt , w l = IT2 ,w l = 4, and
I(T1,w; C) and I(T2,wi C) preserve 38% and 20% of the
original information l(W; C), respectively. On the other
hand, at the same stage, I(T1,w, T2,w; C) preserves almost
80% of I(W; C). Thus, only 8 word-clusters are enough to
preserve most of the information about the category vari­
able.
10

is the probability that a random word-position (e.g., word
2 1 8 in document 1 255) in this collection is equal to w and
at the same time the category of the document is c. To
obtain such a joint distribution we performed several pre­
processing steps: We removed file headers, transformed all
words to lower case, and removed stop words and words
containing digits or non-alphanumeric characters. We then
sorted all words by the contribution to the mutual informa­
tion about the category variable. More formally, we sorted
all words by I(w) = P (w) L:cEC P(c I w) log P�(�)) , and
used the subset of the top 300 most "informative" words.
After re-normalization, we had a joint probability matrix
with I W I = 300 and I CI 20.
We first used the symmetric bottleneck algorithm to clus­
ter both dimensions of this matrix into two sets of clus­
ters: clusters of words, Tw. and clusters of categories, Tc.
The hierarchy found in Tc, shown in Figure 4(a) is in high
agreement with the natural hierarchy one would construct.
Additionally, each of the word clusters is in high correla­
tion with one of these category clusters. For example, for
the second word cluster, the 5 most probable words (i.e. the
5 words that maximize P(t w2 I W)), were 'islamic ', 'reli­
gious ', 'homosexual', 'peace ' and 'religion '. Accordingly
P(Tc I tw2 ) was maximized for the "religion" cluster in Tc
(left cluster in Figure 4(a)).
As already explained, the general mapping scheme we
use is a "soft" one. That is, each object could be assigned
to each cluster with some normalized probability. The clus­
tering of C into Tc was typically "hard" (for every c E C,
P(Tc I c) was approximately 1 for one cluster and 0 for
the others). However, the clustering of W into Tw utilized
the "soft" aspect of the clustering to deal with words that
are relevant to several category clusters. Thus, some of the
words were assigned to more than one cluster. For exam­
ple, the word 'Clinton ' was assigned to two different word
clusters dealing with politics. The word 's exual ' was as­
signed to the same two clusters, and also (with lower prob­
ability) to a cluster of words dealing with religious issues.
For the same data we used also the parallel compression
(C(l) of Example 6 . 1 ). In this case we have two compres=

UAI 2001

Discussion

We presented a novel general framework for data analysis.
This new framework provides a natural generalization of
the information bottleneck method. Moreover, as we have
shown, it immediately suggests new bottleneck like con­
structions, and provides generic tools to implement them.
Many connections with other data analysis methods
should be explored. The general structure of the itera­
tive procedure is reminiscent of EM and k-means proce­
dures. Other connections are, for example, to dimension­
ality reduction techniques, such as ICA [2] . The parallel
bottleneck construction provides an ICA-like decomposi­
tion with an important distinction. In contrast to ICA, it is
aimed at preserving information about specific aspects of
the data, defined by the user.
The suggested framework allows us to extract structure
from data in numerous ways. In our examples, we explored
only few relatively simple cases, but clearly this is the tip of
the iceberg. We are currently working on several additional
applications under this framework. These include analysis
of gene expression data, neural coding and DNA sequence
analysis, document clustering, and computational linguistic
applications.
Acknowledgements
This work was supported in part by the Israel Science Founda­
tion (IS F), the Israeli Ministry of Science, and by the US-Israel
Bi-national Science Foundation (BSF). N. Slonim was also sup­
ported by an Eshkol fellowship. N. Friedman was also supported
by an Alon fellowship and the Harry & Abe Sherman Senior Lec­
tureship in Computer Science. Experiments reported here were
run on equipment funded by an ISF Basic Equipment Grant.

References
[1]
[2]
[3]
[4]

L. D. Baker and A. K. McCallum. Distributional clustering
of words for text classification. In ACM SIGIR 98. 1998.

A.J. Bell & T.J. Sejnowski. An information-maximization
approach to blind separation and blind deconvolution. Neur.
Comp. 7, 1 1 29- 1 159, 1995.

T. M. Cover and J. A. Thomas. Elements of lnfonnation
Theory. John Wiley & Sons, New York, 199 1 .

T. Hofmann. Probabilistic latent semantic indexing. In ACM
SIGIR 99, pages 50-57. 1999.

UAJ 2001

FRIEDMAN ET Al.

[5] K. Lang. Learning to filter netnews. In 12th lnt. Conf on
Machine Learning, pages 331-339. 1995.
[6] J. Pearl . Probabilistic Reasoning in Intelligent Systems.
Morgan Kauffman, San Francisco, 1988.
F. C. Pereira, N. Ti shby, and L. Lee. Distributional clustering
of Engli sh words. In 30th Annual Meeting ofthe Association
for Computational Linguistics, pages 1 83-190. 1993.

[7J

[8] K. Rose.

Deterministic annealing for clustering, com­
pression, classification, regres sion , and related optimization
problems . Proceedings of the IEEE, 86:2210-2239, 1998.

[9] N. Slonim & N. Tishby. Agglomerative Information Bottle­
neck. in Advances in Neural Information Processing Sys­
tems (NIPS) 12, pp. 617-623, 1999.

[ 10] N. Slonim, R. Somerville, N. Tishby, and 0. Lahav. Objec­
tive spectral classification of galaxies using the information
bottleneck method. in "Monthly Notices of the Royal As­
tronomical S ociety" , MNRAS, 323, 270, 2001 .

[ 1 1 ] N. Slonim and N. Tishby. Document clustering using word

clusters via the information bottleneck method. In ACM SI­
GIR 2000, pages 208-2 15. 2000.

N. Slonim and N. Tishby. The power of word clusters for
text classificatio n. In 23rd European Colloquium on Infor­
mation Retrieval Research. 2001 .

[ 12]

[ 1 3] N . Tishby, F. Pereira, and W. Bi alek. The information bot­

tleneck method. In Proc. 3 7th Allerton Conference on Com­
munication and Computation.

1999.

[ 14] N. Tishby and N. Slonim. Data clustering by Markovian
relaxation and the information bottleneck method. In Neural
Information Processing Systems (NIPS-00). 2000 .

A

Proofs

We now sketch the proofs of the two main theorems. We start
with Theorem 7 . 1 .
Proof: The basic idea i s to find stationary points of _c(l) subject
to the normalization constraints. Thus, we add Lagrange multi­
pliers and get the Lagrangian
J = IG ;n � {JIG out _ I: · I:
Au; I:e . P(tj I Uj )

u1

J

To differentiate J we use the following

Lemma A.l :

aa;l�I�J)

=

1

lemma.

.

P(uj ) (EP( · I t; .u; ) [log Pg��) ]

�

1).

We now can differentiate each mutual information term that ap­
pears in J. Note that we can ignore terms that do not depend on
the value of T1 , since these will be absorbed by the normaliza­
tion constant. Thus, a term of the form EP( - I t; ,u; ) [lo g P(Y

I Z)]

Z U {Y}

can be ignored. Collecting terms that do
where Tj ¢
refer to tj , equating to 0, and dividing by P (u1 ) we get the fol­
lowing equation.

log P(ti )
+/3
--;

+/3

I:

EP ( · It;, u; ) [log P(X; I Vx; ) ]

I:

EP( · It; ,u; ) [log P (Tt I Vrt )l

i:T; EVx;
l:T; EVTt

+f3EP ( · I t; ,u; ) [log

(8)

Cj Uj )

where
(
is a term that does not depend on t; .
To get the desired form of the self-consistent equations,
we apply several manipulations.
First, we can write

EP(·It; ,u; ) (log P ( X;

y;_T; , tj )]
'

I

Vx; )]

since all the variables in

EP( - I u; ) (l og P(X;

y;_�;
.

i.

T; given U
Second, we can transform the latter term into
a KL divergence by extracting the term - EP (- I u; ) [log P(X; I

v;_T; . Uj )) from Cj ( uj ) .
te�s that deal with Tt .

EP(- I t ; , u; ) [log

P(Vr; )

Similar transformation applies to the

P(t; IVT- )
P (t;J ]

Third, we use Bayes rule to rewrite
=

P(VT· It; )
EP ( · I t; ,u; ) [log P(V T l ) .
;

I

are independent of

.

S mce

does not involve tJ we can ignore it. To get a KL di­

vergence term, we subtract the term

EP (· I t;, u ; J [log Vr,

I Uj]­

Finally, w e apply the normalization constraints for each distribu­
tion P(TJ Uj ) to get the desired equations. I

I

We now turn to the proof of Theorem 8. 1 .
Proof: To prove convergence it suffices to prove that unless
we are at a stationary point, each application of the assignment

G

G

Eq. (7) reduces the Lagrangian _c(l) [P] = I '" � {JI ""' . Re­
call, that we require that P
Also , recall that when {3 < 1 ,
minimizing this Lagrangian is equivalent to minimizing the La­

f= Gin .

.C ( 2l [P]

grangian

= IG 'n

�D(PI Gout ) .

+

To show convergence, we will introduce an auxiliary La­

D( P I R) + �D(PIQ) subject to the
Q f= Gout. and R f= G0, where
G0 is the DAG without edges. It is easy to see that .C(2) and :F
TJ; P(X; I V x, ) TJ i P (Tj I VT; ) and
coincide when Q
R
TI, P (X;) TI; P(TJ ) That is, when Q and R are the KL­
projections of P onto the space of distributions consistent with
Gout and G;n .
Using properties of KL-projections, we get.
Lemma A.2 : For any choice of P, R and Q that are consistent
with G;n, Gout and G0, respectively, t.:C2l [P] :5 :F [P, Q, Rj, with
equality if and only if Q and R are the projections of P onto G out
grangian:

F[P, R, Q)
P f=

constraints that

=

G;n,

=

=

and

.

G0. respectively.

Assume that Q and R are fixed, and suppose we want to modify
P(TJ I UJ ) to minimize F. Taking derivatives of F with respect
to P(t;
and equating to 0, we get the following self consis­
tent equations:

I Uj )

P(t .
J

I

u )
J
·

=

R (tj )
Z({3, uj ) e

P' (EP( lt; , u; J [Iog Q]-Iog R(t; ) )

It is easy to verify that the right hand side of this equation does
not depend on P(ti I Uj ) . Thus, the assignment

P' (tJ· I

·)

uJ

t-

R ( tj )
Z(/3, UJ ) e

P' {EP(·It;.uj l [logQ]-Iog R(t;))

and P' (tt I Ut) t- P(tl I Ut ) for e i= j results in a distri­
bution P' such that :F[ P' , Q , R] is a stationary point with re­
spect to changes in P(T; I Ui ) . Moreover, it is easy to ver­
ify that the second derivative of :F with respect to P (tJ / Uj )
is positive, and thus this point is a local minima. We conclude
that :F[P' , Q, R] ::; F[P, Q , R] , with equality if and only if
P (Tj I U1 ) minimizes :F (with respect to fixed choice of Q,
R, and P(Tt I Ut) for e -1- j).
We now put all these together. S uppose that Q and R are the
projections of P on Gout and G0, respectively. Then,

d2l [P'] :5 :F[P' , Q , R] :5 F[P, Q, R]

P(ti i Vr; )
]
P(t; )

+ci (uJ )

1 61

=

d2l [PJ.

Moreover, we have equality only if P' = P. This shows that an
update step reduces the value of the Lagrangian. The only situa­
tion where the value remains the same is when the self consistent
equation for P(TJ I Uj ) is satisfied.
The only remaining issue is to show that this iteration is equiv­
alent to the asynchronous iteration of Eq. (7) when Q and R are
the projections of
on Gout and G 0 , respectively. This is can be
easily verified by comparing to Eq. (8). I

P

