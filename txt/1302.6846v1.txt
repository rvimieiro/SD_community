538

A probabilistic approach to hierarchical model-based diagnosis

Sampath Srinivas*
Computer Science Department
Stanford University
Stanford, CA 94305
srinivas@cs.stanford.edu

Abstract
Model-based diagnosis reasons backwards
from a functional schematic of a system to
isolate faults given observations of anoma­
lous b ehavior. We develop a fully proba­
bilistic approach to model based diagno­
sis and extend it to support hierarchical
models. Our scheme translates the func­
tional schematic into a Bayesian network
and diagnostic inference takes place in the
Bayesian network.
A Bayesian network
diagnostic inference algorithm is modified
to take advantage of the hierarchy to give
computational gains.

1

INTRODUCTION

Fault diagnosis in engineering systems is a very im­
portant problem. The problem is as follows: From
observations of anomalous behavior of a system one
has to infer what components might be at fault.
Diagnosis fundamentally involves uncertainty.
For any reasonable sized system, there is a very
large number of possible explanations for anoma­
lous behavior. Instead of reasoning with all of them
we want to concentrate on the most likely explana­
tions. In this paper we describe a method for doing
model-based diagnosis with a fully coherent proba­
bilistic approach. To do so, we translate the system
model into a Bayesian network and perform diag­
nostic computations within the Bayesian network.
We then extend the notion of system models to
include hierarchical models. Hierarchical composi­
tional modeling is an all pervasive technique in en­
gineering practice. It allows modularization of the
modeling problem, thus aiding the modeling process.
In addition, the hierarchy allows gains in computa-

* Also with Rockwell International Science Center,
Palo Alto Laboratory, Palo Alto, CA 94301.

tional tractability. We show how this improvement
in tractability extends to diagnosis by describing a
hierarchical version of a Bayesian network inference
algorithm which takes advantage of the hierarchy in
the model to give computational gains.

2

THE TRANSLATION SCHEME

In this section we describe how the Bayesian network
is created from the system functional schematic.
The system functional schematic consists of a set
of components. Each component has a set of dis­
crete valued inputs I1, !2, . . . , In and a discrete val­
ued output 0. The component also has a discrete
valued mode variable M. Each state of M is asso­
ciated with an operating region of the device. Each
state of M is associated with a specific input-output
behavior of the component.
The component specification requires two pieces
of information- a function F : /1 X h . . . In X M -+ 0
and a prior distribution over M. The prior distri­
bution quantifies the a priori probability that the
device functions normally. As an example, a com­
ponent might have only two possible mode states
broken and ok. If it is very reliable we might have
a very high probability assigned to P(M = ok), say
0.995. The components are connected according to
the signal flow paths in the device to form the system
model ( we do not allow feedback paths) .
A Bayesian network fragment is created for a
component as follows. A node is created for each
of the input variables, the mode variable and the
output variable. Arcs are added from each of the
input variables and the mode to the output variable.
The distribution P(OI/t, h . . . , In, M) is specified
1
by the component function F. That is , P( 0 =
ol/1 = i1,/2 = i2, ... ,/n = in,M = m) = 1 iff
F(it, i2, . . . , in, m) = o. Otherwise the probability is
0. The variable M is assigned the prior distribution
1

We use

x

to denote a state of a discrete variable X.

A Probabilistic Approach to Hierarchical Model-based Diagnosis

given as part of the component specification.
The network fragments are now interconnected

as follows: W henever the output variable 0 1 of a
component C1 is connected to the input !2 of a

C2 ,
01 of C1

component

1

an arc is added from the output

node
to the input node
of C2. This
arc needs to enforce an equality constraint and so
we enter the following distribution into node !2·.

P{Ij

IJ

= pl01 = q) =

)

1 iff p = q, otherwise the
probability is 0. After interconnecting the Bayesian
network fragments created for each component we
have a nearly complete Bayesian network. We now
make some observations. The network created is in­
deed a DAG (and hence fulfills one of the necessary
conditions for us to claim it is a Bayesian network).
This is so because we did not allow any feedback in
the original functional schematic.
The probability distribution for every non-root
node in the Bayesian network has been specified.
This is because every non-root node is either (a)
an output node or (b) an input node which is con­
nected to a preceding output node. The probability
distribution for every output node has been specified
when creating the Bayesian network fragments. The
probability distribution for every input node which
�as an output node as a predecessor has been spec­
tfied when the fragments were interconnected.
The root nodes in the network fall into two
classes. The first class consists of nodes correspond­
ing to mode variables and the second class consists of
nodes corresponding to some of the input variables.
We note that the marginal probability distributions
of all nodes in the first class (i.e, mode variables)
have been specified.
The set of variables associated with this second
class of nodes are those variables which are inputs
to the entire system - that is, these variables are
inputs of components which are not downstream of
other components. We will call this set of variables
system input variables. Let us assume that the in­
puts coming from the environment to the system are
all independently distributed. Further let us assume
for now that we have access to a marginal distribu­
tion for each system input variable2. We enter the
marginal distribution for each system input variable
into its corresponding node. We now have a fully
specified Bayesian network.
Consider the original functional schematic. We
can interpret every component function and inter­
connection in the original functional schematic as
a constraint on the values that variables in the
schematic can take (in the constraint satisfaction
2

If every observation of the system is guaranteed to

contain a full specification of the state of the input, then
the actual choice of priors is irrelevant [Srinivas94].

539

sense). We note that the Bayesian network that we
have constructed enforces exactly those constraints
that are present in the original schematic and no
others. Further, it explicitly includes all the infor­
mation we have about marginal distributions over
the mode variables and the system input variables.
The Bayesian network is therefore a representation
of the joint distribution of the variables in the func­
tional schematic and the mode variables.
We proceed now to use the Bayesian network
for diagnosis in the standard manner. Say we make
an observation. An observation consists of observing
the states of some of the observable variables in the
system. As an example, we might have a observa­
tion which consists of the values (i.e., states) of all
the system input variables and the output values of
some of the components. We declare the observa­
tion in the Bayesian network. That is, we enter the
states of every observed variable into the Bayesian
network and then do a belief update with any stan­
dard Bayesian network inference algorithm (for ex­
ample, (Lauritzen88],(Jensen90]).
Say an observation 0

Y2, ... , Yk

=

=< Yt = Y1, Y2 =
Yk > has been made. After a Bayesian

network algorithm performs a belief update we have
the posterior distribution

P(XIO)

available at ev­

ery node X in the Bayesian network. The posterior
distribution on each of the mode variables gives the
updated probability of the corresponding component
b eing in each of its modes. This constitutes diagno­
_

SIS.

We illustrate our scheme with a simple exam­
ple from the domain of Boolean circuits. The circuit
is shown in Fig 1(a). We treat this circuit as our

input functional schematic. A particular observa­
tion (i.e., input and output values) is marked on the
figure. We note that if the circuit was functioning
correctly the output for the marked inputs should

be 0. Instead the output is a 1. We assume, for this
example, that each gate has two possible states for
the mode variable, ok and broken. The modeler
provides a prior on the mode of each gate - for each
�ate the prior probability of it being in the ok state
IS shown next to it in Fig l(a). We also require a
full fault model- i.e., for each gate we should have a
fully �pecified function relating inputs to the output
even 1f the mode of the gate is broken. We assume

a "stuck-at-0" fault model - i.e., if the gate is in
state broken the output is 0 irrespective of what
the input is. W hen the gate is in state ok the func­
tion relating the inputs to the output is the usual
Boolean function for the gate.
The Bayesian network corresponding to this
schematic is shown in Fig 1{b). We assume that
the inputs are independently distributed. We also

540

Srinivas

11

mation to complete the hierarchy - she has to relate
the modes of the subcomponents to the modes of the
component. To make this more concrete, consider a
component which has two states for its mode vari­
able
ok and broken. Say that it is modeled at
a lower level of detail with 4 subcomponents, each
of which has two possible states. If we consider the
possible combinations of mode states at the lower
level of abstraction there are 24 = 16 possibilities.
However at the higher level of abstraction there are
only two possibilities, i.e., the granularity is not fine
enough to distinguish individually between the 16
different possibilities at the lower level.
To relate the lower level to the higher level the
modeler has to provide a function describing how
the lower level combinations of mode states relate
to the higher level mode state. In other words, the
modeler has to provide a categorization which sep­
arates the lower level state combinations into a set
of bins. Each bin corresponds to one of the states of
the mode variable at the higher level of abstraction.
This function could be a simple rule. One possibil­
ity, for example, is the rule "If anything is broken at
the lower level then consider the component broken
at the higher level". This means, in our example,
that 15 possibilities at the lower level fall into the
broken bin at the higher level while only 1 possi­
bility (i.e., no subcomponents broken) falls into the
the ok bin at the higher level.
Once this function is specified the hierarchical
model is complete. We will call this function the
abstraction function. Note that we can have mul­
tiple levels of hierarchy. We also note two salient
points - the modeler does not need to provide a
component function at higher levels of the hierar­
chy. In addition the modeler does not need to pro­
vide a prior on the mode variable at higher levels of
hierarchy. In other words, if a component is mod­
eled at a lower level of detail then only the low level
functional schematic and the abstraction function
are required. The component function and prior are
required only for a component which is being mod­
eled "atomically", i.e., it is not being modeled at any
finer level of detail.
As an example of hierarchical modeling, con­
sider an exclusive-OR (XOR) gate. We might rep­
resent the XOR gate at a lower level of detail and
show that it is synthesized using AND gates, OR
gates and inverters (Fig 2). We use the following
rule as the abstraction function: "If anything is bro­
ken at the lower level then the X 0 R gate is broken".
-

Key:

�

�

Prior

(a)

Posterior

P(M=Ok}

Figure 1: An example: (a) functional schematic (b)
corresponding Bayesian network.

assume a uniform distribution as the prior for each
of the inputs h, h and 13. Note that in this exam­
ple, any (strictly positive) prior could be assumed
without affecting the results of the diagnosis. This
is because the state of the input is fully known when
the diagnosis is performed. The observation is en­
tered into the network and inference is performed.
The posterior probability of being in the ok state
for each gate is as shown in Fig 1(a).

3

HIERARCHICAL MODELS

Consider a situation where the modeler has con­
ceptually broken up an engineering artifact into a
set of component subsystems. She would probably
not have a complete functional description (i.e., the
function relating inputs to outputs) at this level of
abstraction Each of the component systems has to
be modeled at a lower level of detail. We extend our
scheme to support such a feature. The modeler first
fully specifies the inputs, output and the mode vari­
able of the component. By full specification we mean
that the modeler specifies the number of inputs, the
possible states of each input variable, the possible
states of the output variable and the possible states
of the mode variable.
If the modeler would now like to model the com­
ponent at a lower level of abstraction she can specify
a new functional schematic as a detailed description
of the component. This new functional schematic
would have new components (we will call them sub­
components) which are interconnected to form a
functional schematic. This lower level schematic is
constrained in the following way: The system input
variables of this functional schematic should be the
same as the input variables to the component speci­
fied at the higher level. Similarly the system output
variable of the schematic should be the same as the
component output variable at the higher level.
The modeler has to provide a final piece of infor.

A Probabilistic Approach to Hierarchical Model-based Diagnosis

.... ....

\
\
\

.... ....

... ....

541

....

Xl

n.,.._-e--t----1

112

\

........
. �
.. ---1
X2

0

Inv2 X4 r:::
. ... ::�

�

Figure 2: An XOR gate: An example of a hierarchi­
cal schematic.

3.1

INCORPORATING HIERARCHY IN
THE TRANSLATION

When a component is modeled at a lower level,

the translation proceeds as follows: Assume that
the higher level abstraction does not exist and just
plug in the lower level functional schematic be­
tween the system inputs and outputs and do the
translation. In the resulting Bayesian network in­
troduce a new variable for the higher level mode.
Call this Mh. Add an arc from the mode vari­
able of each of the subcomponents to the higher
level mode variable. Call the lower level mode vari­
ables M11, M12, ... , M1n. Fill out the conditional
probability distribution of the higher level mode
variable as follows: P(mhlm11,m12, ... m1.-.) = 1 iff
m" = Ab(m11, m12,
, m1n ) , 0 otherwise. Here Ab
is the abstraction function relating combinations of
mode states of the subcomponents to the mode of
the higher level component. Fig 3(a) shows the
Bayesian network for the XOR gate example.
Hierarchical models usually have two major and
related advantages. The first advantage is that mod­
eling becomes easier. This is because the system is
decomposed in a compositional fashion into compo­
nents with well defined boundaries and interactions.
The second advantage is that computation with the
model becomes easier. As a first cut, diagnosis with
a hierarchical functional model can proceed exactly
as described with non-hierarchical models. If we
want a fine grain diagnosis we look at the updated
posterior probabilities of the subcomponent modes.
If we want a coarse grained diagnosis we look at the
updated posterior of the mode variable of the com­
ponent at the higher level of abstraction. However,
this simplistic solution does not get any computa­
tional gains from the hierarchy.
To get computational gains we need to be able
to reason with the higher level model in a way such
•

.

•

Figure 3: (a) Bayesian network fragment for XOR
schematic (b) The fragment after "compilation".
that the detail of the lower level model has been
"compiled away" into a more succinct higher level
model. We now describe a scheme for doing so.
Consider a component C" which is modeled at a
lower level of abstraction with a functional schematic
consisting of subcomponents C11, C12, ... , C1n. The
mode variable of C" is Mh and the mode variable of
subcomponent cu is M1;. Let the inputs of C" be
If, I�, ... , I�. Let the output of Ch be Qh. Let all
the internal variables of the lower level functional
schematic (i.e., the input and output variables of
the subcomponents excluding the system inputs and
outputs) be X1, X2, ... , XkFor simplicity, let us assume that all the inputs
of ch are system inputs- i.e., there are no compo­
nents upstream of C". We also assume, as described
before, that we have a prior on each system input.
Now consider the Bayesian network fragment cre­
ated by the translation scheme for Ch. We note
that this fragment happens to be a fully specified
Bayesian network.
A Bayesian network is a structured representa­
tion of the joint distribution of all the variables in
the network. In this case the network is a represen­
tation following distribution P( If, Ig, ..., It:,, O",
, M1n, X1, X2, .. ., Xk ) . Call
Mh , M11, l>.lf12,
this the lower level distribution.
If now, we wanted to have a Bayesian network
representation at the higher level of abstraction we
would not want to explicitly represent the detail
about internal variables of the lower level functional
schematic or the mode variables of the subcompo­
nents. In other words we would like to have a
Bayesian network which represents the joint distri­
bution only the input, mode and output variables of
C", i.e., the distribution P(If, Ig, ...,I�, O", Mh) .
Call this the higher level distribution.
We can generate the higher level distribution
from the lower level distribution by simply marginal­
izing out all the irrelevant variables, vi z , M 11,
.

.

•

542

Srinivas

Figure 4: A hierarchical schematic.

M12,... , M1n, X1, X2, . . . , X�:. Ideally, we should
do this marginalization in some efficient way. Such
efficient marginalization is possible using topological
transformations of Bayesian networks [Shachter86].
Specifically, we can use the arc reversal and node
absorption operations as follows:
1. Successively reverse the arcs

M12 ...... Mh, ... , M1"
this step M h is a root

.......

M11 ...... Mh,

Mh. At the end of

node.

2. Let X be the set of internal variables of the
lower level functional schematic, i.e., X
{ M11, M12, .. . , M1", X 1, X2, . .. , X k}. Sort X
into a sequence X, eg in inverse topological or­
=

der (descendants first). Successively absorb the
nodes in Xuq (in order) into Oh.
This completes the process and leaves us with
the topology shown in Fig 3(b) . The successive ab­
sorption in the last step is always possible since there
is no node N in the Bayesian network such that (a)
N is not in X, eq and (b) the position of N has to nec­
essarily be between two nodes contained in X5eq in a
global topological order [Shachter86]. Note that the
topology which results from the marginalization pro­
cess described above is the same as the one we would
get if we had directly modeled ch as an atomic com­
ponent.
For simplicity of exposition, the description
above assumes that Ch's inputs are system inputs.
However, this assumption is unnecessary. The iden­
tical marginalization process is possible for any hi­
erarchically modeled component. We can consider
the marginalization process that gives us the higher
level distribution as a "compilation process" which
is carried out after the model is created.
3.2

INTEGRATING HIERARCHY AND
DIAGNOSTIC INFERENCE

The hierarchy in the functional schematic can
be exploited to improve diagnostic performance. We

Figure 5:
Bayesian
schematic of Fig 4.

network

Be

created

from

now describe a method of tailoring the cluster­
ing algorithm [Lauritzen88, Jensen90, Pearl88] for
Bayesian network inference to take advantage of the
hierarchy. This is the most widely used algorithm
in practice. The clustering algorithm operates by
constructing an tree of cliques from the Bayesian
network as a pre-processing step. This construction
is by a process called triangulation [Tarjan84]. The
resulting tree is called the join tree. Each clique has
some of the Bayesian network nodes as its members.
As evidence arrives, a distributed update algorithm
is applied to the join tree and the results of the up­
date are translated back into updated probabilities
for the Bayesian network nodes. The update process
mentioned above can be carried out on any join tree
that is legal for the Bayesian network.
We will now describe a method of constructing
a legal join tree that is tailored to exploit the hierar­
chy. We explain by means of an example. Consider
the hierarchical functional schematic shown in Fig 4.
This results in the hierarchical Bayesian network Be
shown in Fig 5.
After the lower level detail is compiled out we
get the network Bh in Fig 6(a). We add a dummy
node D h to this Bayesian network such that Ms, Is
and 03 are parents of Dh. If we run a triangula­
tion algorithm on this network we get a join tree
JTh (Fig 7(a)). We note there exists a clique 8h in
JTh such that I3, Ms and 03 belong to 8h. This is
because I3, Ms and Os are parents of Dh. Triangu­
lation guarantees that a Bayesian network node and
its parents will occur together in at least one clique
in the join tree.
Now consider the lower level network fragment
by itself (Fig 6(b)). Call this B 1• Say we create a
dummy node D1 and add arcs into it from Is, Ms
and 03 as shown in the figure. If we triangulate the
graph we get a join tree JT1 (Fig 7(b)). Once again,

543

A Probabilistic Approach to Hierarchical Model-based Diagnosis

1_3, 1_4-1. M_3, 0-3, IY'h
_r---""""'�-)----<.o•h ---

M_4, 1_4-1, 1_4·2. 0_4

0_3, 1_4·1, 1_4-2

M..1.U,0-1

0.1.

C4-1, U

U.L4-1,U

o_z, L2, 1_4-1
(a)

, ..;��11

.1- -�'
1
.:- ...

,..."'

0_3, 1_3, M_3, D•t

0_3I.-2, 1_3, M_3, M_3I.-2, L3L-2
,_

0-__

...::�--------------......
-- -

__

l_3L-2, M_3, 1_3, M_3L-2, 0_3L-1

0

�----·�-

--0

M_3L-2. 0_3L-l, 1_3, M_3I.-1, M_3

3 I 3 M 3 0 3L-2

(')_____
- ---o

--·

M_3 L- 1 , 1_3, 1_3I.-1, 0-3L-1

(b)

(b)

Figure 6: (a) Compiled network Bh. (b) Lower level
Bayesian network fragment B 1 •

we

are

guaranteed that there is

a

clique 61 in JT1

such that !3, M3 and 03 belong to 81.
Now we construct a composite join tree JTc
from JTh and JT1• This is done by adding an link
from bh to 61 (shown as a dotted line in Fig 7). This
composite join tree is a valid join tree for the network
Be shown in Fig 5 (see next section for proof).
The composite join tree JTc has the following
interesting property. If the user is not interested in
details about the lower level nodes, then the update
operation can be confined purely to the JTh segment
of the join tree since only JTh has any variables
of interest. More precisely, if there is no evidence
available regarding the states of the lower level nodes
and in addition, the user is not interested in details
of the lower level nodes posterior distributions, then
the update can be confined to JTh.
Now suppose the user has finished an update in
JTh. She then decides that she does want to view
more detail. In that case, the update process can be
restarted and continued locally in JT1• That is, the
update process through the whole of JTh need not
be repeated - the information coming from the rest
of JTh is summarized in the message that 8h sends 81
when the update process is restarted. The restarted
update process, in fact, is an incremental update
which occurs only within JT1• This incremental up­
date can be performed at the user's demand- for ex­
ample, in a graphical interface, the user may "open
the window" corresponding to a "iconified" compo­
nent. This can be interpreted as a request for more

JT•l

Figure 7: (a) JTh (b) JT1• Adding the link shown
as a dotted line creates the composite tree JTc.

detailed information.
Along similar lines, if the user discovers evi­
dence pertaining to a subcomponent, then she can
"de-iconify" the containing component and assert
the evidence. In this case, the update process be­
gins in JT1 and proceeds through JTh to make a
global update. If one has multiple levels of hierar­
chy, the composite join tree has multiple levels of
hierarchy too. At any time, the update process only
affects that segment of the join tree that the user
is interested in. This gives substantial savings in
computation.
The dummy nodes D11 and D1 have been used
only for ease of presentation. In practice, one only
has to ensure that the join tree algorithm forces the
nodes of interest to occur together in at least one
clique.
3.3

JTc IS A VALID JOIN

TREE

A valid join tree is constructed for a Bayesian net­
work B as follows [Pearl88):
( 1) The Bayesian network B is converted into a
Markov network G by connecting the parents of each
node in the network and dropping the directions of
the arrows in the DAG. G is an undirected graph.

(2) A chordal supergraph d is created from G by
a process called triangulation. A chordal graph is
one where any loop of length 4 or more has a chord
(an arc connecting two non-consecutive edges in the
loop). Basically, the triangulation process adds arcs
to the G until it becomes chordal. (3) The maximal

544

Srinivas

cliques of the chordal graph d are assembled into
a tree JT. Each maximal clique is a vertex in the
tree. The tree has the join tree property. The join
tree property is the following: For every node n of
B, the sub-tree of JT consisting purely of vertices
which contain node n is a connected tree.
It can be proved that JTc is a valid join tree
for the Bayesian network Be. We do so by first
describing the construction of a particular chordal

(similarly, JT1). The only nodes which appear in
both Bh and B1 are M3, l3 and 03. Since these
nodes appear both in oh and 81 we see that the run­
ning intersection property holds for them too. O
Theorem: JT0 is a valid join tree for the Bayesian
network Be.
Proof: This follows directly from Lemmas 1 and 2.

supergraph cc of the Markov network of Be. JTC

The dummy nodes Dh and D1 are present solely to
force a particular topology on the join t rees JTh
and JT1• After the triangulation process they can
be dropped from the cliques which contain them.
This might sometime result in a simplification of the
composite join tree. Consider the case where 61 is
reduced to {Ma, h Oa} after D1 is dropped In this
situation, 81 can be merged with oh since it is a sub­
set of 8h. Similarly oh can be merged with 61 if 81
reduces to {M3,/3, 03} after oh is dropped. JTc

'

'
is a valid join tree constructed from ac . We have
included proof sketches below, the full proof is in
[TechReport94].
Consider a graph ac' constructed as follows: Bh
is converted into a Markov network Gh. Similarly,
B1 is converted into a Markov network G1. Each of
these networks are triangulated giving the chordal
graphs ah' and G1'. ah' and G1' are merged to form

'

a graph ac . This "merging" of the graphs is don e as
follows: The nodes

M3,

13 and 03 in Gh' are merged

0

.

continues to be a valid join tree after such mer gers.

with with the corresponding nodes in G1'. That is,
ac' has only one copy of each of these nodes. Any
link between any of these nodes and

a

node in ah' is

also present in ac'. Similarly any link between any
of these nodes and a node in 01' is also present in
ac'.
Lemma 1: ac' is a chordal supergraph of a Markov

network ac of Be.

Proof sketch: We note that the nodes in the set S
= {M3, 13, 03} are the only nodes common to the
'

subgraphs Gh' and G1 . Any loop L that lies partly
in 01' and cc' has to necessarily pass through S
twice. We see that in the M3, Is and 03 are nec­
essarily connected to each other in both Gh' and

G1'. Hence the loop L has a chord that breaks it
into two subloops Lh and L1 which lie purely in the

chordal graphs Gh' and G1' respectively. Hence cc

'

is chordal. It is easily proved that ac' is a super­
graph of a Markov netw ork ac of B". o
Lemma 2: JTC

'
is a valid join tree created from cc .

Proof sketch: We note that any maximal clique in

G1' which contains at least one node n which does
'
not occur in Gh' is also a maximal clique in ac .

We now observe that e1;ery maximal clique in G1'
contai ns at least one node which does not occur in
Gh'. We make a similar argument for the maximal

cliques of ah'. This implies that vertices of

JTC are

the maximal cliques of cc'. We note that the run­
ning intersection property (r.i.p) holds for any node
n of Be which appears ::;olely in Bh (similarly, B1
since n appears purely in the vertices of true in JT

�

4

RELATED WORK

Geffner and Pearl [Geffner87] describe a scheme for
doing distributed diagnosis of systems with multiple
faults. They devise a message passing scheme by
which, given an observation, a most likely explana­
tion is devised. An explanation is an assignment of
a mode state to every component in the schematic.
The translation scheme described in this paper can
be used to achieve an isomorphic result. That is,
instead of using a Bayesian network update algo­
rithm to compute updated probabilities of individ­
ual faults we could use a dual algorithm for comput­
ing composite belief [Pearl87] and compute exactly
the same result. From the perspective of this pa
per, [Geffner87] have integrated the inference in the
Bayesian network into the schematic as a message
passing scheme. Separating out the network trans­
lation explicitly allows features such as hierarchical
diagnosis, computation of updated probabilities in
individual components as against component beliefs
and many others (see below).
Mozetic [Mozetic92] lays out a formal basis for
diagnostic hierarchies and demonstrates a diagnos­
tic algorithm which takes advantage of the hierar chy.
The approach is not probabilistic. However, he in­
cludes a notion of non-determinism in the following
sense: Given the mode of a component he allows the
input-output mapping of a component to be relation
instead of a function - there can be multiple possible
outputs for a given input. The notion of hierarchy
we have described here co r responds to one of three
possible schemes of hierarchical modeling that he de­
scribes. Our scheme can be expanded to support a
­

A Probabilistic Approach to Hierarchical Model-based Diagnosis

probabilistic generalization of the other two schemes
of modeling and his notion of non-determinism.
Genesereth [Genesereth84] describes a general
approach to diagnosis including hierarchies. He dis­
tinguishes between structural abstraction and be­
In structural abstraction a
havioral abstraction.
component's function is modeled as the composi­
tion of the functions of subcomponents whose de­
tail is suppressed at the higher level. This is similar
to what we have described. Behavioral abstraction
corresponds to a difference in how the function of
a device is viewed - for example, in a low level de­
scription of a logic gate one might model input and
output voltages while a high level description might
model them as "high" and "low'. Behavioral ab­
straction often corresponds to bunching sets of in­
put values at the low level into single values at the
higher level. Our method extends to support such
abstractions in a straightforward manner.
Yuan [Yuan93] describes a framework for con­
structing decision models for hierarchical diagnosis.
The decision model is comprised of the current state
of knowledge, decisions to test or replace devices
and a utility function that is constructed on the fly.
A two step cycle comprising model evaluation and
progressive refinement is proposed. The cycle ends
when the fault is located (a single fault assumption is
made). Model refinement is in accordance with the
structural hierarchy of the device. The goal is to pro­
vide decision theoretic control of search in the space
of candidate diagnoses. Such a framework needs a
scheme for computing the relative plausibility of can­
didate diagnoses. Our work provides such a scheme
in a general multiple fault setting.

5

CONCLUSION

The translation scheme described in this paper is
a first step in an integrated approach to diagno­
sis, reliability engineering, test generation and op­
timal repair in hierarchically modeled dynamic dis­
crete systems. The approach is probabilistic/utility­
theoretic. We have made variety of assumptions
in this paper for simplicity of exposition. The as­
sumptions are: (a) non-correlated faults (b) full
fault models (c) fully specified input distributions
(d) components with single outputs (e) restricted
form of hierarchy and (f) systems without dynam­
ics or feedback. Each of these are relaxed in the
general approach [Srinivas94]. We also discuss the
temporal aspect of the "prior probability of failure"
notion and relate it to standard quantities found in
the reliability literature.

545

References
[Geffner87] Geffner, H. and Pearl, J. (1987) Dis­
tributed Diagnosis of Systems with Multiple
Faults. In Proceedings of the 3rd IEEE Confer­
ence on AI Applications, Kissimmee, FL, Febru­
ary 1987. Also in Readings in Model based Diag­
nosis, Morgan Kauffman.
[Genesereth84] Genesereth, M. R. (1984) The use of
design descriptions in automated diagnosis, Arti­
ficial Intelligence 24, pp. 411-436.
[Jensen90] Jensen, F. V., Lauritzen S. L. and Olesen
K. G. (1990) Bayesian updating in causal proba­
bilistic networks by local computations. Compu­
tational Statistics Quarterly 4, pp 269-282.
[Lauritzen88] Lauritzen, S. L. and Spiegelhalter,
D. J. (1988) Local computations with probabili­
ties on graphical structures and their applications
to expert systems. J. R. Statist. Soc. B, 50, No.
2, 157-224.
[Mozetic92] Mozetic, I. (1992) Hierarchical Model­
Based Diagnosis. Readings in Model-Based diag­
nosis, pp 354-372. Morgan Kaufmann Publishers,
Inc., San Mateo Calif.
[Pearl87] Pearl, J. ( 1987)
of composite beliefs.
33(1987), pp 173-215.

Distributed revision
Artificial Intelligence,

[Pearl88] Pearl, J. (1988) Probabilistic Reasoning
zn Intelligent Systems: Networks of Plausible In­
ference. Morgan Kaufmann Publishers, Inc., San
Mateo, Calif.
[Shachter86] Shachter, R. D. (1986) Evaluating in­
fluence diagrams. Operations Research 34 (6),
871-882.
[Tarjan84] Tarjan, R. E. and Yannakakis, M. (1984)
Simple linear-time algorithms to test chordality of
graphs, test acyclicity of hypergraphs and selec­
tively reduce hypergraphs. SIAM J. Computing
13:566-79.
[Srinivas94] Srinivas, S. (1994) Building diagnostic
models from functional schematics. Technical Re­
port No. KSL-94-15, Knowledge Systems Labora­
tory, Stanford University, Stanford CA 94304.
[TechReport94] Srinivas, S. (1994) A probabilistic
approach to hierarchical model-based diagnosis.
Technical Report No. KSL-94-14, Knowledge Sys­
tems Laboratory, Stanford University, Stanford
CA 94304.
[Yuan93] Yuan, S. (1993) Knowledge-based decision
model construction for hierarchical diagnosis: A
preliminary report. In Proceedings of the 9th Conf
on Uncertainty in Artificial Intelligence, pp. 274281.

