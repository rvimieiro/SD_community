28

Laplace's Method Approximations for Probabilistic Inference in
Belief Networks with Continuous Variables

Adriano Azevedo-Filho*

Ross D. Shachter

adriano@leland. stanford. edu

shachter@camis.stanford. edu

Department of Engineering-Economic Systems
Stanford University, CA 94305-4025

Abstract

Laplace's method, a family of asymptotic
methods used to approximate integrals, is
presented as a potential candidate for the
tool box of techniques used for knowledge
acquisition and probabilistic inference in be­
lief networks with continuous variables. This
technique approximates posterior moments
and marginal posterior distributions with
reasonable accuracy [errors are 0(n- 2) for
posterior means] in many interesting cases.
The method also seems promising for com­
puting approximations for Bayes factors for
use in the context of model selection, model
uncertainty and mixtures of pdfs. The lim­
itations, regularity conditions and computa­
tional difficulties for the implementation of
Laplace's method are comparable to those as­
sociated with the methods of maximum like­
lihood and posterior mode analysis.
1

Introduction

This paper provides an introduction to Laplace's
method, a family of asymptotic techniques used to ap­
proximating integrals. It argues that this method or
family of methods might have a place in the tool box
of available techniques for dealing with inference prob­
lems in belief networks using the continuous variable
framework. Laplace's method seems to accurately ap­
proximate posterior moments and marginal posterior
distributions in belief nets with continuous variables,
in many interesting situations. It also seems useful in
the context of modeling and classification when used to
approximate Bayes factors and posterior distributions
of alternative models.
The ideas behind Laplace's method are relatively old
and can be traced back, at least, to the developments
•

Also with the University of Sao Paulo, Brazil.

presented by Laplace in one of his first major articles
[Laplace 1774, p.366-367]. Since then they have been
successfully applied in many disciplines. Some im­
provements in the implementation of Laplace's method
introduced during the 80s induced a renewed interest
in this technique, especially in the field of statistics,
and will be discussed in the next section.
Initially, Section 2 presents an introduction to
Laplace's method and its use in approximations for
posterior moments and marginal posterior distribu­
tions. It also includes a d iscussion on the use of
Laplace's method in approximations to Bayes factors
and posterior pdfs of alternative models in general and
in the particular case of mixtures of distributions. Sec­
tion 3 discusses some implementation issues and limi­
tations usually associated with the method. Section 4
illustrates Laplace's method with an inference problem
from the medical domain. Finally, Section 5 presents
some conclusions and recommendations.
2

Laplace's Method and
Approximations for Probabilistic
Inference

The approaches for probabilistic inference in belief net­
works with continuous variable usually consider tech­
niques like: (a) analytical methods using conjugate
priors [Berger 1985] ; (b) linear iterative approxima­
tions using transformed variables and Gaussian influ­
ence diagrams results [Shachter 1990]; (c) numerical
integration methods [Naylor and Smith 1982] ; (d) sim­
ulation and importance sampling [Geweke 1989, Eddy
et al. 1992]; (e) posterior mode analysis and maximum
likelihood estimates [Eddy et al. 1992]; (f) discrete ap­
proximations [Miller and Rice 1983]; (g) mixtures of
probability distributions [Poland and Shachter 1993,
West 1993]; and (h) moment matching methods [Smith
1993]. Frequently these approaches can be combined
and all of them can be useful in the context of specific
problems.

Laplace's Method for Probabilistic Inference in Belief Nets

These techniques are, to varying degrees, well es­
tablished in the belief networks/artificial intelli­
gence/decision analysis literature and have been useful
for d ealing with many problems associated with learn­
ing and probabilistic inference. As an example, some
of them are major elements in the confidence profile
method [Eddy et al. 1 992], a current belief network
approach to deal with the synthesis of probabilistic
evidence from scientific studies (meta-analysis) aimed
primarily for the medical domain.
There is, however, another family of techniques based
on asymptotic approximations of integrals and usually
associated with the denomination Laplace's me thod or
Laplace's approximation that also seems promising for
dealing with some interesting instances from the same
class of problems. This approach is introduced in the
following paragraphs with some historical remarks.
Laplace's method and similar developments are in­
spired by the ingenious procedure used by Laplace in
one of his first major papers [Laplace 1774, p. 366367] to evaluate a particular instance of the integral
b

I,...

=

J b(t) exp(

-n

r(t)) dt,

(1)

a

where n is a large positive number, r(t) is continuous,
unimodal, and twice differentiable, having a minimum
at i E (a, b) and b(t) is continuous, differentiable, and
nonzero at i. The general idea behind the solution
arises from the recognition that with a "large" n the
most important contribution to the value of the inte­
gral comes from the region close to i, the minimum of
r(t). An intuitive argument for the approximation is
presented in sequence. First, the Taylor series for r(t)
and b(t) is expanded at i, leading to
b

In �

J (b(i)

+

b'(i)(t- £))

·

a

e-n(r(f)+r'(i)(t-f)+!r"(f)(t-£)2) dt,

then, recogmzmg that r' (i)
leading terms,
In� b(t)e-n r(i)

b

J

=

(2)

0, and keeping only

e-'tr"(i)(t-i)2 dt;

(3)

a

finally, the limits of the integral are heuristically ex­
tended to oo and an unnormalized Gaussian pdf is rec­
ognized and integrated1. From this result follows the
1 A sufficiently large n can make the contribution from
the region on the domain that does not include [i-f, i + <]

to the value of the integral arbitrarily small for any fixed
small f. Similar argument can be used to eliminated the
contribution of the term that includes

b'(i)(t- i).

29

usual formula for Laplace's approximation in one di­
menswn:

The approximation of In by equation (4) is a stan­
dard result in the literature on asymptotic techniques
shown to have an error term that is 0 ( n -1 ) . Rigorous
proofs for the approximation and behavior of errors,
as well as lengthy discussions on assumptions and ex­
tensions are found in references like [De Bruijn 1961,
p.36-39], [Wong 1989, p.55-66] and [Barndorff-Nielsen
and Cox 1989, p.58-6 8] . Important extensions of these
results include the cases: (a) r(t) also d ependent on n
[Barndorff-Nielsen and Cox 1989, p.61]; (b) i E [a, b];
and (c) multimodal functions.
Similar results also hold for more than one dimension.
If now t E �m and the integration is performed over
am d imensional domain, Laplace's approximation for
the integral in equation ( 1) is just the m dimensional
extension of equation (4) [Wong 1989, p.495-500] :
(5)
where t is a point in �m where \7r(t), the gradient
of r(t) at t, is zero, and �i, the inverse of the Hes­
sian of r ( - ) evaluated at t, is assumed positive definite
(meaning that r(t) has a strict minimum at t). The
general assumptions for this result are not unreason­
able: unimodality, continuity on b(t) and continuity
on the second order derivatives of r(t) in the neigh­
borhood oft [Wong 1989, p.498].
These results have had important applications in
statistics. Laplace himself developed and used the pro­
cedures in a proof associated with what seems to be
the first bayesian developments [Laplace 1774] after
Bayes. Nevertheless, only during the last d ecades have
these results started to be considered more seriously
by statisticians in the context of practical applications
[Mosteller and Wallace 1964, Johnson 1970, Lindley
1980, Leonard 1982].
A clever development presented by Tierney and
Kadane [1 986], later followed by a sequence of pa­
pers that also included the author R. E. Kass, in­
spired renewed interest on Laplace's method in re­
cent years.Tierney and Kadane [1986] argued in favor
of a specific implementation of Laplace's approxima­
tion, called by them fully exponential, that produces
results that are more accurate than the ones obtained
using other approaches. Instead of errors that are typ­
ically 0( n -1 ) for the conventional Laplace's approxi­
mation, they found that with their approach the errors
are O(n-2) due to the cancellation of O(n-1) error

30

Azevedo-Filho and Shachter

Vectors of Evidence

Important results associated with Laplace's method
are examined in the next subsections.
2.1

Approximations for posterior moments

To start the developments consider the d efinition of
E(g(0)JX) in terms of the likelihood function and prior
pdf on e, a random vector:

f g(0)L(XJ0)7r(e) de
E(g(0)JX) = ne J
L(XJ0)7r(0) d0
ne

Continuous Function
{sometimes multidimensional)

Figure 1: Belief net for the general problem
terms. In previous developments [Mosteller and Wal­
lace 1964, Johnson 1970, Lindley 1980] the same ac­
curacy is achieved only when terms including deriva­
tives of h igher order are not dropped from the ap­
proximations, leading to formulas that are often dif­
ficult to apply in practice. In addition to that, Tier­
ney and Kadane [1986] presented procedures to com­
pute marginal posterior distributions, extending some
ideas originally suggested by Leonard [1982] . In a se­
quence of papers [Kass et al. 1988, Tierney et al. 1989a;
1989b, Kass et al. 1990] the original intuitive develop­
ments presented by Tierney and Kadane [1986] were
augmented by more formal derivations of the results.
Laplace's method results have indeed a more general
interpretation that can be extended to the context of
belief networks and influence diagrams. The class of
problems considered in the next sections is described
by the belief net depicted on Figure 1. An imp ortant
issue in this case might be the implementation of pro­
cedures to perform probabilistic inference on a func­
tion g(0) of a vector 0 = { 81, (h, . . . , Bm } of contin­
uous variables conditional on evidence represented by
X= {xl, x2,
, xn}· This requires, generally speak­
ing, constructing an arc from X to g(0). Another
important issue might be the selection of models it­
self conditional on the evidence and prior beliefs. In
both cases, when the conditions for applicability hold,
Laplace's method seems to be a valuable technique.
Each instance of the evidence relates to 0 usually
through a likelihood function that considers at least
some of the elements from e as parameters of con­
tinuous probability density functions. This represen­
tation does not characterize the relationship among
the elements of 0 that can be quite complex in some
problems. The elements of 0 will frequently be called
parameters because at least some of them will (pos­
sibly) be parameters of a specific probability density
function.
·

·

(6)

The first step in deriving Laplace's approxima­
tion to equation (6) involves the restatement of
g(0)L(XJ0)7r(0) and L(XJ0)7r(0) in the forms
b1(6) exp(-nr1(6)) and b2(8) exp(-nr2(6)) so that
the result in equation (5) can be easily applied. There
are, indeed, infinite choices for the functions b; and
r; in this case. The choice selected by Tierney and
Kadane [1986], called fully exponential, leads to im­
proved accuracy and considers:
bl(0)

=

b2(0)

=

1

r1(6)= - n-1log(g(0)L(XI0) 7r(0))
r2(8)

=

-n-1log(L(XJ0) 7r(0)) .

(7)
(8)
(9)

Using these choices Tierney and Kadane [1986] argued
that an approximation for E(g(0)JX) with O(n-2)
error terms can be obtained from the quotient of
Laplace's approximation by equation (5) of each of the
integrals on the numerator and denominator of equa­
tion (6). The improved result is derived from the con­
venient cancellation of 0( n -l ) errors terms from each
approximation. The expression for this approximation
for E(g(0)JX) is

·

where 81 and 82 are, respectively, the minimizers for

r1(0) and r2(0) and Ee, is the inverse of the Hessian

of ri(0) evaluated at 8i. In this case
E(g(0)IX)

=

En(g(0) IX) (1 + O(n-2)).

(11)

This result depends on a set of conditions more specific
than those required for the conventional application of
Laplace's Method that is referred as Laplace regular­
ity. The conditions for Laplace regularity require, in
addition to other aspects, that the integrals in equa­
tion (10) must exist and be finite, the determinant of
the Hessians be bounded away from zero at the op­
timizers, and that the log-likelihood be differentiable
(from first to sixth order) on the parameters and all the

Laplace's Method for Probabilistic Inference in Belief Nets

partial derivatives be bounded in the neighborhood of
m i ld as­
sumptions, asymptotic normality of the posterior. For
formal proofs for the asymptotic results and extensive
technical details on Laplace regularity see Kass et al.
[1990].

Beta Posterior

the optimizers. These conditions imply, under

- Example

31

1

10
%error
log scale

The application of equation (10) to approximate
Var(g(G)IX) and Cov(g1(8),g2(8)IX) using the ex­
pressions (omitting 8):

1

Post. Mode

App�ox:

·

0.1
0.01

0. 00 l

(12)

L-...L.__,L__j____J__j_.L__...J....___j___J

10 20 30 40 50 60 70 80 90 100
n

Figure 2: Errors in Approximations for E(BIX)

En(9192IX)
-E,.(g1IX)E,.(g2IX) (13)

and

equation (9), can be easily computed and are, respeca-l
+a
Then,
and o· 2 = p p+
. ely, e· 1 = + p
tiV
p q+a+b-!
+q+a+b-2.
making the substitutions in equation {10), recalling
that in this unidimensional example det �;(0;) is just
the inverse of the second derivative of r;(O) evaluated
at the appropriate minimizer and letting s = (p +a)
and r = (q +b), it follows that

An aspect of using equation (8) that might seem re­
strictive in certain cases is the implied assumption that
g(8) must be a nonnegative function (as L(XI8)1r(8)
is always nonnegative). This case can be addressed
by at least two alternative approaches. The first one
considers setting h(G, s) = exp(s g(G)) (that is al­
ways nonnegative), computing Laplace's approxima­
tion for E(h( 0) ), the moment generating function ( mgf
) for g(8), fixing s at a convenient value where the
mgf is defined, and then using the approximation
E(g(8)) = &E(�(�,•ll ls=D (the definition of expecta­
tion from a mgf of a random variable). The second
alternative consider setting h(8) = g(G) + c, where
c is a large positive value, computing Laplace's ap­
proximation for E(h{8)) and using the approximation
E(g(G)) = E(h(G))- c. Both alternatives are shown
[Tierney et al. 1989b] to be equivalent when c---. oo,
having absolute approxima t ion errors that are 0( n- 2).

In this example, the error of Laplace's approximation
can be easily examined as the analytical expression for
E(eiX) can be computed. In this case the posterior for
() is a Beta(p +a, q +b) pdf so E(OIX) = P+�$�+b and
Mode(8IX) = /$���-2 . The asymptotic behavior
of the error is � ll�strated in Figure 2 in a situation
where the n = 10 k is the number of flips, p = 2 k is
the number of observed heads, q = 8 k is the num­
ber of tails and a ::::: b = 1 are the parameters of the
prior knowledge on 0. As k increases from 1 to 10,
and n varies from 10 to 100, the relative error from
Laplace's approximation decreases with n, in a way
that seems consistent with the expected asymptotic
behavior. The same figure presents, for comparison,
the behavior of the relative error from an approxima­
tion to the posterior mean using the posterior mode.

An example is presented in sequence to illustrate the
application of these results.

2.2

also leads to accurate results [Tierney and Kadane
1986] as:
V ar(giX)

=

V�r,.(giX)(1 + O( n-2))

(14)

1 (Beta posterior): Experimental results
showed that a coin flipped n times produced p heads
and q tails. Let () be probability of heads and as­
sume that our prior knowledge on () is represented by
a Beta(a, b) pdf. Suppose that we want to compute an
approximation for the posterior expected value of() us­
ing Laplace's method.

Example

In this case g(e)
() and L(XI8)1r(8) = c8P+a-l(18)Hb-l (c is some constant). The minimizers ofri(O)
and r2( () ) , expressions defined in equation (8) and
=

Approximations for Marginal Probability
Distributions

Laplace's method can be also useful for approxima­
tions of marginal distributions. Two important cases
are examined in this section: the approximation for a
marginal posterior distribution and the general case of
an approximation to a nonlinear function of parame­
ters, conditional on the evidence X.
Let 8 be partitioned into the subsets Bp and Gq (q are
the number of elements in each subset) and suppose
that we are interested in computing the marginal pos-

32

Azevedo-Filho and Shachter

terior distribution for 0p in the light of the evidence X
considering the same generic model described in Fig­
ure 1. Explicitly:
1r(8piX) = c

·

j L(XI0p, 89)1r(8p, 8q)d 8q

ne.

(16)

for a constant c that can be analytically defined by:
c=

j

ne,' ne.

L(XI8p, 89) 1T'(0p, 8q)d8p, 0q.

( 1 7)

= c

dent measurements from a gaussian population with
parameters

8

{()�",();} = {ft,u2}.

=

(det

k
k
Ek,B. k) t e-nr( , Bq( ))
(

)

where Bq(k) is the minimizer of r(8p

=

(18)

k, 09) and

Ek e (k is the inverse of the Hessian of r(8p = k, 89)
, . )

evaluated at Bq(k). In this case c is computed by
an external numerical procedure or even heuristically
adjusted if what is needed is just a rough graphical
characterization of the distribution.
The relative errors in the approximation of equa­
tion (16) by the first approach are 0( n -1). The second
approach, c computed numerically from the integra­
tion of the unnormalized marginal, leads to more ac­
curate approximations, with errors that are 0( n - �).
Another important aspect of these procedures is that
they are surprisingly accurate in the approximation of
the tail behavior of the distribution as the errors are
0( n -1) uniformly on all bounded neighborhoods of the
mode [Kass et aL 1988]. In addition, it is also possible
to show that this approximation has properties that
are comparable to those obtained from saddlepoint ap­
proximations, a family of techniques that consider the
application of Laplace's method in the domain of com­
plex numbers (see Reid (1988] for more details on this
technique).

Assume that

the prior belief on the parameters is represented by

7r(()�",()") = 1/()u, 8u

>

0.

Suppose that we want to

compute the margin al posterior probability 1T'(8u IX) us­

ing Laplace's method.
In this example

L(XI8)1T'(8)

An approximation for equation (16) can be easily
found for ()P = k using two alternative approaches.
The first approach considers the use of Laplace's
method to approximate both the integral part in equa­
tion (16) and the constant c defined in equation ( 17).
In the second approach the constant c is approximated
by an external procedure, usually numerical integra­
tion, that is very effective in low dimensions (and fre­
quently p = 1 or 2) , according to Naylor and Smith
[1982]. In this case, a set of approximated values for
the integral in equation (16) is computed by repeated
application of Laplace's method with 8p set to conve­
niently chosen values from its domain. The implemen­
tation of the second approach, using the fully exponen­
tial procedure, in equation (5) leads to the following
approximation for the marginal pdf in equation (16) at
8p = k:
?T,.(kiX)

The following example illustrates these results.
Example 2 ( Gaussian):Let X be a set of n indepen­

where it and

s2,

ex

();(n+1l e-

(n-1) ••+n (il-B,.)2

28;

the mean and the variance of the

measurements, summarize all the evidence from

X.

Consider equation {18) to approximate the posterior

k, using equation {9) to define r (0 ).
81-1 = it is the minimizer of r(k, ()!-') and

marginal at()"
In this case

�k ' & "

ex

k2 .

=

This leads to the following approximation

for the posterior marginal:

This approximation is indeed proportional to the ex­
act expression for the posterior marginal of()" that is
an inverted gamma.

Laplace's approximation for the

marginal posterior ofBJJ is also proportional to the ex­

pdf that is a Student t in this case.
A more general situation considers Laplace's approx­
imation for a pdf of a nonlinear function g(8). The
necessary conditions for using Laplace's approxima­
tion to this case require that, in the neighborhood of
the mode of the density of e, the gradient of g(8) is
nonnull or the Jacobian of g(e) is full rank (g(8) can
be am dimensional function). This condition ensures
local invertibility of g(8) in the region close to the
mode of the distribution (the distribution is assumed
unimodal). If these conditions hold, an approxima­
tion to the density of g(B) can be constructed using
Laplace's Method, as showed by Tierney et al. [1989a].
A possible alternative for this approximation is
act

1Tn(g(8) = kiX) = cAe-nr(S(k))

(19)

where
A=

(

det�e(k)
det(V'g(El(k))TEe(k)V'g(El(k)))

)

1
'

(20)

and
(21)

e and B(k) are, respectively, the minimizers of
r(8) and r(8) subject to the constraint g{8) = k;
V'g(El(k)) is the gradient (or Jacobian) of g(8) evalu­
ated at B( k) (a p X kmatrix), with p being the number

Laplace's Method for Probabilistic Inference in Belief Nets

of elements in e and

the dimensions of the function
1; and, Ee(k) is the inverse of
the Hessian of r(8) evaluated at the appropriate min­
imizer.
The same considerations about the normalization con­
stant discussed previously also apply to this case. The
use of equation (21) as an approximation to the con­
stant in equation (19) does not ensure that it will inte­
grate to one. However, it can be a good approximation
for many purposes associated with graphical charac­
terization of the probability function. The errors in
this approximation follow the same behavior as in the
first situation analyzed in this subsection, as showed
by Tierney et al. (1989a].
m

g(8), that is frequently

2.3

Approximations for Bayes Factors,
Model Selection and Mixtures

33

where m; is the dimension of 8;. It has an approxima­
tion error that is 0( n-l). A variant of this approach
considers the maximum likelihood estimator for 8; in­
stead of the minimizer of r(8) and the inverse of the
expected information matrix instead of the inverse of
the Hessian in equation (25). This alternative might be
convenient when there is statistical software available
that is able to perform these computations. In this
case the error in the approximation is also O(n-1) but
if the prior is informative the result is likely to be less
accurate than the result from Laplace's approximation
[Kass and Raftery 1994].
The following example illustrates these results.
Example 3 (Bayes factor)�A group of n subjects
was randomly split in two subgroups that were, re­
spectively, submitted to two alternative treatments,
and

In recent years there has been a renewed interest in
the use of the Bayes factors as an important tool for
testing hypothesis, selecting models, and dealing with
model uncertainty. In a general context, for a set of
evidence X and two alternative models or hypothesis
M1 and M2 that include, respectively, the sets of con­
tinuous parameters 81 and 82, Bayes factor is defined
by

T2.

1 or 2. Let

each subject are either

j

=

1,

T1

The possible outcomes of the experiment for

2,

n;j, i

:::::

1, 2

and

be the number of subjects that received treat­

ment i and presented outcome j and

0;

be the proba­

bility of a subject that received trea t m en t i p rese nted
outcome

Suppose that we want to get some evi­

1.

dence about whether the treatments induced different

(MI)

or identical

(M2)

outcomes using the Bayes fac­

tor. Assume that the prior knowledge is represented by

7r(Ot, 02IM1)

=

1

and

1r(OIM2) =

1.

In this case the analytical e xpre ssi on for the Bayes fac­
tor is

B(n11 +I, n12 + I)B(n21 +I, n22 + 1)
b12 =
B(n 1 1 + n21 + 1, n12 + nn + 1)

so that
(23)

where

Analytical solutions for Bayes factors are only avail­
able in specific cases. For more general situations,

B(-, ·)

is t h e beta function.

Laplace's approxi­

mation can be easily found using equation

(25):

Bayes factors are computed by Monte Carlo simula­

tion, numerical integration and approximation meth­
ods that include Laplace's method and some variants­
see Kass and Raftery [1994] for an extensive overview
that includes many applications.
A possible approximation for the Bayes factor using
Laplace's method considers e quation (5) with b(-) = 1
and

r(8;jM;)::::: -n-1 log(L(Xj6;, Mi)1r(8;jM;)), (24)
i = 1, 2, to approximate both the numerator and de­
nominator of equation (22)2. The expression for this
approximation is

bt2 = (27r)

m1-m2(det:E,o,)2
2

"''

det E62

e

-n(r(Ell)-r(Elo))
•

•

(25)

2This case uses a modified version of equation (5) that is
derived using r( ·) defined directly as a function of n so that
the term on

n

under

1r

and

s ==

(nu +ntz),
(n12 + nn ) .

=

does not appear in the expression.

q

=

(n 2 1 +n22), r

=

(nu +n21)

The asymptotic behavior of the

error in the approximation is presented in Figure

nn

=

3k, n12

=

2k,n21 = 4k,

nn

=

1k,

n = lOk,

3 for

with

k increasing from 1 to 10.
In the context of model uncertainty, Bayes factors can
be used to compute the posterior probability of pos­
sible models, given the evidence provided by X, as a
direct extension of equations (22) and (23):

7r(M; IX)

l

•

where p

=

"(Mi) . b·.
7r(Mj
L.<enM 1r(M1) bkj

�

(26)

·

Laplace's approximation is derived by replacing the
exact values of the Bayes factors in equation (26) by
approximations using equation (25) with the appro­
priate indexes. An important issue in the use of these

Azevedo-Filho and Shachter

34

%error

18
16
14
12
10
8
6
4
2
0

.-.-.--.-.--,-,-,--,-,
Bayes Factor - Example 3

10 20 30 40 50 60 70 80 90 100
n

Figure 3: Errors in Approx. for the Bayes Factor
approximations is whether the conditions for Laplace
regularity hold in a particular case to ensure the appli­
cability of Laplace's method. This issue is examined in
the next paragraphs in the context of models involving
mixtures of distributions.

A promissing area for Laplace's method, closely re­
lated to model uncertainty, is the approximation of
posterior probabilities of the number of components
in mixtures of distributions and group classification.
This problem is a particular instance of equation (26)
where each model relates to a certain discrete value for
the number of components in the mixture. Laplace's
method is applied using the same approximation sug­
gested for equation (24) considering in this case Mk =
{number of components in the mixture is k} and

where nk

I:J:::1..\j

=

=

{UJ:::1 0i,UJ:::1 ..\i}, ..\i E [0, 1] with
1, X = {x1, x2,···,x,..} and Pj(-), j =

1, . . , k, are pdfs (usually from the same parametric
family).
Two aspects make this problem interesting in the con­
text of Laplace's method. First, if the mixture is i de n­
tifiable
and mixtures with pdfs from the same para­
metric family are identifiable in most cases - there
is still an obvious problem of multimodality in equa­
tion (27) due to the possibility of label switching. This
problem can be overcome with constraints to avoid la­
bel switching in the process of finding the minimizer
for equation (24). Second, there is the concern about
whether Laplace regularity holds in this case. Indeed
it does hold in this case at least for a large class of pdfs
that includes the exponential family as well as other
important parametric families. For more details on
Laplace's method in mixtures as well as proofs for the
regularity conditions and some applications see Craw­
ford [1994].
.

-

3

Implementation Issues and
Limitations

The computational implementation of Laplace's
method is relatively straightforward. It depends on
the availability of the same computer routines that im­
plement -cmsgical optimization procedures used by the
methods of maximum likelihood and posterior mode
analysis.
The approximation of marginal distribu­
tions usually requires numerical integration procedures
- if accurate approximations for the integration con­
stant or probabilities of regions of the distribution are
needed - as well as plotting procedures.
Typical applications of Laplace's method to the ap­
proximation of moments involve the computation of
two minimizers for the expressions in equations (8) and
(9), one of them being usually the posterior mode. Fre­
quently the second minimizer is found with only a few
steps of Newton's method (1 to 3 usually) when one
of the minimizers is used as the starting point for the
second optimization process. Similar procedure can be
used in the case of marginals, considering in these case
information from previous approximations.
This means that the computational effort needed to
implement Laplace's method is only marginally greater
than that needed for the posterior mode analysis or
method of maximum likelihood.
One aspect that
seems critical, however, is the availability of improved
computational procedures to find numerical approxi­
mations to gradients and specially to Hessians if the
analytical expressions are not available. This point
was stressed by Naylor [1988] and also applies to other
methods in some extension (when Newton's method is
used for example).
Reparametrization is shown to be a useful practice
in some cases to make the posterior distribution look
closer to a Gaussian [Tierney et al. 1989b, Crawford
1994].
Other limitations are in general related to possible
problems with multimodality in the distribution and
the lack of practical diagnostic procedures to ensure a
priori the asymptotic properties of the method. Even
if the asymptotic behavior holds in a particular situa­
tion it does not mean that a particular approximation
is accurate. In this case experience seems to be the
best guide [Kass et al. 1988].
4

An Application to a Medical
Inference Problem

This section illustrates the use of Laplace's method
with a problem from the medical domain (Figure 4).
The problem involves the use of tamoxifen and was
previously modeled and analyzed elsewhere [Eddy
et al. 1992, p.l83-189}. Part of this previous study

Laplace's Method for Probabilistic Inference in Belief Nets

ii-(ciX)
�-iment1
Contol

Elq>o<imo<H 2
TfMt.

Figure 4: Belief net for the medical problem
Table 1: Errors in Approximations for E(ciX).
E(ciX)

Method
Monte Carlo

Laplace's method
Posterior Modeb

0.05694
(0.00054)"
0.05686
(0.14%)
0.05832
(2.42%)c

astandard deviation.
bEddy et a.l. [1992] reported

0.059.

<compared to Monte Carlo.

considered the posterior mode analysis to access the
increase in the probability of one year survival with­
out disease from alternative treatments, in the light of
the evidence provided by two medical studies. This
quantity is represented here by €.
To allow some comparison among alternative methods,
an approximation for the posterior expected value of E
was computed using a Monte Carlo procedure with im­
portance sampling that considered 106 samples. Then,
an implementation of Laplace's method, involving nu­
merical methods to compute gradients and Hessians,
was used to find an approximation for the posterior ex­
pected value of E the posterior mode of 7r(ciX) was
found as an intermediate step in the computations.
T hese results are presented in Table 1.
If the Monte Carlo approximation is arbitrarily taken
as the "gold standard" for E(ciX), the relative errors
of Laplace's and posterior mode approximations are,
respectively, 0.14% and 2.42%. A realistic benchmark
for alternative techniques would certainly require more
extensive study. Nonetheless, this example illustrates
the application of Laplace's method to a realistic prob­
lem, even though the results don't change the conclu­
sions from the previous analysis of that experiment
done by Eddy et al. [1992].
An interesting extension for cases like this is the ap­
proximation of the marginal posterior pdf as a way to
get extra insights into the problem. Laplace's approx­
imation for 7r(ciX) is presented in Figure 5. It was
-

14
12
10
8
6
4
2
0
-0.05

0.00

0.05

€

0.10

35

0.15

Figure 5: Laplace's Approximation for 7r(ciX)
computed

using equation (18) with the constant c esti­
mated by numerical integration. The approximations
were found for values off on the interval [-0.08, 0.20]
considering points spaced 0.005 apart using Newton's
method to compute the minimizers. Figure 5 also
shows a Monte Carlo approximation for 7r(<:: IX) using
2 x 104 samples classified into 50 classes in the inter­
val [ -0.08, 0.20]. The points in the figure represent
the frequency density of each class at the center of
the class. The computations for Monte Carlo (includ­
ing classification) took roughly 20 times longer than
Laplace's method computations.
5

Final Considerations

Laplace's method can be viewed as an interesting ex­
tension of methods like posterior mode analysis and
maximum likelihood as it uses similar implementa­
tion procedures, shares with them some of the same
problems, but extends their functionality. Laplace's
method directly computes an approximation for mo­
ments that seems reasonably accurate for many uses,
possibly avoiding Monte Carlo methods in some cases.
This feature might be useful in problems where the mo­
ment of the quantity of interest has special meaning
(e.g. expected utility in the context of influence dia­
grams in decision analysis). Another useful extension
is the approximation for marginal distributions so that
they can be easily plotted or numerically integrated to
produce probabilistic statements about the quantity
of interest. An area of potential interest for investiga­
tion seems to be the combination of Laplace's met ho d
with Monte Carlo methods (e.g. in approximations
of the posterior using mixtures of parametric distribu­
tions and in the formulation of importance sampling
distributions).
Acknowlegements

A. Azevedo-Filho's research was supported by a grant
from FAPESP, BraziL We would like to thank three
anonymous referees for their helpful suggestions.

Azevedo-Filho and Shachter

36

References

Barndorff-Nielsen, 0. E. and Cox, D. R. (1989).
Asymptotic Techniques for Use in Statistics. Chap­
man and Hall, London, first edition.

Berger, J. 0. (1985).

Bayesian Analysis.

T. ( 1982) Commentson: A simple predictive
density function. J. Amer. Statist. Assoc., 77 : 657685.

Leonard,

Statistical Decision Theory and

Springer-Verlag, New York, sec­

ond edition.
Bernardo, J. M., DeGroot, M. H., and Lindley, D. V.,
editors (1988). Bayesian Statistics 3. O xford Uni­
versity Press.
Bernardo, J. M., DeGroot, M. H., Lindley, D. V., and
Smith, F. M., editors (1980). Bayesian Statistics,
Valencia. University Press.
Crawford, S. L. ( 1994). An application of the laplace
method to finite mixtures. J. Amer. Statist. Assoc.,
89(425):259-267.
De Bruijn, N. G. ( 1961). Asymptotic
ysis. North Holland, Amsterdam.

Methods in Anal­

D. M., Hasselblad, V., and Shachter, R. D.
(1992). Meta-Analysis by the Confidence Profile
Method. Academic Press, San Diego.

Eddy,

Geisser, S. , Hodges, J. S., Press, S. J., a nd Zellner,
A., editors (1990). Bayesian and Likelihood Meth­
ods in Statistics and Econometrics. Elsevier Science
Publisher.

Geweke, J. ( 1 989) . Bayesian inference in econometric
models using monte carlo integration. Economet­
rica, 57:73-90.
Heckerrnan, D. and Mamdani, A., editors (1993). Un­
certainty in Artificial Intelligence 93, San Mateo,
California. Proceedings of the Ninth Conference,
Morgan Kaufmann Publishers.
Johnson, R. A. ( 1970). Asymptotic expansions as­
sociated with posterior distributions. Ann. Math.
Statist., 41:851-864.
Kass, R. E. and Raftery, A. E. (1994). Bayes factors.
J. Amer. Statist. Assoc. ( to appear) .
Kass, R. E., Tierney, L., and Kadane, J. B. (1988).
Asymptotics in bayesian computations. In Bernardo
et a!. [ 1988}.
Kass, R. E., Tierney, L., and Kadane, J. B. (1990). The
validity of posterior expansions based on laplace's
method. In Geisser et al. [1990].
Laplace, P. S. (1774). Memoir on the probability of
causes of events. Memoires de Mathematique et de
Physique, Tome Sixieme. ( English translation by S.
M. Stigler 1986. Statist. Sci., 1(19):364-378).

.

Levitt, L., Kanal , N., and Lemm er , J. F., editors
( 1990). Uncertainty in Artificial Intell!gence 4- El­
sevier Science Publishers.
Lindley, D. V.

(1980). Approximate bayesian methods.
In Bernardo et a!. [1980].

Miller, A. and Rice, T. C. (1983). Discrete approxima­
tions of probability distributions. Management Sci.,
29:352-362.
Mosteller, F. and Wallace, D. L. (1964). Applied
Bayesian and Classical Inference. Springer-Verlag,
New York.
Naylor, J. C. (1988). Comments on: Asymptotics on
bayesian computations. In Bernardo et al. [1 988].
Naylor, J. C. and Smith, F. M. (1982). Applications of
a method for the efficient computation of posterior
distributions. App. Statist., 31:214-225.
Poland, W. B. and Shachter, R. D. (1993). Mixtures of
gaussians and minimum relative entropy techniques
for modeling continuous uncertainties. In Hecker­
man and Mamdani [1993}, pages 183-190.
Reid, N. ( 1988). Saddlepoint methods and statistical
infere nce . Statist. Sci., 3(2):213-238.
Shachter, R. D. (1990).
A linear approximation
method for probabilistic inference. In Levitt et a!.
[1990], pages 93-103.
Smith, J. E. (1993). Moment methods for decision
analysis. Management Sci., 39:340-358.
Tierney, L. and Kadane, J. B. ( 1986). Accurate ap­
proximations for posterior moments and marginal
distributions. J. Amer. Statist. Assoc., 81:82-86.
Tierney, L., Kass, R. E., and Kadane, J. B. (1989a).
Approximate marginal densities of nonlinear func­
tions. Biometrika, 76(3):425-433.
Tierney, L., Kass, R. E., and Kadane, J. B. ( 19 8 9 b).
Fully exponential laplace approximations to expec­
tations and variances of nonpositive function. J.
Amer. Statist. Assoc., 84:710-716.
West, M. (1993)
Approximating posterior distributions by mixtures. J. Roy. Statist. Soc., B,
55(2):409-422.
Wong, R. (1989). Asymptotic Approximations
grals. Academic Press, San Diego.

of Inte­

