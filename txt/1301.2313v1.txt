VAN ALLEN ET AL.

522

UAI2001

Bayesian Error-Bars for Belief Net Inference

Tim Van Allen

Russell Greiner

Peter Hooper

digiMine, Inc.

Dep't of Computing Science

Dep't of Mathematical Sciences

10500 NE 8th St. Floor 13
Bellevue, WA 98004

University of Alberta

University of Alberta

Edmonton, AB T6G 2H l Canada

Edmonton, AB T6G 2G 1 Canada

timv@digimine.com

greiner@cs.ualberta.ca

hooper@stat.ualberta.ca

Pr{Cancer=trueiSmoke=true, Gender=male}.

Abstract

These values, referred to as query responses, clearly de­
pend on the training sample used to instantiate the param­

A Bayesian Belief Network (BN) is a model of

eter values - i.e., different training samples will produce

a joint distribution over a finite set of variables,

different parameters and hence different responses.

with a DAG structure to represent the immedi­

This paper investigates how sampling variability in the

ate dependencies between the variables, and a
set of parameters (aka CPTables) to represent the

training data is related to uncertainty about a query re­

local conditional probabilities of a node, given

sponse. We follow the Bayesian paradigm, where uncer­

each assignment to its parents. In many situa­

tainty is quantified in terms of random variation, and we

tions, the parameters are themselves treated as

present a technique for computing Bayesian credible in­

random variables- reflecting the uncertainty re­

tervals (aka "error-bars") for query responses. Our algo­

maining after drawing on knowledge of domain

rithm takes as inputs a belief net structure (which we as­

experts and/or observing data generated by the

sume is correct- i.e., an accurate /-map of true distribu­

network. A distribution over the CPtable param­

tion [Pea88]); a data sample generated from the true belief

eters induces a distribution for the response the

net distribution; and a specific query of the form "'What is

BN will return to any "W hat is

Q =

Pr{ HIE} ?"

Pr{ H

=

hIE

= e } ?". After determining the

This paper investigates the distribution

conditional (posterior) distribution of the belief net param­

of this response, shows that it is asymptotically

eters given the sample, the algorithm produces an estimate

query.

(posterior mean value) of Q: e.g., estimate Q to be

asymptotic variance. We show that this compu­

To quantify uncertainty about this estimate, the algorithm

tation has the same complexity as simply com­

computes an approximate posterior variance for Q and uses

puting the

this variance to construct error-bars (a Bayesian credible in­

(mean value of the) response -i.e.,

O(n exp(w)),
ables and

w

where

n

terval) for Q; e.g., assert that Q is in the interval

is the number of vari­

is the effective tree width.

with 90% probability.

We

also provide empirical evidence showing that the

0.3 ± 0.1

There are several obvious applications for these error-bars.

error-bars computed from our estimates are fairly

First, error-bars can help a user make decisions, especially

accurate in practice, over a wide range of belief

in safety-critical situations - e.g., take action if we are

net structures and queries.

99%

sure that Q =

Pr{ H

=

hIE

= e

}

is on one

side of a decision boundary. Second, error-bars can

1

Introduction

can make appropriate guarantees about the answers to cer­

model of a joint probability distribution, are used in
an ever increasing range of applications [Hec95].
nets

are

typically

built by

Be­

first finding an ap­

propriate structure (either by interviewing an expert,
or by

selecting

a

good model

from training

data),

then using a training sample to fill in the parame­
ters

sug­

gest that more training data is needed before the system

Bayesian belief nets (BNs), which provide a succinct

lief

0.3.

normal, and derives expressions for its mean and

[Hec98]. T he resulting belief net is then used to an­

swer questions, e.g., compute the conditional probability

tain queries. This information is especially valuable when
additional training data, while available, is costly, and its
acquisition needs to be justified. Similarly, the user might
decide that more evidence is needed about a specific in­
stance, before he can render a meaningful decision.

Fi­

nally, if an expert is available and able to provide "correct
answers" to some specific questions, error-bars can be used
to validate the given belief net structure. E.g., if the expert
claims that Q

=

0.5 but our algorithm asserts that Q is in

UAJ 2001

VAN ALLEN ET AL.

0
�
a
I

0

91,110
0.400

523

91,0!0

0.600

G(�l
"x@4�
x4

Figure

1:

o

1
0

Simple Example: Diamond Graph

the interval 0.30 ± 0.04 with 99.9% probability, then we
may question whether the structure provided is correct (as­
suming we believe the expert). By contrast, we might not
question this structure if our algorithm instead asserted that
Q is in the interval 0.30 ± 0.25 with 99.9% probability.
Section 2 provides background results and notation con­
cerning belief nets and Dirichlet distributions for belief net
parameters. Section 3 presents the theoretical results under­
lying our error-bars: a derivation of an approximate poste­
rior variance for a query probability Q, and a proof that the
posterior distribution of Q is asymptotically normal. Com­
putational issues related to calculation of the variance are
briefly discussed. Section 4 presents the results of an em­
pirical study using Monte Carlo simulations to validate our
error-bar methodology over a wide range of belief net struc­
tures and queries. Section 5 briefly surveys related work,
placing our results in context.
2

1
0
0

Belief nets and Dirichlet distributions

We encode the joint distribution of a vector of discrete ran­
dom variables X = (Xv}vEV as a belief net (aka Bayesian
network, probability net). A belief net (V, A, 8) is a
directed acyclic graph whose nodes V index the random
variables and whose arcs A represent dependencies. Let
Pa(v) C V be the immediate parents of node v, and let
Fv = (Xw)wEPa(v) be the corresponding vector of parent
variables. In a belief net, a variable Xv is independent of
its nondescendents, given Fv. The elements of the vector
8 are the CPtable entries

Let Xv and Fv = TiwEPa(v)Xw be the domains of Xv and
Fv· We assume that the domains are finite. The CPtable
for Xv contains IXvl X IFvl entries 0v,xJf·
Figure 1 provides a simple example of a belief network
with specific CPtable entries. Here X1 has no parents, so
we write F1 = (}.We have F2 :::��: {Xt), Fa= (Xt), F 4 =
{X2, Xa); and for each value a, b, c, d, we have 01,al0 =
Pr{ xl = a IE>}, e2,bJa = Pr{ x2 = b I Xt = a, E> },
and e4,djb,c = Pr { x4 = d I x2 = b, X a = c, e }.

(Hence, using Figure 1, we have 81,110 = 0.4.) Note that
the values in each row add up to 1. In general, the variables
need not be binary, but can have larger (finite) domains.
The CPtable entries are estimated using training data and
(possibly) expert opinion. The latter information is incor­
porated using the Bayesian paradigm, where 8 is mod­
eled as a random variable and expert opinion is expressed
through an a priori distribution for 8. We adopt indepen­
dent Dirichlet priors1 for the various CPtable rows. Specit1cally, let 8vl/ = (9v,zl/)xEX. denote the CPtable row for
Fv = f- e.g., e4J(1,0) = (04,1](1,0), 04,0](1,0}) denotes
the entries for the X4 variable associated with the parental
assignment X2 = 1 and X3 = 0. We assume that, be­
fore observing the training data, the evil are independent
"Dir( a::,,11, x E Xv )"random vectors, where a:;,,11 > 0.
An absence of expert opinion is often expressed by setting
a;,xlf = 1 for all (v,x, f)- e.g., 84J(1,o) ""Dir( I, 1)
- which yields a uniform (flat) prior. Stronger opinion is
expressed through larger values of a:*v,x 11. Expressions for
the mean and variance of a Dirichlet distribution are given
below.
Now suppose that the training data consist of m indepen­
dent replicates of vectors X, generated using the given
structure and a fixed set of CPtable entries e. Let
mv,xlf denote the number of cases in the training set with
(Xv, Fv) = (x, f). Under the posterior distribution (the
conditional distribution given the training data), the E> vlf
are independent Vir( O:v,zl/• x E Xv) random vectors, with
O:v,xlf = a;,xlf + m v,xlf [BFH95]. This posterior distri­
bution underlies our derivation of Bayesian credible inter­
vals. Several properties of the Dirichlet distribution will be
needed.
Setting O:v,.J/ = l::xEXv O:v,xJ f• the posterior means and
(co)variances for CPtable entries are [BFH95]:

1

E{G v,xJf}

=

Cov{0v,zJ/• 0v,yJ/}

=

f.lv,xlf

O:v,zl/

= --

(1)

O:v,-J/
llv,xjt(c5xy- llv,yJf)
(2)
O:v,-J/ + 1

Readers unfamiliar with these assumptions, or with Dirichlet

distributions, are referred to [Hec98]. Note that a Dirichlet distri­
bution over a binary variable is a Beta distribution.

VAN ALLEN ET AL.

524

1 if x == y and c5,y = 0 otherwise. The ran­
Elvlf are asymptotically normal, in the limit
as min, av,xl! --t oo [Aki96]. More precisely, the nor­
malized variables ..;av,.1f(8v,x1J - 1-Lv,xJJ) converge in

where

c5xy =

dom vectors

distribution to jointly normal random variables with mean
zero and covariances

J.tv,xJJ(o,y- J.tv,yJJ).

This asymp­

totic framework is applicable as the amount of training data
increases

8v,xJJ

(m --t oo) provided all of the CPtable entries

are positive. This condition occurs with probability

one under a Dirichlet prior.

3

Bayesian Credible Intervals for Query Re­
sponses

It is well-known that the CPtable entries determine the

joint distribution of X:

Pr{ Xv = Xv, v

E

vIe}

=

I1vEV ev,x. II.] where(! vlvEV is determined by (xv ) vEV ;
see [Pea88]. Users are typically interested in one or more

specific "queries" asked of this joint distribution, where a
query is expressed as a conditional probability of the form
Q

=

q(B)

=

Pr{H=hiE=e,e},

(3 )

UAI2001

partial derivatives. Let Pv (h, x,fIe) denote the probabil­

ity

Pr{ H =h, Xv = x, Fv =fIE= e, e
and let pv (x , f

=

f1. },

le),Pv(h,f le),pv(f le), andp(h le) be

defined in a similar manner. Note that the subscript

v

is

Xv or F u is involved,
and all probabilities are evaluated at e
/-£. Let q� /
.zl
denote the partial derivative 8q(B)/8Bv,zlf evaluated at
0 := IL· We will use the following identity, derived by
needed to identify the node when

=

[GGS97, DarOO]:
1

qv,zlf

Pv(h,x,f le) - p(h le)pv(x,f le)
J.tv,zlf

_
-

We now derive an expression for

i7�,

·

(S)

and demonstrate

asymptotic validity of the credible interval (Equation

4)

given a sufficiently large training sample.

1 We assume that 8 is a random vector with
posterior Dirichlet distribution described in Section 2, and
approximate the variance of Q
q(8) by

Theorem

=

where H and E are subvectors of X, and h and e are legal

assignments to these subvectors. Note also the dependency
one.
In our Bayesian context, Q is a random variable with a (the­
oretically) known distribution determined by the posterior

ij�

the posterior mean J.LQ =

E{ Q}.

the iden tity [CH92]:
=

E{ q(8)}
Set It =

defined

E{E>}

where the components

by Equation

J.tv,zJ/

of 11- are

1.

W hile a point estimate 11-Q

=

q(�t)

can be useful, one

often requires some information concerning the potential
error in the estimate.

In the Bayesian context, this can

be achieved by plotting the posterior distribution of Q.
Alternatively, one may construct a 100(1

-

r5)%

cally not analytically tractable, but simple approximations
We

ill show that the distribution of Q is

w

approximately normal, and derive an approximation i7Q for

the standard deviation of Q. We then propose the following
interval as an approximate 100(1

J.LQ

±

-

r5)% credible interval:

zo/2 ifQ,

where zo/2 = <I>-1 (1 - 8/2) is the upper
standard normal distribution.

(4)

J/2

value of the

Our derivation is based on a first-order Taylor expansion of

q(E>)

about

q(�J-).

Some notation is needed to express the

-

Bvf )/(av,.Jf

+

1),

(6)

{Pv(h,x,!Je)-p(hle)pv(x,f]e)P
,
J.t v, zlf

Consider an asymptotic framework where the poste­
rior means J.tv,xlf are fixed, positive values, and
min { nv,zl/}
T hen the random variable
-+
oo.
(Q- J.LQ )/i7Q converges in distribution to the standard
nonnal distribution.
Proof. Our proof uses the Delta method

sider the Taylor expansion

credi­

ble interval for Q; i.e., an interval (L, U) defined so that
Pr{ L :::; Q :::; U} = 1- o. Exact calculations are typi­

are available.

L

T his value can be calcu­

q( E{E>} ).

vEV !E:Fv

where

distribution of e. For a point estimate of Q, one may use

lated using

L L (Avt

=

q(8)

=

q(fJ.) + D

+

[BFH95]. Con­

R,

where

D

L L L q�,l:l/ (E>v,zlf - /Jv,z!f ).

vEV /E:Fv zEXv

(7)

and the remainder term R can be expressed in terms of the
matrix of second derivatives of q( e) evaluated at a pointe
between Band J.t. Since the variances for E>u,zJ! in EqUCJ­
tion 2 are of order 1/av,zlf --t 0, and since the second
derivatives remain bounded in a neighbourhood of f.i, the
remainder R is asymptotically negligible compared with D.

&b

We define
to be the variance of D (Equation 7). As
the CPtable rows El vlf are statistically independent, but

UAI2001

VAN ALLEN ET AL.

e ntries within a row are correlated, the variance of
be expressed as

D

can

525

Table 1: Gold Standard for Validity Estimates
d

After substituting Equation 2 for the covariances and sim­
plifying, we obtain Equation 6 with

L (q�,xlt)21Lx,vlf'

Avf

A substitution of Equation 5 then yields the equivalent ex­
pressions/or Avt and Bvt within Equation 6.

Dfuq

10%
20%
30%
40%

Mean
2.38

3.15
3.63

3.88

Std.Dev.
1.86

2.41
2.79
2.96

q�,xlf in time O(n2w);

see [DarOO]. Given these deriva­
tives, the summations in Equation 6 can be performed with
one additional pass over the values, of time 0 (n).

The extended paper [VGHOl] describes an algorithm for
computing UQ. The main challenge, computing all of
the derivatives q� xlf' is accomplished by "back propagat­
ing" intermediate' results obtained by the Bucket Elimina­
tion [Dec98] algorithm.

We observe that
is a random variable with mean 0
and variance 1. It remains to show that D / ijQ is asymp­
totically normal. This result follows from the asymptotic
multivariate normality of the components of 8 (after suit­
able standardization- see Section 2), and the fact that D
0
is a linear function ofEl.

[VGHOl] also provides additional comments on the proper
interpretation and application of this theorem.

There are exceptional situations where
the posterior distribution of q(9) is analytically tractable
and exact credible intervals are available. In the degener­
ate situation where the network structure has arcs connect­
ing all pairs of nodes (and hence imposes no assumptions
about conditional independence), the assumption of inde­
pendent Dirichlet distributions for CPtable rows is equiva­
lent to an assumption of a single Dirichlet distribution over
unconditional probabilities Pr{ Xv = Xv, v E V}. It is
then straightforward to derive the distribution of the query
probability using properties of the Dirichlet distribution;
see [Mus93].2 Note that this exact approach is not cor­
rect in general
i.e., it does not hold for networks with
non-trivial structure.3

(8)

4

Empirical Study

Theorem 1 proves that the interval /-LQ ± z6;2uq is asymp­
totically valid. More precisely, let

Degenerate Case:

-

The computational problem of
computing J.LQ = q(p,) is known to be NP-hard [Coo90];
when all variables Xv are binary, the most effective ex­
act algorithms require time O(n2w), where n = lVI is the
number of nodes and w is the induced tree width of the
graph [Dec98, LS99]. The variance uq can also be com­
puted in time O(n2w). This result follows from the exis­
tence of algorithms that can compute all of the derivatives
Computational Issues:

Assuming a uniform prior and a sample of size m, we can
compute the posterior variance of Pr{ HIE} as P{HIE} x (1F{HIE} )/((m x P{E})+3), where F(x) is the expected value
of x, wrt the given belief net.
3This follows from a dimensionality argument: in a non-trivial
structure, the 2n-dimensional vector of unconditional probabili­
ties is constrained to lie in a lower-dimensional submanifold of
t e 2n -!-dimensional simplex. This cannot be represented by a
smgle Dirichlet distribution because, wpl, the constraints would
not be satisfied.
2

�

be the probability that the query response Q falls outside of
the credible interval, based on our UQ estimate of standard
deviation, Equation 6. The values 1 o and 1 � are the
nominal and actual coverage probabilities for the credible
interval. The value� is a function of o, the graph (V, A),
the query q, and the posterior distribution of 0. The pos­
terior distribution depends on the prior distribution and the
training sample. Thus� typically varies from one applica­
tion to the next. While Theorem 1 implies that� � o when
the training sample is sufficiently large, it does not tell us
whether this approximation is valid in practice, particularly
for small samples. In general, the validity of the approxi­
mation depends on all of the factors determining �. We
carried out a number of experiments to assess how these
factors affect validity.
-

-

Given a fixed set of factors, we estimate the correspond­
ing� by a simple Monte Carlo strategy. Using the (fixed)
posterior distribution of 0, calculate ILQ and uq. Simulate
r replicates ei from the posterior distribution, calculate
Qi = q(0;), then let.&. be the proportion of the {Qi} with
IQ; /-LQI > Zof2UQ. In our experiments, each � was
based on r 100 replicates.
-

=

To quantify the validity of the approximation�
employ average absolute differences:
validity estimate

=

average I.&- Jl.

�

o, we

(9)

The absolute differences are averaged as we vary one or
more of the the factors determining �- The validity es­
timates are presented as percentages in our tables. When

VAN ALLEN ET AL.

526

Diamood Gqaph Qu,riat wit'! lilO% E'rorBa,.·· E�e (r"' 100

m

UAI2001

=50}

Analyiic &r11 •
Qyery�II)OnM +
Mon'le Carlo 81.1'1 D
..•

11

•..

lo
.,
i

•..

�-1

..•

-2

01

QO

02

Figure 2:

Results for the Diamond Graph

We studied the following inferential patterns in the dia­
mond graph (Figure I):
Pr{Xl

=

1 IE>}

Q2 = Pr{X1

=

1IX2

=

1 IX2X3

Qi

=

=

=

el,ll()
11
=

E>}

=

1,

E>}

Q3

=

Pr{Xt

Q4
Qs

=

IIX1 = 1, E>}
Pr{X2X3
1 IX4
11 E>}
Pr{X1

Q6

=

=

=

-.2

1

Standard Normal Ouanmea

-1

0

2

(B) QQ-plot showing relation to Normal
J E {10%, 20%, 30%, 40% }. The resulting validity es­
timates are listed in Table 2. Each cell in the table is an
average of 30 values.

Figure 2(A) shows the error-bars returned by our approx­
imation, and also the Monte Carlo system, on a random
network posterior, for the error-bars for 90% credible inter­
vals. We see the two methods give similar answers.
Figure 2(B) uses a quantile-quantile (QQ) plot to address
the validity of the normality assumption, independently of
the linear approximation. Each "line" in this figure corre­
sponds to z-scores of the 100 query responses generated
by our Monte Carlo simulation, plotted against standard
normal quantiles. This figure shows six such lines, each
corresponding to a single query in { Q1, . .. , Q6}, given a
sample of size m 10. A straight-line would correspond
to data produced by a "perfect" normal distribution; we see
each dataset is close. (Of course, this is only suggestive; the
real proof comes first from Theorem 1, and then from the
data (e.g., Table 2) which demonstrates that our approach,
which assumes normality, produces reasonable results.)
=

8�·'1' 9'·'1°
La e2,lla e,,,,o
=

=

e2.'1' e3,qt e,,,IO

La e2,tla e3,t!a e,,,,o

82.111 83,111

=

=

Lb1c e4,t!b,c e2.bll es.c11 e,,,IO
L,.,b,c e4,llb,c e2,bla e3,cla e,,,,o

=

Pr{X4

=

-3

(A) Examples of Error Bars;

viewing these values, it is helpful to have a gold standard
for comparison. Consider the validity estimate 1.& - 51 for
a single ..:l. The minimum expected value is obtained when
..:l == S; i.e., when 100.& has the Binomial(lOO, S) distribu­
tion. Table 1 presents means and standard deviations under
these ideal circumstances. Now suppose a validity estimate
is obtained by averaging k independent terms. Its standard
deviation is typically greater than the value Std.Dev./ v'k
suggested by Table 1 because there is usually variation in
the underlying � values.
4.1

""""

1IXt

=

1, E>}

=

L:o,c 84,tlb,c 82,olt 83,cll

The six queries cover a range of different inferential pat­
terns. The first is basically a "sanity check", as it is a triv­
ial inference; the fourth is also straightforward, although it
does involve a multiplication. The sixth is slightly more
complex, but it is still only a summation of a set of prod­
ucts. The remaining queries involve divisions of increas­
ingly complicated expressions.
For each m E {10, 20, 30, 40} , we carried out 30 trials
of the form: (1) generate E> from a uniform Dirichlet
prior distribution, (2) generate a training sample of size m
based on E> and use the result to obtain a posterior dis­
tribution, (3) generate 100 Monte Carlo replicates from
the posterior distribution and use these to obtain an esti­
mate 6. for each pair (Q, J), for Q E {QI. ... , Q6} and

4.2

Results for Alarm Network

The Alarm network [BSCC89] is a benchmark network
based on a medical diagnosis domain, commonly used in
belief network studies. The network variables are all dis­
crete, but many range over 3 or more values. The network
includes a CPtable for each node; i.e., a particular 0 is spec­
ified.
Table 3 summarizes the results for experiments on the
Alarm network, where we varied both J and m. For each
m, we generated a single random sample of size m from
0, and used this to determine a posterior distribution (as­
suming a uniform prior). Validity estimates were obtained
by averaging over randomly chosen queries. The queries
Pr{H =hIE= e, 0} were chosen by determining an as­
signment H = h to one randomly chosen query variable,
and assignments E = e to five randomly chosen evidence
variables. (Here, we used [HC91] to determine which vari-

527

VAN ALLEN ET AL.

UAI2001

Table 4: Results for Random Networks
#E
#H
2
3
4
5

Table 2: Results for Diamond Graph
m

Ql

Q2

10

2.37

2.77

20

2.67

3.33

30
40

2.60
2.60

3.03
2.97

10
20
30

3.50
4.60
2.90

5.00
6.27
5.07

40

4.07

5.27

3.97
5.20
4.50
4.90

5.63

4.70

30
40

3 . 70
5.13

7.20
5.80

3.33
3.90

6 = 20%
4.87
4.70
7.03
5.13
5.97
4.43
4.93
4.93

6 = 40%
7.53
5.33
9.27
5.73
6.47
5.00
6.73
5.97

3.63

30
40

2.00

3.10

6.97

10

4.33
4 .73

6 = 10%
3.27
3.10
3.37
2.50
3.13
2.40

6 = 30%
7.23
6.10
11.13
6.83
7.20
5.47
6.63
6.03

20

10
20

Q4

Q3

Q5

Q6

2.20
2.90
2.77

3.93
3 .50
3.70
2.90

3.43
4.03

5.57
4.53

3.97
4;03

4.87
3.87

2.60

5.13
5.30

6.27

3.50
4.27

5.30
4.27

1
2
3
4
5

1
2

3
4
5

6.03

4.40

6.63

5.20
4.27
4.43

5.97
4.97
4.47

1
2
3
4

5

fJ
10%

20%

30%

40%

50

2.47

4.37

4.48

4.07

100
150

2.66
3.04

4.95
5.35

5.97
6.45

4.87
5.66

200

2.65

4.80

5.43

5.42

abies could be query as opposed to evidence variables.)
Some or all of the evidence variables might have had no
effect on the query variable, others might have had a pro­
found effect. Each cell in Table 3 represents an average
from 100 queries on a single posterior distribution.

4.3

Results for Random Networks

Although random networks tend not to reflect typical (or
natural) domains, they complement more focussed studies
by exposing methods to a wide range of inputs and help to
support claims of generality. We carried out experiments
on networks with 10 binary variables and 20 links, gener­
ating gold models from a uniform prior distribution on e.
and generating random queries of various types. Here we
used sample size m = 1 00 throughout, and varied the type
of query. Table 4 displays the results of our experiments.
Each query was of the form Pr{H =hIE= e, 9}, with
varying dimensionalities forE and H. Let #E and #H
denote the number of variables comprising E and H, re­
spectively. Each cell of Table 4 is based on 100 trials: I 0
queries on 10 networks, with both structure and posterior
generated randomly.

2.72

2.59
2.49

2.50
2.59

2.57
2.72

2.26
2.53

0

6
3.06
3.60
3.50
4.12

4.16
3.63
4.16
4.46

4.67

5.76

0

4.4

4.56
5.64

5.07
6.31

6.63

7.85

8.17

30%
4.75
5.11
5.02
5.13

5.97

=

5.61

5.69

7.63

7.43
9.45

5.15
4.95

4.39
4.96
5.78
7.14

4.98

4.20

2.79

4.71

5.14

4.43
4.11

2.56
2.88

4.38

4.74

2

5

20%
4.41
4.19

3.00
2.45

=

4.7 8
4.28

1
3
4

= 10%
2.84
3.20
3.13
2.93
2.62
2.38
2.S4
2.58
2.61
3.05

4.17
4.46
4.12

0

Table 3: Results for Alarm Network
m

2.16

6.39
8.71

6.33
7.25
10.81

12.51

13.99

4.90
4.24

40%
4.56
5.00
4.81
5.44

4.62
4.52

4.80
6.47

5.95
8.36

7.14

8.95

13.05

=

5.62
6.43
7.12
10.99
16.15

Discussion

Our hypothesis was that our Bayesian error-bars algorithm
would be accurate for essentially all cases. We tried to falsify our hypothesis by varying the following experimental
factors:
•

Network structure (V, A)

•

Credibility level 1

•

Query type (Diamond network, Alarm)

•

Number of evidence variables (Random networks)

•

Number of query variables (Random networks)

-

c5

In no case did we observe a result where averagelto
81
exceeded 20%. In most cases, the validity estimate was less
than 8/3. As noted in Table 1, even if our error-bars were
exact, we would still get positive validity estimates due to
the variance in to about �- We therefore believe that these
results comfortably bound the expected error of our method
under the experimental conditions . None of the factors that
we manipulated had a profound effect. T he strongest ef­
fect, observed in Table 4, was that increasing the number
of variables assigned in a query tended to increase the er­
ror I� 81; see also [Kle96]. One possible explanation is
that, as #E and #H increase, the query function q tends
to become more complex, and the local linear approxima­
tion of q becomes less reliable. Another possibility is that
-

-

VAN ALLEN ET AL.

528

the query probability

Q

tends to become very small, mak­

UAI2001

bution over CPtables, but for different purposes. For exam­

ing the normal approximation less accurate. Further exper­

ple, Cooper and Herskovits [CH92] use it to compute the

iments could address this issue.

expected response to a query; by contrast, we also approx­

We found these results very encouraging.

Our method

appears to give reasonable error-bars for a wide range of
queries and network types.

This makes the technique a

promising addition to the array of data-analysis tools re­
lated to belief networks, especially as the algorithm is rea­
sonably efficient, (only) roughly doubling the computation
time per inference. W hile there may be pathological cases

imate the posterior variance in that response. Similarly,
while many BN-learning algorithms compute the posterior
distribution over CPtables [Hec98], most of these systems
seek a single set of CPtable entries that maximize the like­
lihood, which again is different from our task;

e.g.,

their

task is not relative to a specific query (but see [GGS97]).
Many other projects consider sensitivity analyses, provid­

where our method will not give reasonable results - per­

ing mechanisms for propagating ranges of CPtable values

haps because the local linear approximation and the asymp­

to produce a range in the response; cf., [BKRK97, Las95,

totic normality are far off the truth-

we

did not find such

CNKE93, DarOO]. W hile these papers assume the user is
explicitly specifying the range of a local CPtable value, our

cases in our experiments.

work considers the source of these variances based on a
Other Experiments:

We also ran a number of other ex­

periments. One set computed the average{ A

8}

data sample. This also means our system must propagate

scores

all of the "ranges"; most other analyses consider only prop­

in each situation, to determine if there was any systematic

agating a single range. The [DarOO] system is an excep­

-

bias. (Note this score differs from Equation 9 by not tak­

tion, as it can simultaneously produce all of the derivatives.

ing absolute values.) We found that our bounds were typi­

However, Darwiche does not consider our error-bar appli­

cally a bit too wide for most queries- i.e., we often found
the

1

-

a-interval included slightly more than

1

-

8 of the

cases. We are currently investigating this, to see if there are
straight-forward refinements we can incorporate.

cation, and so does not include the additional optimizations
we could incorporate.
Excluding the [DarOO] result, none of the other projects
provides an efficient way to compute that information.

We also computed error-bars based on the (incorrect!)

Also, some of those other papers focus on properties of

"complete structure" assumption, which implies the re­

this derivative - e.g., when it is

sponse will have a simple Dirichlet distribution; see Foot­

able entry.

note

ately from our expression (Equation 6). Finally, our anal­

2. We found that, as anticipated, the approach de­

scribed in this paper, using Equation 6, consistently out­

0

for some specific CPt­

Note that this information falls out immedi­

ysis holds for arbitrary structures; by constrast some other

performed that case, in that our approach was consistently

results (e.g., [CNKE93]) deal only with singly connected

closer to the Monte Carlo estimates.

networks (trees).

[VGHOl] discusses these results in detail.

It also inves­

Lastly, our analysis also connects to work on abstractions,

tigates techniques for dealing with extreme values, where

which also involves determining how influential a CPtable

the normal distribution may be sub-optimal.

entry is, with respect to a query, towards deciding whether

5

typically computational efficiency in computing that re­

to include a specific node or arc [GDSOl]. Their goal is

Related Work

sponse. By contrast, our focus is in computing the error­
Our results provide a way to compute the variance of

bars around the response, independent of the time required

a BN's response to a query,

to determine that result.

which depends on the

posterior distribution over the space of CPtable entries,
based on a data sample.
method" [BFH95]:

This is done using the "Delta

first determine the variance of each

CPtable row, then propagate this variance using a sensi­
tivity analysis

(i.e., the partial derivatives); see Equation 6.

Kleiter [Kle96] performs a similar computation; parts of his
analysis are more general, in that he considers incomplete

(1) discuss how to deal
structures, (2) show how to deal

data. However, he does not

with

general graphical

with

the correlations encountered with general Dirichlet distri­
butions, nor

(3)

provide an efficient way to compute this

information. Moreover, our empirical data provide addi­
tional evidence that the approximations inherent in this ap­
proach are appropriate, even for small sample sizes.
Several other researchers also consider the posterior distri-

6

Conclusion

Further Extensions:

Our current system has been im­

plemented, and works very effectively. There are several
obvious ways to extend it.

One set of extensions corre­

spond to discharging assumptions underlying Theorem

1:

computing error bars when the data was used to learn the
structure, as well as the parameters; dealing with param­
eters that are drawn from a distribution other than inde­
pendent Dirichlets, perhaps even variables that have con­
tinuous domains; dealing with a training sample whose
instances are

not completely specified.

Our work deals

with fully-parameterized CP tables. It would be interesting
to investigate techniques capable of dealing with CPtables

UAI2001

VAN ALLEN ET AL.

represented as, say, decision tree functions [BFGK96], etc.
Contributions:
Many real-world systems work by rea­
soning probabilistically, based on a given belief net modeL
When knowledge concerning model parameters is condi­
tioned on a random training sample, it is useful to view
the parameters as random variables; this characterizes our
uncertainty concerning the responses generated to specific
queries in terms of random variation. Bayesian error-bars
provide a useful summary of our current knowledge about
questions of interest, and so provide valuable guidance for
decision-making or learning.

This paper addresses the challenge of computing the error­
bars around a belief net's response to a query, from a
Bayesian perspective. We first motivated and formally de­
fined this task- finding the 100(1 - o)% credible interval
for a query response with respect to its posterior distribu­
tion, conditioned on a training sample. We then investi­
gated an application of the "Delta method" to derive these
intervals. This required determining both the covariance
matrix interrelating all of the parameters, and the derivative
of the query response with respect to each parameter. We
produced an effective system that computes these quanti­
ties, and then combines them to produce the error-bars.
The fact that our approximation is guaranteed to be cor­
rect in the limit does not mean it will work well in practice.
We therefore empirically investigated these claims, by test­
ing our system across a variety of different belief nets and
queries, and over a range of sample sizes and credibility
levels. We found that the method works well throughout.

Acknowledgements
We are grateful for the many comments and suggestions we
received from Adnan Darwiche and the anonymous review­
ers, and for the fairness of the UAI'0 1 programme chairs.
All authors greatfutly acknowledge the generous support
provided by NSERC, iCORE and Siemens Corporate Re­
search. Most of this work was done while the first author
was a student at the University of Alberta.
References

529

[BSCC89] I. Beinlich, H. Suermondt, R. Chavez, and
G. Cooper. The ALARM monitoring system: A case
study with two probabilistic inference techniques for be­
lief networks. In Proc. Second European Conf Artificial
Intelligence in Medicine, August 1989.
[CH92] G. Cooper and E. Herskovits. A Bayesian method
for the induction of probabilistic networks from data.
MU, 9:309-347, 1992.
[CNKE93] P. Che, R. Neapolitan, J. Kenevan, and
M. Evens. An implementation of a method for com­
puting the uncertainty in inferred probabilities in belief
networks. In UA/-93, pages 292-300, 1993.
[Coo90] G. Cooper. The computational complexity of
probabilistic inference using Bayesian belief networks.
Artificial Intelligence, 42(2-3):393--405, 1990.
[DarOO] A Darwiche. A differential approach to inference
in bayesian networks. In UA/'00, 2000.
[Dec98] R. Dechter. Bucket elimination: A unifying
framework for probabilistic inference. In Learning and
Inference in Graphical Models, 1998.

[GDSOl] R. Greiner, C. Darken, and I. Santoso. Efficient
reasoning. Computing Surveys, 33:1-30,2001.
[GGS97] R. Greiner, A Grove, and D. Schuurmans.
Learning Bayesian nets that perform welL In UAI-97,
1997.
[HC91] E. Herskovits and C. Cooper. Algorithms for
Bayesian belief-network precomputation. In Methods of
Information in Medicine, pages 362-370, 1991.
[Hec95] March 1995. Special issue of "Communications
of the ACM", on Bayesian Networks.
[Hec98] D. Heckerman. A tutorial on learning with
Bayesian networks. In Learning in Graphical Models,
1998.
[Kle96] G. Kleiter. P ropagating imprecise probabilities in
bayesian networks. Artificial Intelligence, 88, 1996.
[Las95] K. Laskey. Sensitivity analysis for probability as­
sessments in Bayesian networks. IEEE Transactions on
Man, Cybernetics and Systems, 25(6):901-909, 1995.

[Aki96] Y. Akimoto. A note on uniform asymptotic nor­
mality of Dirichlet distribution. Mathematica Japonica,
44:25-30, 1996.

[LS99] V. Lepar and P. P. Shenoy. A comparison of
Lauritzen-Spiegelhalter, Hugin, and Shenoy-Shafer ar­
chitectures for computing marginals of probability dis­
tributions. In UA/98, 1999.

[BFGK96] C. Boutilier, N. Friedman, M. Goldszmidt, and
D. Koller. Context-specific independence in Bayesian
networks. In UAI-96, 1996.

[Mus93] R. Musick. Minimal assumption distribution
propagation in belief networks. In UA/93, 1993.

[BFH95] Y. Bishop, S. Fienberg, and P. Holland. Dis­
crete Multivariate Analysis- Theory and Practice. MIT
Press, 1995.
[BKRK97] J. Binder, D. Koller, S. Russell, and Ke.
Kanazawa. Adaptive probabilistic networks with hidden
variables. Machine Learning, 29:213-244, 1997.

[P ea88] J. Pearl. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann, 1988.
[V GH01] T. Van Allen, R. Greiner, and P. Hooper.
Bayesian error-bars for belief net inference. Technical
report, University of Alberta, 2001.

