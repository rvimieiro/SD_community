92

An entropy-based learning algorithm of Bayesian conditional trees

Dan Geiger

dang@cs.technion.ac.il
Computer Science Department
Technion�Israel Institute of Technology
Haifa, Israel, 32000

1

Abstract

assumption, however, is too restrictive and not really
needed.

This article offers a modification of Chow
and Liu's learning algorithm in the context
of handwritten digit recognition. The modi­
fied algorithm directs the user to group digits
into several classes consisting of digits that
are hard to distinguish and then construct­
ing an optimal conditional tree representa­
tion for each class of digits instead of for
each single digit as done by Chow and Liu
(1968). Advantages and extensions of the
new method are discussed. Related works of
Wong and Wang (1977) and Wong and Poon
(1989) which offer a different entropy-based
learning algorithm are shown to rest on inap­
propriate assumptions.

A more sophisticated approach was taken by Chow and
Liu (1968). They devised an algorithm that selects,
for each digit c0, a probability distribution p(x I co )
representable by a dependence tree ( to be defined)
that minimizes the divergence to p(x I c0 ) among all
discrete probability distributions representable by a
dependence tree. The divergence between p and p,
also known as the Kullback-Leiber measure and cross­
entropy, is defined as follows:
"'
p(x I co)
D(p(x I c0),p(x I co))= L....J p(x I co) ·log
· (I)
p(x I co)

Introduction

Much work is recently being devoted to learning ar­
bitrary Bayesian networks from data (e.g., Pearl and
Verma (1991), Cooper and Herskovits (1991)). The
purpose of these investigations is to uncover a Bayesian
network that represents a phenomena of interest from
measurements. This article develops, in the context of
handwritten digit recognition, a learning algorithm of
Bayesian networks that have a specific topology called
a conditional tree. The proposed algorithm is very ef­
ficient and it has an entropy-based performance guar­
antee similar to that of Chow and Liu's (1968). The
algorithm learns also Bayesian networks that have a
small cutset of root nodes but its performance deteri­
orates as the size of the cutset increases.
The most accurate representation of a digit using just
a given set of possible measurements is clearly the joint
distribution p(x, c ) where c is a digit and x is a vec­
tor of measurements. However, such a representation
requires exponential number of parameters which are
infeasible to collect, and hard to use in computing the
most likely digit. The simplest solution to this com­
binatorial explosion is to assume that all variables are
conditionally independent given any digit. Such an

•

r

Information theoretic justifications for the use of this
measure can be found for example in (Lewis, 1959).
Chow and Liu's algorithm turned out to be remarkably
efficient; rather than examining each possible depen­
dence tree, the algorithm computes a set of weights
and identifies the maximum weight spanning tree us­
ing these weights. For each pair of variables (xi ,xi)
the appropriate weight is found to be the instanta­
neous mutual information I( Xi, Xj I c = co ) defined
by
I(x;, xi I c =co ) =
p(xi , Xj I co )
L p(x;, Xj I co ) ·log p( 1:j I Co ) p( Xj I Co )
x
Experiments have shown that the error rate of digit
recognition drops to about half compared to the er­
ror rate obtained using the naive assumption of condi­
tional independence of all x,'s given any digit (Chow
and Liu, 1968).
·

.

One should notice that Chow and Liu construct sev­
eral dependence trees, one for each digit, and not a
single dependence tree for the entire domain. Such a
collection of networks was recently termed a Bayesian
multinet (Geiger and Heckerman, 1991). Chow and
Liu apparently observed that the assumptions of con­
ditional independence encoded in a single dependence
tree do not represent the digits as good as the as­
sumptions of (asymmetric) conditional independence
encoded in a set of dependence trees each having a
different structure that depends on the digit.

An Entropy-based Learning Algorithm of Bayesian Conditional Trees

This article offers a modification of Chow and Liu's
learning algorithm. The modified algorithm directs
the user to group digits into several classes consisting
of digits that are hard to distinguish and then con­
structing an optimal dependence tree representation
for each class of digits instead of for each single digit
as done by Chow and.Liu. The algorithm finds a max­
imum weight spanning tree for each class of digits D
where the weight between two variables z; and z; is
found to be the conditional mutual information defined
by
L p(c�c) I(z;, x; I c = c�c).
The main importance of this modification is that
for each class of similar digits D, a dependence tree
can now be constructed using only features that are
judged (manually) to be helpful in making the dis­
tinctions between digits in D. Consequently, noise
that is due to limited-sized samples can be removed
with the help of qualitative knowledge of the digit's
prototypical forms. The new learning algorithm com­
bines an entropy-based optimization criterion together
with Heckerman 's similarity networks representation
scheme (1991).
Another contribution made herein is an investigation
into the relationship between minimization of Bayes
error rate, divergence, and conditional entropy. Re­
lated works of Wong and Wang (1977) and Wong
and Poon (1989) which offer a different entropy-based
learning algorithm are shown to rest on inappropriate
assumptions.
Although this article is self contained knowledge is
assumed of the definition and usage of Bayesian net­
works. For details consult (Pearl, 1988), (Pearl, 1986)
or (Lauritzen and Spiegelhalter, 1988).
2

Figure 1: A conditional tree

·

CkED

Learning Conditional Trees

In this section, the class of functions among which an
approximation for p(x, c) is selected are those probabil­
ity distributions representable by a conditional depen­
dence tree, where z consists of a finite set of variables
XI,. .. , Zn each having a finite domain and c's domain
is finite as well.
Definition A probability distribution p(x, c) is rep­
resentable by a conditional dependence tree if it is of
the form:
n
(
(2)
)
=
p
)
x
P ,c
(c IJp(Xm; I Za(m;)•C),
·

i=l

where Xm,, ... , Zm,. is a permutation of x1, .. . Xn and
a : {1, . . . , n} -+ {0, ... , n} is a function that identifies
for each variable zi a single variable xa(j) (designated
as the parent of Zj ), such that 0 � a(j) < j. When
a (j) = 0, then Xj has no parent other than possibly
c.

For example p(xt, z2, xa, c) = p(c) p(z1 I c)
p(x2 lx 1 , c) · p(za I c) is representable by the condi­
tional tree shown in Figure 1. (Throughout, as a short
hand notation, x; is written instead of Xm; . )
·

·

Equivalently, a probability distribution p is repre­
sentable by a conditional tree if there exists a Bayesian
network representation of p for which node c is a root
node and when node c is removed, the remaining net­
work becomes a set of trees.
The probability distribution f>(x, c) given by Eq. (2)
is called in the literature a product approximation of
p(x, c) because it consists of a product of lower order
components of p(x, c) (Lewis, 1959). We return to this
point in section 4.
The optimization problem can now be stated as fol­
lows. Find a probability distribution p(z, c) of the
form dictated by Eq. (2) such that the divergence,
D(p(x, c) , p(z, c) )

=

is minimized.

L p(
x,c

x,

( z c)
c) . log �( ' ,
p x, c )

(3)

Note the difference between Eq. (1) and Eq. (3). In
the former equation which is used by Chow and Liu,
a minimization procedure is exercised afresh for each
digit co, while in Eq. (3), a minimization procedure is
exercised once for all values of c together. The mo­
tivation for this modification is clarified in the next
section.
The optimization criterion is obtained by plugging the
formula for p(x, c) (Eq. 2) into the divergence measure
(Eq. 3):
D(p(X' c) ' p( X' c))

=

L P(x, c ) log p(
"'c

=

p(x , c)
c). rri=l p(z; I Za(i)o c)
n

p(x, c) ""'
L., p(x, c) log p(c - L., p(z,c)·
)
r,c
z,c

""'

93

94

Geiger

.[

t

log

a.(i)�O, i=l

=

�

p(x;, Xa(i)• c)

1

+ a(i)=O, i=l og

p(Xa(i), c)

p(x;,c)]
�

�

p (x c)
, - LP(x, c)
logp(x; I c)
LP(x, c) log
pC( )
({:00
X0C
j:::;l
- LP(x, c)
:r,c

t

log

a(i)ji!O,i:::;l

p(x;, Xa(i) I c)
p(x ; I c). p(xa(i) I c)

Note that all summands beside the last one are not
affected by the choice of a(i). Therefore, minimizing
D(p(x,c),P(x,c)) over all choices of a ( i) is equivalent
to maximizing the summand,
LP(x,c)
<l',e

t

p (x;,Xa(i) I c)
p(x; I c) .p(xa(i) I c)'

log

a(i);tO,i:::;l

2:

L p(c) I(x;,Xa(i) I c).
·

(4)

The optimization algorithm is now evident.:
•
•

•

For every pair of variables (x;, xi) compute their
mutual weight LeP(c) I(x;,x; I c).
Select the maximum weight spanning tree over the
Xi's using known algorithms ( Even, 1979) and the
weights computed in the previous step.
Add links from c to each x;,unless the equality
p(x; I Xa(i)• c)= p(x; I Xa(i)) holds for every value
of c .
·

The first two steps are justified by Eq. 4).
( The last
step stems from the definition of a link in a Bayesian
network.
3

Conditional trees and Similarity
networks

The last equation is simply the sum of conditional
mutual information over all edges (xi,Xa(i))of the con­
ditional tree,

a(i);to, i:::;l

Rebane and Pearl (1987) extend Chow and Liu's al­
gorithm to learn polytrees provided the given distribu­
tions are polytree-asomorph. Their algorithm can now
be extended to learn any Bayesian network that has
a small cutset of root-nodes. The only change needed
in their algorithm is that the conditional mutual in­
formation must be computed instead of the marginal
mutual information as Rebane and Pearl do. Again,
this procedure is feasible only if the joint sample space
of Yl, ... , Yr is small enough and Yt, ... , Yr are known in
advance to be root nodes.
4

which is equivalent to maximizing the sum,

n

learning algorithm for this type of networks computes
the conditional mutual information between every pair
(x;, Xj) conditioned on every combination of values
for Yt, ... , Yl and selects the maximum weight spanning
tree over the x; 's according to these weights as in the
previous section. Obviously, this procedure is feasi­
ble only if the joint sample space of y1, . .. , Yl is small
enough and Yt, . . . , Yl are known in advance to be root
nodes.

Learning Networks with Small
Cutset of Root Nodes

The above algorithm can be used also to learn more
general Bayesian networks. Suppose y1, . .., Yl are root
nodes and when removing them, the remaining net­
work becomes a tree. The y; 's are called a cutset
because they break all cycles in the network. The

Section 2 develops a formula for edge weights that al­
lows one to build a single conditional tree instead of
building a tree for each digit as done by Chow and
Liu. One justification for this modification is the re­
duction in storage obtained by replacing a set of trees
with a single conditional tree. However this reduction
is quite small because the total number of conditional
probabilities remains unchanged. The only savings are
due to the lesser overhead of storing fewer trees. The
penalty in performance, on the other hand, might be
quite high because if optimal trees of distinct digits
are different from each other, then grouping them to­
gether yields a lesser approximation. An explicit ex­
ample of this phenomena is described by Wong and
Poon (1989).
The genuine motivation for the use of conditional trees
is provided by Beckerman's remarkable work on Simi­
larity networks (1991). Beckerman worked in the con­
text of medical diagnosis but his ideas extend natu­
rally to many pattern recognition tasks. His main ob­
servation, when translated to the terminology of digit
recognition, is that not all measurements help to dis­
tinguish between every pair of digits. For example, the
ratio between height and width ( r ) does not help to
distinguish between a six and a nine, although it does
help to distinguish between a six and a one. Therefore
it is advantageous to seek a probability distribution
in which the equality p(r I six) = p(r I nine) is en­
forced, although this equality is not necessarily present
in any finite sample due to noise. This is an example
of qualitative knowledge that reduces the parameter
space and therefore provides us with the opportunity
to consider additional measurements without assum­
ing larger samples.

An Entropy-based Learning Algorithm of Bayesian Conditional Trees

Figure 2: A similarity network
Figure 3: A global Bayesian network
Similarity networks are defined below. A preliminary
definition is needed. These definitions are borrowed
from (Geiger and Beckerman, 1991).
A cover of a set A is a collection
{A1, ... , A�:} of non-empty subsets of A whose union
is A. Each cover is a hypergraph, called the similarity
hypergraph, where the A; 's are called edges and ele­
ments of A are called nodes. A cover is connected if
the similarity hypergraph is connected.

responding to A;

L

cEA,

Definition

For example, {{0, 6, 9}, {1, 7}, {4, 7, 9}, {5, 6}, {2, 3},
{3, 6, 8}} is a connected cover of the ten digits.
Let {x1,
, Xn, c} be a finite set of variables each hav­
ing a finite set of values, and let p(x1, . . • , Xn, c) be a
probability function having the cross product of these
sets of values as its sample space. Let At, ... , A�: be a
connected cover of the values of c.
• • •

local network D; of p (as­
sociated with A;) is a Bayesian network of the proba­
bility distribution p( x1 ...Xn lA;), i.e, D; is a Bayesian
network of p(xt, . .. , xn) given c draws its values only
from A;. The network obtained from D; by removing
nodes that are not relevant to distinguishing between
elements in A; is called an ordinary local network. The
set of k ordinary local networks is called a similarity
n e two rk of p.
Definition A comprehensive

For example, Figure 2 is an example of a similar­
ity network representation of p(x1, ... , x5, c) where
c has fiv e values Ct, ... , c5. This similarity network
uses the cover At = { c1, c2, c3}, A2 = {C3, c4} and
A3 = {c4, cs}. Variable x1 is the only measurement
that helps to distinguish between c4 and c5, and vari­
able X4 is the only variable that does not help to distin­
guish among { Ct, c2, c3}. Note that many parameters
of the joint distribu tions are not explicitly given. For
example, p (x2 I Xt, cs ) is not given explicitly but it is
equal to p(x2 I x1,c3) because x2 does not appear in
the local network for { c3, c4} and for { c4, c5}.

The similarity network of Figure 2 consists oflocal net­
works each of which is a conditional tree. Each of these
local networks can be constructed using the algorithm
of the previous section subject to minor modifications;
The weights for a link (x;, xi) in a local network cor-

=

{ c;1,

• • •

, c;1} are computed by

p(c) I(x;, Xj I c),
·

where the sum is taken only over the values in A; in­
stead of over all values of c as done in Eq. (4), and
these weights are computed only for vari ables z;, Xj
that help to distinguish among { c; 1, • • • , c;,}.

The following procedure finds an approximation for
p( Xt, , Xn, c) among those probability distributions
representable by a similarity network that consists of
a set of conditional trees.
. • .

1. Select a connected cover { A1,
, A�:} for the do­
main of c.
2. For each A;, select the set of variables F; � x that
help to distinguish between the elements in A;.
3. Construct an optimal conditional tree representa­
tion for each A; using only the variables in F;.
4. Apply arc-reversal transformations (see Shachter,
1986, 1990) until all local networks have a com­
mon ordering of nodes.
5. Combine the local networks constructed in Step
4 by taking the union of their edges (Becker­
man,1990).
• . .

The first step is heuristics because it leaves the user
to select any connected cover for the domain of c. An
appropriate selection is the one that minimizes the car­
dinality of U7=1 F; where F; is defined in Step 2. Step
3 uses the algorithm of the previous section subject to
the above modifications. This step is the main addi­
tion to Beckerman's work. Step 4 is required in order
to prevent loops in the network produced by Step 5.
Step 5 is best explained by an example; The similarity
network of Figure 2 becomes, after applying Step 5,
the Bayesian network shown in Figure 3. Beckerman
( 1990) provides the justification for Step 5. Geiger and
Heckerman (1991) describe an alternative for Step 5.
The main benefit of this algorithm is attained whe n
only a small fraction of relevant variables help to dis­
tinguish between any given pair of classes c; and ci.

95

96

Geiger

This condition holds in the domain of Lymph node
pathology (Heckerman, 1990) where each Ci denotes
a disease. It seems to hold also for digit recognition
where each ci denotes a prototype of some digit (the
number of prototypes is about thirty). Experimental
results are forthcoming.
5

Related optimization procedures

A simple justification for the use of the divergence
measure as an approximation criterion is due to Lewis
(1959). He observed that whenever f> (x, c) is a product
approximation of p(x, c), then the divergence,
D(p(x,c) , f>(x, c)) ;;:;:

where H(p(c)) = L:c p(c) ·logf>(c) and H(f>(c I z)) =
L:.., c p(x, c) · logf>(c I x) . Eq. (7) shows that max­
imi�ing I(x, c) is equivalent to the minimization of
H(p(c I x)),and Eq. (6) shows that H(p(c I x) is
an upper bound of Bayes error rate.
While Wong and Wang's optimization criteria is plau­
sible its computational implementation required addi­
tional assumptions which are shown below to be inap­
propriate. Wong and Wang's optimization problem is
stated as follows. Find a product approximation f> ofp
that minimizes H(f>(c I x)) such that, similar to Chow
and Liu 's work,
n

p(x,c)=p(c) ITp(xi I Xa(i),c),
·

L p(x, c) ·logp(x I c)- L p(x, c) ·logp(x I c),
.,,c

.,,c

reduces to:

i=l

(8)

and, contrary to Chow and Liu's work,
n

L p(x, c) ·logp(x I c)- L f>(x, c) ·logf>(x I c) .

f>(x)= Lfi(x,c)= rrp(xi I Xa(i))·

The first summand remains constant for all selections
of p. The second summand is called the conditional
entropy and it can be rewritten as

The minimization of H(p(c I x)) is achieved by finding
a maximum weight spanning tree over the Xi's such
that the weight for each pair {Xi, Xj ) is

:�:,c

:�:,c

c

i=l

(9)

H(f>(x I c))=- Lf>(c) L f>(x I c) ·logf>( x I c).
C

X

Consequently, minimizing the divergence is a equiva­
lent to minimizing the conditional entropy as long as
the approximate distribution is a product of lower or­
der components ofp. This is an appealing observation
because as H(p(x I c)) is minimized, the distribution
p(x I c) becomes sharper, and hence, on the average,
each measurements' vector x is more inductive about
c. If pis not a product approximation, then the mini­
mization of conditional entropy and cross entropy (di­
vergence) are not necessarily equivalent.
The method of minimizing the divergence evolved as a
practical substitute for the genuine optimization prob­
lem of reducing overall error rate of recognition. It is
not clear that this substitute is the best feasible one to
attain this goal. Wong and Wang {1977) suggest an­
other optimization criterion: the maximization of the
mutual information between x and c,
p(x,c)
I (x, c)= "',
Ll p(x,c) ·log
p(x). p(c) .
.,,c

(5)

Wong and Wang justify their optimization criterion
by showing that it is equivalent to the minimization of
an upper bound of the Bayes error rate. They use an
inequality between Bayes error rate up and H(p(c I x)),
Up�

� ·H(p(c I

x)),

(6)

obtained by Hellman and Raviv (1970) and an alter­
native definition for I(x,c):
(7)
I(x, c)= H(p(c))- H(f>(c I x))

( Actually, this formula is Wong and Poon's version)
Notably, Wong and Wang did not notice that the as­
sumptions encoded by Eq. {8) stand in contrast to
those encoded by Eq. (9) as can be seen by the fol­
lowing example. Suppose only two classes Ct and c2
are considered and suppose x consists of two binary
variables x1 and x2. Suppose further that Xt and x2
are conditionally independent given c, i.e., for c1 and
for c2,
p(xt, x2 I c;)=p(xt I ci) ·p(x2 I Ci) ·
This is a common assumption which realizes Eq. (8).
However, when augmented with the restriction im­
posed by Eq. (9),
p(x1, x2) ;;:;: p(xt) p(x2),
·

it follows that either x 1 is independent of {c, x2} or
x2 is independent of {c,xt} (weak transitivity; Pearl,
1988). In other words, if x1 and x2 are conditionally
independent measurements, then Eq. (9) implies that
one measurement must be completely irrelevant to the
classification process contrary to any reasonable inter­
pretation.
The correct minimization problem must, therefore, ig­
nore Eq. (9). However, minimizing
H(p(c I x)) = H(p(c))- H(p(x)) + H(p(x I c)), (10)

over all selections of pwhere f>(x) = L:c f>( c, x) does
not lead itself to a computationally feasible algorithm
because the second summand cannot be reduced to an

An Entropy-based Learning Algorithm of Bayesian Conditional Trees

additive form. This complexity is perhaps the reason
Wong and Wang introduced Eq. (9).
Wong and Poon (1989) undertook the task of show­
ing that Chow and Liu's algorithm can be derived by
minimizing H(p(c I z)) instead of minimizing the di­
vergence (Eq. 1). However, Chow and Liu's criteria is
actually equivalent to minimizing H(p(z I c)) which is
different from minimizing H(p(c I z)), as can be seen
from Eq. (10). A closer look on Wong and Wang's
result reveals that they assume that H(p(z )) remains
constant for all selections of p in which case the two cri­
teria are indeed identical due to Eq. (10) and the fact
that p(c) = p(c). The added assumption, however, has
no mathematical or methodological justification.

6

Summary

A learning algorithm is presented that combines an
entropy-based optimization criterion with similarity
networks. This algorithm looks promising for domains
of pattern recognition where a network cannot be con­
structed manually. A critical review of related work
provides insight into the relationship between the com­
plexity of the proposed learning algorithm viz-a-viz
various entropy-based optimization criteria.

References
[Cooper and Herskovits,

1991)

ety, B, 50(2), 157-224.

1988] Pearl, J. (1988). Probabilistic Reasoning
in intelligent systems: Networks of plausible in­
ference. Morgan Kaufmann, San Mateo, CA.

[Pearl,

[Pearl, 1986] Pearl, J. (1986). Fusion, propagation,
and structuring in belief networks. Artificial In­

telligence, 29:241-288.

[Pearl and Verma, 1991] Pearl, J. and Verma, T.S.
(1991). A theory of inferred causality. Proceed­
ings of the second conference on the principles o f
knowledge representation and reasoning, Boston,
MA (1991).
[Rebane and Pearl, 1987] Rebane, G. and Pearl J.
(1987). The recovery of causal poly-trees from
statistical data. Uncertainty in AI 9, eds. Kana!,
Levitt, and Lemmer. Elsevier s cience publishers
(North Holland), 1989.
[Shachter, 1986] Shachter, R. (1986). Evaluating influ­
ence diagrams. Operations Research, 34:871-882.
[Shachter,

1990]

Shachter,

R.

(1990).

An

examination of influence diagrams.

20:535-563.

ordered

Networks,

[Wang and Wong, 1979] Wang, C. C., and Wong, A.
K. C. 1979. Classification of discrete data with
feature space transformation. IEEE Trans. on au­

tomatic control, 24(3), 434-437.

Cooper, G., and Her­

skovits E. (1991). Proceedings of the 7th confer­
ence on uncertainty in artificial intelligence. Mor­
gan Kaufmann publishers, CA.

[Chow and Liu, 1968) Chow, C. K., and Liu, C. N.
1968. Approximating discrete probability distri­
butions with dependence trees. IEEE Trans. on

information theory, 14(3), 462-467.

[Even, 1979) Even S. {1979). Graph algorithms. Com­
puter s cience press.

1991] Geiger D., and Hecker­
D. (1991). Advances in probabilistic reason­

[Geiger and Beckerman,
man,

(with discussion). Journal Royal Statistical Soci­

ing. Proceedings of the seventh conference on un­
certainty in artificial intelligence, Morgan Kauf­
man, 118-126.

1991) Beckerman, D. (1991). Probabilis­
tic similarity networks. ACM Doctoral disserta­

[Beckerman,

tion award series, MIT press,

1991.

[Hellman and Raviv, 1970) Hellman, M.E. and Raviv
J. 1970. Probability of error, equivocation, and
the Chernoff bound. IEEE Trans. on information

theory, 16(4), 368-372.

[Lewis, 1959] Lewis, P.M. II. 1959. Approximating
probability distributions to reduce storage re­
quirements. Information and control, 2, 214-225.
[Lauritzen and Spiegelhalter, 1988]
Lauritzen, S.L.; and Spiegelhalter, D.J. 1988. Lo­
cal computations with probabilities on graphical
structures and their application to expert systems

[Wong and Wang, 1977] Wong, A. K. C., and Wang,
C. C. 1977. in Proceedings of the seventh confer­
ence cybernetics society, Washington, 19-21.
[Wong and Poon, 1989] Wong, S. K. M., and Poon, F.
C. S. 1989. Comments on approximating discrete
probability distributions with dependence trees.

IEEE Trans. on PAM!, 11(3), 333-335.

97

