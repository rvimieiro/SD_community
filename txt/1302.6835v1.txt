454

A Probabilistic Calculus of Actions

Judea Pearl
Cognitive Systems Laboratory
Computer Science Department
University of California, Los Angeles, CA

90024

judea@cs. ucla. edu

Abstract
We present a symbolic machinery that admits
both probabilistic and causal information
about a given domain and produces proba­
bilistic statements about the effect of actions
and the impact of observations. The calculus
admits two types of conditioning operators:
ordinary Bayes conditioning, P(ylX = x) ,
which represents the observation X = x, and
causal conditioning, P(yldo( X = x)), read
the probability of Y = y conditioned on hold­
ing X constant (at x) by deliberate action.
Given a mixture of such observational and
causal sentences, together with the topology
of the causal graph, the calculus derives new
conditional probabilities of both types, thus
enabling one to quantify the effects of ac­
tions (and policies) from partially specified
knowledge bases, such as Bayesian networks
in which some conditional probabilities may
not be available.

1

INTRODUCTION

Probabilistic methods, especially those based on
graphical models, have proven useful in tasks of predic­
tion, abduction and belief revision [Pearl 1988, Beck­
erman 1990 , Goldszmidt 1992, Darwiche 1993]. In
planning, however, they are less popular, 1 partly due
to the unsettled, strange relationship between proba­
bility and actions. In principle, actions are not part
of standard probability theory, and understandably
so: probabilities capture normal relationships in the
world, while actions represent interventions that per­
turb those relationships. It is no wonder, then, that ac­
tions are treated as foreign entities throughout the lit­
erature on probability and statistics; they serve neither
as arguments of probability expressions nor as events
for conditioning such expressions. Even in the decision
theoretic literature, where actions are the target of op-

1Works by Dean & Ka.naza.wa. [1989)
[1993] notwithstanding.

et al.

and Kushmerick

timization, the symbols given to actions serve merely
as indices for distinguishing one probability function
from another, not as propositions that specify the im­
mediate effects of the actions. As a result, if we are
given two probabilities, PA and Ps, denoting the prob­
abilities prevailing under actions A or B, respectively,
there is no way we can deduce from this input the prob­
ability PAAB corresponding to the joint action A 1\ B,
or any Boolean combination of the propositions A and
B. This means that, in principle, the impact of all
anticipated joint actions would need to be specified in
advance-an insurmountable task by any standard.
The peculiar status of actions in probability theory
can be seen most clearly in comparison to the status
of observations. By specifying a probability function
P( s) on the possible states of the world, we automat­
ically specify how probabilities would change with ev­
ery conceivable observation e, since P(s) permits us
to compute (using Bayes rule) the posterior probabil­
ities P(Eie) for every pair of events E and e. How­

ever, specifying P(s) tells us nothing about how our
probabilities should be revised as a response to an ex­
ternal action A. In general, if an action A is to be de­
scribed as a function that takes P( s ) and transforms it
to PA ( s ) , then Bayesian conditioning is clearly inade­
quate for encoding this transformation. For example,
consider the statements: "I have observed the barom­
"
eter reading to be x and "I intervened and set the
barometer reading to x" If processed by Bayes con­
ditioning on the event "the barometer reading is x" ,
these two reports would have the same impact on our
current probability function, yet we certainly do not
consider the two reports equally informative about an
incoming storm.
.

The engineering solution to this problem is to include
the acting agents as variables in the analysis, construct
a distribution function including the behavior of those
agents, and infer the effect of the action by condition­
ing those "agent variables" to a particular mode of
behavior. Thus, for example, the agent manipulat­
ing the barometer would enter the system as a vari­
able such as, "Squeezing the barometer" or "Heating
the barometer". After incorporating this variable into
the probability distribution, we could infer the impact

A Probabilistic Calculus of Actions

of manipulating the barometer by simply conditioning
the distribution on the event "Squeezing the barometer
reached level x". This is, in effect, the solution adopted
in influence diagrams (IDs), the graphical tool pro­
posed for decision theory [Howard & Matheson 1981,
Shachter 1986]. Each anticipated action is represented
as a variable (a node in the diagram), and its impact
on other variables is assessed and encoded in terms of
conditional probabilities, similar to the impact of any
other parent node in the diagram.
The difficulties with this approach are twofold. First ,
the approach is procedural (rather than declarative)
and therefore lacks the semantics necessary for sup­
porting symbolic derivations of the effects of actions.
We will see in Section 3 that such derivations be­
come indispensable in processing partially specified di­
agrams. Second, the need to anticipate and represent
all relevant actions in advance renders the elicitation
process unduly cumbersome. In circ u i t diagnosis, for
example, it would be awkward to represent every con­
ceivable act of component replacement (similarly, ev­
ery conceivable connection to a voltage source, cur­
rent source, etc.) as a node in the diagram. Instead,
the effects of such actions are implicit in the circuit
diagram itself and can be inferred directly from the
(causal) Bayesian network that represents the work­
ings of the circuit.2 We therefore concentrate our
discussion on knowledge bases where actions are not
represented explicitly. Rather, each action will be in­
dexed by a proposition which describes the condition
we wish to enforce directly. Indirect consequences
of these condi tions will be inferred from the causal
relationships among the variables represented in the
knowledge base.
As an alternative to Bayesian conditioning, philoso­
phers [Lewis 1976] have studied another probability
transformation called "imaging" which was deemed
useful in the analysis of subjunctive conditionals and
which more adequately represents the transformations
associated with actions. Whereas Bayes condition­
ing P(sle) transfers the entire probability mass from
states excluded by e to the remaining states (in pro­
portion to their current P(s)), imaging works differ­
ently : each excluded state s transfers its mass in­
dividually to a select set of states S*(s), considered
"closest" to s. While providing a more adequate and
general framework for actions, imaging leaves the pre­
cise specification of the selection function s· ( s ) almost
unconstrained. The task of formalizing and represent­
ing these specifications can be viewed as the proba2 Causal information can in fact be viewed as an implicit
encoding of responses to future actions, and, in practice,
causal information is assumed and used by most decision
analysts. The ID literature's insistence on divorcing the
links in the ID from any causal interpretation [Howard &
Matheson 1981, Howard 1989] is, therefore, at odds with
prevailing practice. Section 2 of this paper can be viewed
as a way to formalize and reinstate the causal reading of
influence diagrams.

455

bilistic version of the infamous frame problem and its
two satellites, the ramification and concurrent a ctions
problems.
An assumption commonly found in the literature is
that the effect of an elementary action do(q) is merely
to change q to q where the current state satisfies q
and, otherwise, to leave things unaltered.3 We can call
this assumption the "delta" rule, variants of which are
embedded in STRIPS as well as in probabilistic plan­
ning systems. In BURIDAN [Kushmerick et al. 1993],
for example, every action is specified as a probabilistic
mixture of several elementary actions, each operating
under the delta rule.
•

•

The problem with the delta rule and its variants is that
they do not take into account the indirect ramifications
of an action such as, for example, those triggered by
chains of causally related events. To handle such ram­
ifications, we must construct a causal theory of the
domain, specifying which event chains are likely to be
triggered by a given action (the ramification problem)
and how these chains interact when triggered by sev­
eral actions (the concurrent action problem). Elabo­
rating on the works of Dean and Wellman [1991], this
paper shows how the frame, ramification, and concur­
rency problems can be handled effectively using the
language of causal graphs, (see also [Darwiche & Pearl
1994]).
The key idea is that causal knowledge can efficiently
be organized in terms of just a few basic mechanisms,
each involving a relatively small number of variables
and each encoded as a set of functional constraints
perturbed by random disturbances. Each external el­
ementary action overrules just one mechanism while
leaving the others unaltered. The specification of
an action then requires only the identification of the
mechanisms that are overruled by that action. Once
these mechanisms are identified, the effect of the ac­
tion (or combinations thereof) can be computed from
the constraints imposed by the remaining mechanisms.
The semantics behind causal graphs and their relations
to actions and belief networks have been discussed in
prior publications [Pearl & Verma 1991, Goldszmidt
& Pearl 1992, Druzdzel & Simon 1993, Pearl 1993a,
Spirtes et al . 1993, Pearl 1993b]. In Spirtes Elt al.
[1993] and later in Pearl [ 1993b], for example, it was
shown how graphical representation can be used to
facilitate quantitative predictions of the effects of in­
terventions, including interventions that were not con­
templated during the network's construction. Section
2 reviews this aspect of causal networks, following the
formulation in [Pearl 1993b].
The main problem addressed in this paper is quantifi­
cation of the effects of interventions when the causal
graph is not fully parameterized, that is, when we are
3This assumption corresponds to Dalal's [1988]
database update, which uses the Hamming distance to de­
fine the �closest world" in Lewis's imaging.

456

Pearl

given the topology of the graph but not the conditional
probabilities on all variables. In this situation, numer­
ical probabilities are given to only a subset of vari­
ables, in the form of unstructured conditional prob­
ability sentences. This is a unless you have a com­
parative realistic setting in AI applications, where the
user/designer might not have either the patience or
the knowledge necessary for specification of a com­
plete distribution function; some combinations of vari­
ables may be too esoteric to be assigned probabilities,
and some variables may be too hypothetical (e.g., "life
style" or "attitude") to even be parameterized numer­
ically.
To manage this problem, this paper introduces a calcu­
lus that operates on whatever probabilistic and causal
information is available and, using symbolic transfor­
mations on the input sentences, produces probabilis­
tic assessments of the effects of actions. The calculus
admits two types of conditioning operators: ordinary
Bayes conditioning, P(y\X = x ) ; and causal condi­
tioning, P(yido(X = x)), that is, the probability of
Y = y conditioned on holding X constant (at x) by
deliberate external action.4 Given a causal graph and
an input set of conditional probabilities, the calcu­
lus derives new conditional probabilities of both the
Bayesian and the causal types and, whenever possi­
ble, generates closed form expressions for the effect of
interventions in terms of the input information.
2

THE MANIPULATIVE READING
OF CAUSAL NETWORKS: A
REVIEW

The connection between the probabilistic and the ma­
nipulative readings of directed acyclic graphs (DAGs)
is formed through Simon's [1977] mechanism-based
model of causal ordering.5 In this model, each child­
parent family in a DAG G represents a determinist ic
function
(1)
where pa; are the parents of variable X; m G, and
f;, 0 < i < n, are mutually independent, arbitrarily
distributed random disturbances. A causal theory is
a pair < P, G >, where G is a DAG and P is the
probability distribution that results from the functions
/; in ( 1 ) .
Characterizing each child-parent relationship as a de­
terministic function, instead of the usual conditional
probability P(x; I pa1), imposes equivalent indepen­
dence constraints on the resulting distributions and
4The notation set(X = x) was used in [Pearl 1993b),
while do(X = x) was used in [Goldszmidt and Pearl1992).
5This mechanism-based model was adopted in [Pearl &
Verma 1991] for defining probabilistic causal theories. It
has been elaborated in Druzdzel & Simon [1993] and is
also the basis for the "invariance" principle of Spirtes et
al. [1993].

leads to the same recursive decomposition
P(xt. . .. , Xn) = IJ P(x; I pa1)

(2)

that characterizes Bayesian networks [Pearl 1988].
This is so because each f; is independent on all non­
descendants of X;. However, the functional character­
ization X; = f,(pa;, £;)also specifies how the resulting
distribution would change in response to external in­
terventions, since, by convention, each function is pre­
sumed to remain constant unless specifically altered.
Moreover, the nonlinear character of/; permits us to
treat changes in the function/; itself as a variable, F;,
by writing
(3)
X; = /[{pa;, F;, t:; )
where

ff(a, b, c) = f; (a, c) whenever b = f;
Thus, any external intervention F; that alters /; can
be represented graphically as an added parent node
of X;, and the effect of such an intervention can be
analyzed by Bayesian conditionalization, that is, by
simply setting this added parent variable to the ap­
propriate value f;.
The simplest type of external intervention is one in
which a single variable, say X;, is forced to take on
some fixed value, say, x�. Such intervention, which we
call atomic, amounts to replacing the old functional
mechanism X; = /;(pa;, £i) with a new mechanism
X; = x� governed by some external force F; that sets
the value x�. If we imagine that each variable Xi could
potentially be subject to the influence of such an ex­
ternal force F;, then we can view the causal network G
as an efficient code for predicting the effects of atomic
interventions and of various combinations of such in­
terventions.

G

G'

Figure 1: Representing external intervention F; by an
augmented network G' = G U { F; -+ X;}.
The effect of an atomic intervention do(X; = xD is
encoded by adding to G a link F; --+ X; (see Fig­
ure 1), where F; is a new variable taking values in
{do(xD, idle}, xi ranges over the domain of X,, and
idle represents no intervention. Thus, the new parent
set of X; in the augmented network is pa: = pa1 U{ Fi},
and it is related to X; by the conditional probability

{

P(x; I paD
P(x; I pa;)
=
0
1

if F; = idle
if F; = do(xi) and x; #- x�
if F; = do(xi) and x; =xi

(4)

A Probabilistic Calculus of Actions

The effect of the intervention do( .:r:D is to transform
the original probability function P(.:r:1, . .. , Xn) into a
new function P.,•(xio . .. , .:r:n), given by
'

P.,•(Xt, ... ,.:r:n)
'

=

P'(.:r:t, . . . ,.:r:n I F;:::: do(.rD)

(5 )

where P' is the distribution specified by the augmented .
network G' = GU {F;-+ X;} and Eq. (4), with an ar­
bitrary prior distribution on F;. In general, by adding
a hypothetical intervention link F; -+ X; to each node
in G, we can construct an augmented probability func­
tion P'(.r1, . .. , xn; Ft, ..., Fn) that contains information
about richer types of interventions. Multiple interven­
tions would be represented by conditioning P' on a
subset of the F;'s (taking values in their respective
do(xD), while the pre-intervention probability func­
tion P would be viewed as the posterior distribution
induced by conditioning each F; in P' on the value
idle.
This representation yields a simple and direct trans­
formation between the pre-intervention and the post­
intervention distributions:6

) { :fzt,� ,xn)

pz: (Xt, ... ,Xn =

..
pa,

if
if

x;

=

x�

(6 )
of X�
This transformation reflects the removal of the term
P(x; I pa;) from the product decomposition of Eq.
(2), since pa; no longer influence X;. Graphically, the
removal of this term is equivalent to removing the links
between pa ; and X;, while keeping the rest of the net­
work intact.
0

X;

Xj

The transformation (6) exhibits the following proper­
ties:

1. An intervention do(x;) can affect only the descen­

dants of X; in G.
2. For any set S of variables, we have

P:�:;(S I pa;) = P(S I x;, pa;)
(7)
In other words, given X; = x; and pa;, it is super­
fluous to find out whether X; = x; was established
by extern al intervention or not. This can be seen
directly from the augmented network G' (see Fig­
ure 1), since {X;} U pa; d-separates F; from the
rest of the network, thus legitimizing the condi­
tional independence S JL F; I (X;, pa;).

3. A sufficient condition for an external intervention
do(X; = x;) to have the same effect on X; as the
passive observation X; = x; is that X; d-separates
pai from Xi, that is,
II pa; I X;
P'(x;ldo(x;)) = P(x; I xi) iff X; (8)
6Eq. (6) is a special case of the Manipulation The­
orem of Spirtes et al. (1993] which deals with interven­
tions that modify several conditional probabilities simulta­
neously. According to this source, Eq. (6) was "indepen­
dently conjectured by Fienberg in a seminar in 1991". An
additive version of Eq. (6) was independently presented in
(Goldszmidt & Pearl 1992}.

457

The immediate implication of Eq. (6) is that, given the
structure of the causal network G, one can infer post­
intervention distributions from pre-intervention distri­
butions; hence, we can reliably estimate the effects
of interventions from passive (i.e., nonexperimental)
observations. However, use of Eq. (6) is limited for
several reasons. First, the formula was derived under
the assumption that the pre-intervention probability
P is given by the product of Eq. (2), which represents
general domain knowledge prior to making any spe­
cific observation. Second, the formula in Eq. (6) is
not very convenient in practical computations, since
the joint distribution P(x1, ... , Xn) is represented not
explicitly but implicitly, in the form of probabilistic
sentences from which it can be computed. Finally,
the formula in Eq. (6) presumes that we have suffi­
cient information at hand to define a complete joint
distribution function. In practice, a complete specifi­
cation of P is rarely available, and we must predict the
effect of actions from a knowledge base containing un­
structured collection of probabilistic statements, some
observational and some causal.
The first issue is addressed in [Pearl 1993a and Balke
& Pearl 1994] , where assumptions about persistence
are added to the knowledge base to distinguish prop­
erties that terminate as a result of an action from those
that persist despite that action. This paper addresses
the latter two issues It offers a set of sound (and pos­
sibly complete) inference rules by which probabilistic
sentences involving actions and observations can be
transformed to other such sentences, thus providing
a syntactic method of deriving (or verifying) claims
about actions and observations. We will assume, how­
ever, that the knowledge base contains the topological
structure of the causal network G, that is, some of
its links are annotated with conditional probabilities
while others remain unspecified. Given such a par­
tially specified causal theory, our main problem will
be to facilitate the syntactic derivation of expressions
of the form P(xildo(x;)).
3
3.1

A CALCULUS OF ACTIONS
PRELIMINARY NOTATION

Let X, Y, Z, W be four arbitrary disjoint sets of nodes
in the DAG G. We say that X andY are independent
given Z in G, denoted (X II YI Z)a, if the set Z d­
_
separates X fromY in G. We denote by Gx (G�
respectively) the graph obtained by deleting from G
all arrows pointing to (emerging from, respectively)
nodes in X.
_ _,

Finally, we replace the expression P(yldo(x), z ) by a
shorter expression P(yli:, z), using the ' symbol to
identify the variables that are kept constant externally.
In words, the expression P(yli:, z ) stands for the prob­
ability ofY = y given that Z = z is observed and X
is held constant at x.

458

Pearl

3.2

3.3

INFERENCE RULES

Armed with this notation, we are now able to formu­
late the three basic inference rules of the proposed cal­
culus.

Theorem

3.1 Given a causal theory < P, G
any sets of variables X,Y,Z, W we have:

>,

Rule

(Bayes

1 Insertion/deletion of observations
conditioning)

for

EXAMPLE

We will now demonstrate how these inference rules can
be used to quantify the effect of actions, given partially
specified causal theories. Consider the causal theory
< P(x, y,z), G >, where G is the graph given in Fig­
ure 2 below and P(x, y, z) is the distribution over the

o

P(ylx,z,w) = P(ylx,w) if (Y _II ZIX, W)Gx

Rule 2 Action/observation exchange
P(ylx, i,w)

Rule

3

=

/

.
X

actions

P(yx,z,
1" " w) = P(yx,w
1' ) 1f (Y 11 ZIX W) G
'
-x zcw)
whereZ ( W) is the set ofZ nodes that are not
ancestors of any W node in Gx.

Each of the inference rules above can be proven from
the basic interpretation of the "do{x)" operation as a
replacement of the causal mechanism t.hat connects X
to its parent prior to the action with a new mechanism
X = x introduced by the intervening force (as in Eqs.
(4)- (5)).

Rule 3 provides conditions for introducing (or delet­
ing) an external intervention do(Z = z) without affect­
ing the probability ofY == y. Such operation would
be valid if the d-separation (Y II FziX, W) is satis­
fied in the augmented graph G�, since it implies that
the manipulating variables Fz have no effect on Y .
The condition used in Rule 3, Y
II ZjX, W)G( X Z(W)
translates the one above into d-separation betweenY
andZ (in the unaugmented graph) by pruning the ap­
propriate links entering Z.

�

y

observed variables X,Y,Z. Since U is unobserved, the
theory is only partially specified; it will be impossible to infer all required parameters, such as P(u) or
P(yz
l ,u). We will see, however, that this structure
still permits us to quantify, using our calculus, the effeet of every action on every observed variable.
The applicability of each inference rule requires that
certain d-separation conditions hold in some graph,
whose structure will vary with the expressions to be
manipulated. Figure 3 displays the graphs that will
be needed for the derivations that follow.

�

/0�

·-·-·

Rule 1 reaffirms d-separation as a legitimate test for
Bayesian conditional independence in the distribution
determined by the intervention do(X = x ) , hence the
graph Gx·
Rule 2 provides conditions for an external intervention
doZ
( = z) to have the same effect. on Y as the passive
observationZ = z. The condition is equivalent to Eq.
( 8), since Gxz eliminates all paths from Z toY (in
Gy) which do-not go through paz.7

z

(Unobserved)

Figure 2

P(ylx, z,w) if Y
( II ZIX, W) GX�

Insertion/deletion of

.

U

•

X

•

Y

Z

X

•

Z

X

Gi

z

I

1-1
r x
z

Y

Gz=�

0�/�
I

•

I

�

1-1
rx
z

I

r

Figure 3
Task-1, compute P(zlx)
This task can be accomplished in one step, since G
satisfies the applicability condition for Rule 2, namely
X II Z in Gx (because the path X <- U -+Y <-Z
is blocked by the collider atY ), and we can write
(9)
P(zlx) == P(zlx)

-·

7

This condition was named the "back-door" criterion

in [Pearl 1993b], echoing the requirement that only indi­
rect paths from Z to Y be d-separated; these paths can be
viewed as entering Z through the back door. An equiva­
lent, though more complicated, graphical criterion is given
in Theorem

7.1 of (Spirtes et al.

1993].

Task-2, com pute P(ylz)
Here we cannot apply Rule 2 to substitute z for z
because Gz contains a back-door path fromZ toY .
Naturally, we would like to "block" this path by con­
ditioning on variables (such as X) that reside on that
path. Symbolically, this operation involves condition­
ing and summing over all values of X,
l )
P(yz
l ) = 2:: P(ylx, i)P(xi
(10)

A Probabilistic Calculus of Actions

We now have to deal with two expressions involving
z, P(ylx, z) and P(xi.Z). The latter can be readily
computed by applying Rule 3 for action deletion:
P(xjz)

=

P(x) if (Z _II X)az-

(11)

noting that, indeed, X and Z are d-separated in Gz.
(This can be seen immediately from Figure 2; manip­
ulatingZ will have no effect on X.) To reduce the
former quantity, P(yjx, i), we consult Rule 2
P(yjx, z)

=

( _II YjX)a�
P(yjx, z) if Z
�

P(yli) =

L P(yjx, z)P(x) = ExP(yl:c, z)
X

(13)

which is a special case of the back-door formula [Pearl
1993b, Eq. (11)] with S = X. This formula ap­
pears in a number of treatments on causal effects
(e.g., [Rosenbaum & Rubin 1983, Pratt & Schlaifer
1988, Rosenbaum 1989;])

where the legitimizing condi­

tion, ( Z II Y IX)a z, was given a variety of names, all
based oncondition�l-independence judgments about
counterfactual variables. Action calculus replaces such
judgments by formal tests ( d-separation) on a single
graph (G) that represents the domain knowledge.
We are now ready to tackle the evaluation of P(yjx),
w hich cannot be reduced to an observational expres­
sion by direct application of any of the inference rules.

Task-3,

compute

Writing
P(ylx)

P(yjx)

=

L P(ylz, x)P(zjx)

(14)

we see that the term P(z!x) was reduced in Eq. (9)
while no rule can be applied to eliminate the manipu­
lation symbol from the term P(yjz, i:). However, we
can add a symbol to this term via Rule 2
·

·

P(yjz,i:)

=

P(yji,x)

(15)

since Figure 3 shows
(Y II Z IX)a�
X.f
We can now delete the action x from P(ylz, i:) using
Rule 3, sinceY � � X!Z holds in Gxz· Thus, we have
�
�

P(yjz, i:)

=

P(ylz)

( 16)

which was calculated in Eq. (13). Substituting Eqs.
(13), (16), and (9) back into Eq. (14) finally yields
P(yji:)

=

L P(zjx) L P(ylx', z)P(x')
z

x'

intermediate variable Z that is affected by X.
Task-4, compute

(17)

In contrast to the back-door formula of Eq. (13), Eq.
(17) computes the causal effect of X on Y using an

P(y, zji:)

P(y, z!i:) = P(yjz, i:)P(z!x)
(18)
The two terms on the r.h.s. were derived before in
Eqs. (9) and (16), from which we obtain
P(y, zlx)

(12)

and note that X d-separates Z from Y in Gz. This
allows us to write Eq. (10) as

459

3.4

=

=

P(yji)P(zjx)
P(z\x) I:x' P(yj:c', z)P(x')

DISCUSSION

Computing the effects of actions by using partial theo­
ries in which probabilities are specified on a select sub­
set of (observed) variables is an extremely important
task in statistics and socio-economic modeling, since it
determines when causal effects are "identifiable" (i.e.,
estimable consistently from non-experimental data)
and this when randomized experiments are not needed.
The calculus proposed here, reduces the problem of
identifiability to the problem of finding a sequence of
transformations, each conforming to one of the infer­
ence rules in Theorem 3.1, which reduces an expression
of the form P(yli:) to a standard (i.e., hat-free) prob­
ability expression. Note that whenever a reduction is
possible, the calculus provides a closed form expression
for the desired causal effect.
The proposed calculus uncovers many new structures
that permit the identification of causal effects from
nonexperimental observations. For example, the struc­
ture of Figure 3 represents a large class of observa­
tional studies in which the causal effect of an action
(X) can be determined by measuring a variable (Z)
that mediates the interaction between the action and
its effect (Y ). Most of the literature on statistical ex­
perimentation considers the measurement of interme­
diate variables, affected by the action, to be useless,
if not harmful, for causal inference [Cox 1958, Pratt
& Schlaifer 1988]. The relevance of such structures
in practical situations can be seen, for instance, if we
identify X with smoking, Y with lung cancer, Z with
the amount of tar deposited in a subject's lungs, and
U with an unobserved carcinogenic genotype that, ac­
cording to the tobacco industry, also induces an in­
born craving for nicotine. In this case, Eq. (17) would
provide us with the means to quantify, from nonexper­
imental data, the causal effect of smoking on cancer.
(Assuming, of course, that the data P(x, y, z) is made
available and that we believe that smoking does not
have any direct causal effect on lung cancer except
that mediated by tar deposits).
In this example, we were able to compute answers to
all possible queries of the form P(yjz, i:) where Y, Z,
and X are subsets of observed variables. In general,
this will not be the case. For example, there is no
general way of computing P(yji:) from the observed
distribution whenever the causal model contains the
subgraph shown in Figure 4, where X and Y are adja-

460

Pearl

cent and the dashed line represents a path traversing

L\
y

X

z

using the steps that led to Eq. {16). Note that this
derivation is still valid when we add a common cause
to X and Z, which is the most general condition un­
der which the transitivity of causal relationships holds.
In [Pearl 1994] we present conditions for transforming
P(yjx) into expressions in which only members of Z
obtain the hat symbol. These would enable an agent to
measure P(yjx) by manipulating a surrogate variable,
Z, which is easier to control than X.

Figure 5

Figure 4

3.5

unobserved variable.8 Similarly, our ability to com­
pute P(yji) for every pair of singleton variables does
not ensure our ability to compute joint distributions,
such as P(Yt, Y2Jx). Figure 5, for example, shows a
causal graph where both P(yd.i) and P(y2]x) are com­
putable, but P(y1, Y2Ji) is not; consequently, we can­
not compute P(zji). Interestingly, the graph of Figure
5 is the smallest graph that does not contain the pat­
tern of Figure 4 and still presents an uncomputable
causal effect. Graphical criteria for identifiability and
nonidentifiability are given in [Pearl 1994}.
Another interesting feature demonstrated by the net­
work in Figure 5 is that it is often easier to compute
the effects of a joint action than the effects of its con­
stituent singleton actions9. In this example, it is pos­
sible to compute P(zji, !h) and P(zji:, 111), yet there is
no way of computing P(zji). For example, the former
can be evaluated by invoking Rule 2, giving
P(z]i,1h) =

CONDITIONAL ACTIONS AND
STOCHASTIC POLICIES

The interventions considered thus far were uncondi­
tional actions that merely force a variable or a group
of variables X to take on some specified value x. In
general, interventions may involve complex policies in
which a variable X is made to respond in a specified
way to some set Z of other variables, say through a
functional relationship X = g( Z) or through a stochas­
tic relationship whereby X is set to x with probability
P*(xlz). We will show that computing the effect of
such policies is equivalent to computing the expression
P(yjx, z).
Let P(yjdo(X = g(Z))) stand for the distribution (of
Y) prevailing under the policy (X = g(Z)). To com­
pute P(yjdo(X = g(Z))), we condition on Z and write
P(yido(X = g(Z)))

L P(yjdo(X
z

L P(z]yt, X.!h)P(yl]idh)
!/I

=

g(z)), z)P(zjdo(X = g(z)))

L P(yjx, z)i:.=g(z)P(z)
z

E. [P(yj£, z) l:r.=g( •)J

On the other hand, Rule 2 cannot be applied to the
computation of P(y1Ji:, y2) because, conditioned on
Y2 , X and Yt are d-connected in Gx (through the
dashed lines). We conjecture, however, that when­
ever P(yji:;) is computable for every singleton Xi, then
P(y]i1. .i2, . .. , xt) is computable as well, for any subset
of variables {Xt, ..., X1}.
Our calculus is not limited to the derivation of causal
probabilities from noncausal probabilities; we can de­
rive conditional and causal probabilities from causal
expressions as well. For example, given the graph
of Figure 2 together with the quantities P(zjx) and
P(yji), we can derive an expression for P(yjx),
P(yJ.i)

=

L P(yJi)P(zlx)
z

(19)

80ne can calculate strict upper and lower bounds on

P(yjx) and these bounds may coincide for special distribu­
tions, P( x, y, z ) [Balke & Pearl 1994), but there is no way
of computing P(ylx) for every distribution P(x, y, z).

9The fact that the two tasks are not equivalent was
brought to my attention by James Robins, who has worked
out many of these computations in the context of sequential
treatment management [Robins 1989).

The equality
P(zjdo(X

=

g(z)))

=

P(z)

stems, of course, from the· fact that Z cannot be a
descendant of X, hence, whatever control one exerts
on X, it can have no effect on the distribution of Z.
Thus, we see that the causal effect of a policy X =
g(Z) can be evaluated directly from the expression of
P(yjx, z), simply by substituting g(z) for x and taking
the expectation over Z (using the observed distribu­
tion P(z)).
The identifiability condition for policy intervention is
somewhat stricter than that for a simple intervention.
Clearly, whenever a policy do(X = g(Z)) is identifi­
able, the simple intervention do(X = x ) is identifi­
able as well, as we can always get the latter by setting
g(Z) = X. The converse, does not hold, however,
because conditioning on Z might create dependencies
that will prevent the successful reduction of P(yjx, z )
to a hat-free expression.
A stochastic policy, which imposes a new conditional
distribution P*(xjz) for x, can be handled in a similar

A Probabilistic Calculus of Actions

manner. We regard the stochastic intervention as a
random process in which the unconditional interven­
tion do(X = x) is enforced with probability P*(xiz).
Thus, given Z = z, the intervention set(X = x) will
occur with probability P*(xiz) and will produce a
causal effect given by P(y!x, z). Averaging ove r x and
z gives
P(yiP*(xlz))

=

L L P(ylx, z)P*(xiz)P(z)
r

z

Since P'"(x!z) is specified externally, we see again that
the identifiability of P(ylx, z) is a necessary and suffi­
cient condition for the identifiability of any stochastic
policy that shapes the distribution of X by the out­
come of Z.
Of special importance in planning is a STRIP-like ac­
tion whose immediate effects X = x depend on the
satisfaction of some enabling precondition C(w) on a
set W of variables. To represent such actions, we let
Z = WUpax and set

P*(xlz) =

{

P(xlpax) if C(w) =false
if C(w) = true and X = x
1
if C(w) = true and X :f. x
0

It should be noted, however, that in planning appli­
cations the effect of an action may be to invalidate
its preconditions. To represent such actions, tempo­
rally indexed causal networks are necessary [Dean &
Kanazawa 1989, Pearl 1993a, Balke & Pearl 1994) .
4

CONCLUSIONS

The calculus proposed in this paper captures in sym­
dist in ct ion between
seeing and doing. While many systems have imple­
mented this obvious distinction-from early systems
of adaptive control to their modern AI counterparts of
[Dean and Kanazawa 1989] and [Draper et al. 1994) ­
the belief-changing operators of seeing and doing can
now enjoy the power of symbolic manipulations. The
calculus permits the derivation of expressions for states
of belief that result from sequences of actions and ob­
bols and graphs the conceptual

servations, which, in turn, should permit the identifi­

cation of variables and relationships that are crucial
for the success of a given plan or strategy. The ex­
ercise in Section 3.3, for example, demonstrates how
predictions about the effects of actions can be derived
from passive observations even though portions of the
knowledge base (connected with the unobserved vari­
able U) remain inaccessible. Another possible applica­
tion of the proposed calculus lies in the area of learn­
ing , where it migh t facilitate the in t egrati on of the two
basic modes of human learning: learning by manipu­
lation and learning by observation.
The immediate beneficiaries of the proposed calculus
would be social scientists and clinical trilists, as the

461

calculus enables experimental researchers to translate
complex considerations of causal interactions into a
formal language, thus facilitating the following tasks:
1. Explicate the assumptions underlying the model.
2. Decide whether the assumptions are sufficient for
obtaining consistent estimates of the target quan­
tity: the total effect of one variable on another.
3. If the answer to item 2 is affirmative, the method
provides a closed-form expression for the target
qu antit y, in terms of distributions of observed
quantities.
4. If the answer to item 2 is negative, the method
suggests a set of observations and experiments
which, if performed, would render a consistent es­
timate feasible.
The bizzare confusion and controversy surrounding the
role of causality in statistics stems largely from the
lack of mathematical notation for defining, expressing,
and manipulating causal relationships. Statisticians
will benefit, therefore, from a calculus that integrates
both statistical and causal information, and in which
causal influences are kept distinct from probabilistic
dependencies.
There are also direct applications of action calculus
to expert systems and Bayesian networks technology.
One conceptual contribution, mentioned in Section 1,
is the appeal to causality for inferring the effect of
certain actions without those actions being explicitly
encoded in the knowledge base. This facility simplifies
the knowledge elicitation process by focusing atten­
tion on causal relationships and by dispensing with
the specification of actions whose effects can be in­
ferred from those relationships.
A second contribution involves the treatment of hid­
den variables. Such variables represent factors that the
expert chooses to exclude from formal analysis, either
because they lie beyond the scope of the domain or be­
cause they are inaccessible to measurement. The ex­
ample of Section 3.3 demonstrates that certain queries
can be answered precisely without the parameters as­
sociated with hidden variables assessing. Action cal­
culus should identify the conditions under which such
assessments can be saved.
Acknowledgments

This investigation was inspired by Goldszmidt's for­
maliza.tion of actions in nonmonotonic reasoning
[Goldszmidt 1992) and by [Spirtes et al. 1993) , in which
a graphical account of manipulations was first pro­
posed. The investigation also benefitted from discus­
sions with Adnan Darwiche, Phil Dawid, Arthur Gold­
berger, Ed Leamer, John Pratt, James Robins, Donald
Rubin, Keunkwan Ryu, Glenn Shafer, and Michael So­
bel. The research was partially supported by Air Force
grant #F49620-94-1-0173, NSF grant #IRI-9200918,
and Northrop-Rockwell Micro grant #93-124.

462

Pearl

References

Lewis, D.K. (1976) Probability of Conditionals and
Conditional Probabilities. Philosophical Review, 85,
297-315.

Balke, A . , and J. Pearl ( 1 994) Counterfactual Proba­
bilities: Computational Methods, Bounds, and Appli­
cations. This volume.

Pearl, J . (1988) Probabilistic Reasoning i n Intelligent
Systems: Networks of Plausible Inference. San Mateo,

Cox, D.R. (1958) The Planning
York: John Wiley and Sons.

Dalal, M. (1988) Investigations into a Theory of
Knowledge Base Revision : Preliminary Report. In

Pearl, J. ( 1993a) From Conditional Oughts to Qualita­
tive Decision Theory. in Proceedings of the Ninth Con­
ference on Uncertainty in A rtificial Intelligence (eds.
D. Heckerman and A. Mamdani) , 12-20.

Proceedings of the Seventh National Conference on A r­
tificial Intelligence, 475-479.

Pearl , J . (1993b) Graphical Models, Causality, and In­
tervention. Statistical Science, 8(3), 266-273.

Darwiche, A. ( 1993) Argument Networks: Explicating
Independence in Propositional Databases. In Proceed­

Pearl, J. (1994) Causal Diagrams for Experimental Re­
search. Technical Report R-218-L, UCLA Cognitive
Systems Laboratory.

of Experiments,

New

ings of the Ninth Conference on Uncertainty in A rti­
ficial Intelligence (eds. D. Heckerman and A. Mam­

dani), 420-427.

Darwiche, A. and Pearl, J. ( 1994) Symbolic Causal
Networks for Reasoning about Actions and Plans.
Symposium Notes of the 1 994 AAAI Spring Sympo­
sium on Decision- Theoretic Planning, 41-47.

Dean, T., and K. Kanazawa ( 1989) A Model for Rea­
soning about Persistence and Causation. Computa­
tional Intelligence, 5, 142- 150.
Dean, T., and M. Wellman ( 1991 ) Planning
trol. San Mateo, CA: Morgan Kaufmann .

and Con­

Draper, D . , S. Hanks, and D. Weld ( 1 994) A Proba­
bilistic Model of Action for Least-Commitment Plan­
ning with Information Gathering. This volume.
Druzdzel, M .J . , and H.A. Simon ( 1993) Causality in
Bayesian Belief. In Proceedings of the Ninth Confer­
ence on Uncertainty in A rtificial Intelligence { eds. D.
Beckerman and A. M amdani) , CA, 3- 1 1 .
Goldszmidt, M . ( 1 992) Qualitative Probabilities: A
Normative Framework for Commonsense Reasoning.
Technical Report R- 190, UCLA Cognitive Systems
Laboratory; Ph.D . thesis, UCLA.
Goldszmidt, M., and J . Pearl (1992) Default Rank­
ing: A Practical Framework for Evidential Reason­
ing, Belief Revision and Update. In Proceedings of the
Third International Conference on Knowledge Repre­
sentation and Reasoning, 66 1-672.

Heckerman, D. ( 1990) Probabilistic Similarity Net­
works. Networks, 20 (5) , 607-636.
Howard, R. (1989) Knowledge Maps.
35(8), 903-922.

Management

Science,

Howard, R. and J.E. Matheson (1981) Influence Dia­
grams. Principles and Applications of Decision A nal­
ysis , Strategic Decisions Group, Menlo Park, CA.
Kushmerick , N . , S. Hanks, and D. Weld ( 1 993) An
Algorithm for Probabilistic Planning. Technical Re­
port 93-06-03, Department of Computer Science and
Engineering, University of Washington .

CA: Morgan Kaufmann.

Pearl , J . , and T. Verma ( 1991) A Theory of Inferred
Causation. In Principles of Knowledge Representa­
tion and Reasoning: Proceedings of the Second Inter­
national Conference (eds. J .A. Allen, R. Fikes, and E.

Sandewall), 441-452.

Pratt, J . , and R. Schlaifer ( 1988) On the Interpretation
and Observation of Laws. Journal of Economics, 39,
23-52.
Robins, J .M. (1989) The Analysis of Randomized and
Non-Randomized AIDS Treatment Trials using a New
Approach to Causal Inference in Longitudinal Stud­
ies. In Health Service Research Methodology: A Focus
on A IDS (eds. L. Sechrest, H . Freeman, and A. Mul­
ley), 1 13-1 59. Washington, DC: NCHSR, U .S. Public
Health Service.
Rosenbaum, P.R. ( 1989) The Role of Known Effects
in Observational Studies. Biometrics, 45, 557-569.
Rosenbaum, P. , and D. Rubin ( 1983) The Central
Role of Propensity Score in Observational Studies for
Causal Effects. Biometrica, 70, 4 1-55.
Simon, H.A. ( 1977)

Models of Discovery: and Other
Topics in the Methods of Science. Dordrecht, Holland:

D. Reidel.
Shachter, R.D. ( 1986) Evaluating influence diagrams.
Operations Research, 34(6), 871-882.
Spirtes, P., C. Glymour, and R. Schienes (1993) Cau­
sation, Prediction, and Search. New York: Springer­
Verlag.

