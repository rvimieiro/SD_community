Sensitivity Analysis for Threshold Decision Making with Dynamic Networks

Theodore Charitos and Linda C. van der Gaag
Department of Information and Computing Sciences, Utrecht University
P.O Box 80.089, 3508 TB Utrecht, The Netherlands
{theodore,linda}@cs.uu.nl

Abstract
The effect of inaccuracies in the parameters of
a dynamic Bayesian network can be investigated
by subjecting the network to a sensitivity analysis. Having detailed the sensitivity functions
involved in our previous work, we now study
the effect of parameter inaccuracies on a recommended decision in view of a threshold decisionmaking model. We describe the effect of varying
one or more parameters from a conditional probability table and present a computational procedure for establishing bounds between which
assessments for these parameters can be varied
without inducing a change in the recommended
decision. We illustrate the various concepts by
means of a real-life dynamic network in the field
of infectious disease.

1 Introduction
Probabilistic graphical models are often used in contexts
where human decision makers have to make a decision in
uncertainty. The marginal probability distributions yielded
by the model then are taken as input to a decision-making
model. The simplest model for choosing between alternative decisions is the threshold decision-making model, in
which an output probability is compared against a number
of fixed threshold probabilities which demarcate the boundaries for the various decisions [13]. In our application for
ICU care, for example, a clinician has to decide whether or
not to start antibiotics treatment for a patient who is suspected of having ventilator-associated pneumonia (VAP),
based upon the probability of VAP being present.
Probabilistic graphical models are typically learned from
data or constructed with the help of domain experts. Due
to incompleteness of data and partial knowledge of the domain under study, the numerical parameters of the model
tend to be inaccurate to at least some degree. The inaccuracies may affect the output probabilities of the model as well
as the decisions based upon these probabilities. The effects

of inaccuracies in the parameters of a network on its output probabilities can be studied by subjecting the network
to a sensitivity analysis [2, 7, 8, 9]. In view of a decisionmaking model, however, robustness of the output of a probabilistic graphical model pertains not just to the computed
output probabilities but also to the decisions based upon
these probabilities. In this paper, we study this type of robustness for dynamic Bayesian networks (DBNs) in view
of the threshold decision-making model.
Previous work on sensitivity analysis of Bayesian networks
(BNs) in general showed that any posterior probability for
an output variable is a quotient of two linear functions in
any of the network’s parameters [8]; the posterior probability can further be expressed as a sum of such functions
in all parameters from a single conditional probability table
[2]. Building upon these results, we show that in sensitivity
analysis of DBNs a quotient of polynomial functions is obtained, where the order of these polynomials is linear in the
time scope taken into consideration [4, 6]. We further show
how the resulting functions can be used to study the robustness of a decision that is based upon an output probability
of the network. By doing so, we focus not just on the effect of varying a single parameter, but also on the effect of
varying all parameters from a given conditional probability table. In our medical application, for example, we thus
provide for studying the extent to which the sensitivity and
specificity rates of a particular diagnostic test can be varied without affecting the clinician’s decision for treatment.
We illustrate the various concepts involved by means of a
sensitivity analysis of our dynamic network in the field of
infectious disease [5].
Establishing the sensitivity functions for a DBN has a time
complexity similar to that of performing exact inference in
such a model. Using quotients of higher-order polynomials
for further processing in view of the threshold decisionmaking model can also be highly demanding from a computational point of view [6]. To handle the complexity involved, we present an approximate method for sensitivity
analysis that reduces the runtime requirements involved yet
incurs only a small loss in accuracy.

three input processes (summarised in immunological status), three input observable variables (hospitalisation, mechanical ventilation, and previous antibiotics), one hidden
input variable (aspiration), and seven output observable
variables (summarised in symptoms-signs). Per time step,
representing a single day, the model includes 30 variables.
Each of the interacting processes consists of seven subprocesses that are a-priori independent. The transition matrices of these subprocesses are moderately stochastic. Figure
1 shows the dVAP network in a compact way.
Figure 1: The dVAP model for the diagnosis of VAP for two
consecutive time steps; clear nodes are hidden, shaded nodes are
observable. The dashed boxes indicate the hidden processes.

3 Sensitivity analysis revisited

2 Preliminaries

Having been studied extensively in the context of BNs, sensitivity analysis has received less attention in DBNs. In
this section, we briefly review previous work on sensitivity
analysis in BNs and extend it to a dynamic context.

The simplest type of dynamic network is a hidden Markov
model (HMM) H = (X, Y, A, O, Γ) involving a single
stochastic process [14]. We use Xn to denote the hidden
variable of the modelled process at time step n. Xn has
the possible states xni , i = 1, . . . , l, l ≥ 2. The transition matrix for Xn is denoted as A = {ai,j } with elements
ai,j = p(Xn+1 = xn+1
| Xn = xni ), i, j = 1, . . . , l, for all
j
n. We denote the observable variables by Yn , with values yjn , j = 1, . . . , m, m ≥ 2. The value of Yn is generated
from the state of the hidden variable according to a timeinvariant observation matrix O = {oi,j } with oi,j = p(Yn =
yjn | Xn = xni ), i = 1, . . . , l, j = 1, . . . , m, for all n. We
further denote by Γ = {γi } the initial probability vector for
the hidden variable, with γi = p(X1 = x1i ), i = 1, . . . , l. A
DBN of more general structure is an extension of an HMM,
capturing a compound process that involves a collection of
hidden variables. We assume that our dynamic networks
are time invariant, that is, neither the topology nor the parameters of the model change across time steps.
In this paper, we focus on the task of monitoring in DBNs,
that is, on computing marginal distributions for the model’s
hidden variables for some time step n given the observations that are available up to and including that time
step. For this purpose, the interface algorithm is available
[11], which basically is an extension of the junction-tree
algorithm for probabilistic inference in graphical models
in general. The interface algorithm efficiently exploits the
concept of forward interface, which is the set of variables
at time step n that affect some variables at time step n + 1
directly; in the sequel, we use F I to denote this set of variables. The computational complexity of the interface algorithm is exponential in the number of hidden variables at
each time step, which is prohibitive for larger models.
Throughout the paper we will use the dVAP network for illustration purposes. The dVAP network is a real-life DBN
for diagnosing VAP in ICU patients and is destined for
use in clinical practice [5]. The network includes two interacting hidden processes (colonisation and pneumonia),

3.1 Sensitivity analysis of BNs
Sensitivity analysis of a BN amounts to establishing, for
each of the network’s parameter probabilities, a function
that expresses an output probability of interest in terms of
the parameter under study [7, 9, 16]. We take the posterior
probability p(b | e) for our probability of interest, where
b is a specific value of the variable B and e denotes the
available evidence; we further let θ = p(hi | π) be our
parameter under study, where hi is a value of the variable
H and π is a specific combination of values for the parents
of H. The sensitivity of the probability p(b | e) to variation
of the parameter θ now is expressed by a sensitivity function
p(b | e)(θ). If the parameters p(hj | π), hj 6= hi , specified
for H are co-varied proportionally according to
½
θ
if j = i
p(hj | π)(θ) =
1−θ
p(hj | π) · 1−p(h
otherwise
i |π)
for p(hi | π) < 1, then this function is a quotient of two
linear functions in θ, that is,
p(b | e)(θ) =

p(b, e)(θ)
c1 · θ + c0
=
p(e)(θ)
d1 · θ + d0

where c1 , c0 , d1 and d0 are constants with respect to θ [7].
Under the assumption of proportional co-variation, therefore, any sensitivity function is characterised by at most
three constants. Note that for parameters of which the probability of interest is algebraically independent, the function
simply equals the posterior probability p(b | e). Any computations can therefore be restricted to the sensitivity set
for the variable of interest, which can be readily established
from the network’s graphical structure. An efficient scheme
for sensitivity analysis is available [8], which requires an
inward propagation in the junction-tree for processing evidence and an outward propagation for establishing the constants of the sensitivity functions for all parameters per output probability.

3.2 Sensitivity analysis of DBNs

0.35
0.3

n−1
+ . . . + c1n,r · θa + c0n,r
p(xnr )(θa ) = cn−1
n,r · θa
0
cn−1
n,r , . . . , cn,r

are constants with respect to θa dewhere
pendent on n; note that the function is a polynomial of
order n − 1 in the parameter under study. For an initial
parameter θγ = γi ∈ Γ, the function is linear
p(xnr )(θγ ) = c1n,r · θγ + c0n,r
where c1n,r and c0n,r are constants with respect to θγ . For an
HMM in which no evidence has been entered, the observable variables do not belong to the sensitivity set of the hidden variable. Its prior probability therefore is algebraically
independent of any observation parameter.
We now assume that some evidence has been entered into
the model; we use en to denote the cumulated evidence up
to and including time step n. For the sensitivity function
that expresses the posterior probability p(xnr | en ) in terms
of a transition parameter θa = ai,j ∈ A, we find that
n−1
+ . . . + c1n,r · θa + c0n,r
p(xnr , en )(θa ) cn−1
n,r ·θa
= n−1 n−1
p(en )(θa )
dn,r ·θa + . . . + d1n,r ·θa + d0n,r
0
n−1
0
where cn−1
n,r , . . . , cn,r , dn,r , . . . , dn,r again are constants
with respect to θa . The function thus is a quotient of two
polynomials of order n − 1. For an observation parameter
θo = oi,j , the sensitivity function is

p(xnr , en )(θo ) cbn,r ·θob + . . . + c1n,r · θo + c0n,r
= n
p(en )(θo )
dn,r ·θon + . . . + d1n,r ·θo + d0n,r
where b = n if r = i and b = n − 1 otherwise;
cbn,r , . . . , c0n,r , dnn,r , . . . , d0n,r are constants with respect to
θo . The order of the polynomials involved again grows linearly with n. For an initial parameter θγ , to conclude, we
have that the sensitivity function is a quotient of two linear
functions. Similar results hold for probabilities of interest
belonging to any possible time step no < n or no > n [4].
The results for HMMs are readily generalised to DBNs. We
consider the posterior probability of interest p(bnr | en ) of
the state br of the hidden variable Bn given the evidence en .
Then, for any variable Hn ∈ Sens(Bn , en ), the sensitivity
function expressing p(bnr | en ) in θ = p(hni | π) is a quotient

0.25

p(vap4  e4)

The main difference with sensitivity analysis of BNs is that
a parameter occurs multiple times in a DBN. In previous
work [4], we derived functional forms to express the sensitivity of a probability of interest of an HMM in terms of
a parameter under study. We briefly review these sensitivity functions. We begin by studying the sensitivities of an
HMM in which no evidence has been entered as yet. The
probability of interest is the prior probability p(xnr ) of some
state xr of the hidden variable Xn . Let θa = ai,j ∈ A be
a transition parameter in the model such that Xn is algebraically dependent on θa . Then,

0.2
0.15
0.1
0.05
0
0

0.2

0.4

θ →

0.6

0.8

1

Figure 2: The sensitivity function expressing the probability of
pneumonia at day 4 given evidence e4 for a patient, in terms of
the parameter θ = p(leucocytosis = yes | pneumonia = yes).

of two polynomials of order n−1 if Hn ∈ F I, or of order
n otherwise.
As an example sensitivity function, Figure 2 depicts, for
the dVAP network, the effect of varying the parameter
θ = p(leucocytosis = yes | pneumonia = yes) on the
probability of pneumonia at day 4 given evidence e4 for a
specific patient. The depicted function is a quotient of two
polynomials of order 4 each. For computing the constants
in the various sensitivity functions, we combined the interface algorithm with the junction-tree scheme for sensitivity
analysis; further details of the resulting algorithm are out
of the scope of this paper.

4 Threshold decision making
BNs in general yield marginal probability distributions for
their output. Often these marginal distributions are input
to a decision maker who has to make a decision. The simplest model for choosing between alternative decisions is
the threshold decision-making model. In this section, we
briefly review the threshold model for decision making and
describe sensitivity analysis of BNs in view of this model.
Although generally applicable, the threshold decisionmaking model is used most notably for patient management
in medicine [13]. Since our dVAP network also pertains
to the field of medicine, we present the model in medical
terms. With the model, a clinician decides whether or not
to gather additional information from diagnostic tests and
whether or not to give treatment based upon the probability
of disease for a patient. The threshold model to this end
builds upon various threshold probabilities of disease.
The treatment threshold probability p∗ is the probability
at which the clinician is indifferent between giving treatment and withholding treatment. If, for a specific patient,
the probability of disease exceeds the treatment threshold
probability, then the clinician will decide to treat the patient
as if the disease were known to be present with certainty.
Alternatively, if the probability of disease is less than p∗ ,
the clinician will basically withhold treatment. As a consequence of the uncertainty concerning the true condition

no treat p−

p+

test
no treat

p

∗

treat

treat

Figure 3: The threshold decision model.
of the patient however, additional information from a diagnostic test may affect the clinician’s basic management
decision. The threshold model therefore includes another
two threshold probabilities. The no treatment-test threshold probability of disease p− is the probability at which
the clinician is indifferent between the decision to withhold
treatment and the decision to obtain additional diagnostic
information. The test-treatment threshold probability p+
is the probability at which the clinician is indifferent between obtaining additional information and starting treatment rightaway. Figure 3 illustrates the various threshold
probabilities employed by the model.
In view of the threshold model for decision making, sensitivity of the output of a network pertains no longer to just a
probability of interest computed from the network, but also
to the decision based upon it. To take the various threshold probabilities employed into consideration, the method
of sensitivity analysis of BNs has been enhanced with the
computation of upper and lower bounds between which
a network’s parameters can be varied without inducing a
change in decision [15]. The computation of these bounds
builds upon the sensitivity functions relating the probability of interest to the network’s parameters. By equating the
function for a specific parameter to the various threshold
probabilities, bounds are obtained between which the parameter can be varied. Since the sensitivity functions for a
BN are either monotonically non-decreasing or monotonically non-increasing, a single lower bound and a single
upper bound are guaranteed to exist.

5 Sensitivity analysis of decisions with DBNs
The probabilities established from a dynamic network are
also often employed for decision making. The goal of
the dVAP model, for example, is to monitor the onset
of ventilator-associated pneumonia in ICU patients and to
start appropriate treatment as soon as possible. Sensitivity
to parameter variation then pertains not just to the probability of VAP but also to the decision that the clinician makes
based upon this probability. To provide for studying this
type of sensitivity, we extend sensitivity analysis of DBNs
in view of threshold decision making.
5.1 Analysis of single parameters
Suppose that a posterior probability p(xnr | en ) of interest
has been computed from a DBN; based upon this probability, a particular decision has been established from the
threshold decision-making model. We now are interested

in the effect of variation of a parameter θ on this decision.
To compute upper and lower bounds between which the
parameter can be varied without inducing a change in decision, the sensitivity function p(xnr | en )(θ) is equated to the
threshold probabilities p− and p+ , respectively. The lower
and upper bounds thus are solutions of the equations
p(xnr , en )(θ)
p(xnr , en )(θ)
= p− and
= p+
p(en )(θ)
p(en )(θ)

(1)

We recall that for DBNs a sensitivity function in general is
a quotient of higher-order polynomials. Contrary to threshold decision making for BNs therefore, there is no guarantee that these functions are monotonically non-decreasing
or non-increasing. The equations stated above may thus
have multiple solutions instead of single ones.
We begin by studying a parameter for which single solutions exist for the two threshold equations above. Suppose
that the lower and upper bounds computed from the equations are θ− and θ+ respectively. If p(xnr | en ) < p− ,
then the decision to withhold treatment remains unaltered
for any value of θ within the interval (−∞, θ− ) ∩ [0, 1].
If p(xnr | en ) > p+ , the decision to start treatment immediately remains unaltered for any value of θ within
(θ+ , +∞) ∩ [0, 1]. If p− ≤ p(xnr | en ) ≤ p+ , then the
decision to gather further information will be the same for
any value of θ within the interval (θ− , θ+ ) ∩ [0, 1].
As an example from the dVAP network, we illustrate the
bounds on variation of the parameter θ = p(leucocytosis =
yes | pneumonia = yes) in view of the management decision for a particular patient at day 4; Figure 2 shows the
sensitivity function that expresses the probability of VAP
for this patient in terms of θ. Suppose that the three threshold probabilities are fixed at p∗ = 0.2, p− = 0.12, and
p+ = 0.64. From the dVAP network, we have that p(vap4 |
e4 ) = 0.134 and hence that p− ≤ p(vap4 | e4 ) ≤ p+ . The
clinician thus decides to gather additional information for
the patient. Solving the two threshold equations from (1),
we find a lower bound on the parameter under study equal
to θ− = 0.676; the upper bound is θ+ = 1.1063. For any
value of the parameter within the interval (0.676, 1], therefore, the decision to gather additional information will remain unaltered. Since the original value of the parameter
has been assessed at 0.7, we conclude that the decision is
not very robust with regard to this parameter.
We now turn to parameters for which the threshold equations have multiple solutions. Suppose that from the notreatment-test threshold probability p− , we find the vector of solutions θ − = (θ1− , θ2− , . . . , θr− ), in which the parameter values θi− are given in ascending order; from the
test-treatment threshold probability p+ , we find the vector
θ + = (θ1+ , θ2+ , . . . , θs+ ), again with the θi+ in ascending
order. We further use v(θi ) to denote the value of the first
derivative of the sensitivity function for the parameter θ at
θi . The value of the first derivative helps in determining

1

treat

0.9

p+=0.88

p(vap4 e4) →

0.8
0.7

test

0.6
0.5

p−=0.34

0.4
0.3
0.2

no treat

0.1
0

0

0.2

0.4

θ →

0.6

0.8

1

Figure 4: Threshold decision making for the treatment of pneumonia when varying the parameter θ = p(temperature = low |
pneumonia = no, a.drugs = yes).

the intervals in which a particular decision holds. Now, if
the output probability p is smaller than p− , the decision
to withhold treatment remains unaltered for any value of θ
that belongs to the compound interval
½
[0, θ1− )∪(θ2− , θ3− )∪. . .∪(θr− , 1]
r is even
Θ−
=
−
+
[0, θ1− )∪(θ2− , θ3− )∪. . .∪(θr−1
, θr− ) r is odd
whenever v(θ1− ) > 0, and for any value of θ belonging to
½ − −
−
(θ1 , θ2 )∪(θ3− , θ4− )∪. . . ∪ (θr−1
, θr− ] r is even
Θ−
=
− −
− −
−
−
(θ1 , θ2 )∪(θ3 , θ4 )∪. . . ∪ (θr , 1]
r is odd
whenever v(θ1− ) < 0. Similarly, if the output probability p
+
is larger than p+ , we find compound intervals Θ+
− and Θ+
for the parameter θ within which the decision to start treatment immediately remains unaltered. Finally, if the output
probability lies between p− and p+ , the vectors θ − and θ +
are merged into the vector θ m = (θ1m , θ2m , . . . , θqm ), q =
r + s. Now, the decision will be the same for any value of
θ within the interval Θm
½
m
, θqm ) v(θ1m ) < 0
[0, θ1m )∪(θ2m , θ3m )∪. . .∪(θq−1
Θm =
m
, θqm ) v(θ1m ) > 0
(θ1m , θ2m )∪(θ3m , θ4m )∪. . .∪(θq−1
Note that in case v(θ1m ) > 0 and the value of the sensitivity
function for θ = 0 is greater than p− , the interval Θm is the
same as when v(θ1m ) < 0.
As an example, Figure 4 depicts the probability of pneumonia at day 4 given evidence e4 in terms of the parameter
θ = p(temperature = low | pneumonia = no, a.drugs =
yes); the original value of θ equals 0.2, giving p(vap4 |
e4 ) = 0.278. The figure also shows the intersection points
with the threshold probabilities, which have been set at
p− = 0.34 and p+ = 0.88. We compute the two lower
bounds to be θ1− = 0.0918 and θ2− = 0.4335; the upper
bound is θ1+ = 0.8781. Using v(θ1− ) < 0, we find that the
decision to withhold treatment remains unaltered for any
value of θ in (0.0918, 0.4335). We conclude that the decision is relatively robust with regard to the parameter.
5.2 Analysis of full conditional probability tables
In addition to single parameters, we may be interested in
the effects of varying multiple parameters, for example

Figure 5: The sensitivity function expressing the probability of
pneumonia given e5 in terms of the sensitivity θse and specificity
θsp rates of the CPT for radiological signs.

from a single conditional probability table (CPT) [3]. In a
medical application for instance, we may wish to study the
robustness of a decision in terms of both the sensitivity and
the specificity of a particular diagnostic test and not just in
one of these rates. Recall that these rates express the probabilities that a test result is found to be positive (negative)
in a patient who does (does not) have the disease. We now
extend the previous results for single parameters to CPTs
to provide for such an analysis.
For BNs, any posterior probability for the output variable
is a quotient of sums of linear functions in the parameters
of a CPT [3, 8]. For a dynamic network we obtain sums of
polynomial functions. For a joint probability p(bnr , en ) we
find that
|πCn |
X
p(bnr , en ) =
gi (θi )
i=1

where |πCn | denotes the number of parent configurations
of the variable Cn and gi (θi ) represents a polynomial function in the parameter θi for a specific parent configuration
for Cn . The polynomial functions gi (θi ) are all of the same
order and can be computed individually using the considerations of the previous section. For the joint probability
p(en ) a similar result holds. We conclude that, for a DBN,
the sensitivity function for a CPT is a quotient of sums of
higher-order polynomials in the parameters under study.
As an example, Figure 5 illustrates the effect of varying the
sensitivity and specificity rates of the CPT for radiological
signs of pneumonia at day 5 given evidence e5 for a specific
patient. The depicted sensitivity function is a quotient of
sums of two polynomial functions of order 5 each.
Upon studying the effects of varying all parameters from
a CPT in view of threshold decision making, we have to
solve threshold equations similar to (1). For a single parameter, we identified intervals for the parameter’s value
within which a decision remains unaltered. For a CPT, we
now identify areas in higher-space with the same meaning.
In the remainder of this section, we consider a CPT with

1

•

0.5
0.45

treat

0.4

+

p =0.64

0.3

θ

se

0.35

test

0.25

•

0.2
0.15
0.1

−

•

p =0.12

0.05

no treat

0
0

0.1

0.2

0.3

0.4

0.5

θsp

0.6

0.7

0.8

0.9

1

Figure 6: Threshold decision making when varying the sensitivity and specificity rates θse , θsp of the CPT for radiological signs.

sensitivity and specificity rates as in the previous example;
similar results hold for more complex CPTs.
We begin again by studying a CPT for which single lower
−
−
and θsp
rebounds exist for the two rates, denoted as θse
spectively. By re-arranging the individual polynomial func−
tions included, we can express the relationship between θse
−
and θsp as
−
−
ge(θse
) = gb(θsp
)
where ge and gb are polynomials of the same order. If ge is
invertible in [0, 1], we have that
−
−
θse
= ge−1 (b
g (θsp
))
−
−
which defines the relationship between the θse
and θsp
. The
horizontal line test can be used for checking whether ge is
invertible in [0, 1]. Establishing ge−1 , however, is computationally expensive if not infeasible [10].
−
−
and
and θsp
To determine the relationship between θse
thereby study the robustness of a recommended decision,
we use a numerical approximation procedure. We repeat0−
0−
edly pick a value θse
∈ [0, 1] and solve for θsp
∈ [0, 1].
0− 0−
From the pairs (θse , θsp ) thus obtained, we construct a line
−
−
. A
and θsp
l− representing the relationship between θse
+
similar approach is used to find a line l that represents the
+
+
relationship between the upper bounds θse
and θsp
for the
two rates. We note that our procedure requires solving just
a single polynomial equation, which is feasible in general
[12]. For larger CPTs, however, the procedure becomes
computationally more demanding, since a larger sample of
points is required to assure good results.

We now have that, if the probability of interest p(xnr | en )
falls below p− , the decision to withhold treatment remains
unaltered for any pair (θse , θsp ) below l− . If p(xnr | en ) >
p+ , the decision to start treatment remains unaltered for
any pair (θse , θsp ) above l+ . Finally, if p− ≤ p(xnr | en ) ≤
p+ , the decision will be the same for any pair (θse , θsp )
between l− and l+ .
Figure 6 now illustrates the sensitivity analysis. With our
approximation procedure, two lines are established that

serve to divide the unit square into three areas in which
different decisions apply. For our example patient, we have
that p(vap5 | e5 ) = 0.9842 > p+ . Since the original values
for the sensitivity and specificity rates under study are 0.9
and 0.95 respectively, we conclude that the decision to start
treatment right away is quite robust with regard to the CPT.
The three bullets in the figure highlight three other interesting cases. For the bullet with θsp = 0.6, we observe that
the decision to test is only moderately robust since a small
change in θsp or θse can alter the decision. For the bullet
with θsp = 0.8, the decision to treat is quite robust since
only a major change in θsp or θse can induce another decision. Finally, for the bullet with θsp = 0.87, the decision
to test is not robust at all since a small change in θsp or θse
suffices to alter the decision.
To conclude, we note that when the function ge−1 is not invertible, our sampling procedure will result in multiple solutions. The unit square for θse and θsp will then be divided
in compound areas per decision, similar to the compound
intervals in the single-parameter case.

6 An approximate scheme
The number of constants in the sensitivity functions of a
DBN and the number of propagations required to compute
these constants grows linearly with n. Moreover, the computational burden of solving polynomials of high order can
grow dramatically [12]. For a larger time scope, therefore,
sensitivity analysis in view of threshold decision making
can become quite hard. To reduce the order of the polynomials in the sensitivity functions and thereby the runtime
requirements, we present a method for approximate sensitivity analysis that builds on the concept of contraction of a
Markov process [1]. We discuss our method for DBNs with
a single hidden process and review its extension to DBNs
with multiple processes.
We consider two probability distributions µ and µ0 over a
variable W . Conditioning on a set of observations is known
to never increase the relative entropy of these distributions.
Denoting the conditioning by o(·), we thus have that
D[o(µ)ko(µ0 )] ≤ D[µkµ0 ]

(2)

where D stands for the relative entropy. Now, consider the
extreme case where µ and µ0 have their entire probability
mass on two different states wi and wk respectively. We
denote by A(·) the distribution that results from processing through the transition matrix A. Even though µ and µ0
do not agree on any state, processing will cause them to
place some mass
£ on some state wj . They then
¤ agree for a
mass of min A(µ(wj ; wi )), A(µ0 (wj ; wk )) on that state
wj . Based on this property, the minimal mixing rate of the
matrix A is defined as
X
£
¤
δA = min
min A(µ(wj ; wi )), A(µ0 (wj ; wk ))
i,k

j

Given the minimal mixing rate of a transition matrix A, the
following theorem now holds [1]:
D[A(µ)kA(µ0 )] ≤ (1 − δA ) · D[µkµ0 ]
We say that the stochastic process with transition matrix
A contracts with probability δA . Combining equation (2)
with the previous theorem gives
D[A(o(µ))kA(o(µ0 ))] ≤ (1 − δA ) · D[µkµ0 ]
Performing conditioning on two different distributions and
subsequently transitioning them, will thus result in two new
distributions whose distance in terms of relative entropy is
reduced by a factor smaller than one.
Our approximate method for sensitivity analysis now builds
on the contraction property reviewed above. Suppose that
we are interested in the probability of some state of the hidden variable Xn at time step n. After entering the available
evidence en into the model, we can compute the exact posterior distribution p(Xn | en ). Building on the contraction
property, however, we can also compute an approximate
distribution pe(Xn | en ) starting from time step nφ , with
1 < nφ < n, without losing too much accuracy. We define
φ
the backward acceptable window ωn,²
for time step n with
a specified level of accuracy ², to be the number of time
steps we need to use from the past to compute the probability distribution of the hidden variable at time step n within
an accuracy of ². We now propose to perform sensitivity
analysis for time step n considering only the backward acφ
. Note that the resulting functions
ceptable window ωn,²
then include polynomials of order O(n − nφ ) rather than
of order O(n) compared to the true functions.
For a given level of accuracy ², we have to determine the
maximum value of nφ for which
D[p(Xn | en )ke
p(Xn | en )] ≤
n−nφ
(1 − δA )
· D[p(Xnφ | enφ )kp(X1 )] ≤ ²
where pe(Xn | en ) denotes the approximate distribution of
φ
Xn that is computed using ωn,²
. Solving for nφ , we find
that
(
$
¡
¢ %)
log ²/D[p(Xnφ | enφ )kp(X1 )]
nφ = max 1, n−
log(1 − δA )
(3)
where b·c stands for the integer part. Starting from nφ =
n and decreasing the value of nφ one step at a time, we
can readily establish the value of nφ that first satisfies the
equation (3). To this end, the interface algorithm needs to
have computed and stored the exact posterior distributions
p(Xno | eno ) for all no ≤ n, given evidence eno .
The procedure to compute the optimal value nφ requires at
most n computations of (3) and thus is not very demanding from a computational point of view. We recall, how-

ever, that for the computation of nφ , the interface algorithm needs to have established the exact posterior distributions given the available evidence. Now in a full sensitivity analysis, the effects of parameter variation are being
studied for a number of evidence profiles. The above procedure may then become rather demanding since for every
such profile a full propagation with the interface algorithm
is required. An alternative way would then be to approximate nφ given ² from the start and perform the entire analφ
ysis with the backward acceptable window ωn,²
. If we assume that D[p(Xnφ )kp(X1 )] is bounded from above by a
known constant M , we find that an approximate value for
nφ would satisfy
%)
(
$
log(²/M )
nφ ≈ max 1, n −
log(1 − δA )
Note that for a given ² and δA , the higher the value of M ,
the smaller the value of nφ and hence the larger the backward acceptable window. Knowledge of the domain under
study can help in determining a suitable value for M . For a
patient profile for example, M can be determined by inserting worst-case scenario observations for the first time step
and computing for that time the posterior probability distribution for the hidden variable from which M can be readily
established. The complexity that our method now entails
is just the complexity of computing M which is similar to
performing a single propagation for a single time step. This
computational burden is considerably less than the burden
of performing nφ time steps of exact inference, which we
thereby forestall in the sensitivity analysis. Note that for
some patients the computation of nφ based upon this value
M will lead to a larger backward acceptable window than
the one computed directly from equation (3).
In view of sensitivity analysis, we observe that the value
of nφ that is established as outlined above, is based on the
original values of all parameters of the model under study.
We further observe that the minimal mixing rate δA used in
the computation of nφ is algebraically dependent only on
φ
the model’s transition parameters. Using ωn,²
based upon
nφ for sensitivity analysis, therefore, is guaranteed to result
in approximate sensitivity functions within an accuracy of
² for any non-transition parameter. For transition parameters, this guarantee does not hold in general. We note, however, that for the original value of a transition parameter,
the difference between the true probability of interest and
the approximate one is certain to be smaller than ². Since
the value nφ changes with δA in a stepwise manner only,
this property holds for a range of values for the parameter. Figure 7 illustrates the relationship between nφ and δA
given particular values for n, ² and M . We observe from
the figure that there is a range of values of δA for which the
value of nφ stays the same. We expect a similar property
to hold for a range of values for the transition parameter
θa . We are currently studying this issue and hope to report
results in the near future.

n = 10, ε = 0.001

References

M = 0.1

10

M = 0.01
9

M = 0.3

8

nφ →

7
6
5
4
3
2
1
0

0

0.2

0.4

δA →

0.6

0.8

1

Figure 7: The relation between nφ and δA .

In general, a DBN with multiple interacting subprocesses
can be represented as a single-process stochastic model
with a global transition matrix AG by enumerating all combinations of values for the subprocesses. We can show that
a lower bound on the global mixing rate δAG can be computed from knowledge of the contraction rates of the individual subprocesses of the model [6]. For a DBN composed
of several sparsely interacting subprocesses each of which
is fairly stochastic, we expect a reasonable lower bound on
the mixing rate δAG . We recall that the larger the mixing
rate, the larger nφ and the smaller the backward window
that we can acceptably use for the sensitivity analysis. For
all patients in our application for ICU care for example,
we found that instead of using the observations for 10 days
upon performing a sensitivity analysis for the probability of
VAP, we could use the observations from day 5 on with an
average error smaller than ² = 0.003. This result is quite
promising as it shows that even if the dynamic processes
of a DBN are only moderately stochastic, the backward
acceptable window can still be small enough to allow for
good approximations of the sensitivity functions.

7 Conclusions
In this paper we presented functional forms to express the
sensitivity of a probability of interest of a DBN in terms of
one or more parameters from a single CPT. We used these
sensitivity functions for studying the robustness of the output of the network in view of the threshold decision making. In addition, we presented an approximate scheme for
computing the constants involved in the sensitivity functions that is less demanding than an exact algorithm yet incurs only a small loss in accuracy. We illustrated our results
with a real-life dynamic network for ICU care.

Acknowledgements
This research was (partly) supported by the Netherlands
Organization for Scientific Research (NWO).

[1] Boyen, X. 2002. Inference and Learning in Complex
Stochastic Processes. Ph.D. diss, Stanford University.
[2] Chan, H. and Darwiche, A. 2002. When do numbers really
matter? Journal of Artificial Intelligence Research, 17: 265287.
[3] Chan, H. and Darwiche, A. 2004. Sensitivity analysis in
Bayesian networks: From single to multiple parameters.
Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence, pp. 67-75.
[4] Charitos, T. and Van der Gaag, L.C. 2004. Sensitivity properties of Markovian models. Proceedings AISTA Conference in Cooperation with IEEE Computer Society, ISBN:
2-9599776-8-8.
[5] Charitos, T., Van der Gaag, L.C., Visscher, S., Schurink, K.
and Lucas, P. 2005. A dynamic Bayesian network for diagnosing ventilator-associated pneumonia in ICU patients.
Working notes of the 10th Workshop on Intelligent Data
Analysis in Medicine and Pharmacology, pp. 32-37.
[6] Charitos, T. and Van der Gaag, L.C. 2006. Sensitivity analysis of Markovian models. Proceedings of the 19th International Florida Artificial Intelligence Research Society Conference, pp. 806-811.
[7] Coupé, V.M.H. and Van der Gaag, L.C. 2002. Properties of
sensitivity analysis of Bayesian belief networks. Annals of
Mathematics and Artificial Intelligence 36: 323-356.
[8] Kjaerulff, U. and Van der Gaag, L.C. 2000. Making sensitivity analysis computationally efficient. Proceedings of the
16th Conference on Uncertainty in Artificial Intelligence,
pp. 317-325.
[9] Laskey, K.B. 1995. Sensitivity analysis for probability assessments in Bayesian networks. IEEE Transactions on Systems, Man and Cybernetics 25: 901-909.
[10] Mourrain, B. 2006. Personal communication.
[11] Murphy, K.P. 2002. Dynamic Bayesian Networks: Representation, Inference and Learning. Ph.D. diss, University of
California Berkley.
[12] Pan, V.Y. 1997. Solving a polynomial equation: Some history and recent progress. SIAM Review 39(2): 187-220.
[13] Pauker, S.G and Kassirer, J.P. 1980. The threshold approach to clinical decision making. New England Journal
of Medicine 302: 1109-1117.
[14] Rabiner, L.R. 1989. A tutorial on hidden Markov models
and selected applications in speech recognition. Proceedings of the IEEE 77(2): 257-286.
[15] Van der Gaag, L.C. and Coupé, V.M.H 2000. Sensitivity
analysis for threshold decision making with Bayesian belief networks. AI*IA 99: Advances in Artificial Intelligence,
Lecture Notes in Artificial Intelligence, pp. 37-48.
[16] Van der Gaag, L.C. and Renooij, S. 2001. Analysing sensitivity data from probabilistic networks. Proceedings of the
17th Conference on Uncertainty in Artificial Intelligence,
pp. 530-537.

