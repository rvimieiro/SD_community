arXiv:1402.6763v1 [math.OC] 27 Feb 2014

Linear Programming for Large-Scale Markov Decision Problems
Yasin Abbasi-Yadkori
Queensland University of Technology
yasin.abbasiyadkori@qut.edu.au

Peter L. Bartlett
UC Berkeley and QUT
bartlett@eecs.berkeley.edu

Alan Malek
UC Berkeley
malek@eecs.berkeley.edu
February 28, 2014

Abstract
We consider the problem of controlling a Markov decision process (MDP) with a large state
space, so as to minimize average cost. Since it is intractable to compete with the optimal
policy for large scale problems, we pursue the more modest goal of competing with a lowdimensional family of policies. We use the dual linear programming formulation of the MDP
average cost problem, in which the variable is a stationary distribution over state-action pairs,
and we consider a neighborhood of a low-dimensional subset of the set of stationary distributions
(defined in terms of state-action features) as the comparison class. We propose two techniques,
one based on stochastic convex optimization, and one based on constraint sampling. In both
cases, we give bounds that show that the performance of our algorithms approaches the best
achievable by any policy in the comparison class. Most importantly, these results depend on
the size of the comparison class, but not on the size of the state space. Preliminary experiments
show the effectiveness of the proposed algorithms in a queuing application.

1

Introduction

We study the average loss Markov decision process problem. The problem is well-understood when
the state and action spaces are small (Bertsekas, 2007). Dynamic programming (DP) algorithms,
such as value iteration (Bellman, 1957) and policy iteration (Howard, 1960), are standard techniques
to compute the optimal policy. In large state space problems, exact DP is not feasible as the
computational complexity scales quadratically with the number of states.
A popular approach to large-scale problems is to restrict the search to the linear span of a small
number of features. The objective is to compete with the best solution within this comparison class.
Two popular methods are Approximate Dynamic Programming (ADP) and Approximate Linear
1

Programming (ALP). This paper focuses on ALP. For a survey on theoretical results for ADP see
(Bertsekas and Tsitsiklis, 1996, Sutton and Barto, 1998), (Bertsekas, 2007, Vol. 2, Chapter 6), and
more recent papers (Sutton et al., 2009b,a, Maei et al., 2009, 2010).
Our aim is to develop methods that find policies with performance guaranteed to be close to
the best in the comparison class but with computational complexity that does not grow with the
size of the state space. All prior work on ALP either scales badly or requires access to samples
from a distribution that depends on the optimal policy.
This paper proposes a new algorithm to solve the Approximate Linear Programming problem
that is computationally efficient and does not require knowledge of the optimal policy. In particular,
we introduce new proof techniques and tools for average cost MDP problems and use these techniques to derive a reduction to stochastic convex optimization with accompanying error bounds.
We also propose a constraint sampling technique and obtain performance guarantees under an
additional assumption on the choice of features.

1.1

Notation

Let X and A be positive integers. Let X = {1, 2, . . . , X} and A = {1, 2, . . . , A} be state and action

spaces, respectively. Let âˆ†S denote probability distributions over set S. A policy Ï€ is a map from
the state space to âˆ†A : Ï€ : X â†’ âˆ†A . We use Ï€(a|x) to denote the probability of choosing action a

in state x under policy Ï€. A transition probability kernel (or transition kernel) P : X Ã— A â†’ âˆ†X

maps from the direct product of the state and action spaces to âˆ†X . Let P Ï€ denote the probability

transition kernel under policy Ï€. A loss function is a bounded real-valued function over state and
action spaces, â„“ : X Ã— A â†’ [0, 1]. Let Mi,: and M:,j denote ith row and jth column of matrix M
P
respectively. Let kvk1,c = i ci |vi | and kvkâˆž,c = maxi ci |vi | for a positive vector c. We use 1 and

0 to denote vectors with all elements equal to one and zero, respectively. We use âˆ§ and âˆ¨ to denote

the minimum and the maximum, respectively. For vectors v and w, v â‰¤ w means element-wise
inequality, i.e. vi â‰¤ wi for all i.

1.2

Linear Programming Approach to Markov Decision Problems

Under certain assumptions, there exist a scalar Î»âˆ— and a vector hâˆ— âˆˆ RX that satisfy the Bellman
optimality equations for average loss problems,
"

Î»âˆ— + hâˆ— (x) = min â„“(x, a) +
aâˆˆA

X

xâ€² âˆˆX

â€²

#

P(x,a),xâ€² hâˆ— (x )

.

The scalar Î»âˆ— is called the optimal average loss, while the vector hâˆ— is called a differential value
function. The action that minimizes the right-hand side of the above equation is the optimal action
in state x and is denoted by aâˆ— (x). The optimal policy is defined by Ï€âˆ— (aâˆ— (x)|x) = 1. Given â„“ and

2

P , the objective of the planner is to compute the optimal action in all states, or equivalently, to
find the optimal policy.
The MDP problem can also be stated in the LP formulation (Manne, 1960),
max Î» ,

(1)

Î»,h

s.t.

B(Î»1 + h) â‰¤ â„“ + P h ,

where B âˆˆ {0, 1}XAÃ—X is a binary matrix such that the ith column has A ones in rows 1 + (i âˆ’ 1)A

to iA. Let vÏ€ be the stationary distribution under policy Ï€ and let ÂµÏ€ (x, a) = vÏ€ (x)Ï€(a|x). We can
write
X

Ï€âˆ— = argmin
Ï€

xâˆˆX

= argmin
Ï€

vÏ€ (x)

X

X

Ï€(a|x)â„“(x, a)

aâˆˆA

ÂµÏ€ (x, a)â„“(x, a) = argmin ÂµâŠ¤
Ï€â„“ .
Ï€

(x,a)âˆˆX Ã—A

In fact, the dual of LP (1) has the form of
min ÂµâŠ¤ â„“ ,

(2)

ÂµâˆˆRXA

s.t. ÂµâŠ¤ 1 = 1, Âµ â‰¥ 0, ÂµâŠ¤ (P âˆ’ B) = 0 .
The objective function, ÂµâŠ¤ â„“, is the average loss under stationary distribution Âµ. The first two
constraints ensure that Âµ is a probability distribution over state-action space, while the last constraint ensures that Âµ is a stationary distribution. Given a solution Âµ, we can obtain a policy via
P
Ï€(a|x) = Âµ(x, a)/ aâ€² âˆˆA Âµ(x, aâ€² ).

1.3

Approximations for Large State Spaces

The LP formulations (1) and (2) are not practical for large scale problems as the number of variables and constraints grows linearly with the number of states. Schweitzer and Seidmann (1985)
propose approximate linear programming (ALP) formulations. These methods were later improved by de Farias and Van Roy (2003a,b), Hauskrecht and Kveton (2003), Guestrin et al. (2004),
Petrik and Zilberstein (2009), Desai et al. (2012). As noted by Desai et al. (2012), the prior work
on ALP either requires access to samples from a distribution that depends on optimal policy or
assumes the ability to solve an LP with as many constraints as states. (See Section 2 for a more
detailed discussion.) Our objective is to design algorithms for very large MDPs that do not require
knowledge of the optimal policy.
In contrast to the aforementioned methods, which solve the primal ALPs (with value functions
as variables), we work with the dual form (2) (with stationary distributions as variables). Analogous
3

to ALPs, we control the complexity by limiting our search to a linear subspace defined by a small
number of features. Let d be the number of features and Î¦ be a (XA) Ã— d matrix with features as

column vectors. By adding the constraint Âµ = Î¦Î¸, we get
min Î¸ âŠ¤ Î¦âŠ¤ â„“ ,
Î¸

s.t.

Î¸ âŠ¤ Î¦âŠ¤ 1 = 1, Î¦Î¸ â‰¥ 0, Î¸ âŠ¤ Î¦âŠ¤ (P âˆ’ B) = 0 .

If a stationary distribution Âµ0 is known, it can be added to the linear span to get the ALP
min(Âµ0 + Î¦Î¸)âŠ¤ â„“ ,

(3)

Î¸

s.t. (Âµ0 + Î¦Î¸)âŠ¤ 1 = 1, Âµ0 + Î¦Î¸ â‰¥ 0, (Âµ0 + Î¦Î¸)âŠ¤ (P âˆ’ B) = 0 .
Although Âµ0 + Î¦Î¸ might not be a stationary distribution, it still defines a policy1
Ï€Î¸ (a|x) = P

[Âµ0 (x, a) + Î¦(x,a),: Î¸]+
,
â€²
aâ€² [Âµ0 (x, a ) + Î¦(x,aâ€² ),: Î¸]+

(4)

We denote the stationary distribution of this policy ÂµÎ¸ which is only equal to Âµ0 + Î¦Î¸ if Î¸ is in the
feasible set.

1.4

Problem definition

With the above notation, we can now be explicit about the problem we are solving.
Definition 1 (Efficient Large-Scale Dual ALP). For an MDP specified by â„“ and P , the efficient
large-scale dual ALP problem is to produce parameters Î¸b such
n
o
âŠ¤
ÂµâŠ¤
â„“
â‰¤
min
Âµ
â„“
:
Î¸
feasible
for
(3)
+ O(Ç«)
Î¸
b
Î¸

(5)

in time polynomial in d and 1/Ç«. The model of computation allows access to arbitrary entries of Î¦,
â„“, P , Âµ0 , P âŠ¤ Î¦, and 1âŠ¤ Î¦ in unit time.
Importantly, the computational complexity cannot scale with X and we do not assume any
knowledge of the optimal policy. In fact, as we shall see, we solve a harder problem, which we
define as follows.
Definition 2 (Expanded Efficient Large-Scale Dual ALP). Let V : â„œd â†’ â„œ+ be some â€œviolation

functionâ€ that represents how far Âµ0 +Î¦Î¸ is from a valid stationary distribution, satisfying V (Î¸) = 0

if Î¸ is a feasible point for the ALP (3). The expanded efficient large-scale dual ALP problem is to
1

We use the notation [v]âˆ’ = v âˆ§ 0 and [v]+ = v âˆ¨ 0.

4

produce parameters Î¸b such that
ÂµâŠ¤
â„“
Î¸b



1
âŠ¤
d
â‰¤ min ÂµÎ¸ â„“ + V (Î¸) : Î¸ âˆˆ â„œ + O(Ç«),
Ç«

(6)

in time polynomial in d and 1/Ç«, under the same model of computation as in Definition 1.
Note that the expanded problem is strictly more general as guarantee (6) implies guarantee
(5). Also, many feature vectors Î¦ may not admit any feasible points. In this case, the dual ALP
problem is trivial, but the expanded problem is still meaningful.
Having access to arbitrary entries of the quantities in Definition 1 arises naturally in many
situations. In many cases, entries of P âŠ¤ Î¦ are easy to compute. For example, suppose that for any
state xâ€² there are a small number of state-action pairs (x, a) such that P (xâ€² |x, a) > 0. Consider
Tetris; although the number of board configurations is large, each state has a small number of
possible neighbors. Dynamics specified by graphical models with small connectivity also satisfy
this constraint. Computing entries of P âŠ¤ Î¦ is also feasible given reasonable features. If a feature Ï•i
is a stationary distribution, then P âŠ¤ Ï•i = B âŠ¤ Ï•i . Otherwise, it is our prerogative to design sparse
feature vectors, hence making the multiplication easy. We shall see an example of this setting later.

1.5

Our Contributions

In this paper, we introduce an algorithm that solves the expanded efficient large-scale dual ALP
problem under a (standard) assumption that any policy converges quickly to its stationary distribution.
Our algorithm take as input a constant S and an error tolerance Ç«, and has access to the various
products listed in Definition 1. Define Î˜ = {Î¸ : Î¸ âŠ¤ Î¦âŠ¤ 1 = 1 âˆ’ ÂµâŠ¤
0 1, kÎ¸k â‰¤ S}. If no stationary
distribution is known, we can simply choose Âµ0 = 0. The algorithm is based on stochastic convex
optimization. We prove that for any Î´ âˆˆ (0, 1), after O(1/Ç«4 ) steps of gradient descent, the algorithm
finds a vector Î¸b âˆˆ Î˜ such that, with probability at least 1 âˆ’ Î´,
ÂµâŠ¤
â„“ â‰¤ÂµâŠ¤
Î¸â„“+
Î¸b

1
1
k[Âµ0 + Î¦Î¸]âˆ’ k1 +
(P âˆ’ B)âŠ¤ (Âµ0 + Î¦Î¸)
Ç«
Ç«

1

+ O(Ç« log(1/Î´))

holds for all Î¸ âˆˆ Î˜; i.e., we solve the expanded problem for V (Î¸) equal to the L1 error of the

violation. The second and third terms are zero for feasible points (points in the intersection of
the feasible set of LP (2) and the span of the features). For points outside the feasible set, these
terms measure the extent of constraint violations for the vector Âµ0 + Î¦Î¸, which indicate how well
stationary distributions can be represented by the chosen features.

The above performance bound scales with 1/Ç« that can be large when the feasible set is empty
and Ç« is very small. We propose a second approach and show that this dependence can be eliminated
if we have extra information about the MDP. Our second approach is based on the constraint
5

sampling method that is already applied to large-scale linear programming and MDP problems
(de Farias and Van Roy, 2004, Calafiore and Campi, 2005, Campi and Garatti, 2008). We obtain
performance bounds, but under the condition that a suitable function that controls the size of
constraint violations is available. Our proof technique is different from previous work and gives
stronger performance bounds.
Our constraint sampling method takes two extra inputs: functions v1 and v2 that specify the
amount of constraint violations that we tolerate at each state-action pair. The algorithm samples
O( d log 1 ) constraints and solves the sampled LP problem. Let Î¸e denote the solution of the sampled
Ç«

Ç«

ALP and Î¸âˆ— denote the solution of the full ALP subject to constraints v1 and v2 . We prove that
with high probability,

â„“âŠ¤ ÂµÎ¸e â‰¤ â„“âŠ¤ ÂµÎ¸âˆ— + O(Ç« + kv1 k1 + kv2 k1 ) .

2

Related Work

de Farias and Van Roy (2003a) study the discounted version of the primal form (1). Let c âˆˆ RX

be a vector with positive components and Î³ âˆˆ (0, 1) be a discount factor. Let L : RX â†’ RX be
P
the Bellman operator defined by (LJ)(x) = minaâˆˆA (â„“(x, a) + Î³ xâ€² âˆˆX P(x,a),xâ€² J(xâ€² )) for x âˆˆ X . Let

Î¨ âˆˆ RXÃ—d be a feature matrix. The exact and approximate LP problems are as follows:
max câŠ¤ J ,

max câŠ¤ Î¨w ,

JâˆˆRX

s.t.

wâˆˆRd

LJ â‰¥ J ,

s.t.

LÎ¨w â‰¥ Î¨w .

which can also be written as
max câŠ¤ J ,

max câŠ¤ Î¨w ,

JâˆˆRX

wâˆˆRd

s.t. âˆ€(x, a), â„“(x, a) + Î³P(x,a),: J â‰¥ J(x) ,

s.t.

(7)

âˆ€(x, a), â„“(x, a) + Î³P(x,a),: Î¨w â‰¥ (Î¨w)(x) .

The optimization problem on the RHS is an approximate LP with the choice of J = Î¨w. Let
 Pâˆž t

JÏ€ (x) = E
t=0 Î³ â„“(xt , Ï€(xt ))|x0 = x be value of policy Ï€, Jâˆ— be the solution of LHS, and Ï€J (x) =

argminaâˆˆA (â„“(x, a) + Î³P(x,a),: J) be the greedy policy with respect to J. Let Î½ âˆˆ âˆ†X be a probability

distribution and define ÂµÏ€,Î½ = (1 âˆ’ Î³)Î½ âŠ¤ (I âˆ’ Î³P Ï€ )âˆ’1 . de Farias and Van Roy (2003a) prove that

for any J satisfying the constraints of the LHS of (7),
kJÏ€J âˆ’ Jâˆ— k1,Î½ â‰¤
Define Î²u = Î³ maxx,a

P

xâ€²

1
kJ âˆ’ Jâˆ— k1,ÂµÏ€ ,Î½ .
J
1âˆ’Î³

(8)

P(x,a),xâ€² u(xâ€² )/u(x). Let U = {u âˆˆ RX : u â‰¥ 1, u âˆˆ span(Î¨), Î²u < 1}. Let

6

wâˆ— be the solution of ALP. de Farias and Van Roy (2003a) show that for any u âˆˆ U ,
kJâˆ— âˆ’ Î¨wâˆ— k1,c â‰¤

2câŠ¤ u
min kJâˆ— âˆ’ Î¨wkâˆž,1/u .
1 âˆ’ Î²u w

(9)

This result has a number of limitations. First, solving ALP can be computationally expensive as
the number of constraints is large. Second, it assumes that the feasible set of ALP is non-empty.
Finally, Inequality (8) implies that c = ÂµÏ€Î¨wâˆ— ,Î½ is an appropriate choice to obtain performance
bounds. However, wâˆ— itself is function of c and is not known before solving ALP.
de Farias and Van Roy (2004) propose a computationally efficient algorithm that is based on
a constraint sampling technique. The idea is to sample a relatively small number of constraints
and solve the resulting LP. Let N âŠ‚ Rd be a known set that contains wâˆ— (solution of ALP). Let
V
V
ÂµVÏ€,c (x) = ÂµÏ€,c (x)V (x)/(ÂµâŠ¤
Ï€,c V ) and define the distribution ÏÏ€,c (x, a) = ÂµÏ€,c (x)/A. Let Î´ âˆˆ (0, 1)
P
and Ç« âˆˆ (0, 1). Let Î² u = Î³ maxx xâ€² P(x,Ï€âˆ— (x)),xâ€² u(xâ€² )/u(x) and

(1 + Î² V )ÂµâŠ¤
Ï€âˆ— ,c V
sup kJâˆ— âˆ’ Î¨wkâˆž,1/V ,
D=
âŠ¤
2c Jâˆ—
wâˆˆN

16AD
mâ‰¥
(1 âˆ’ Î³)Ç«



2
48AD
+ log
d log
(1 âˆ’ Î³)Ç«
Î´



.

Let S be a set of m random state-action pairs sampled under ÏVÏ€âˆ— ,c . Let w
b be a solution of the

following sampled LP:

max câŠ¤ Î¨w ,

wâˆˆRd

s.t.

w âˆˆ N , âˆ€(x, a) âˆˆ S, â„“(x, a) + Î³P(x,a),: Î¨w â‰¥ (Î¨w)(x) .

de Farias and Van Roy (2004) prove that with probability at least 1 âˆ’ Î´, we have
kJâˆ— âˆ’ Î¨wk
b 1,c â‰¤ kJâˆ— âˆ’ Î¨wâˆ— k1,c + Ç« kJâˆ— k1,c .

This result has a number of limitations. First, vector ÂµÏ€âˆ— ,c (that is used in the definition of D)
depends on the optimal policy, but an optimal policy is what we want to compute in the first
place. Second, we can no longer use Inequality (8) to obtain a performance bound (a bound on
kJÏ€Î¨wb âˆ’ Jâˆ— k1,c ), as Î¨w
b does not necessarily satisfy all constraints of ALP.

Desai et al. (2012) study a smoothed version of ALP, in which slack variables are introduced

that allow for some violation of the constraints. Let D â€² be a violation budget. The smoothed ALP
(SALP) has the form of
max câŠ¤ Î¨w ,

max câŠ¤ Î¨w âˆ’

w,s

s.t.

Î¨w â‰¤ LÎ¨w + s,

w,s

ÂµâŠ¤
Ï€âˆ— ,c s

â€²

â‰¤ D , s â‰¥ 0,

7

2ÂµâŠ¤
Ï€âˆ— ,c s
,
1âˆ’Î³

s.t. Î¨w â‰¤ LÎ¨w + s, s â‰¥ 0 .

The ALP on RHS is equivalent to LHS with a specific choice of D â€² . Let U = {u âˆˆ RX : u â‰¥ 1}

be a set of weight vectors. Desai et al. (2012) prove that if wâˆ— is a solution to above problem, then
kJâˆ— âˆ’ Î¨wâˆ— k1,c â‰¤ inf kJâˆ— âˆ’ Î¨wkâˆž,1/u
w,uâˆˆU

2(ÂµâŠ¤
Ï€âˆ— ,c u)(1 + Î²u )
c u+
1âˆ’Î³
âŠ¤

!

.

The above bound improves (9) as U is larger than U and RHS in the above bound is smaller than
RHS of (9). Further, they prove that if Î· is a distribution and we choose c = (1âˆ’Î³)Î· âŠ¤ (I âˆ’Î³P Ï€Î¨wâˆ— ),
then

JÂµÎ¨wâˆ— âˆ’ Jâˆ—

1,Î·

1
â‰¤
1âˆ’Î³

inf kJâˆ— âˆ’ Î¨wkâˆž,1/u

w,uâˆˆU

2(ÂµâŠ¤
Ï€âˆ— ,Î½ u)(1 + Î²u )
c u+
1âˆ’Î³
âŠ¤

!!

.

Similar methods are also proposed by Petrik and Zilberstein (2009). One problem with this result
is that c is defined in terms of wâˆ— , which itself depends on c. Also, the smoothed ALP formulation
uses Ï€âˆ— which is not known. Desai et al. (2012) also propose a computationally efficient algorithm.
Let S be a set of S random states drawn under distribution ÂµÏ€âˆ— ,c . Let N â€² âŠ‚ Rd be a known set

that contains the solution of SALP. The algorithm solves the following LP:
max câŠ¤ Î¨w âˆ’
w,s

s.t.

X
2
s(x) ,
(1 âˆ’ Î³)S
xâˆˆS

âˆ€x âˆˆ S, (Î¨w)(x) â‰¤ (LÎ¨w)(x) + s(x), s â‰¥ 0, w âˆˆ N â€² .

Let w
b be the solution of this problem. Desai et al. (2012) prove high probability bounds on
the approximation error kJâˆ— âˆ’ Î¨wk
b 1,c . However, it is no longer clear if a performance bound
on kJâˆ— âˆ’ JÏ€Î¨wb k1,c can be obtained from this approximation bound.

Next, we turn our attention to average cost ALP, which is a more challenging problem and is

also the focus of this paper. Let Î½ be a distribution over states, u : X â†’ [1, âˆž), Î· > 0, Î³ âˆˆ [0, 1],

PÎ³Ï€ = Î³P Ï€ + (1 âˆ’ Î³)1Î½ âŠ¤ , and LÎ³ h = minÏ€ (â„“Ï€ + PÎ³Ï€ h). de Farias and Van Roy (2006) propose the

following optimization problem:

min s1 + Î·s2 ,

(10)

w,s1 ,s2

s.t.

LÎ³ Î¨w âˆ’ Î¨w + s1 1 + s2 u â‰¥ 0, s2 â‰¥ 0 .

Let (wâˆ— , s1,âˆ— , s2,âˆ— ) be the solution of this problem. Define the mixing time of policy Ï€ by
Ï„Ï€ = inf

(

Ï„ :

tâˆ’1
1 X âŠ¤ Ï€ tâ€²
Ï„
Î½ (P ) â„“Ï€ âˆ’ Î»Ï€ â‰¤ , âˆ€t
t â€²
t
t =0

)

.

Let Ï„âˆ— = lim inf Î´â†’0 {Ï„Ï€ : Î»Ï€ â‰¤ Î»âˆ— + Î´}. Let Ï€Î³âˆ— be the optimal policy when discount factor is
8

Î³. Let Ï€Î³,w be the greedy policy with respect to Î¨w when discount factor is Î³, ÂµâŠ¤
Î³,Ï€ = (1 âˆ’
Pâˆž t âŠ¤ Ï€ t
Î³) t=0 Î³ Î½ (P ) and ÂµÎ³,w = ÂµÎ³,Ï€Î³,w . de Farias and Van Roy (2006) prove that if Î· â‰¥ (2 âˆ’
Î³)ÂµâŠ¤
Î³,Ï€Î³âˆ— u,

Î»w âˆ— âˆ’ Î»âˆ— â‰¤

(1 + Î²)Î· max(D â€²â€² , 1)
min hâˆ—Î³ âˆ’ Î¨w
w
1âˆ’Î³

âˆž,1/u

+ (1 âˆ’ Î³)(Ï„âˆ— + Ï„Ï€wâˆ— ) ,

âŠ¤
where Î² = maxÏ€ kI âˆ’ Î³P Ï€ kâˆž,1/u , D â€²â€² = ÂµâŠ¤
Î³,wâˆ— V /(Î½ V ) and V = LÎ³ Î¨wâˆ— âˆ’ Î¨wâˆ— + s1,âˆ— 1 + s2,âˆ— u.

Similar results are obtained more recently by Veatch (2013).

An appropriate choice for vector Î½ is Î½ = ÂµÎ³,wâˆ— . Unfortunately, wâˆ— depends on Î½. We should
also note that solving (10) can be computationally expensive. de Farias and Van Roy (2006) propose constraint sampling techniques similar to (de Farias and Van Roy, 2004), but no performance
bounds are provided.
Wang et al. (2008) study ALP (3) and show that there is a dual form for standard value function
based algorithms, including on-policy and off-policy updating and policy improvement. They also
study the convergence of these methods, but no performance bounds are shown.

3

A Reduction to Stochastic Convex Optimization

In this section, we describe our algorithm as a reduction from Markov decision problems to stochastic convex optimization. The main idea is to convert the ALP (3) into an unconstrained optimization over Î˜ by adding a function of the constraint violations to the objective, then run stochastic
gradient descent with unbiased estimated of the gradient.
For a positive constant H, form the following convex cost function by adding a multiple of the
total constraint violations to the objective of the LP (3):
c(Î¸) = â„“âŠ¤ (Âµ0 + Î¦Î¸) + H k[Âµ0 + Î¦Î¸]âˆ’ k1 + H (P âˆ’ B)âŠ¤ (Âµ0 + Î¦Î¸)
âŠ¤

âŠ¤

1

= â„“ (Âµ0 + Î¦Î¸) + H k[Âµ0 + Î¦Î¸]âˆ’ k1 + H (P âˆ’ B) Î¦Î¸
1
X
X
âŠ¤
= â„“ (Âµ0 + Î¦Î¸) + H
[Âµ0 (x, a) + Î¦(x,a),: Î¸]âˆ’ + H
(P âˆ’ B)âŠ¤
:,xâ€² Î¦Î¸ .

(11)

xâ€²

(x,a)

We justify using this surrogate loss as follows. Suppose we find a near optimal vector Î¸b such that
b â‰¤ minÎ¸âˆˆÎ˜ c(Î¸) + O(Ç«). We will prove
c(Î¸)
1. that

bâˆ’
[Âµ0 + Î¦Î¸]

1

and

b
(P âˆ’ B)âŠ¤ (Âµ0 + Î¦Î¸)

Lemma 2 in Section 3), and

b â‰¤ minÎ¸âˆˆÎ˜ c(Î¸) + O(Ç«).
2. that â„“âŠ¤ (Âµ0 + Î¦Î¸)
9

1

are small and Âµ0 + Î¦Î¸b is close to ÂµÎ¸b (by

Input: Constant S > 0, number of rounds T , constant H.
Let Î Î˜ be the Euclidean projection onto Î˜.
Initialize Î¸1 = 0.
for t := 1, 2, . . . , T do
Sample (xt , at ) âˆ¼ q1 and xâ€²t âˆ¼ q2 .
Compute subgradient estimate gt (12).
Update Î¸t+1 = Î Î˜ (Î¸t âˆ’ Î·t gt ).
end forP
Î¸bT = T1 Tt=1 Î¸t .
Return policy Ï€Î¸bT .
Figure 1: The Stochastic Subgradient Method for Markov Decision Processes

As we will show, these two facts imply that with high probability, for any Î¸ âˆˆ Î˜,
ÂµâŠ¤
â„“ â‰¤ ÂµâŠ¤
Î¸â„“+
Î¸b

1
1
k[Âµ0 + Î¦Î¸]âˆ’ k1 +
(P âˆ’ B)âŠ¤ (Âµ0 + Î¦Î¸)
Ç«
Ç«

1

+ O(Ç«) ,

which is to say that minimization of c(Î¸) solves the extended efficient large-scale ALP problem.
Unfortunately, calculating the gradients of c(Î¸) is O(XA). Instead, we construct unbiased
estimators and use stochastic gradient descent. Let T be the number of iterations of our algorithm.
Let q1 and q2 be distributions over the state-action and state space, respectively (we will later
discuss how to choose them). Let ((xt , at ))t=1...T be i.i.d. samples from q1 and (xâ€²t )t=1...T be i.i.d.
samples from q2 . At round t, the algorithm estimates subgradient âˆ‡c(Î¸) by
(P âˆ’ B)âŠ¤
Î¦(xt ,at ),:
:,xâ€²t Î¦
I{Âµ0 (xt ,at )+Î¦(xt ,at ),: Î¸<0} + H
s((P âˆ’ B)âŠ¤
gt (Î¸) = â„“ Î¦ âˆ’ H
:,xâ€²t Î¦Î¸).
q1 (xt , at )
q2 (xâ€²t )
âŠ¤

(12)

This estimate is fed to the projected subgradient method, which in turn generates a vector Î¸t .
P
After T rounds, we average vectors (Î¸t )t=1...T and obtain the final solution Î¸bT = T Î¸t /T . Vector
t=1

Âµ0 + Î¦Î¸bT defines a policy, which in turn defines a stationary distribution ÂµÎ¸bT .2 The algorithm is
shown in Figure 1.
2

Recall that ÂµÎ¸ is the stationary distribution of policy
Ï€Î¸ (a|x) = P

[Âµ0 (x, a) + Î¦(x,a),: Î¸]+
.
â€²
â€²
aâ€² [Âµ0 (x, a ) + Î¦(x,a ),: Î¸]+

With an abuse of notation, we use ÂµÎ¸ to denote policy Ï€Î¸ as well.

10

3.1

Analysis

In this section, we state and prove our main result, Theorem 1. We begin with a discussion of
the assumptions we make then follow with the main theorem. We break the proof into two main
ingredients. First, we demonstrate that a good approximation to the surrogate loss gives a feature
vector that is almost a stationary distribution; this is Lemma 2. Second, we justify the use of
unbiased gradients in Theorem 3 and Lemma 5. The section concludes with the proof.
We make a mixing assumption on the MDP so that any policy quickly converges to its stationary
distribution.
Assumption A1 (Fast Mixing)
for all distributions d and

dâ€²

For any policy Ï€, there exists a constant Ï„ (Ï€) > 0 such that

over the state space, kdP Ï€ âˆ’ dâ€² P Ï€ k1 â‰¤ eâˆ’1/Ï„ (Ï€) kd âˆ’ dâ€² k1 .

Define
C1 =

max

(x,a)âˆˆX Ã—A

Î¦(x,a),:
,
q1 (x, a)

C2 = max
xâˆˆX

(P âˆ’ B)âŠ¤
:,x Î¦
.
q2 (x)

These constants appear in our performance bounds. So we would like to choose distributions q1
and q2 such that C1 and C2 are small. For example, if there is C â€² > 0 such that for any (x, a) and
i, Î¦(x,a),i â‰¤ C â€² /(XA) and each column of P has only N non-zero elements, then we can simply

choose q1 and q2 to be uniform distributions. Then it is easy to see that
Î¦(x,a),:
â‰¤ Câ€² ,
q1 (x, a)

(P âˆ’ B)âŠ¤
:,x Î¦
â‰¤ C â€² (N + A) .
q2 (x)

As another example, if Î¦:,i are exponential distributions and feature values at neighboring states
are close to each other, then we can choose q1 and q2 to be appropriate exponential distributions so
that Î¦(x,a),: /q1 (x, a) and (P âˆ’ B)âŠ¤
:,x Î¦ /q2 (x) are always bounded. Another example is when

âŠ¤ Î¦ / B âŠ¤ Î¦ < C â€²â€²3 and we have access to
there exists a constant C â€²â€² > 0 such that for any x, P:,x
:,x
P
P
âŠ¤ Î¦ and can sample
an efficient algorithm that computes Z1 = (x,a) Î¦(x,a),: and Z2 = x B:,x

from q1 (x, a) =

Î¦(x,a),: /Z1 and q2 (x) =

âŠ¤ Î¦ /Z . In what follows, we assume that such
B:,x
2

distributions q1 and q2 are known.
We now state the main theorem.
Theorem 1. Consider an expanded efficient large-scale dual ALP problem. Suppose we apply the
stochastic subgradient method (shown in Figure 1) to the problem. Let Ç« âˆˆ (0, 1). Let T = 1/Ç«4 be
the number of rounds and H = 1/Ç« be the constraints multiplier in subgradient estimate (12). Let Î¸bT
be the output of the stochastic subgradient method after T rounds and let the learning rate be Î·1 =
âˆš
âˆš
P
Â· Â· Â· = Î·T = S/(Gâ€² T ), where Gâ€² = d + H(C1 + C2 ). Define V1 (Î¸) = (x,a) [Âµ0 (x, a) + Î¦(x,a),: Î¸]âˆ’
3

This condition requires that columns of Î¦ are close to their one step look-ahead.

11

and V2 (Î¸) =

P

xâ€²

(P âˆ’ B)âŠ¤
:,xâ€² (Âµ0 + Î¦Î¸) . Then, for any Î´ âˆˆ (0, 1), with probability at least 1 âˆ’ Î´,
â„“
ÂµâŠ¤
Î¸bT

â‰¤ min
Î¸âˆˆÎ˜



ÂµâŠ¤
Î¸â„“


1
+ (V1 (Î¸) + V2 (Î¸)) + O(Ç«) ,
Ç«

(13)

where constants hidden in the big-O notation are polynomials in S, d, C1 , C2 , log(1/Î´), V1 (Î¸),
V2 (Î¸), Ï„ (ÂµÎ¸ ), and Ï„ (ÂµÎ¸bT ).
Functions V1 and V2 are bounded by small constants for any set of normalized features: for any
Î¸ âˆˆ Î˜,
V1 (Î¸) â‰¤ kÂµ0 k1 + kÎ¦Î¸k1 â‰¤ 1 +
V2 (Î¸) â‰¤
â‰¤

X

âŠ¤
P:,x
â€² (Âµ0 + Î¦Î¸) +

xâ€²

X

P:,xâ€²

xâ€²

!âŠ¤

X

(x,a)

X

Î¦(x,a),: Î¸ â‰¤ 1 + Sd ,
âŠ¤
B:,x
â€² (Âµ0 + Î¦Î¸)

xâ€²

[Âµ0 + Î¦Î¸]+ +

X

B:,xâ€²

xâ€²

= (2)1âŠ¤ [Âµ0 + Î¦Î¸]+

!âŠ¤

[Âµ0 + Î¦Î¸]+

â‰¤ (2)1âŠ¤ |Âµ0 + Î¦Î¸|
= 2 + 2S .
Thus V1 and V2 can be very small given a carefully designed set of features. The output Î¸bT is a
random vector as the algorithm is based on a stochastic convex optimization method. The above

theorem shows that with high probability the policy implied by this output is near optimal.
p
The optimal choice for Ç« is Ç« = V1 (Î¸âˆ— ) + V2 (Î¸âˆ— ), where Î¸âˆ— is the minimizer of RHS of (13)
and q
not known in advance. Once we obtain Î¸bT , we can estimate V1 (Î¸bT ) and V2 (Î¸bT ) and use input
Ç« = V1 (Î¸bT ) + V2 (Î¸bT ) in a second run of the algorithm. This implies that the error bound scales
p
like O( V1 (Î¸âˆ— ) + V2 (Î¸âˆ— )).
The next lemma, providing the first ingredient of the proof, relates the amount of constraint
violation of a vector Î¸ to resulting stationary distribution ÂµÎ¸ .
Lemma 2. Let u âˆˆ RXA be a vector. Let N be the set of points (x, a) where u(x, a) < 0 and S be
complement of N . Assume
X
x,a

u(x, a) = 1,

X

(x,a)âˆˆN

|u(x, a)| â‰¤ Ç«â€² , uâŠ¤ (P âˆ’ B)

1

â‰¤ Ç«â€²â€² .

Vector [u]+ / k[u]+ k1 defines a policy, which in turn defines a stationary distribution Âµu . We have

that

kÂµu âˆ’ uk1 â‰¤ Ï„ (Âµu ) log(1/Ç«â€² )(2Ç«â€² + Ç«â€²â€² ) + 3Ç«â€² .
12

Proof. Let f = uâŠ¤ (P âˆ’ B). From uâŠ¤ (P âˆ’ B)
X

(x,a)âˆˆS

such that

P

xâ€²

u(x, a)(P âˆ’ B)(x,a),xâ€² = âˆ’

â‰¤ Ç«â€²â€² , we get that for any xâ€² âˆˆ X ,

1

X

(x,a)âˆˆN

u(x, a)(P âˆ’ B)(x,a),xâ€² + f (xâ€² )

|f (xâ€² )| â‰¤ Ç«â€²â€² . Let h = [u]+ / k[u]+ k1 . Let H â€² = hâŠ¤ (B âˆ’ P ) 1 . We write
Hâ€² =

X
xâ€²

=

X

(x,a)âˆˆS

1 X
1 + Ç«â€² â€²
x

h(x, a)(B âˆ’ P )(x,a),xâ€²
X

(x,a)âˆˆS

u(x, a)(B âˆ’ P )(x,a),xâ€²

X
1 X
âˆ’
u(x, a)(B âˆ’ P )(x,a),xâ€² + f (xâ€² )
1 + Ç«â€² â€²
x
(x,a)âˆˆN
ï£«
ï£¶
X
X
X
1 ï£­
âˆ’
u(x, a)(B âˆ’ P )(x,a),xâ€² +
â‰¤
f (xâ€² ) ï£¸
1 + Ç«â€²
xâ€²
xâ€²
(x,a)âˆˆN
ï£«
ï£¶
X
X
1 ï£­ â€²â€²
Ç« +
|u(x, a)| (B âˆ’ P )(x,a),xâ€² ï£¸
â‰¤
1 + Ç«â€²
â€²
(x,a)âˆˆN x
ï£¶
ï£«
â€²
â€²â€²
X
1 ï£­ â€²â€²
ï£¸ â‰¤ 2Ç« + Ç«
Ç«
+
2
|u(x,
a)|
â‰¤
1 + Ç«â€²
1 + Ç«â€²
=

(x,a)âˆˆN

â€²

â€²â€²

â‰¤ 2Ç« + Ç« .

Vector h is almost a stationary distribution in the sense that
hâŠ¤ (B âˆ’ P )
Let kwk1,S =

P

(x,a)âˆˆS

1

â‰¤ 2Ç«â€² + Ç«â€²â€² .

(14)

|w(x, a)|. First, we have that
kh âˆ’ uk1 â‰¤ h âˆ’

u
1 + Ç«â€²

1

+ uâˆ’

u
1 + Ç«â€²

1,S

â‰¤ 2Ç«â€² .

Next we bound kÂµh âˆ’ hk1 . Let Î½0 = h be the initial state distribution. We will show that as we

run policy h (equivalently, policy Âµh ), the state distribution converges to Âµh and this vector is close

âŠ¤
â€²
â€²â€²
h
to h. From (14), we have ÂµâŠ¤
0 P = h B + v0 , where v0 is such that kv0 k1 â‰¤ 2Ç« + Ç« . Let M be a
h
X Ã— (XA) matrix that encodes policy h, M(i,(iâˆ’1)A+1)
-(i,iA) = h(Â·|xi ). Other entries of this matrix

13

are zero. We get that
hâŠ¤ P M h = (hâŠ¤ B + v0 )M h = hâŠ¤ BM h + v0 M h = hâŠ¤ + v0 M h ,
âŠ¤
h
where we used the fact that hâŠ¤ BM h = hâŠ¤ . Let ÂµâŠ¤
1 = h P M which is the state-action distribution

after running policy h for one step. Let v1 = v0 M h P = v0 P h and notice that as kv0 k1 â‰¤ 2Ç«â€² + Ç«â€²â€² ,
we also have that kv1 k1 = P hâŠ¤ v0âŠ¤

1

â‰¤ kv0 k1 â‰¤ 2Ç«â€² + Ç«â€²â€² . Thus,

âŠ¤
âŠ¤
ÂµâŠ¤
1 P = h P + v1 = h B + v0 + v1 .

By repeating this argument for k rounds, we get that
âŠ¤
h
ÂµâŠ¤
k = h + (v0 + v1 + Â· Â· Â· + vkâˆ’1 )M

and it is easy to see that
(v0 + v1 + Â· Â· Â· + vkâˆ’1 )M h

1

â‰¤

kâˆ’1
X
i=0

kvi k1 â‰¤ k(2Ç«â€² + Ç«â€²â€² ).

Thus, kÂµk âˆ’ hk1 â‰¤ k(2Ç«â€² + Ç«â€²â€² ). Now notice that Âµk is the state-action distribution after k rounds of
policy Âµh . Thus, by mixing assumption, kÂµk âˆ’ Âµh k1 â‰¤ eâˆ’k/Ï„ (h) . By the choice of k = Ï„ (h) log(1/Ç«â€² ),

we get that kÂµh âˆ’ hk1 â‰¤ Ï„ (h) log(1/Ç«â€² )(2Ç«â€² + Ç«â€²â€² ) + Ç«â€² .

The second ingredient is the validity of using estimates of the subgradients. We assume access
to estimates of the subgradient of a convex cost function. Error bounds can be obtained from
results in the stochastic convex optimization literature; the following theorem, a high-probability
version of Lemma 3.1 of Flaxman et al. (2005) for stochastic convex optimization, is sufficient.
Theorem 3. Let Z be a positive constant and Z be a bounded subset of Rd such that for any

z âˆˆ Z, kzk â‰¤ Z. Let (ft )t=1,2,...,T be a sequence of real-valued convex cost functions defined over

Z. Let z1 , z2 , . . . , zT âˆˆ Z be defined by z1 = 0 and zt+1 = Î Z (zt âˆ’ Î·ftâ€² ), where Î Z is the Euclidean

projection onto Z, Î· > 0 is a learning rate, and f1â€² , . . . , fTâ€² are unbiased subgradient estimates such
âˆš
that E [ftâ€² |zt ] = âˆ‡f (zt ) and kftâ€² k â‰¤ F for some F > 0. Then, for Î· = Z/(F T ), for any Î´ âˆˆ (0, 1),

with probability at least 1 âˆ’ Î´,

T
T
X
X
âˆš
ft (z) â‰¤ ZF T +
ft (zt ) âˆ’ min
t=1

zâˆˆZ

t=1

Proof. Let zâˆ— = argminzâˆˆZ

PT

t=1 ft (z)

s

(1 +

4Z 2 T )





1
Z 2T
2 log + d log 1 +
.
Î´
d

(15)

and Î·t = ftâ€² âˆ’ âˆ‡ft (zt ). Define function ht : Z â†’ R by

ht (z) = ft (z) + zÎ·t . Notice that âˆ‡ht (zt ) = âˆ‡ft (zt ) + Î·t = ftâ€² . By Theorem 1 of Zinkevich (2003),
14

we get that
T
X
t=1

ht (zt ) âˆ’

T
X
t=1

ht (zâˆ— ) â‰¤

T
X
t=1

ht (zt ) âˆ’ min
zâˆˆZ

T
X
t=1

âˆš
ht (z) â‰¤ ZF T .

Thus,
T
X
t=1

Let St =

Ptâˆ’1

s=1 (zâˆ— âˆ’zs )Î·s ,

ft (zt ) âˆ’

T
X
t=1

T
X
âˆš
ft (zâˆ— ) â‰¤ ZF T +
(zâˆ— âˆ’ zt )Î·t .
t=1

which is a self-normalized sum (de la PenÌƒa et al., 2009). By Corollary 3.8

and Lemma E.3 of Abbasi-Yadkori (2012), we get that for any Î´ âˆˆ (0, 1), with probability at least

1 âˆ’ Î´,

v
!
u


tâˆ’1
u
X
Z 2t
1
t
2
|St | â‰¤
(zt âˆ’ zâˆ— )
2 log + d log 1 +
1+
Î´
d
s=1
s



1
Z 2t
2
â‰¤ (1 + 4Z t) 2 log + d log 1 +
.
Î´
d

Thus,
T
X
t=1

ft (zt ) âˆ’ min
zâˆˆZ

T
X
t=1

âˆš
ft (z) â‰¤ ZF T +

s

(1 +

4Z 2 T )





Z 2T
1
.
2 log + d log 1 +
Î´
d

Remark 4. Let BT denote RHS of (15). If all cost functions are equal to f , then by convexity of
P
f and an application of Jensenâ€™s inequality, we obtain that f ( Tt=1 zt /T ) âˆ’ minzâˆˆZ f (z) â‰¤ BT /T .
As the next lemma shows, Theorem 3 can be applied in our problem to optimize cost function

c.
Lemma 5. Under the same conditions as in Theorem 1, we have that for any Î´ âˆˆ (0, 1), with
probability at least 1 âˆ’ Î´,

SGâ€²
c(Î¸bT ) âˆ’ min c(Î¸) â‰¤ âˆš +
Î¸âˆˆÎ˜
T

s

1 + 4S 2 T
T2





S 2T
1
2 log + d log 1 +
.
Î´
d

Proof. We prove the lemma by showing that conditions of Theorem 3 are satisfied.

(16)
We be-

gin by calculating the subgradient and bounding its norm with a quantity independent of the
number of states. If Âµ0 (x, a) + Î¦(x,a),: Î¸ â‰¥ 0, then âˆ‡Î¸ [Âµ0 (x, a) + Î¦(x,a),: Î¸]âˆ’ = 0. Otherwise,

15

âˆ‡Î¸ [Âµ0 (x, a) + Î¦(x,a),: Î¸]âˆ’ = âˆ’Î¦(x,a),: . Calculating,
âˆ‡Î¸ c(Î¸) = â„“âŠ¤ Î¦ + H
= â„“âŠ¤ Î¦ âˆ’ H

X

(x,a)

X

âˆ‡Î¸ [Âµ0 (x, a) + Î¦(x,a),: Î¸]âˆ’ + H
Î¦(x,a),: I{Âµ0 (x,a)+Î¦(x,a),: Î¸<0} + H

X
xâ€²

X
xâ€²

(x,a)

âˆ‡Î¸ (P âˆ’ B)âŠ¤
:,xâ€² Î¦Î¸

âŠ¤
(P âˆ’ B)âŠ¤
:,xâ€² Î¦s((P âˆ’ B):,xâ€² Î¦Î¸) ,

(17)

where s(z) = I{z>0} âˆ’ I{z<0} is the sign function. Let Â± denote the plus or minus sign (the exact

sign does not matter here). Let G = kâˆ‡Î¸ c(Î¸)k. We have that

v
v
ï£¶2
ï£«
ï£«
ï£«
ï£¶ï£¶2
u
u
uX
uX
d
d
X
X
X
u
u
ï£­
ï£­
ï£­Â±
G â‰¤ Ht
Î¦(x,a),i ï£¸ .
(P âˆ’ B)(x,a),xâ€² Î¦(x,a),i ï£¸ï£¸ + â„“âŠ¤ Î¦ + H t
i=1

xâ€²

i=1

(x,a)

(x,a)

Thus,
v
v
ï£«
u
u d
uX
d
uX
X
âˆš
u
ï£­
G â‰¤ t (â„“âŠ¤ Î¦:,i )2 + H d + H t
i=1

i=1

(x,a)

X
Â±
(P âˆ’ B)(x,a),xâ€²
xâ€²

v
ï£«
ï£¶2
u
uX
d
X
âˆš
âˆš
âˆš
u
ï£­2
Î¦(x,a),i ï£¸ = d(1 + 3H) ,
â‰¤ d + H d + Ht
i=1

!

ï£¶2

Î¦(x,a),i ï£¸

(x,a)

where we used â„“âŠ¤ Î¦:,i â‰¤ kâ„“kâˆž kÎ¦:,i k1 â‰¤ 1.

Next, we show that norm of the subgradient estimate is bounded by Gâ€² :
(P âˆ’ B)âŠ¤
âˆš
Î¦(xt ,at ),:
:,xâ€²t Î¦
d + H(C1 + C2 ) .
+H
â‰¤
kgt k â‰¤ â„“ Î¦ + H
q1 (xt , at )
q2 (xâ€²t )
âŠ¤

Finally, we show that the subgradient estimate is unbiased:
E [gt (Î¸)] = â„“âŠ¤ Î¦ âˆ’ H

X

q1 (x, a)

(x,a)

+H
= â„“âŠ¤ Î¦ âˆ’ H

X

Î¦(x,a),:
I
q1 (x, a) {Âµ0 (x,a)+Î¦(x,a),: Î¸<0}

X
xâ€²

â€²

q2 (x )

(P âˆ’ B)âŠ¤
:,xâ€² Î¦
q2 (xâ€² )

s((P âˆ’ B)âŠ¤
:,xâ€² Î¦Î¸)

Î¦(x,a),: I{Âµ0 (x,a)+Î¦(x,a),: Î¸<0} + H

X
âŠ¤
(P âˆ’ B)âŠ¤
:,xâ€² Î¦s((P âˆ’ B):,xâ€² Î¦Î¸)
xâ€²

(x,a)

= âˆ‡Î¸ c(Î¸) .
The result then follows from Theorem 3 and Remark 4.

16

With both ingredients in place, we can prove our main result.
Proof of Theorem 1. Let bT be the RHS of (16). Equation (16) implies that with high probability
for any Î¸ âˆˆ Î˜,
â„“âŠ¤ (Âµ0 + Î¦Î¸bT ) + H V1 (Î¸bT ) + H V2 (Î¸bT ) â‰¤ â„“âŠ¤ (Âµ0 + Î¦Î¸) + H V1 (Î¸) + H V2 (Î¸) + bT .

(18)

From (18), we get that

1
def
V1 (Î¸bT ) â‰¤
(2(1 + S) + H V1 (Î¸) + H V2 (Î¸) + bT ) = Ç«â€² ,
H
1
def
V2 (Î¸bT ) â‰¤
(2(1 + S) + H V1 (Î¸) + H V2 (Î¸) + bT ) = Ç«â€²â€² .
H

(19)
(20)

Inequalities (19) and (20) and Lemma 2 give the following bound:

ÂµâŠ¤
â„“ âˆ’ (Âµ0 + Î¦Î¸bT )âŠ¤ â„“ â‰¤ Ï„ (ÂµÎ¸bT ) log(1/Ç«â€² )(2Ç«â€² + Ç«â€²â€² ) + 3Ç«â€² .
Î¸b
T

(21)

From (18) we also have

â„“âŠ¤ (Âµ0 + Î¦Î¸bT ) â‰¤ â„“âŠ¤ (Âµ0 + Î¦Î¸) + H V1 (Î¸) + H V2 (Î¸) + bT ,

which, together with (21) and Lemma 2, gives the final result:

ÂµâŠ¤
â„“ â‰¤ â„“âŠ¤ (Âµ0 + Î¦Î¸) + H V1 (Î¸) + H V2 (Î¸) + bT + Ï„ (ÂµÎ¸bT ) log(1/Ç«â€² )(2Ç«â€² + Ç«â€²â€² ) + 3Ç«â€²
Î¸b
T

â€²
â€²
â€²â€²
â€²
â‰¤ ÂµâŠ¤
Î¸ â„“ + H V1 (Î¸) + H V2 (Î¸) + bT + Ï„ (ÂµÎ¸bT ) log(1/Ç« )(2Ç« + Ç« ) + 3Ç«

+ Ï„ (ÂµÎ¸ ) log(1/V1 (Î¸))(2V1 (Î¸) + V2 (Î¸)) + 3V1 (Î¸) .
âˆš
Recall that bT = O(H/ T ). Because H = 1/Ç« and T = 1/Ç«4 , we get that with high probability,
1
âŠ¤
for any Î¸ âˆˆ Î˜, ÂµâŠ¤
b â„“ â‰¤ ÂµÎ¸ â„“ + Ç« (V1 (Î¸) + V2 (Î¸)) + O(Ç«).
Î¸T

Letâ€™s compare Theorem 1 with results of de Farias and Van Roy (2006). Their approach is to
relate the original MDP to a perturbed version4 and then analyze the corresponding ALP. (See
Section 2 for more details.) Let Î¨ be a feature matrix that is used to estimate value functions.
Recall that Î»âˆ— is the average loss of the optimal policy and Î»w is the average loss of the greedy
policy with respect to value function Î¨w. Let hâˆ—Î³ be the differential value function when the
restart probability in the perturbed MDP is 1 âˆ’ Î³. For vector v and positive vector u, define the
4

In a perturbed MDP, the state process restarts with a certain probability to a restart distribution. Such perturbed
MDPs are closely related to discounted MDPs.

17

weighted maximum norm kvkâˆž,u = maxx u(x) |v(x)|. de Farias and Van Roy (2006) prove that for

appropriate constants C, C â€² > 0 and weight vector u,
Î»w âˆ— âˆ’ Î»âˆ— â‰¤

C
min hâˆ—Î³ âˆ’ Î¨w
1âˆ’Î³ w

âˆž,u

+ C â€² (1 âˆ’ Î³) .

(22)

This bound has similarities to bound (13): tightness of both bounds depends on the quality of feature vectors in representing the relevant quantities (stationary distributions in (13) and value functions in (22)). Once again, we emphasize that the algorithm proposed by de Farias and Van Roy
(2006) is computationally expensive and requires access to a distribution that depends on optimal
policy.

4

Sampling Constraints

In this section we describe our second algorithm for average cost MDP problems. Using the results on polytope constraint sampling (de Farias and Van Roy, 2004, Calafiore and Campi, 2005,
Campi and Garatti, 2008), we reduce approximate the solution to the dual ALP with the solution
to a smaller, sampled LP. Basically, de Farias and Van Roy (2004) claim that given a set of affine
constraints in Rd and some measure q over these constraints, if we sample k = O(d log(1/Î´)/Ç«)
constraints, then with probability at least 1 âˆ’ Î´, any point that satisfies all of these k sampled

constraints also satisfies 1 âˆ’ Ç« of the original set of constraints under measure q. This result is

independent of the number of original constraints.

Let L be a family of affine constraints indexed by i: constraint i is satisfied at point w âˆˆ Rd
if aâŠ¤
i w + bi â‰¥ 0. Let I be the family of constraints by selecting k random constraints in L with

respect to measure q.

Theorem 6 (de Farias and Van Roy (2004)). Assume there exists a vector that satisfies all con
2
straints in L. For any Î´ and Ç«, if we take m â‰¥ 4Ç« d log 12
Ç« + log Î´ , then, with probability 1 âˆ’ Î´, a
set I of m i.i.d. random variables drawn from L with respect to distribution q satisfies
sup
{w:âˆ€iâˆˆI,aâŠ¤
i w+bi â‰¥0}

q({j : aâŠ¤
j w + bj < 0}) â‰¤ Ç« .

Our algorithm takes the following inputs: a positive constant S, a stationary distribution Âµ0 ,
a set Î˜ = {Î¸ : Î¸ âŠ¤ Î¦âŠ¤ 1 = 1 âˆ’ ÂµâŠ¤
0 1, kÎ¸k â‰¤ S}, a distribution q1 over the state-action space, a
distribution q2 over the state space, and constraint violation functions v1 : X Ã— A â†’ [âˆ’1, 0] and

v2 : X â†’ [0, 1]. We will consider two families of constraints:

L1 = {Âµ0 (x, a) + Î¦(x,a),: Î¸ â‰¥ v1 (x, a) | (x, a) âˆˆ X Ã— A} ,
n
o[n
o
âŠ¤
L2 = (P âˆ’ B)âŠ¤
(Âµ
+
Î¦Î¸)
â‰¤
v
(x)
|
x
âˆˆ
X
(P
âˆ’
B)
(Âµ
+
Î¦Î¸)
â‰¥
âˆ’v
(x)
|
x
âˆˆ
X
.
2
2
:,x 0
:,x 0
18

Input: Constant S > 0, stationary distribution Âµ0 , distributions q1
and q2 , constraint violation functions v1 and v2 , number of samples
k1 and k2 .
For i = 1, 2, let Ii be ki constraints sampled from Li under distribution
qi .
Let I be the set of vectors that satisfy all constraints in I1 and I2 .
Let Î¸e be the solution to LP:
min â„“âŠ¤ (Âµ0 + Î¦Î¸) ,

(24)

Î¸âˆˆÎ˜

s.t.

Î¸ âˆˆ I, Î¸ âˆˆ Î˜ .

Return policy ÂµÎ¸e.
Figure 2: The Constraint Sampling Method for Markov Decision Processes

Let Î¸âˆ— be the solution of
min â„“âŠ¤ (Âµ0 + Î¦Î¸) ,

(23)

Î¸âˆˆÎ˜

s.t.

Î¸ âˆˆ L1 , Î¸ âˆˆ L2 , Î¸ âˆˆ Î˜ .

The constraint sampling algorithm is shown in Figure 2. We refer to (24) as the sampled ALP,
while we refer to (3) as the full ALP.

4.1

Analysis

We require Assumption A1 as well as:
Assumption A2 (Feasibility)

There exists a vector that satisfies all constraints L1 and L2 .

Validity of this assumption depends on the choice of functions v1 and v2 . Larger functions ensure
that this assumption is satisfied, but as we show, this leads to larger error.
The next two lemmas apply theorem 6 to constraints L1 and L2 , respectively.


2
Lemma 7. Let Î´1 âˆˆ (0, 1) and Ç«1 âˆˆ (0, 1). If we choose k1 = Ç«41 d log 12
+
log
Ç«1
Î´1 , then with
P
e âˆ’ â‰¤ SC1 Ç«1 + kv1 k .
probability at least 1 âˆ’ Î´1 , (x,a) [Âµ0 (x, a) + Î¦(x,a),: Î¸]
1

Proof. Applying theorem 6, we have that w.p. 1 âˆ’ Î´1 , q1 (Âµ0 (x, a) + Î¦(x,a),: Î¸e â‰¥ v1 (x, a)) â‰¥ 1 âˆ’ Ç«1 ,

and thus

X

q1 (x, a)I{Âµ0 (x,a)+Î¦

e

(x,a),: Î¸<v1 (x,a)}

(x,a)

19

â‰¤ Ç«1 .

Let L =

P

(x,a)

e âˆ’ . With probability 1 âˆ’ Î´1 ,
[Âµ0 (x, a) + Î¦(x,a),: Î¸]
L=

X

eâˆ’ I
[Âµ0 (x, a) + Î¦(x,a),: Î¸]
{Âµ0 (x,a)+Î¦

(x,a)

+

X

(x,a)

â‰¤
â‰¤
â‰¤

X

eâˆ’ I
[Âµ0 (x, a) + Î¦(x,a),: Î¸]
{Âµ0 (x,a)+Î¦

Î¦(x,a),: Î¸e I{Âµ0 (x,a)+Î¦

(x,a)

X

Î¦(x,a),:

(x,a)

X

e

(x,a),: Î¸â‰¤v1 (x,a)}

e

(x,a),: Î¸â‰¤v1 (x,a)}

Î¸e I{Âµ0 (x,a)+Î¦

SC1 q1 (x, a)I{Âµ0 (x,a)+Î¦

e

(x,a),: Î¸>v1 (x,a)}

+ kv1 k1

e

(x,a),: Î¸â‰¤v1 (x,a)}

e

(x,a),: Î¸â‰¤v1 (x,a)}

(x,a)

+ kv1 k1

+ kv1 k1

â‰¤ SC1 Ç«1 + kv1 k1 .

Lemma 8. Let Î´2 âˆˆ (0, 1) and Ç«2 âˆˆ (0, 1). If we choose k2 =
probability at least 1 âˆ’ Î´2 , (P âˆ’ B)âŠ¤ Î¦Î¸e

1

â‰¤ SC2 Ç«2 + kv2 k1 .

Proof. Applying theorem 6, we have that q2
X
x

Let Lâ€² =

P

x



4
Ç«2


d log

12
Ç«2


e â‰¤ v2 (x) â‰¥ 1 âˆ’ Ç«2 . This yields
Î¦
Î¸
(P âˆ’ B)âŠ¤
:,x

q2 (x)I{|(P âˆ’B)âŠ¤

e|â‰¥v2 (x)}

:,x Î¦Î¸

e
(P âˆ’ B)âŠ¤
:,x Î¦Î¸ . Thus, with probability 1 âˆ’ Î´2 ,
X

â‰¤ Ç«2 .

e
(P âˆ’ B)âŠ¤
e
:,x Î¦Î¸ I{|(P âˆ’B)âŠ¤
:,x Î¦Î¸ |>v2 (x)}
x
X
e
(P âˆ’ B)âŠ¤
+
e
:,x Î¦Î¸ I{|(P âˆ’B)âŠ¤
:,x Î¦Î¸ |â‰¤v2 (x)}
x
X
Î¸e I{|(P âˆ’B)âŠ¤ Î¦Î¸e|>v2 (x)} + kv2 k1
â‰¤
(P âˆ’ B)âŠ¤
:,x Î¦
:,x
x
X
â‰¤
SC2 q2 (x)I{|(P âˆ’B)âŠ¤ Î¦Î¸e|>v2 (x)} + kv2 k1
:,x

Lâ€² =


+ log Î´22 , then with

x

â‰¤ SC2 Ç«2 + kv2 k1 ,
where the last step follows from (25).

20

(25)

a1
d4

Âµ1

d1

Âµ2

d2

Âµ4

d2

Âµ3

a3

server1

server2

Figure 3: The 4D queueing network. Customers arrive at queue Âµ1 or Âµ3 then are referred to queue
Âµ2 or Âµ4 , respectively. Server 1 can either process queue 1 or 4, and server 2 can only process queue
2 or 3.

We are ready to prove the main result of this section. Let Î¸e denote the solution of the sampled

ALP, Î¸âˆ— denote the solution of the full ALP (23), and ÂµÎ¸e be the stationary distribution of the
solution policy. Our goal is to compare â„“âŠ¤ ÂµÎ¸e and â„“âŠ¤ ÂµÎ¸âˆ— .

Theorem 9. Let Ç« âˆˆ (0, 1) and Î´ âˆˆ (0, 1). Let Ç«â€² = SC1 Ç« + kv1 k1 and Ç«â€²â€² = SC2 Ç« + kv2 k1 . If we


4
4
4
12
sample constraints with k1 = 4Ç« d log 12
Ç« + log Î´ and k2 = Ç« d log Ç« + log Î´ , then, with probability
1 âˆ’ Î´,
â„“âŠ¤ ÂµÎ¸e â‰¤ â„“âŠ¤ ÂµÎ¸âˆ— + Ï„ (ÂµÎ¸e) log(1/Ç«â€² )(2Ç«â€² + Ç«â€²â€² ) + 3Ç«â€²
+ Ï„ (Âµâˆ— ) log(1/ kv1 k)(2 kv1 k + kv2 k) + 3 kv1 k .
Proof. Let Î´1 = Î´2 = Î´/2. By Lemmas 7 and 8, w.p. 1 âˆ’ Î´,
e
(P âˆ’ B)âŠ¤ (Âµ0 + Î¦Î¸)

1

â‰¤ Ç«â€²â€² . Then by Lemma 2,

P

(x,a)

e âˆ’ â‰¤ Ç«â€² and
[Âµ0 (x, a) + Î¦(x,a),: Î¸]

e â‰¤ Ï„ (Âµ e) log(1/Ç«â€² )(2Ç«â€² + Ç«â€²â€² ) + 3Ç«â€² .
â„“âŠ¤ ÂµÎ¸e âˆ’ â„“âŠ¤ (Âµ0 + Î¦Î¸)
Î¸

e â‰¤ â„“âŠ¤ (Âµ0 + Î¦Î¸âˆ— ). Thus,
We also have that â„“âŠ¤ (Âµ0 + Î¦Î¸)

â„“âŠ¤ ÂµÎ¸e â‰¤ â„“âŠ¤ (Âµ0 + Î¦Î¸âˆ— ) + Ï„ (ÂµÎ¸e) log(1/Ç«â€² )(2Ç«â€² + Ç«â€²â€² ) + 3Ç«â€²
â‰¤ â„“âŠ¤ ÂµÎ¸âˆ— + Ï„ (ÂµÎ¸e) log(1/Ç«â€² )(2Ç«â€² + Ç«â€²â€² ) + 3Ç«â€²

+ Ï„ (ÂµÎ¸âˆ— ) log(1/ kv1 k)(2 kv1 k + kv2 k) + 3 kv1 k ,
where the last step follows from Lemma 2.

21

5

Experiments

In this section, we apply both algorithms to the four-dimensional discrete-time queueing network
illustrated in Figure 5. This network has a relatively long history; see, e.g. Rybko and Stolyar
(1992) and more recently de Farias and Van Roy (2003a) (c.f. section 6.2). There are four queues,
Âµ1 , . . . , Âµ4 , each with state 0, . . . , B. Since the cardinality of the state space is X = (1 + B)4 , even
a modest B results in huge state-spaces. For time t, let Xt âˆˆ X be the state and si,t âˆˆ {0, 1},

i = 1, 2, 3, 3 denote whether queue i is being served. Server 1 only serves queue 1 or 4, server 2
only serves queue 2 or 3, and neither server can idle. Thus, s1,t + s4,t = 1 and s2,t + s3,t = 1. The
dynamics are as follows. At each time t, the following random variables are sampled independently:
A1,t âˆ¼ Bernoulli(a1 ), A3,t âˆ¼ Bernoulli(a3 ), and Di,t âˆ¼ Bernoulli(di âˆ— si,t ) for i = 1, 2, 3, 4. Using
e1 , . . . , e4 to denote the standard basis vectors, the dynamics are:
â€²
Xt+1
=Xt + A1,t e1 + A3,t e3

+ D1,t (e2 âˆ’ e1 ) âˆ’ D2,t e2
+ D3,t (e4 âˆ’ e3 ) âˆ’ D4,t e4 ,
â€² )) (i.e. all four states are thresholded from below by 0 and above
and Xt+1 = max(0, min((B), Xt+1

by B). The loss function is the total queue size: â„“(Xt ) = ||Xt ||1 . We compared our method against

two common heuristics. In the first, denoted LONGER, each server operates on the queue that is
longer with ties broken uniformly at random (e.g. if queue 1 and 4 had the same size, they are
equally likely to be served). In the second, denoted LBFS (last buffer first served), the downstream
queues always have priority (server 1 will serve queue 4 unless it has length 0, and server 2 will serve
queue 2 unless it has length 0). These heuristics are common and have been used an benchmarks
for queueing networks (e.g. de Farias and Van Roy (2003a)).
We used a1 = a3 = .08, d1 = d2 = .12, and d3 = d4 = .28, and buffer sizes B1 = B4 = 38, B2 =
B3 = 25 as the parameters of the network.. The asymmetric size was chosen because server 1 is the
bottleneck and tend to have has longer queues. The first two features are the stationary distributions
corresponding to two heuristics. We also included two types of non-stationary-distribution features.
For every interval (0, 5], (6, 10], . . . , (45, 50] and action A, we added a feature Ïˆ with Ï•(x, a) = 1 if
â„“(x, a) is in the interval and a = A. To define the second type, consider the three intervals I1 =
[0, 10], I2 = [11, 20], and I3 = [21, 25]. For every 4-tuple of intervals (J1 , J2 , J3 , J4 ) âˆˆ {I1 , I2 , I3 }4

and action A, we created a feature Ïˆ with Ïˆ(x, a) = 1 only if xi âˆˆ Ji and a = A. Every feature was

normalized to sum to 1. In total, we had 372 features which is about a 104 reduction in dimension
from the original problem.

22

loss of running average

total constraint violations

average loss of the running average policy

0.42

0.395

0.4

0.39

âˆ’2.3

10

0.385

0.38

0.38
0.36

âˆ’2.4

10

0.375

0.34
0.37
0.32

0.365

âˆ’2.5

10
0.3

0

1

2

3

4

0

1

2

4

3

4

0.36

0

1

2

3

4

x 10

x 10

4
4

x 10

Figure 4: The left plot is of the linear objective of the running average, i.e. â„“âŠ¤ Î¦Î¸bt . The center
plot is the sum of the two constraint violations of Î¸bt , and the right plot is â„“âŠ¤ ÂµÌƒÎ¸bt (the average loss of
the derived policy). The two horizontal lines correspond to the loss of the two heuristics, LONGER
and LBFS.

5.1

Stochastic Gradient Descent

We ran our stochastic gradient descent algorithm with I = 1000 sampled constraints and constraint
gain H = 2. Our learning rate began at 10âˆ’4 and halved every 2000 iterations. The results of
our algorithm are plotted in Figure 5.1, where Î¸bt denotes the running average of Î¸t . The left plot

is of the LP objective, â„“âŠ¤ (Âµ0 + Î¦Î¸bt ). The middle plot is of the sum of the constraint violations,
[Âµ0 + Î¦Î¸bt ]âˆ’ + (P âˆ’ B)âŠ¤ Î¦Î¸bt . Thus, c(Î¸bt ) is a scaled sum of the first two plots. Finally, the
1

1

right plot is of the average losses, â„“âŠ¤ ÂµÎ¸bt and the two horizontal lines correspond to the loss of the
two heuristics, LONGER and LBFS. The right plot demonstrates that, as predicted by our theory,
minimizing the surrogate loss c(Î¸) does lead to lower average losses.
All previous algorithms (including de Farias and Van Roy (2003a)) work with value functions,
while our algorithm works with stationary distributions. Due to this difference, we cannot use
the same feature vectors to make a direct comparison. The solution that we find in this different
approximating set is slightly worse than the solution of de Farias and Van Roy (2003a).

5.2

Constraint Sampling

For the constraint sampling algorithm, we sampled the simplex constraints uniformly with 10
different sample sizes: 508, 792, 1235, 1926, 3003, 4684, 7305, 11393, 17768, and 27712. Since
XA = 4.1 âˆ— 106 , these sample sizes correspond to less that 1%. The stationary constraints were

sampled in the same proportion (i.e. A times fewer samples). Let a1 , . . . , aAN and b1 , . . . , bN be
the indices of the sampled simplex and stationary constraints, respectively. Explicitly, the sampled

23

LP is:
min(Î¦Î¸)âŠ¤ â„“ ,

(26)

Î¸

s.t. (Î¦Î¸)âŠ¤ 1 = 1, Î¦ai ,; Î¸ â‰¥ 0, âˆ€i = 1, . . . , AN,
Î¦Î¸ âŠ¤ (P âˆ’ B):,bi â‰¤ Ç«s , âˆ€i = 1, . . . , N, kÎ¸kâˆž â‰¤ M

(27)

where M and Ç« are necessary to ensure the LP always has a feasible and bounded solution. This
corresponds to setting v1 = 0 and v2 = Ç«. In particular, we used M = 3 and Ç« = 10âˆ’3 . Using
differenc values of Ç« did not have a large effect on the behavior of the algorithm.
For each sample size, we sample the constraints, solve the LP, then simulate the average loss of
the policy. We repeated this procedure 35 times for each sample size and plotted the mean with
error bars corresponding to the variance across each sample size in Figure 5.2. Note the log scale
on the x-axis. The best loss corresponds to 4684 sampled simplex constraints, or roughly 1%, and
is a marked improvement over the average loss found by the stochastic gradient descent method.
However, changing the sample size by a factor of 4 in either direction is enough to obliterate this
advantage.
average loss produced by the constraint sampling algorithm with variance bars
70
65
60

average loss

55
50
45
40
35
30
25

3

10

4

10
number of simplex constraints sampled

Figure 5: Average loss with variance error bars of the constraint sampling algorithm run with a
variety of sample sizes.
24

First, we notice that the mean average loss is not monotonic. If we use too few constraints, then
the sampled ALP does not reflect our original problem and we expect that the solution will make
poor policies. On the other hand, if we sample too many constraints, then the LP is too restrictive
and cannot adequately explore the feature space. To explain the increasing variance, recall that we
have three families of constraints: the simplex constraints, the stationarity constraints, and the box
constraints (i.e. |Î¸|âˆž â‰¤ M ). Only the simplex and stationarity constraints are sampled. For the

small sample sizes, the majority of the active constraints are the box constraint so Î¸Ìƒ (the minimizer

of the LP) is not very sensitive to the random sample. However, as the sample size grows, so does
the number of active simplex and stationarity constraints; hence, the random constraints affect Î¸Ìƒ
to a greater degree and the variance increases.

6

Conclusions

In this paper, we defined and solved the extended large-scale efficient ALP problem. We proved
that, under certain assumptions about the dynamics, the stochastic subgradient method produces
a policy with average loss competitive to all Î¸ âˆˆ Î˜, not just all Î¸ producing a stationary distri-

bution. We demonstrated this algorithm on the Rybko-Stoylar four-dimensional queueing network

and recovered a policy better than two common heuristics and comparable to previous results on
ALPs de Farias and Van Roy (2003a). A future direction is to find other interesting regularity
conditions under which we can handle large-scale MDP problems. We also plan to conduct more
experiments with challenging large-scale problems.

7

Acknowledgements

We gratefully acknowledge the support of the NSF through grant CCF-1115788 and of the ARC
through an Australian Research Council Australian Laureate Fellowship (FL110100281).

References
Y. Abbasi-Yadkori. Online Learning for Linearly Parametrized Control Problems. PhD thesis,
University of Alberta, 2012.
R. Bellman. Dynamic Programming. Princeton University Press, 1957.
D. P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientific, 2007.
D. P. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena scientific optimization and
computation series. Athena Scientific, 1996.

25

G. Calafiore and M. C. Campi. Uncertain convex programs: randomized solutions and confidence
levels. Mathematical Programming, 102(1):25â€“46, 2005.
M. C. Campi and S. Garatti. The exact feasibility of randomized solutions of uncertain convex
programs. SIAM Journal on Optimization, 19(3):1211â€“1230, 2008.
D. P. de Farias and B. Van Roy. The linear programming approach to approximate dynamic
programming. Operations Research, 51, 2003a.
D. P. de Farias and B. Van Roy. Approximate linear programming for average-cost dynamic
programming. In Advances in Neural Information Processing Systems (NIPS), 2003b.
D. P. de Farias and B. Van Roy. On constraint sampling in the linear programming approach to
approximate dynamic programming. Mathematics of Operations Research, 29, 2004.
D. P. de Farias and B. Van Roy. A cost-shaping linear program for average-cost approximate
dynamic programming with performance guarantees. Mathematics of Operations Research, 31,
2006.
V. H. de la PenÌƒa, T. L. Lai, and Q-M. Shao. Self-normalized processes: Limit theory and Statistical
Applications. Springer, 2009.
V. V. Desai, V. F. Farias, and C. C. Moallemi. Approximate dynamic programming via a smoothed
linear program. Operations Research, 60(3):655â€“674, 2012.
A. D. Flaxman, A. T. Kalai, and H. B. McMahan. Online convex optimization in the bandit
setting: gradient descent without a gradient. In Proceedings of the sixteenth annual ACM-SIAM
symposium on Discrete algorithms, 2005.
C. Guestrin, M. Hauskrecht, and B. Kveton. Solving factored mdps with continuous and discrete
variables. In Twentieth Conf. Uncertainty in Artificial Intelligence, 2004.
M. Hauskrecht and B. Kveton. Linear program approximations to factored continuous-state markov
decision processes. In Advances in Neural Information Processing Systems, 2003.
R. A. Howard. Dynamic Programming and Markov Processes. MIT, 1960.
H. R. Maei, Cs. SzepesvaÌri, S. Bhatnagar, D. Precup, D. Silver, and R. S. Sutton. Convergent
temporal-difference learning with arbitrary smooth function approximation. In Advances in
Neural Information Processing Systems, 2009.
H. R. Maei, Cs. SzepesvaÌri, S. Bhatnagar, and R. S. Sutton. Toward off-policy learning control
with function approximation. In Proceedings of the 27th International Conference on Machine
Learning, 2010.
26

A. S. Manne. Linear programming and sequential decisions. Management Science, 6(3):259â€“267,
1960.
M. Petrik and S. Zilberstein. Constraint relaxation in approximate linear programs. In Proc. 26th
Internat. Conf. Machine Learning (ICML), 2009.
A. N. Rybko and A. L. Stolyar. Ergodicity of stochastic processes describing the operation of open
queueing networks. Problemy Peredachi Informatsii, 28(3):3â€“26, 1992.
P. Schweitzer and A. Seidmann. Generalized polynomial approximations in Markovian decision
processes. Journal of Mathematical Analysis and Applications, 110:568â€“582, 1985.
R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. Bradford Book. MIT
Press, 1998.
R. S. Sutton, H. R. Maei, D. Precup, S. Bhatnagar, D. Silver, Cs. SzepesvaÌri, and E. Wiewiora. Fast
gradient-descent methods for temporal-difference learning with linear function approximation. In
Proceedings of the 26th International Conference on Machine Learning, 2009a.
R. S. Sutton, Cs. SzepesvaÌri, and H. R. Maei. A convergent O(n) algorithm for off-policy temporaldifference learning with linear function approximation. In Advances in Neural Information Processing Systems, 2009b.
M. H. Veatch. Approximate linear programming for average cost mdps. Mathematics of Operations
Research, 38(3), 2013.
T. Wang, D. Lizotte, M. Bowling, and D. Schuurmans. Dual representations for dynamic programming. Journal of Machine Learning Research, pages 1â€“29, 2008.
M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In ICML,
2003.

27

