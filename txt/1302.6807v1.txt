227

Backward Simulation in Bayesian Networks

Robert Fung and Brendan Del Favero

Institute for Decision Systems Research
4894 El Camino Real, Suite II 0
Los Altos, CA 94022

Abstract
Backward simulation is an approximate
inference technique for Bayesian belief
networks. It differs from existing simulation
methods in that it starts simulation from the
known evidence and works backward (i.e.,
contrary to the direction of the arcs). The
technique's focus on the evidence leads to
improved convergence in situations where the
posterior beliefs are dominated by the evidence
rather than by the prior probabilities. Since this
class of situations is large, the technique may
make practical the application of approximate
inference in Bayesian belief networks to many
real�world problems.

1

INTRODUCTION AND MOTIVATION

Because of its sound theoretical foundation in probability
theory, the Bayesian belief network technology has
become, in artificial intelligence, an important alternative
architecture for reasoning to logic�based architectures
(e.g., rule�based systems). Although efficient exact
inference techniques (Shachter, 1986; Lauritzen, 1988,
Shachter, 1990) for Bayesian belief networks can and
have provided excellent solutions to many real�world
problems, their applicability is limited, because exact
inference is NP�complete (Cooper, 1990).
Because of this, significant research has been focused on
finding efficient approximate inference methods. Most
previous research has emphasized simulation methods
(Pearl, 1987; Fung, 1989; Shachter, 1989; Chavez, 1990;
Shwe, 1991a), which repeatedly draw sample values from
the network's nodes using a sampling algorithm, and then
use the relative frequencies of the sample values to
estimate the probabilities of interest. Researchers try to
find methods that converge quickly to the exact result, and
to characterize the convergence properties of the
simulation algorithms.
There are two basic classes of simulation methods:
forward-simulation methods (Fung, 1989; Henrion, 1986;
Shachter, 1989) and stochastic�simulation methods
(Chavez, 1990; Pearl, 1987).
Forward�sirnulation

methods start each trial of the simulation by instantiating
the source nodes (i.e., nodes with no predecessors) and
then proceeding forward along the diagram arcs to
instantiate each downstream node in tum. Because the
sample values from one trial to the next are unrelated, the
trials are independent. In stochastic�simulation methods,
on the other hand, each trial begins by modifying the
previous trial's instantiation. Each node's sample is
chosen with respect to the current instantiations of
neighboring nodes.
Because they are driven by the prior probabilities of
upstream nodes, rather than by the likelihood of the
observed evidence, forward�simulation methods converge
slowly when faced with low�likelihood evidence
(evidence that has low prior likelihood). Because of the
way samples depend on the current instantiation,
stochastic�simulation methods as a group are inefficient
when there are deterministic or near�deterministic
relationships in a network.
In this paper, we present the backward�simulation method
for performing approximate probabilistic inference i n
Bayesian belief networks. Our method i s closely related
to forward�simulation methods, and is not susceptible to
slow convergence in the presence of deterministic
relationships as are stochastic simulation methods. In
addition, the method is not as susceptible to slow
convergence with low�likelihood evidence, the main
problem with other methods of its class.
In Section 2, we present the notation used in this paper.
In Section 3, we discuss forward simulation methods in
detail. Section 4 provides the details on the backward �
simulation method. In Section 5, we give a summary of
the paper, and discuss directions for future research.

2

NOTATION

A Bayesian belief network is a directed acyclic graph D
with an associated probability distribution P. The set of
nodes in a network is denoted by N. Individual nodes are
denoted by capital letters, whereas general references to a
node (such as "node i") are in lowercase italics. Each
node i in the network has a corresponding variable X; in P
and a set of parents Pa( i). If S is a set of nodes, then the

228

Fung and Del Favero

set X s is the set of variables corresponding to the nodes in

s.

The variable

Xi has a corresponding set of conditioning
variables X Pa(i)• called the parents of X1. Each variable
xi has a state space nj and an associated probability
distribution P(X 11Xpa ))· The product of node probability
(i

dis tributions in a network is the joint probability
distribution P of the variables associated with the graph D.

4

BACKWARD SIMULATION

The two defining features of backward simulation are the
direction of simulation and the sampling method for
drawing node values. The direction of sampling in the
backward simulation method is outward from the
evidence. In contrast, forward-simulation methods that
work forward from the source nodes, whereas stochastic­
simulation methods that have no particular direction of

We represent evidence in Bayesian belief networks by

simulation.

setting the values of the appropriate nodes to their

methods, backward simulation permits many possible

observed states. The set of nodes whose values have been

orderings of the nodes to be sampled.

observed is denoted by N e· The nodes that are unobserved
(i.e., that are in

N\Ne) are called state nodes.

Backward simulation is a specialization of the
importance-sampling inference method discussed in

The inference task for Bayesian belief networks is to
compute answers to queries of the

Like forward and stochastic simulation

form P(X 11XK), where

Section 3. Like forward simulation, backward simulation
includes a node ordering step and a simulation step. The

all the observed evidence Ne is typically a subset of the

first step computes an ordering of network nodes, starting

nodes in K. Many exact and approximate algorithms have
been developed for addressing such queries.

general, an ordering will contain only a subset of the

Where there is little potential for confusion, the notation

P(A) will be used to mean P(XA)

.

3

nodes in the network. In the second step, the simulation
trials are performed, with network nodes sampled in the
predetermined order.

1981) is a well-known

technique for improving convergence in Monte Carlo
simulation. It has been adapted for use in forward

simulation models (Shachter, 1989).

It provides the

ability to instantiate the network from an arbitrary

distribution Ps instead of just from the joint distribution P
as in logic sampling. To adjust for sampling from P s
instead of from P, the weight Z associated with each trial
is computed to be the ratio of the likelihood of the sample

based on the network distribution to the likelihood of the
sample based on the sampling distribution:

(I)
The network probability P(XN) is always the product of
node probabilities,

A trial weight Z is computed for

4.1 ORDERING
The

thr ee

requirements

for

node

ordering

in

back ward simulation are flexible and allow significant
variation:

1. A node must be instantiated before it is backward
sampled,

2. A node's predecessors must be instantiated before
the node is forward sampled, and
3. Each node in the network must be either (a) a node
in the ordering or (b) a direct predecessor of a node
in the ordering that is backward sampled.

Items 2 and 3(a) are the usual requirements for an or­
dering in forward simulation.

(2)

Because evidence nodes are instantiated, they can always
be backward sampled.

but the sampling distribution can be arbitrary.

In the

simplest form of the algorithm, the sampling distribution
is the joint distribution over the unobserved nodes (the
state nodes). In that case, where P(XN5)=P5(XN5), the
ratio in Equation 1 is just
likelihood of the evidence.

In

each trial, and is used to increment the counts of each
distribution (i.e., query) of interest.

IMPORTANCE SAMPLING

Importance sampling (Rubinstein,

from the evidence nodes and working outward.

P(XN e IXNs ) the overall
,

An informal argument for why this method works is as
follows. If a large number of trials is done, the frequency
of Xi in the accumulated trials should be approximately
Ps(X1). When multiplied by the weight we get the
probability of X;. P(Xi). A formal proof of the conver­
gence of this method is presented in (Geweke, 1989).

Leaf nodes also can always be

backward sampled, because a dummy evidence node with
uniform likelihoods can be attached to a leaf node without
the posterior distribution being changed.
We shall use the simple network in Figure I to illustrate
the ordering and sampling steps. Each node represents a

probabilistic variable that is conditionally dependent on
the nodes at the ends of the arrows pointing into it. Thus,
the network in Figure 1 represents this probabilistic

factorization:

P(ABCDE) = P(A) P(BIA) P(CIA) P(DIBC) P(E).
Node

D is shaded to indicate that it is an evidence node.

Backward Simulation in Bayesian Networks

229

simulation (the process driving the sampling). Indeed, in
forward simulation, they are linked: forward simulation
involves only forward sampling. However, the process of
backward simulation does involve both backward and
forward sampling.

4.2.1

Backward Sampling

Backward sampling from a node's probability distribution
instantiates those of the node's predecessors that are not
already instantiated. We denote the instantiated value of
node i by X;, the parents of i that are uninstantiated by
Pa ll(i) and the instantiated parents by Pa *ci).
Figure 1: Simple Network.
Four different sampling orders for the example network
shown in Figure 1 are {D,B,E}, (D,E,B}, {D,E,C}, and
{ D,C,E}. First, the evidence node D is sampled,
instantiating nodes B and C. We then have two choices
for instantiating node A, namely by backward sampling
from either B or C. Finally, the value of E is determined
by forward sampling from C.
The nodes in the sampling order will be denoted by Ns.
This set is composed of the nodes to backward sampled,
N b• and the nodes to be forward sampled, Nr· In this
example, if we take Ns to be { D,B ,E}, then N b is { D,B}
andNf is {E}.

The backward-sampling procedure instantiates the
uninstantiated parent nodes according to the following
probability distribution:

ps

Backward sampling differs from forward sampling in
each of these three aspects. First, sampling from a node's
probability distribution occurs only after the node itself
has been instantiated. Second, sampling from a node's
probability distribution does not determine the node's
value but rather the values of the node's predecessors.
Third, the sampling is based not on a particular
conditional probability distribution but rather on
normalized likelihoods of the node's conditional
probability distribution.
Which of the two methods is used to sample a node
depends on the network topology. For nodes with an
evidence node as a descendent, sampling from a node's
conditional probability occurs after the node has been
instantiated, and it determines the values for the node's
predecessors. For nodes with no downstream evidence
nodes, forward sampling is used. Any of the algorithms
developed for forward simulation can be used.
We would like to emphasize the distinction between
sampling (a selection from a set of choices) and

I'

::::

P(x;

xp )

I XP ( )
a

"

r

a

'

'(·)
1

Norm (i)

•

'z E

N b·

(3)

The numerator of the preceding expression is the
likelihood of the current state of node i given a particular
state of the parents of node i. The denominator, Norm(i),
is a normalization constant that ensures that the terms in
this distribution sum to 1. Norm(i) is computed as
follows:

(

u(i)l X; I y, XPa

Norrn(i) = LyeXP(Pa

4.2 SAMPLING
Two sampling methods are used in backward simulation:
forward sampling and backward sampling. In forward
sampling, a node's distribution is sampled only after all
the node's predecessors have been sampled. Forward
sampling a node sets the value for the node itself. The
probability distribution on which the random sampling is
based is determined by the values of the node's
predecessors.

(Pa u { ))

J

• (i

The set XP(Pau(i)) is the set of all possible conditioning
cases of the uninstantiated parents of node i. For nodes
with n binary-valued uninstantiated parents, this set is of
size 2n.
Normalization constants can be precomputed if there is a
fixed order, or they can be cached as computation
proceeds.

4.2.2

Forward Sampling

In forward sampling, the sampling distribution for a node
is the same as the node's probability distribution:

(4)
4.3 SCORING
After all of the nodes have been instantiated, the weight
for each trial can therefore be computed by combining
Equations 1 through 4:

Z(x)=

•e

[

](

(
P (xj I XPa(j) )
rrjENb Norm(J)
n.

NP xi lxPa(i)

)

rrjENf p(

x; XPa(j} ))
I

230

Fung and Del Favero

Tills can be simplified to

(

Z(x) =Die N \ N P x1 I xPa(i)
s

Table

)n

j eN b Norm(})

4: Backward Sampling Distribution for A

(5)

4.4 EXAMPLE
Let us step through an example of backward simulation.

Consider the five-node network in Figure l. All of the
nodes are binary-valued: the values for A, for instance,

are taken to be a1 and a2, whereas the possible joint
values (or states) of nodes B and C are htc1, b 1c2, b2 c1,
and b 2c2• The value of the evidence node D is observed to

be d2 . Suppose that the sampling order { D,B,E} is used.

Table 1 is the conditional probability table for node D.
For instance, P(D= d2 I B=b1, C=c2) = p 122 .

As before, the constant

Suppose that the sampling step selects state a2.
Finally, we would use forward sampling to set node E to

one of the states ('1_, e2) according to the distribution
given in Table 6, which is identical to the second column
of Table 5.

Table I: Probability distribution for D givenB and C

Table 5: Probability ofE given C

P( D IBC )
Pm

Pm

P2 11

P 22 1

P112

P122

P2 12

P222

Since D is observed to be

j) normalizes the terms:

P(E I C )

d.z, the sampling distribution for

the parents of D is based on the second row of Table 1.

Table

The sampling distribution, over the states in XP(Pa 0 (OJ),
{b 1c1, b1 c2, b2c1, b2c2}, is shown in Table 2.

cz

6: Sampling Distribution forE

Table 2: Sampling Distribution forB and C
In forward sampling, the sampling distribution is the same
P tt2

Pt22

P 2 12

P 222

a

0:

a

a

The constant a normalizes the terms to sum to 1, namely:
a"' Nonn(D=dz) = P 1 12 + P 122 + P 2 12 + P 22 2
·

as the probability distribution, so no normalization
constant is necessary (the terms already sum to

Suppose that the sampling step setsE to e 1 .

1).

This example trial has instantiated the network to the joint
state a2b1 c2d2 e 1. The trial score that would be added to
the beliefs of the currently-selected states of each node

would be

Suppose that the sampling step chooses joint state b 1c2
and sets the states of B and C to these values.
Next, we would sample nodeB to set node A to one of the

states { a1, a2} according to the distribution over these
states.
Table 3 shows the conditional probability
distribution of B given A, whereas Table 4 shows the
sampling distribution for A.

Table 3: Probability ofB given A
P(B I A)

4.5 CORRECTNESS AND CONVERGENCE
Backward simulation meets the single constraint for a
valid importance sampling procedure: no point in the

joint state space of the prior distribution with positive
probability can have a probability of zero in the sampling
distribution.

As a form of importance sampling with likelihood

weighting, backward simulation inherits the convergence
properties of importance sampling (Shachter, 1989). That

is, the beliefs generated by the simulation are guaranteed

to converge to their true values, with the errors decreasing
in proportion to the square root of the number of trials.

231

Backward Simulation in Bayesian Networks

5

DISCUSSION

S to s2, and would set the trial score Z to e, a relatively
small number. The belief corresponding to

5.1 BENEFITS AND COSTS

1! 8),

remain at zero. After many trials (on average, about

The backward-simulation method is well-suited to
inference in situations with low-likelihood evidence. By
working from the evidence, the method focuses on
instantiating those scenarios that are most compatible with
the observed network state, rather than with the prior
distribution. The effect

� will be

augmented by this score, whereas the belief of s1 will

of the prior distribution is taken

into account by the trial weights. If the prior probabilities
are diffuse, compared to the evidence likelihoods, the

nodeS will be set to s1 and the trial score Z would be 1-e.

Now, because of this one trial, the belief of s 1 will

discovered, after much work, that s 1 is the most likely

explanation for the evidence observed.
Table

7:

ForwardSampling Distribution forS
P(S)

weights also will be diffuse, and the inference method will
converge to the correct solution much faster than would

for low-likelihood evidence is illustrated by the two-node
network in Figure 2. NodeT has been observed at value
t1, and nodeS has two states, s1 and�-

1-8

8

forward-simulation methods.
This benefit of backward sampling over forward sampling

be

much greater than the belief of s 2; we will have

Table

8 shows the backward sampling distribution for S.

If we use backward sampling fromT to S, S will be set to
state s1 in the preponderance of trials. The belief of s 1
will be augmented in each trial by

Z=o, and the belief of

s2 by zero. Thus, from the start, the simulation is more in
line with the most probable diagnosis, s 1.
Table

8:

BackwardSampling Distribution forS

€

1-€

The main cost of backward sampling is the computational
resources required for computing the normalization
Figure

2: Two-node Network With Low-likelihood
Evidence.

Suppose that the prior and conditional probabilities are as
follows, with 0 < £ <<

� << I:

(
P(S

) o
s2 ) = (1-o)
P{T = t1 IS= st ) (t- e)
=

=

=

)

P T =t 11S=s2 =e
Using exact inference (Bayes's rule), we can show that,
although the prior for state

Although in

general

the

costs

grow

exponentially with the number of predecessors, the costs
can be reduced where there are special network structures
such as invertible continuous functions or noisy-or
relationships

P S = st

(

constants.

� is much less than that for

state s2, s1 is the most likely explanation for the evidence:

Backward simulation is related to the method of evidential
integration (Chin,

1987;

Fung,

1989)

that has been

suggested for use with simulation methods. In evidential
integration, arc reversals are used as a pre-processing step
to integrate the evidence into the network, to convert
extremal

likelihoods

to less

extreme

likelihoods.

Evidence integration is computationally expensive;
backward

simulation

does part of what

evidence

integration does (when it computes the normalization
constants), at a fraction of the cost.
For networks in which the conditional probabilities do not
change, the normalization constants can be precomputed
and cached, taking much of the work out of each trial.

�(1-e)+(1-o)e
8
e
=--=1---"'1
o+e
0+£
e
-=0
P(s2/t1)�1:"u+e
Table

7 shows the forward sampling distribution forS. If

we use forward sampling from S toT, most trials will set

5.2 EXPERIMENTS
We have run some preliminary experiments comparing
the performance of forward simulation with backward

1,
1989). The probabilities are given

simulation. They were based on the network in Figure
as described in (Fung,
inTable
state e1.

9.

D is observed in state

q and E is observed in

232

Fung and Del Favero

The test routines were written in Macintosh Common
Lisp. We tested a forward simulation method against the
Both use likelihood

backward simulation method.
weighted scoring.

evidence is not particularly unlikely, thus there is no
particular benefit for backward sampling.
Most
importantly, Figures 3 and 4 show that backward
simulation works as an inference method.

Table 9: Probabilities for the Experimental Network

P(A)

P(a1)

=0,20

P(BIA)

P(b11a1)

=0,80

P(b11�)

=0.20

P(CIA)

P(c11a1)

=0.20

P(c11�)

=

P(DIBC)

P(d 11b 1c1)

=0,80

P(d 11b2c1)

=0.80

P(d lib tC2)

::::0,80

P(d11b2c2)

=0.05

=0,80

P(d 11b1c1)

=0.60

P(d11b 1c1)

P(EIC)

6
I::

LIJ
._
0
>

0. 05

0.2

0

0

'B
Cl)

0.1

0

As was done in the previously cited paper, we perform a
large number (250) of runs. In each run, we measure the

500

1000, and 2000 trials, using the absolute-value error

2000

Trials

accuracy of each simulation method at 100, 200, 500,
function below:

1500

1000

Figure 4: Standard Deviation vs. Trials:

Forward {Diamonds) and Backward (Squares)
Although both simulation methods will converge to the

same answer, they may do so at different speeds. This is
the motivation of the next set of runs, which were

Here, Bk(xii•t) is the belief (probability estimate) for
state j of node i on trial t of the kth run. The error,
averaged over all runs, is presented in Figure 3.

The

standard deviation of the errors in the runs is presented in

Figure 4.

performed on

the

same

network

with

the

same

distributions, with the exception of the modified entries

listed in Table 10. The evidence in these runs is that node

D is set to d1. These modifications make the observed
evidence much less likely than before.
Table 10: Modifications to Probabilities in Table 9
for Extreme-probability Experiment

0.5

g
�
0

�
:>

<

0.4

P{DIBC)

P(d 11b1c1)
P(d11b1c2)

0.3

=

0.001

P(d11b2c1)

=

0.0001

P(d11b2c2)

=

0.0001

=

0.05

We record the performance of each method at 10, 20, 50,
100, and 200 trials. The average error and standard
deviation are presented in Figures 5 and 6.

0.2
0.1

Figures 5 and

0
0

500

1000

1500

2000

The conclusion is that we can expect good performance of

Trials

backward simulation at a low numbers of trials in

Figure 3: Error vs. Trials for Two Simulation Methods:
Forward (Diamonds) and Backward (Squares)
Figures 3 and

4 show that backward and forward

simulation are performing equivalently well, with the
backward s imulation method showing slightly lower

average error values.

This is to be expected:

given a

large enough number of trials, both methods will

converge to the same answer.

6 show that at low trial numbers, backward

simulation is consistently closer to the true probability
than forward simulation.

In this example, the

networks with low-l ikelihood evidence. Of course, much
more work is necessary to characterize and understand the
conditions under which each simulation method performs

better than the other. There are cases in which backward
simulation would perform worse than forward simulation:

backward simulation is subject to the proof of Dagum
(1993) that, in the worst case, all approximate

probabilistic methods are NP-hard.

Backward Simulation in Bayesian Networks

In backward simulation, on the other hand, the sampling
order can affect the sampling distribution. This may have
a significance in convergence properties of the simulation.
For instance, in the example of Section 4.4, two possible
sampling orders are { D,E,B}and {D,E,C}. Using the
former, A is backward sampled from B, whereas using the
latter, A is backward sampled from C. Depending on the
structure of the joint distribution P(ABC), the sampling
distribution for A could be quite different between the two
orders. One ordering may be better than the other in
terms of simulation performance.

0.5
0.4

j
t
:>

<

0.3
0.2
0.1
0
0

50

100

150

200

Trials
Figure 5: Error vs. Trials for Second Experimental Run:
Forward (Diamonds) and Backward (Squares)

0.3
...
0

t::
UJ

......
0

The added flexibility is exciting, in that it provides an
opportunity to develop heuristics governing when to
sample a node backward or forward. The hope is that
with backward and forward simulation in the probabilistic
tool chest, as well as evidence integration and other
approximate methods, a simulation run can be optimized
based on the particulars of the network being considered.
6

0.1

There are many interesting avenues of research for
backward simulation. Dynamic node ordering (i.e.,
changing Ns within a single trial or between trials) is
possible and may provide a way to improve performance.
Also, it is possible to group nodes for sampling. For
example, in Figure 1, B and C could be aggregated
together for the purpose of sampling the value of A

'1)

"E

The ordering {A,B,C,E} presents a different type of
flexibility, or ambiguity: it is valid under this order to
instantiate C by forward sampling from A or by backward
sampling from D. There may be a difference in
simulation performance under the different interpretations
of this ordering.

0.2

>

0

233

�

0
0

50

100

150

200

Trials
Figure 6: Standard Deviation vs. Trials for Second Exper­
imental Run: Forward (Diamonds) and Backward
(Squares)
5.3 THE SIGNIFICANCE OF NODE ORDERING
Backward simulation provides a new flexibility in
devising strategies for instantiating the network during a
simulation triaL One strategy would be to use backward
sampling wherever possible. Another would use forward
sampling everywhere (this is just forward simulation). In
between these extremes, there are many possibilities for
mixing both forward and backward sampling.
There are often multiple possible node ordering in
forward simulation as welL However, node ordering has
no impact on the sampling distribution for a particular
node: the distribution is always based on the node's
predecessors, which are always instantiated before the
node itself is.

FUTURE RESEARCH AND
APPLICATIONS

One of the most promising areas of research is the
combination of the backward-simulation method with its
dual, forward simulation. The combination would
provide a complete probabilistic architecture that allows
both data-driven and causal reasoning.
Such an
architecture might have the promise of attacking such
problems as natural-language understanding or speech
recognition. We are currently testing this architecture on
two-level networks with noisy-or relationships between
the nodes such as the QMR-DT network (Shwe, 199Ib).
This combination may make feasible the application of
Bayesian belief networks to many real-world settings that
current techniques cannot handle. For example, it should
be applicable to situations that have large and dynamic
state spaces, and strong evidence. Many sensory
situations (e.g., vision) seem to have this flavor. It seems
to match well with how people reason under uncertainty
- by reasoning from evidence to conclusions. This has
the promise of making explanation of the results of
backward simulation more intuitive than for exact
inference methods.

234

Fung and Del Favero

Acknowledgments

This work was supported in part by NSF Grant IRI9120330.
This work has benefited greatly from discussions with
Mark Peot and Kuo-Chu Chang and from the comments
of the reviewers of this paper.
References

Chavez, R. M., & Cooper, G. F. (1990). A randomized
approximation algorithm for probabilistic inference on
Bayesian belief networks. Networks, 20(5), 661-685.
Chin, H. L., & Cooper, G. F. (1987). Stochastic
simulation of Bayesian networks. In Proceedings of the
Third Workshop on Uncertainty in Artificiallntelligence,

106-113. Seattle, Washington.
Cooper, G.F. (1990). The computational complexity of
probabilistic inference using Bayesian belief networks.
Artificial Intelligence , 42(2-3), 393-405.
Dagum, P., & Luby, R. M. (1993). Approximating
probabilistic inference in Bayesian belief networks is NP­
hard. Artificial Intelligence, 60, 141-153.
Fung,

R. M., and Chang, K. C. (1989). Weighing and
integrating evidence for stochastic simulation in Bayesian
networks. In Fifth Workshop on Uncertainty in Artificial
Intelligence, 209-219. Detroit, Michigan.

Geweke, J. (1989). Bayesian inference in econometric
models using monte carlo integration. Econometrica,
57(6), 1317-1339.
Henrion, M. (1986). Propagating uncertainty in Bayesian
networks by probabilistic logic sampling. In J. F. Lemmer
& L. N. Kanal (Eds.), Uncertainty in Artificial
Intelligence, 2. Amsterdam: North-Holland.
Lauritzen, S. L., & Spiegelhalter, D. J. (1988). Local
computations with probabilities on graphical structures
and their application in expert systems. Journal of the
Royal Statistical Society, Series B, 50(2), 157-224.
Pearl, J. (1987). Evidential reasoning using stochastic
simulation of causal models. Artificial Intelligence, 32(2),
245-257.
Rubinstein, R. Y. (1981). Simulation and the Monte Carlo
Method. New York: Wiley.

Shachter, R. D. (1986). Intelligent Probabilistic Inference.
In J. F. Lemmer & L. N. Kanal (Eds.), Uncertainty in
Artificial Intelligence ,2. Amsterdam: North-Holland.
Shachter, R. D., D'Ambrosio, B., & Del Favero, B.
(1990). Symbolic probabilistic inference: A probabilistic
perspective. In Proceedings of the Eighth National Con­
ference on Artificial Intelligence, 157-224. Boston, MA.

Shachter, R. D., & Peat, M. (1989). Simulation
approaches to probabilistic inference for general
probabilisti c inference on belief networks. In Fifth

Workshop on Uncertainty in Artificial Intelligence.

Detroit, Michigan.
Shachter, R. D., & Peat, M. (1993). Evidential reasoning
using likelihood weighting. P ersonal communication with
authors.
Shwe, M., & Cooper, G. F. (199la). An empirical analysis
of likelihood-weighting simulation on a large, multiply
connected medical belief network. Computers and
Biomedical Research, 24(5), 453-475.
Shwe, M. A., Middleton, B., Beckerman, D. E., Henrion,
M., Horvitz, E. J., Lehmann, H. P., & Cooper, G. F.
(1991b). Probabilistic diagnosis using a reformulation of
the INTERNIST-1/QMR knowledge base-l:
The
probabilistic model and inference algorithms. Methods of
Information in Medicine, 30(4), 241-255.

