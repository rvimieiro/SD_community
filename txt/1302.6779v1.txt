8

An evaluation of an algorithm for inductive learning of
Bayesian belief networks using simulated data sets
Constantin F. Aliferis and Gregory F. Cooper

Section of Medical Informatics & Intelligent Systems Program,
University of Pittsburgh, B50A Lothrop Hall, 190 Lothrop St.
Pittsburgh, PA 15261

Abstract

Bayesian learning of belief networks (BLN) is
a method for automatically constructing belief
networks (ENs) from data using search and
Bayesian scoring techniques. K2 is a
particular instantiation of the method that
implements a greedy search strategy. To
evaluate the accuracy of K2, we randomly
generated a number of BNs and for each of
those we simulated data sets. K2 was then
used to induce the generating BNs from the
simulated data. We examine the performance
of the program, and the factors that influence
it We also present a simple BN model,
developed from our results, which predicts the
accuracy of K2, when given various
characteristics of the data set .
1 INTRODUCTION

Bayesian belief networks (ENs) constitute a method for
graphical representation of knowledge, based on
explicitly defining probabilistic dependencies and
independences among variables. A BN consists of a
directed acyclic graph (DAG) that captures the
dependencies and independences among nodes
(corresponding to variables) of the graph, and a set of
functions that give each variable's probability
distribution, conditioned on the values of its parent nodes
[Pearl 1988, Neapolitan 1990]. BNs are a state-of-the-art
formal method for probabilistic modelling in decision­
support systems [Cooper 1989].
Although BNs can reduce dramatically the number of
probabilities that must be specified for a particular
modelling task, relative to methods that do not exploit
the independence relations among the domain variables,
the knowledge acquisition (KA) problem is still
challenging. To cope with the KA "bottleneck",
researchers within the symbolic Artificial Intelligence
(AI) Machine-learning (ML) community have developed

methods for learning representations of knowledge
automatically from collections of data sets [Shavlick
1990]. In the same spirit, researchers in the BN field
have developed techniques which when given a set of
variable observations, will try to find the BN (or
depending on the method, the class of BNs) that most
probably produced the data set (i.e., that best captures the
variables relationships) [Cooper 1992, Pearl 1993, Fung
1990, Lam 1993, Singh 1993, Spirtes 1992, Suzuki
1993].
The pursuit of ML methods for BN construction is
further motivated by the following applications areas: (a)
exploratory statistical analysis,
(b) comparison,
confirmation, and discovery of scientific hypotheses, (c)
partial substitution of classical multivariate analytic
techniques [Cooper 1992, Aliferis 1993].
One method for BN ML is the Bayesian learning of BNs
(BLN) method [Cooper 1992]. This method, when given
a database of observations, searches a space of BNs, and
scores them using a Bayesian scoring function. A
particular instantiation of the method is the algorithm
K2, which uses greedy search as the search strategy. K2
also requires as an input an ordering of the variables,
such that no variable later in the ordering can "cause" (be
the parent of) a variable earlier in the ordering [Cooper
1992]. It is assumed that temporal precedence and
domain knowledge suffice for the determination of such
an ordering. In the conclusions section we discuss
methods for relaxing this assumption.
The goal of the research reported in the current paper is
to investigate the accuracy of K2, and to identify data
attributes that possibly determine its accuracy using
simulated data as contrasted to real data. The problem
with real data is that frequently a gold standard (i.e., the
underlying BN process) is not known. Thus in such cases
researchers measure how well the ML method models the
domain indirectly, by measuring the predictive accuracy
of the produced model. For an initial evaluation of K2
using this method, see [Herskovits 1991].
Using simulated data produced by well-specified

An

models (gold-standard models) on the other hand enables

Evaluation of a BLN Al gor ithm

were performed on this output using a statistical package

us to overcome these difficulties and measure directly

and appropriate techniques [Norusis

how well the ML method learns the model structure. An

in Figure 1 dep icts the experimental design.

admitted limitation, however, is that the simulated data
may not necessarily resemble closely the type of data we
would obtain from samples in the real world. In a
preliminruy evaluation of K2 using this method, Cooper
and Herskovits used simulated data from the ALARM
network (a BN with 37 nodes and 46 arcs, developed to
model the anesthesia emergencies in the operating room
[Beinlich

1989]),

to examine the number of

9

1992].

The diagram

BN Random Generator

Cases Generator

arcs

correctly found, and erroneously added by K2, given

database sizes ranging from 100 to 10000 cases [Cooper

1992].

K2

In this paper we describe experiments that extend

the use of simulation to obtain insight into BN learning
methods. In

particular

we describe the use of simulated

data from a variety of BNs and we discuss not only the

....__�

findings, but also which specific data attributes determine

the accuracy of the algorithm, and how a user can infer
the expected accuracy for a particular learning task.
2 METHODS

Figure l. Flowchart of the Experimental Design

The experiment consists of the following parts:

a)

Generation of a set of BNs, which we call the Gold

standard BNs (BNs-GS). For each belief network the
number of variables was chosen randomly from the
following values:

2, 10, 20, 30, 40, 50.

The number of

arcs was chosen randomly (i.e., a uniform probability
distribution was used), so that between 0 and 10 arcs

would

point

Gold-standard vs Induced BNs
statistics module

to

any

particular

node.

The

ordinality of the variables (i.e., total number of possible
values) was randomly chosen to be either two or three for
all variables in a generated BN. After the structures were
constructed, they were parameterized (i.e., conditional

probabilities functions were determined for each node)

randomly for each prior and conditional probability.
b) The set of generated BNs was given to the case
generator. For each BN, the case gen erator constructed a
set of simulated data using logic sampling [Henrion

1988]. The number

of cases per BN was chosen randomly

between 0 and 2000 cases.

The experiment was performed using an integrated
software package supporting belief network inference,
learning, and simulated BN and case generation and
evaluation, which we have developed. The software is
written in Pascal and runs on

an

IBM RS 6000

workstation. For pragmatic reasons we decided to run the
program in batch mode and analyze the results which
were produced. We additionally developed using K2, a
model

of

K2's

accuracy

(conditioned

upon

empirical

data
accuracy

A total of 67 BN pairs were generated and

analyzed.

attributes)

for

the

purposes

of

prediction.

3 RESULTS

Table

1

presents the descriptive statistics for the data

attributes (number of variables, ordinality of variables,
number of arcs, number of cases). To facilitate analysis,

c) The sets of simulated cases were given to K2, which

we additionally discretized the number of arcs and cases.

constructed for each data set a BN. K2 had access to the

Table

correct

ordering

of the variables for each BN-GS.

We will c all the set of BNs produced by K2 the Induced

2

shows the corresponding information. Tables

3

and 4 present the descriptive statistics for the evaluation
metrics we used, both in their original and discretized
forms. AF. it is evident from Table 4, K2 discovered at

BNs (BNs-1).
d) Finally, the sets of gold-standard BNs and the induced
BNs were compared by a statistics module, which
estimated descriptive statistics and the following two
metrics for each BN-GS and BN-I pair: percentage

is 91.6% and the mean ratio of superfluous arcs (M2) is

of

4.7%.

arcs

in BNs-GS that are present in BNs-1 (metric

Ml), and ratio of number of arcs in BNs-1 that are
absent in BNs-GS to the number of arcs in the
corresponding BN-GS (metric M2). Additional analyses

least

70%

of the arcs

94%

of

94% of the
10% arcs of the BN-GS

the time. In

cases, K2 did not add more than

arcs. The mean percentage of correctly found arcs (Ml)

10

Aliferis and Cooper

Table 1: Descriptive Statistics for Data Attributes of
BNs-GS
freguencl: %

value

variable

number of variables

ordinality of variables

2
10
20
30
40
50
2
3

6.0
16.4
26.9
22.4
14.9
13.4
46.3
53.7

variable

mean

s.d.

number of arcs

60.93

36.77

number of cases

1085.49

544.97

Table 2: Descriptive Statistics for Discretized Data
Attributes
frequency distribution %

number of arcs

number of cases

0-20
21-60
61-100
>100
0-200
201-500
501-1000
1001-1500
>1500

16.4
37.3
25.4
20.9
3.0
17.9
22.4
32.8
23.9

We also analyzed the factors that influence the
performance of K2 . The nature of the data is such that
the influences of the independent variables (number of
variables, number of arcs, number of cases and variable
ordinality) on the dependent ones (i.e., M1, M2), can
not be analyzed appropriately with a linear model.
Although we tried a number of variable transformations
on the variables, an analysis of variance/covariance or
multiple regression model was not applicable, due to
violation of assumptions. Thus we applied a graphical
analysis of the
response surface, followed by fitting a
non-linear regression model to the relationships that
were revealed by the analysis.
Graphs I and II show the relationship between number of
arcs, nwnber of variables and number of cases for the
case where ordinality is 2 or 3 (graphs I & II
respectively). As we would expect from our design, the
number of variables is uniformly distributed across the
number of cases. For each number of variables, there is
small variation of the corresponding arcs number (since
we constrained the incoming arcs per variable in the
generation process - as described in the methods section).
Finally, the same observations hold true when ordinality
is 3, although the spread of data points is somewhat more
constrained. These graphs imply that we can eliminate
the number of arcs from further consideration, since it is
determined by the number of variables. Also they suggest
that we might want to apply two different analyses, one
for cases where variables were binary and one where they
were ternary due to the somewhat different spread of data
points.

Table 3: Descriptive Statistics for Evaluation Metrics

M1 (%)
M2 (%)

mean

s.d.

91.6
4.7

11.7
7.6

Table 4: Descriptive Statistics for Discretized
Evaluation Metrics
value

M1

M2

0-50 %
51-70%
71-90%
91-95%
96-98%
>98%
0-2%
3-5%
6-10%
11-30%
31-50%
>50%

freguency distribution %

1.5
4.5
28.4
11.9
13.4
40.3
47.8
19.4
26.9
4.5
1.5
0

Graph III shows the relationships between M l & M2 and
number of cases for the complete data set (i.e., both cases
containing variables with ordinality 2 and ordinality 3).
Similar relationships exist for the subset with ordinality 2
and the subset with ordinality 3. Graph IV shows the
relationships between M1 & M2 and number of variables
for the complete data set. Again similar plots have been
produced (not shown here) for the subset with ordinality
2 and the subset with ordinality 3.
The graphs shown here support the following: (a) Ml
appears to be asymptotically approaching 100% as cases
increase (graph III),
(b) M2 appears to be asymptotically approaching 0 as
cases increase (graph III),
(c) there is no clear form of covariation of M1, M2 and
number of variables (graph IV).
In addition, even though for both binary and ternary
variables the same nature (i.e. functional form) of
covariation exists between Ml & M2 and cases, the
function parameters should be assessed individually since

An Evaluation of a BLN Algorithm

11

and for M2 is 0.8, when we model these metrics

the relevant plots (not shown here) have different spread

separately for ordinality of

of data points.

2 and 3), and thus these

GRAPH I. Relationship between arcs and cases when ordinality is
2. Data points corresponding to BNs with different number of
variables are separated Into 6 different groups. Numbers for each
group denote number of variables.

--...:.;50:.�
..- .
y
------�
�

140
120
100
ARCS

80
60
•

40

•

•

20
0
2000

1500

1000

500

0

CASES

GRAPH II. Relationship between arcs and cases when ordinality is
3. Data points corresponding to BNs with different numbers of

variables are separated into 6 different groups. Numbers for each
group d enote number of variables.
140

50

120
100
ARCS

80
60
40
20
0
0

1500

1000

500

2000

CASES

The next step in our
the

analysis is to estimate parameters for

relationships we

models can be used for the assessment of the values of

identified. Since the

Ml, M2 given the sample size we use. Finally, we used

exponential in character, we used the iterative algorithm

the expected accuracy of K2, given data attributes. We

functional

functional form of
of SPSS [Norusis
=

1

-

the relationships appears

1992]

to

be

to fit the following models: Ml

e·C1..Jcases and M2

=

c2

·C3..Jcases. The
e
results

of this analysis are given in T able 5. We observe that the

explained variability (i.e., fit of the model) which is
indicated by R2, is quite good (mean R2 for Ml is 0.6

our results and K2 to develop a

BN

model for predicting

utilized the following ordering: [number of variables,

number of

arcs,

dimensionality, number of cases, Ml,

M2]. The BN graph is given in Figure 2, while Appendix
I

contains

distnlmtions.

the

conditional

and

prior

probability

Aliferis and Cooper

12

GRAPH Ill. Relationship between M1 & M2 and the number of
cases.
1

•

•

0.9

.

•

•
••

0.8

•

0.7

•

•

M1 & M2 0.6

•
•

. .. '
•

• • •
•
•

.
•

.

.

•

.. ,... ., .
•

•

•

•

r;M11
�

•

0.5

£1

.....

..

.

.�
•

•

•

0.4
0.3
0.2
0.1

1.'.1

�13

a�<�

rep

c

0 +------o�KD--�D-����zo��a-�
0

1000

500

1500

2000

CASES

GRAPH IV. Relationship between M1 & M2 and the number of
variables.
1

•

0.9

*
•

0.8

•

•

!

•

•

•

I

•

i

•

•

•

0.7

•

*

M1 & M2 0.6

•

•

0.5

•

c

0.4
0.3

�
2

1::1
[J

0.2
0.1

I

8

0
0

10

20

1:1

;

[J

30

40

50

NUMBER OF VARIABLES

K2

and

binary variables, how many cases should we have in

independence relationships among the variables, without

reveals

order for K2 to produce 2% or less extraneous arcs?" We
can use any standard BN inference algorithm to answer

having

any

the

fairly

access

visual/analytical tools

complex

dependences

to

domain

theory,

we

utilized to

reach

or

the

similar

such questions [Henrion 1990].

conclusions. Using this model (under the assumptions
that the underlying data generating process is a BN) we
can answer questions of the type: "If the variables are
binary and our data set consists of
have

1200 cases, and we
20 variables in the model, what is the expected

percentage of correct arcs in the model found by K2?" .
Or we can ask questions like: "If our data set contains . 10

4 CONCLUSIONS
The results of

these experiments are encouraging.

Although we used a fairly small number of cases per BN,
K2 was able to find the gold standard BN with high
accuracy.

13

An Evaluation of a BLN Algorithm

Table 5: Non-linear Regression ofMl,

produce (at a first pass) an ordering and then use K2

M2 by

[Singh 1993].

Number of Cases
Ml

=

Other methods for coping with the ordering assumption

1 - e -C1...Jcases

all cases

are to use multiple random orderings and select the one

ord=2

that leads to the most probable BN [Cooper 1992]. Due to
the huge number of orderings, this approach would be

0.57

C1 (± SE)

=

0.56

0.65

.09 ± .004

.08 ± .004

most practical for BNs with a few variables.

In this experiment we assumed that there were no

.10±.007

missing values. Unfortunately in many real-life databases

this is not the case. Missing values can be handled
ord=2

all cases

0.58

c2 (± SE)
C3 (± SE)

=

1.27 ± .33

0.78

1.88 ± .45

normatively

ord-3

as

described

in

[Cooper

The

1994].

tractability of this method depends on the domain.

0.79

Finally,

we

parameterized

our

gold-standard

BNs

randomly. There is a possibility that BNs that capture

2.10 ± .60

real-life

processes

will

deviate

from

such

parameterizations. With our current state of knowledge
=

0.14 ± .02

0.01 ± .02

however, it seems that this is a reasonable initial design

0.21 ± .02

choice for an experiment.

Acknowledgements

We are indebted to Mr. Thanban I. Valappil, and to Dr.
Allan R. Sampson for their help with the statistical
analysis of the data. Funding was provided by the
National Science Foundation under grant# IRI 9111590.
References

C. Aliferis, E. Chao and G. Cooper "Data Explorer: A
Prototype

Expert

System

for

Statistical

Analysis"

Proceedings of the 17th annual SCAMC, 1993, 389-393.

I. Beinlich, H. Suermondt, M. Chavez, G. Cooper "The

ALARM monitoring system: A case study with two
probabilistic inference techniques for belief networks"

Figure 2: BN M odel (Graph Only) of the Variables

in Medical Care, London, 1989, 247-256.

Relationships

G. Cooper, E. Herskovits: "A Bayesian method for the

We were also able to identify

specific data attributes

that

de�ennine the expected accuracy of the algoritlun, and to
bwld a model for predicting this accuracy. The procedure
strongly resembles the process of power and size analysis
used in classical statistics, the main difference being that
our model was empirically derived. It is important to note
that K2 utilizes a simple search method (one step greedy
search).

In future

work

�

Proceedings of the Conference on Artificial Intelligen

we expect to explore the

performance of BLN when alternative heuristic search
methods are used. Such search methods are likely to
diminish or eliminate the need for specification of a total
ordering of the variables. The ordering constraint also
can be dealt with by using statistical methods similar to

those used in the TETRAD II program [Spines 1992] to

induction of probabilistic networks from data", Machine
Learning, 9: 309-347, 1992

G. Cooper:
development
networks",

"Current
of

expen

Applied

research
systems

Stochastic

directions

based
Models

in

the

on

belief

and

Data

Analysis, 5: 39-52, 1989.

G.

Cooper "A Bayesian method for learning belief

networks that contain hidden variables", to appear

in:

Journal of Intelligent Information Systems, 1994.
R. Fung, S. Crawford "Constructor: A system for the
induction of probabilistic models", Proceedings of AAAI
1990, 762-769.

M.

Henrion:

inference

in

"An introduction to algorithms
belief

networks",

In:

Uncertainty

for
in

14

Aliferis and Cooper

Artificial Intelligence 5, M. Henrion and R. Shachter
(Eds.) 1990; 129-138, Amsterdam:North Holland.
M. Henrion: "Propagating uncertainty in
networks

by

logic

sampling".

In:

(a) VARNUM: it has no
VARNUM value

Bayesian

Uncertainty in

Artificial Intelligence 2, J. Lemmer and L. Kanal, (Eds.)
1988; 149-163, Amsterdam: North Holland.
E. Herskovits "Computer-Based Probabilistic-Network
Constroction" Ph.D. thesis, Stanford, 1991.
W. Lam, F. Bacchus "Using causal information and
local measures to learn Bayesian networks" Proceedings

0.07

1
2

0.16

3

0.26

4

0.22

5
6

0.15
0.14

ofUncertainty in AI 1993,243-250.

R

Neapolitan:

"Probabilistic

reaso ning

in

expert

systems", New York: John Wiley and Sons 1990.
M. Norusis: "SPSS PC+ vers 4.0 Base manual, Statistics
manual, Advanced statistics manual", SPSS Inc 1992.
J. Pearl: "Probabilistic reasoning in intelligent systems",
San Mateo, California, Morgan- Kaufinann 1988.
J.

Pearl , T. Verma

CASES value

causation" Artificial Intelligence frontiers in statistics: AI
and statistics III, Hand DJ (Ed)., New York, Chapman
and Hal11993, p. 327-334.
J. Shavlik, T. Diettrich (Eds.) "Readings in machine

M.

Singh,

M.

California, Morgan-Kaufmann

Valtorta

"An

algorithm

for

the

0.46
0.54

(c) CASES: it has no parents

"A statistical semantics for

learning", San Mateo,
1990.

1
2

p(CASES)

1

0.04

2

0.18

3

0.23

4

0.55

. edb
(d) ARCS: 1t
..lS d etennm
>y VARNUM
ARCS

p(ARCS I

vAR_NUM)

construction of Bayesian network structures from data"

value

Proceedings ofUncertainty in AI 1993,259-265.
P. Spirtes, R Scheines and C. Glymour "Causation,

1

0.63 0.53 0.05 0.05 0.07 0.08

2

0.13 0.33 0.86 0.21 0.07 0.08

3

0.11 0.07 0.05 0.68 0.43 0.08

4

0.13 0.07 0.05 0.05 0.43 0.77

Prediction and Search", New York,
1992.

Springer-Verlag

J. Suzuki "A construction of Bayesian networks from

databases based on an

MDL principle", Proceedings of

Uncertainty in AI 1993, 266-273.

(e) M1 :it is detennined by CASES:

Appendix I
The following conditional (or prior) probabilities apply to
the BN of Figure 2. Note: for each value of the dependent
variable

we

present

the

conditional

probabilities

corresponding to the values of the parent variables,
created so that the leftmost parent changes values slower,
and

the

rightmost

one

faster. M_DIM

stands

for

ordinality. Also for the interpretation of the values see
Tables 1, 2 and 4.

_ill_M2:
M2

M1 value

p(Ml I CASES)

1

0.13 0.11 0.05 0.02

2

0.38 0.11 0.05 0.02

3
4

0.13 0.50 0.19 0.20
0.10 0.06 0.29 0.09

5

0.13 0.06 0.19 0.16

6

0.13 0.17 0.24 0.50

it is detennined by M DIM and CASES:

p(M2j M_DIM,

CASES)

value
1

0.17 0.17 0.10 0.17 0.10 0.53 0.36 0.74

2

0.17 0.17 0.10 0.08 0.20 0.20 0.40 0.09

3

0.17 0.17 0.40 0.58 0.50 0.13 0.16 0.09

4

0.17 0.33 0.30 0.08 0.10 0.07 0.04 0.04

5

0.33 0.17 0.10 0.08 0.10 0.07 0.04 0.04

